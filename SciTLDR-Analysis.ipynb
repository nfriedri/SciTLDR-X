{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59741be4",
   "metadata": {},
   "source": [
    "## SciTLDR Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e3aea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import MBartTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "96362956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data directories containing original SciTLDR data\n",
    "language = \"English\"\n",
    "ft_text_path = os.path.join(language, \"jsonl\", \"FullText\")\n",
    "aic_text_path = os.path.join(language, \"jsonl\", \"AIC\")\n",
    "ao_text_path = os.path.join(language, \"jsonl\", \"Abstracts\")\n",
    "\n",
    "max_mbart = 1024\n",
    "max_zmbart = 2048\n",
    "max_led = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4be89d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load json data from file and output data as array.\n",
    "def load_json_data(file_path):\n",
    "    data = []\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def compute_number_of_tldrs(data):\n",
    "    length = 0\n",
    "    for line in data:\n",
    "        length += len(line[\"target\"])\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f7df16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization functions\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')\n",
    "\n",
    "def tokenize_abstract(array):\n",
    "    text = \"\"\n",
    "    for ele in array:\n",
    "        text += ele\n",
    "    # print(text)\n",
    "    tensors = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # print(tokens)\n",
    "    return tensors.size()[1]\n",
    "\n",
    "def tokenize_data(data):\n",
    "    lengths = []\n",
    "    for line in data:\n",
    "        # print(line[\"source\"])\n",
    "        lengths.append(tokenize_abstract(line[\"source\"]))\n",
    "    return lengths\n",
    "\n",
    "def too_long_elements(data, border):\n",
    "    ao_too_long = []\n",
    "    for ele in data:\n",
    "        if ele >= border:\n",
    "            ao_too_long.append(ele)\n",
    "    return ao_too_long\n",
    "\n",
    "def number_too_long_elements(data, border):\n",
    "    ao_too_long = []\n",
    "    for ele in data:\n",
    "        if ele >= border:\n",
    "            ao_too_long.append(ele)\n",
    "    return len(ao_too_long)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c554f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_avg_words_source(data):\n",
    "    counts = []\n",
    "    for line in data:\n",
    "        content = line[\"source\"]\n",
    "        numbers = 0\n",
    "        for ele in content:\n",
    "            length = len(ele.split(\" \"))\n",
    "            numbers += length\n",
    "        counts.append(numbers)\n",
    "    return sum(counts)/len(counts)\n",
    "\n",
    "def extract_avg_words_target(data):\n",
    "    counts = []\n",
    "    for line in data:\n",
    "        content = line[\"target\"]\n",
    "        for ele in content:\n",
    "            length = len(ele.split(\" \"))\n",
    "            counts.append(length)\n",
    "    return sum(counts)/len(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166b5f5",
   "metadata": {},
   "source": [
    "### Abstract-Only Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e36f71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read abstract only data\n",
    "ao_train = load_json_data(os.path.join(ao_text_path, \"train.jsonl\"))\n",
    "ao_valid = load_json_data(os.path.join(ao_text_path, \"valid.jsonl\"))\n",
    "ao_test = load_json_data(os.path.join(ao_text_path, \"test.jsonl\"))\n",
    "ao_data = [*ao_train, *ao_valid, *ao_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec866708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of TLDRs per split \n",
    "ao_train_tldrs = compute_number_of_tldrs(ao_train)\n",
    "ao_valid_tldrs = compute_number_of_tldrs(ao_valid)\n",
    "ao_test_tldrs = compute_number_of_tldrs(ao_test)\n",
    "ao_data_tldrs = compute_number_of_tldrs(ao_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d8bb88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data and compare lengths\n",
    "ao_tokenized_sizes = tokenize_data(ao_data)\n",
    "ao_mean_length = sum(ao_tokenized_sizes) / len(ao_tokenized_sizes)\n",
    "ao_too_long_mbart = number_too_long_elements(ao_tokenized_sizes, max_mbart)\n",
    "ao_too_long_zmbart = number_too_long_elements(ao_tokenized_sizes, max_zmbart)\n",
    "ao_too_long_led = number_too_long_elements(ao_tokenized_sizes, max_led)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ea7191d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AO-TRAIN     - Number of elements: 1992 | Number of TLDRs: 1992\n",
      "AO-VALID     - Number of elements:  619 | Number of TLDRs: 1453\n",
      "AO-TEST      - Number of elements:  618 | Number of TLDRs: 1967\n",
      "AO-COMPLETE  - Number of elements: 3229 | Number of TLDRs: 5412\n",
      "Mean length of tokenized abstracts: 248.94673273459276\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  0\n",
      "ZmBART (token length>=2048):  0\n",
      "LED    (token length>=16384): 0\n"
     ]
    }
   ],
   "source": [
    "# Standard measures:\n",
    "print(f\"AO-TRAIN     - Number of elements: {len(ao_train)} | Number of TLDRs: {ao_train_tldrs}\")\n",
    "print(f\"AO-VALID     - Number of elements:  {len(ao_valid)} | Number of TLDRs: {ao_valid_tldrs}\")\n",
    "print(f\"AO-TEST      - Number of elements:  {len(ao_test)} | Number of TLDRs: {ao_test_tldrs}\")\n",
    "print(f\"AO-COMPLETE  - Number of elements: {len(ao_data)} | Number of TLDRs: {ao_data_tldrs}\")\n",
    "print(f\"Mean length of tokenized abstracts: {ao_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {ao_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {ao_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {ao_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3f47892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of tokenized abstracts: 252.2766990291262\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  0\n",
      "ZmBART (token length>=2048):  0\n",
      "LED    (token length>=16384): 0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data and compare lengths\n",
    "ao_tokenized_sizes = tokenize_data(ao_test)\n",
    "ao_mean_length = sum(ao_tokenized_sizes) / len(ao_tokenized_sizes)\n",
    "ao_too_long_mbart = number_too_long_elements(ao_tokenized_sizes, max_mbart)\n",
    "ao_too_long_zmbart = number_too_long_elements(ao_tokenized_sizes, max_zmbart)\n",
    "ao_too_long_led = number_too_long_elements(ao_tokenized_sizes, max_led)\n",
    "print(f\"Mean length of tokenized abstracts: {ao_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {ao_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {ao_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {ao_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c5e04887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number average words in document AO: 163.0242718446602\n",
      "Number average words in summary AO:  18.91103202846975\n",
      "Compression ratio AO:                8.62059096506389\n"
     ]
    }
   ],
   "source": [
    "ao_source_words = extract_avg_words_source(ao_test)\n",
    "ao_target_words = extract_avg_words_target(ao_test)\n",
    "comp_ratio = ao_source_words / ao_target_words\n",
    "print(f\"Number average words in document AO: {ao_source_words}\")\n",
    "print(f\"Number average words in summary AO:  {ao_target_words}\")\n",
    "print(f\"Compression ratio AO:                {comp_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e2b9b8",
   "metadata": {},
   "source": [
    "### Abstract Introduction Conclusion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56519404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read abstract introduction, conclusion data\n",
    "aic_train = load_json_data(os.path.join(aic_text_path, \"train.jsonl\"))\n",
    "aic_valid = load_json_data(os.path.join(aic_text_path, \"valid.jsonl\"))\n",
    "aic_test = load_json_data(os.path.join(aic_text_path, \"test.jsonl\"))\n",
    "aic_data = [*aic_train, *aic_valid, *aic_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2bd94953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of TLDRs per split \n",
    "aic_train_tldrs = compute_number_of_tldrs(aic_train)\n",
    "aic_valid_tldrs = compute_number_of_tldrs(aic_valid)\n",
    "aic_test_tldrs = compute_number_of_tldrs(aic_test)\n",
    "aic_data_tldrs = compute_number_of_tldrs(aic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ae064fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1585 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data and compare lengths\n",
    "aic_tokenized_sizes = tokenize_data(aic_data)\n",
    "aic_mean_length = sum(aic_tokenized_sizes) / len(aic_tokenized_sizes)\n",
    "aic_too_long_mbart = number_too_long_elements(aic_tokenized_sizes, max_mbart)\n",
    "aic_too_long_zmbart = number_too_long_elements(aic_tokenized_sizes, max_zmbart)\n",
    "aic_too_long_led = number_too_long_elements(aic_tokenized_sizes, max_led)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "55440def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC-TRAIN     - Number of elements: 1992 | Number of TLDRs: 1992\n",
      "AIC-VALID     - Number of elements:  619 | Number of TLDRs: 1453\n",
      "AIC-TEST      - Number of elements:  618 | Number of TLDRs: 1967\n",
      "AIC-COMPLETE  - Number of elements: 3229 | Number of TLDRs: 5412\n",
      "Mean length of tokenized abstracts: 1528.5385568287395\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  2573\n",
      "ZmBART (token length>=2048):  601\n",
      "LED    (token length>=16384): 0\n"
     ]
    }
   ],
   "source": [
    "# Standard measures:\n",
    "print(f\"AIC-TRAIN     - Number of elements: {len(aic_train)} | Number of TLDRs: {aic_train_tldrs}\")\n",
    "print(f\"AIC-VALID     - Number of elements:  {len(aic_valid)} | Number of TLDRs: {aic_valid_tldrs}\")\n",
    "print(f\"AIC-TEST      - Number of elements:  {len(aic_test)} | Number of TLDRs: {aic_test_tldrs}\")\n",
    "print(f\"AIC-COMPLETE  - Number of elements: {len(aic_data)} | Number of TLDRs: {aic_data_tldrs}\")\n",
    "print(f\"Mean length of tokenized abstracts: {aic_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {aic_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {aic_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {aic_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c17d9ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of tokenized abstracts: 1571.0291262135922\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  522\n",
      "ZmBART (token length>=2048):  125\n",
      "LED    (token length>=16384): 0\n"
     ]
    }
   ],
   "source": [
    "aic_tokenized_sizes = tokenize_data(aic_test)\n",
    "aic_mean_length = sum(aic_tokenized_sizes) / len(aic_tokenized_sizes)\n",
    "aic_too_long_mbart = number_too_long_elements(aic_tokenized_sizes, max_mbart)\n",
    "aic_too_long_zmbart = number_too_long_elements(aic_tokenized_sizes, max_zmbart)\n",
    "aic_too_long_led = number_too_long_elements(aic_tokenized_sizes, max_led)\n",
    "print(f\"Mean length of tokenized abstracts: {aic_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {aic_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {aic_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {aic_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5e04887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number average words in document AIC: 1023.6731391585761\n",
      "Number average words in summary AIC:  18.91103202846975\n",
      "Compression ratio AIC:                54.13100340676701\n"
     ]
    }
   ],
   "source": [
    "aic_source_words = extract_avg_words_source(aic_test)\n",
    "aic_target_words = extract_avg_words_target(aic_test)\n",
    "comp_ratio = aic_source_words / aic_target_words\n",
    "print(f\"Number average words in document AIC: {aic_source_words}\")\n",
    "print(f\"Number average words in summary AIC:  {aic_target_words}\")\n",
    "print(f\"Compression ratio AIC:                {comp_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ccd9b",
   "metadata": {},
   "source": [
    "### Full Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b4e85309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full text data\n",
    "ft_train = load_json_data(os.path.join(ft_text_path, \"train.jsonl\"))\n",
    "ft_valid = load_json_data(os.path.join(ft_text_path, \"valid.jsonl\"))\n",
    "ft_test = load_json_data(os.path.join(ft_text_path, \"test.jsonl\"))\n",
    "ft_data = [*ft_train, *ft_valid, *ft_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "85298d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of TLDRs per split \n",
    "ft_train_tldrs = compute_number_of_tldrs(ft_train)\n",
    "ft_valid_tldrs = compute_number_of_tldrs(ft_valid)\n",
    "ft_test_tldrs = compute_number_of_tldrs(ft_test)\n",
    "ft_data_tldrs = compute_number_of_tldrs(ft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ad3e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data and compare lengths\n",
    "ft_tokenized_sizes = tokenize_data(ft_data)\n",
    "ft_mean_length = sum(ft_tokenized_sizes) / len(ft_tokenized_sizes)\n",
    "ft_too_long_mbart = number_too_long_elements(ft_tokenized_sizes, max_mbart)\n",
    "ft_too_long_zmbart = number_too_long_elements(ft_tokenized_sizes, max_zmbart)\n",
    "ft_too_long_led = number_too_long_elements(ft_tokenized_sizes, max_led)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dc122949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT-TRAIN     - Number of elements: 1992 | Number of TLDRs: 1992\n",
      "FT-VALID     - Number of elements:  619 | Number of TLDRs: 1453\n",
      "FT-TEST      - Number of elements:  618 | Number of TLDRs: 1967\n",
      "FT-COMPLETE  - Number of elements: 3229 | Number of TLDRs: 5412\n",
      "Mean length of tokenized abstracts: 7602.2313409724375\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  3124\n",
      "ZmBART (token length>=2048):  3083\n",
      "LED    (token length>=16384): 42\n"
     ]
    }
   ],
   "source": [
    "# Standard measures:\n",
    "print(f\"FT-TRAIN     - Number of elements: {len(ft_train)} | Number of TLDRs: {ft_train_tldrs}\")\n",
    "print(f\"FT-VALID     - Number of elements:  {len(ft_valid)} | Number of TLDRs: {ft_valid_tldrs}\")\n",
    "print(f\"FT-TEST      - Number of elements:  {len(ft_test)} | Number of TLDRs: {ft_test_tldrs}\")\n",
    "print(f\"FT-COMPLETE  - Number of elements: {len(ft_data)} | Number of TLDRs: {ft_data_tldrs}\")\n",
    "print(f\"Mean length of tokenized abstracts: {ft_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {ft_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {ft_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {ft_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4a408d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of tokenized abstracts: 7867.3236245954695\n",
      "Number Tokens too long for model inputs:\n",
      "mBART  (token length>=1024):  604\n",
      "ZmBART (token length>=2048):  603\n",
      "LED    (token length>=16384): 7\n"
     ]
    }
   ],
   "source": [
    "ft_tokenized_sizes = tokenize_data(ft_test)\n",
    "ft_mean_length = sum(ft_tokenized_sizes) / len(ft_tokenized_sizes)\n",
    "ft_too_long_mbart = number_too_long_elements(ft_tokenized_sizes, max_mbart)\n",
    "ft_too_long_zmbart = number_too_long_elements(ft_tokenized_sizes, max_zmbart)\n",
    "ft_too_long_led = number_too_long_elements(ft_tokenized_sizes, max_led)\n",
    "print(f\"Mean length of tokenized abstracts: {ft_mean_length}\")\n",
    "print(\"Number Tokens too long for model inputs:\")\n",
    "print(f\"mBART  (token length>={max_mbart}):  {ft_too_long_mbart}\")\n",
    "print(f\"ZmBART (token length>={max_zmbart}):  {ft_too_long_zmbart}\")\n",
    "print(f\"LED    (token length>={max_led}): {ft_too_long_led}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c5e04887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number average words in document FT: 5153.211974110032\n",
      "Number average words in summary FT:  18.91103202846975\n",
      "Compression ratio FT:                272.49765990307094\n"
     ]
    }
   ],
   "source": [
    "ft_source_words = extract_avg_words_source(ft_test)\n",
    "ft_target_words = extract_avg_words_target(ft_test)\n",
    "comp_ratio = ft_source_words / ft_target_words\n",
    "print(f\"Number average words in document FT: {ft_source_words}\")\n",
    "print(f\"Number average words in summary FT:  {ft_target_words}\")\n",
    "print(f\"Compression ratio FT:                {comp_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cfbbeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of too long inputs for LED: 3225.5714285714284\n",
      "Tokens over #16384: 746\n",
      "Tokens over #16384: 3419\n",
      "Tokens over #16384: 181\n",
      "Tokens over #16384: 601\n",
      "Tokens over #16384: 2066\n",
      "Tokens over #16384: 14661\n",
      "Tokens over #16384: 905\n"
     ]
    }
   ],
   "source": [
    "# Display how many tokens more than border are used and mean length over max\n",
    "too_long_led = too_long_elements(ft_tokenized_sizes, max_led)\n",
    "mean_too_long_led = (sum(too_long_led) - (len(too_long_led)*max_led)) / len(too_long_led)\n",
    "print(f\"Mean length of too long inputs for LED: {mean_too_long_led}\")\n",
    "for ele in too_long_led: \n",
    "    print(f\"Tokens over #{max_led}: {ele-max_led}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a5b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
