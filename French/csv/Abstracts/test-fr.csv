source,source_labels,rouge_scores,paper_id,title,target
"Incremental class learning involves sequentially learning classes in bursts of examples from the same class.This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting.Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale.Here, we propose FearNet for incremental class learning.FearNet is a generative model that does not store previous examples, making it memory efficient.FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex.Memory consolidation is inspired by mechanisms that occur during sleep.FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJ1Xmf-Rb,FearNet: Brain-Inspired Model for Incremental Learning,"﻿FearNet est un réseau de neurones à mémoire efficace, inspiré de la formation de la mémoire dans le cerveau des mammifères, capable d'apprendre des classes de manière incrémentielle sans oubli catastrophique."
"Incremental class learning involves sequentially learning classes in bursts of examples from the same class.This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting.Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale.Here, we propose FearNet for incremental class learning.FearNet is a generative model that does not store previous examples, making it memory efficient.FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex.Memory consolidation is inspired by mechanisms that occur during sleep.FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJ1Xmf-Rb,FearNet: Brain-Inspired Model for Incremental Learning,Cet article présente une nouvelle solution à un problème de classification incrémentielle basée sur un système à double mémoire. 
"Multi-view learning can provide self-supervision when different views are available of the same data.Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.One framework uses a generative objective and the other a discriminative one.In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.","[1, 0, 0, 0, 0, 0]",[],S1xzyhR9Y7,Improving Sentence Representations with Multi-view Frameworks,L'apprentissage multi-vues améliore l'apprentissage non supervisé de la représentation des phrases
"Multi-view learning can provide self-supervision when different views are available of the same data.Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.One framework uses a generative objective and the other a discriminative one.In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.","[1, 0, 0, 0, 0, 0]",[],S1xzyhR9Y7,Improving Sentence Representations with Multi-view Frameworks,L'approche utilise des codeurs différents et complémentaires de la phrase d'entrée et la maximisation du consensus.
"Multi-view learning can provide self-supervision when different views are available of the same data.Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.One framework uses a generative objective and the other a discriminative one.In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.","[1, 0, 0, 0, 0, 0]",[],S1xzyhR9Y7,Improving Sentence Representations with Multi-view Frameworks,L'article présente un cadre multi-vues pour améliorer la représentation des phrases dans les tâches NLP en utilisant des architectures objectives génératives et discriminatives.
"Multi-view learning can provide self-supervision when different views are available of the same data.Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.One framework uses a generative objective and the other a discriminative one.In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.","[1, 0, 0, 0, 0, 0]",[],S1xzyhR9Y7,Improving Sentence Representations with Multi-view Frameworks,Cet article montre que les cadres multi-vues sont plus efficaces que l'utilisation d'encodeurs individuels pour l'apprentissage des représentations de phrases.
"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion.During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","[1, 0, 0, 0, 0]",[],HJDUjKeA-,Learning objects from pixels,"Nous montrons comment des objets discrets peuvent être appris de manière non supervisée à partir de pixels, et comment effectuer un apprentissage par renforcement en utilisant cette représentation d'objets."
"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion.During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","[1, 0, 0, 0, 0]",[],HJDUjKeA-,Learning objects from pixels,Une méthode pour apprendre des représentations d'objets à partir de pixels pour faire de l'apprentissage par renforcement. 
"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion.During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","[1, 0, 0, 0, 0]",[],HJDUjKeA-,Learning objects from pixels,"L'article propose une architecture neuronale pour mettre en correspondance des flux vidéo avec une collection discrète d'objets, sans annotations humaines, en utilisant une perte de reconstruction de pixels non supervisée. "
"Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps.Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization.As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.","[0, 0, 1, 0, 0, 0]",[],BJgLg3R9KQ,Learning what and where to attend,"Un ensemble de données à grande échelle pour l'entraînement des modèles d'attention pour la reconnaissance d'objets permet une reconnaissance d'objets plus précise, plus interprétable et plus proche de l'homme."
"Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps.Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization.As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.","[0, 0, 1, 0, 0, 0]",[],BJgLg3R9KQ,Learning what and where to attend,"Les gains récents en matière de reconnaissance visuelle proviennent de l'utilisation des mécanismes d'attention visuelle dans les réseaux convolutifs profonds, qui apprennent où se concentrer grâce à une forme faible de supervision basée sur les étiquettes de classes d'images."
"Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps.Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization.As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.","[0, 0, 1, 0, 0, 0]",[],BJgLg3R9KQ,Learning what and where to attend,Présente une nouvelle approche de l'attention dans laquelle un grand ensemble de données sur l'attention est collecté et utilisé pour entraîner un NN de manière supervisée afin d'exploiter l'attention humaine autodéclarée.
"Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived ""top-down"" attention maps.Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than ""bottom-up"" saliency features for rapid image categorization.As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers.","[0, 0, 1, 0, 0, 0]",[],BJgLg3R9KQ,Learning what and where to attend,"Cet article propose une nouvelle approche pour utiliser des signaux plus informatifs, plus précisément les régions que les humains jugent importantes sur les images, pour améliorer les réseaux neuronaux convolutifs profonds."
"In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BklpOo09tQ,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,Nous avons proposé une méthode de défense efficace en termes de temps contre les attaques adverses à une étape et itératives.
"In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BklpOo09tQ,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,"Proposer une nouvelle méthode efficace en termes de calcul, appelée e2SAD, qui génère des ensembles de deux échantillons adverses d'entraînement pour chaque échantillon d'entraînement propre."
"In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BklpOo09tQ,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,"L'article présente une méthode de défense contradictoire en deux étapes, qui consiste à générer deux exemples contradictoires par échantillon propre et à les inclure dans la boucle d'apprentissage réelle afin d'atteindre la robustesse et de prétendre qu'elle peut surpasser les méthodes itératives plus coûteuses."
"In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BklpOo09tQ,EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS,L'article présente une approche en deux étapes pour générer des exemples contradictoires forts à un coût bien moindre par rapport aux récentes attaques contradictoires itératives à plusieurs étapes.
"Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.","[0, 0, 0, 1]",[],BJLmN8xRW,Character Level Based Detection of DGA Domain Names,Une comparaison de cinq architectures de réseaux neuronaux profonds pour la détection de noms de domaine malveillants montre étonnamment peu de différences.
"Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.","[0, 0, 0, 1]",[],BJLmN8xRW,Character Level Based Detection of DGA Domain Names,Les auteurs proposent d'utiliser cinq architectures profondes pour la tâche de cybersécurité consistant à détecter les algorithmes de génération de domaine.
"Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.","[0, 0, 0, 1]",[],BJLmN8xRW,Character Level Based Detection of DGA Domain Names,Applique plusieurs architectures NN pour classer les URL entre celles qui sont bénignes et celles qui sont liées à des logiciels malveillants.
"Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting.","[0, 0, 0, 1]",[],BJLmN8xRW,Character Level Based Detection of DGA Domain Names,Cet article propose de reconnaître automatiquement les noms de domaine comme malveillants ou bénins par des réseaux profonds entraînés à classer directement la séquence de caractères comme telle.
"Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks.Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts.We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases.Our experiments demonstrate that our models are more robust to new NLI datasets.","[0, 0, 0, 0, 1]",[],rkMlSnAqYX,Mitigating Bias in Natural Language Inference Using Adversarial Learning,Les méthodes d'apprentissage adversarial encouragent les modèles NLI à ignorer les biais spécifiques aux ensembles de données et aident les modèles à se transférer entre les ensembles de données.
"Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks.Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts.We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases.Our experiments demonstrate that our models are more robust to new NLI datasets.","[0, 0, 0, 0, 1]",[],rkMlSnAqYX,Mitigating Bias in Natural Language Inference Using Adversarial Learning,L'article propose une configuration contradictoire pour atténuer les artefacts d'annotation dans les données d'inférence en langage naturel.
"Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks.Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts.We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases.Our experiments demonstrate that our models are more robust to new NLI datasets.","[0, 0, 0, 0, 1]",[],rkMlSnAqYX,Mitigating Bias in Natural Language Inference Using Adversarial Learning,Cet article présente une méthode pour éliminer les biais d'un modèle d'implication textuelle par le biais d'un objectif de formation contradictoire. 
"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","[0, 0, 1, 0, 0, 0]",[],HkgEQnRqYQ,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,Une nouvelle approche de pointe pour l'intégration de graphes de connaissances.
"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","[0, 0, 1, 0, 0, 0]",[],HkgEQnRqYQ,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,"Présente une fonction de notation de prédiction de liens neuronaux qui peut déduire des modèles de symétrie, d'anti-symétrie, d'inversion et de composition de relations dans une base de connaissances."
"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","[0, 0, 1, 0, 0, 0]",[],HkgEQnRqYQ,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,Cet article propose une approche de l'intégration des graphes de connaissances en modélisant les relations comme des rotations dans l'espace vectoriel complexe.
"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.","[0, 0, 1, 0, 0, 0]",[],HkgEQnRqYQ,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,propose une méthode d'intégration des graphes à utiliser pour la prédiction des liens.
"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification.This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks.However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms.In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels.To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps.Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels.Thereby, weights and kernels are collectively optimized for learning of image classification models robust toadversarial attacks without employment of additional target detection and rejection algorithms.We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkeDJ04Mf,HyperNetworks with statistical filtering for defending adversarial examples,Nous avons modifié le CNN en utilisant des HyperNetworks et avons observé une meilleure robustesse contre les exemples adverses.
"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification.This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks.However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms.In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels.To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps.Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels.Thereby, weights and kernels are collectively optimized for learning of image classification models robust toadversarial attacks without employment of additional target detection and rejection algorithms.We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkeDJ04Mf,HyperNetworks with statistical filtering for defending adversarial examples,Amélioration de la robustesse et de la fiabilité des réseaux neuronaux profonds à convolution par l'utilisation de noyaux de convolution dépendant des données
"Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.This requires back-propagating errors through the solver steps.While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,"Nous proposons une approche de méta-apprentissage pour la classification de quelques images qui permet d'obtenir de bonnes performances à grande vitesse par rétro-propagation à travers la solution de solveurs rapides, tels que la régression ridge ou la régression logistique."
"Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.This requires back-propagating errors through the solver steps.While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,"L'article propose un algorithme de méta-apprentissage qui revient à fixer les caractéristiques (c'est-à-dire toutes les couches cachées d'un NN profond), et à traiter chaque tâche comme ayant sa propre couche finale qui pourrait être une régression ridge ou une régression logistique."
"Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.This requires back-propagating errors through the solver steps.While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],HyxnZh0ct7,Meta-learning with differentiable closed-form solvers,"Cet article propose une approche de méta-apprentissage pour le problème de la classification en quelques coups, ils utilisent une méthode basée sur la paramétrisation de l'apprenant pour chaque tâche par un solveur à forme fermée."
"While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.Each answer eliminates some classes, leaving the learner with a partial label.The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.Active learning with partial labels requires(i) a sampling strategy to choose (example, class) pairs, and(ii) learning from partial labels between rounds.Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJfSEnRqKQ,Active Learning with Partial Feedback,"Nous offrons une nouvelle perspective sur la formation d'un modèle d'apprentissage automatique à partir de zéro dans un cadre d'étiquettes hiérarchiques, c'est-à-dire en le considérant comme une communication bidirectionnelle entre l'homme et les algorithmes, et nous étudions comment nous pouvons à la fois mesurer et améliorer l'efficacité. "
"While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.Each answer eliminates some classes, leaving the learner with a partial label.The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.Active learning with partial labels requires(i) a sampling strategy to choose (example, class) pairs, and(ii) learning from partial labels between rounds.Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJfSEnRqKQ,Active Learning with Partial Feedback,"Introduit un nouveau cadre d'apprentissage actif dans lequel l'oracle propose une étiquette partielle ou faible au lieu de demander l'étiquette d'un exemple particulier, ce qui simplifie la recherche d'informations."
"While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.Each answer eliminates some classes, leaving the learner with a partial label.The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.Active learning with partial labels requires(i) a sampling strategy to choose (example, class) pairs, and(ii) learning from partial labels between rounds.Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJfSEnRqKQ,Active Learning with Partial Feedback,Cet article propose une méthode d'apprentissage actif avec rétroaction partielle qui surpasse les lignes de base existantes avec un budget limité.
"While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.Each answer eliminates some classes, leaving the learner with a partial label.The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.Active learning with partial labels requires(i) a sampling strategy to choose (example, class) pairs, and(ii) learning from partial labels between rounds.Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJfSEnRqKQ,Active Learning with Partial Feedback,"L'article considère un problème de classification multiclasse dans lequel les étiquettes sont regroupées dans un nombre donné M de sous-ensembles, qui contiennent toutes les étiquettes individuelles comme singletons."
"Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.","[0, 0, 0, 1, 0, 0, 0]",[],rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,Nous montrons que les espaces de Wasserstein sont de bonnes cibles pour l'intégration de données à structure sémantique complexe.
"Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.","[0, 0, 0, 1, 0, 0, 0]",[],rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,"Apprend les encastrements dans un espace discret de distributions de probabilité, en utilisant une version minimisée et régularisée des distances de Wasserstein."
"Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.","[0, 0, 0, 1, 0, 0, 0]",[],rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,L'article décrit une nouvelle méthode d'incorporation qui incorpore les données dans l'espace des mesures de probabilité doté de la distance de Wasserstein. 
"Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.","[0, 0, 0, 1, 0, 0, 0]",[],rJg4J3CqFm,Learning Embeddings into Entropic Wasserstein Spaces,"L'article propose d'intégrer les données dans des espaces de Wasserstein de faible dimension, qui peuvent capturer la structure sous-jacente des données de manière plus précise."
"Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces.We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.The data is embedded into a lower-dimensional space by a deep autoencoder.The autoencoder is optimized as part of the clustering process.The resulting network produces clustered data.The presented approach does not rely on prior knowledge of the number of ground-truth clusters.Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective.We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms.Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJzMATlAZ,Deep Continuous Clustering,Un algorithme de regroupement qui effectue une réduction non linéaire de la dimension et un regroupement en optimisant un objectif global continu.
"Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces.We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.The data is embedded into a lower-dimensional space by a deep autoencoder.The autoencoder is optimized as part of the clustering process.The resulting network produces clustered data.The presented approach does not rely on prior knowledge of the number of ground-truth clusters.Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective.We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms.Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJzMATlAZ,Deep Continuous Clustering,"Présente un algorithme de regroupement en résolvant conjointement l'auto-codeur profond et le regroupement comme un objectif global continu, montrant de meilleurs résultats que les schémas de regroupement de l'état de l'art."
"Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces.We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.The data is embedded into a lower-dimensional space by a deep autoencoder.The autoencoder is optimized as part of the clustering process.The resulting network produces clustered data.The presented approach does not rely on prior knowledge of the number of ground-truth clusters.Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective.We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms.Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJzMATlAZ,Deep Continuous Clustering,Le clustering continu profond est une méthode de clustering qui intègre l'objectif de l'autoencodeur avec l'objectif du clustering puis s'entraîne en utilisant SGD.
"Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements.Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation.However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning.Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time.Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure.To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning.As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining.For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients.This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure.For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJzYdsAqY7,Spatial-Winograd Pruning Enabling Sparse Winograd Convolution,"Pour accélérer le calcul des réseaux neuronaux convolutifs, nous proposons une nouvelle technique d'élagage en deux étapes qui permet d'obtenir une plus grande sparsité des poids dans le domaine de Winograd sans modifier la structure du réseau."
"Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements.Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation.However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning.Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time.Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure.To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning.As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining.For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients.This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure.For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJzYdsAqY7,Spatial-Winograd Pruning Enabling Sparse Winograd Convolution,Propose un cadre d'élagage spatial-Winograd qui permet de conserver les poids élagués du domaine spatial dans le domaine Winograd et d'améliorer la sparsité du domaine Winograd.
"Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements.Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation.However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning.Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time.Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure.To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning.As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining.For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients.This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure.For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJzYdsAqY7,Spatial-Winograd Pruning Enabling Sparse Winograd Convolution,Propose deux techniques d'élagage des couches convolutives qui utilisent l'algorithme de Winograd.
"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.We develop a Bayesian nonparametric framework for federated learning with neural networks.Each data server is assumed to train local neural network weights, which are modeled through our framework.We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling.We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.","[0, 1, 0, 0, 0]",[],SygHGnRqK7,Probabilistic Federated Neural Matching,Nous proposons un modèle bayésien non paramétrique pour l'apprentissage fédéré avec des réseaux neuronaux.
"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.We develop a Bayesian nonparametric framework for federated learning with neural networks.Each data server is assumed to train local neural network weights, which are modeled through our framework.We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling.We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.","[0, 1, 0, 0, 0]",[],SygHGnRqK7,Probabilistic Federated Neural Matching,Utilise le processus bêta pour faire une correspondance neuronale fédérée.
"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.We develop a Bayesian nonparametric framework for federated learning with neural networks.Each data server is assumed to train local neural network weights, which are modeled through our framework.We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling.We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.","[0, 1, 0, 0, 0]",[],SygHGnRqK7,Probabilistic Federated Neural Matching,"L'article considère l'apprentissage fédéré des réseaux neuronaux, où les données sont distribuées sur plusieurs machines et la répartition des points de données est potentiellement inhomogène et déséquilibrée."
"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution.Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed.We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second.Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling.Python source code will be open-sourced with the camera-ready paper.","[1, 0, 0, 0, 0]",[],B1n8LexRZ,Generalizing Hamiltonian Monte Carlo with Neural Networks,"Méthode générale pour entraîner des noyaux MCMC expressifs paramétrés avec des réseaux neuronaux profonds. Étant donné une distribution cible p, notre méthode fournit un échantillonneur à mélange rapide, capable d'explorer efficacement l'espace d'état."
"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution.Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed.We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second.Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling.Python source code will be open-sourced with the camera-ready paper.","[1, 0, 0, 0, 0]",[],B1n8LexRZ,Generalizing Hamiltonian Monte Carlo with Neural Networks,Propose une HMC généralisée en modifiant l'intégrateur saute-mouton à l'aide de réseaux neuronaux pour que l'échantillonneur converge et mélange rapidement. 
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.To solve this we propose a continuation approach that learns failure modes in related but less robust agents.Our approach also allows reuse of data already collected for training the agent.We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1xhQhRcK7,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,"Nous montrons que des défaillances rares mais catastrophiques peuvent être entièrement manquées par les tests aléatoires, ce qui pose des problèmes pour un déploiement sûr. L'approche que nous proposons pour les tests contradictoires résout ce problème."
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.To solve this we propose a continuation approach that learns failure modes in related but less robust agents.Our approach also allows reuse of data already collected for training the agent.We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1xhQhRcK7,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,"Propose une méthode qui apprend un prédicteur de probabilité de défaillance pour un agent appris, ce qui permet de prédire quels états initiaux provoquent la défaillance d'un système."
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.To solve this we propose a continuation approach that learns failure modes in related but less robust agents.Our approach also allows reuse of data already collected for training the agent.We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1xhQhRcK7,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,"Cet article propose une approche d'échantillonnage par importance pour l'échantillonnage des cas de défaillance pour les algorithmes RL, basée sur une fonction apprise via un réseau neuronal sur les défaillances qui se produisent pendant la formation de l'agent."
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.To solve this we propose a continuation approach that learns failure modes in related but less robust agents.Our approach also allows reuse of data already collected for training the agent.We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1xhQhRcK7,Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,Cet article propose une approche contradictoire pour identifier les cas de défaillance catastrophique dans l'apprentissage par renforcement.
"The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique.By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods.In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior.In this paper, we investigate posterior collapse from the perspective of training dynamics.We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target.As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs.Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update.Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work.Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rylDfnCqF7,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,"Pour résoudre le problème de l'effondrement postérieur dans les VAE, nous proposons une procédure de formation nouvelle mais simple qui optimise de manière agressive le réseau d'inférence avec davantage de mises à jour. Cette nouvelle procédure de formation atténue l'effondrement postérieur et permet d'obtenir un meilleur modèle VAE. "
"The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique.By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods.In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior.In this paper, we investigate posterior collapse from the perspective of training dynamics.We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target.As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs.Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update.Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work.Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rylDfnCqF7,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,"Examine le phénomène d'effondrement postérieur, montrant qu'un entraînement accru du réseau d'inférence peut réduire le problème et conduire à de meilleurs optima."
"The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique.By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods.In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior.In this paper, we investigate posterior collapse from the perspective of training dynamics.We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target.As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs.Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update.Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work.Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rylDfnCqF7,Lagging Inference Networks and Posterior Collapse in Variational Autoencoders,"Les auteurs proposent de modifier la procédure de formation des VAE uniquement comme solution à l'effondrement postérieur, sans toucher au modèle ni à l'objectif."
"Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies.To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form.To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge.Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs.The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs.Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method.Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.","[0, 0, 1, 0, 0, 0, 0]",[],BJhxcGZCW,Generative Discovery of Relational Medical Entity Pairs,"Découvrez de manière générative de nouvelles paires d'entités significatives ayant une certaine relation médicale en apprenant purement à partir des paires d'entités significatives existantes, sans avoir besoin d'un corpus de texte supplémentaire pour l'extraction discriminante."
"Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies.To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form.To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge.Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs.The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs.Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method.Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.","[0, 0, 1, 0, 0, 0, 0]",[],BJhxcGZCW,Generative Discovery of Relational Medical Entity Pairs,Présente un auto-codeur variationnel pour générer des paires d'entités à partir d'une relation dans un contexte médical.
"Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies.To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form.To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge.Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs.The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs.Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method.Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel.","[0, 0, 1, 0, 0, 0, 0]",[],BJhxcGZCW,Generative Discovery of Relational Medical Entity Pairs,"Dans le contexte médical, cet article décrit le problème classique de la ""complétion de la base de connaissances"" à partir de données structurées uniquement."
"Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. The code for our model is available at \url{https://github.com/daib13/TwoStageVAE}.","[0, 0, 0, 1, 0, 0]",[],B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,Nous analysons de près la fonction objectif du VAE et tirons de nouvelles conclusions qui conduisent à des améliorations simples.
"Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. The code for our model is available at \url{https://github.com/daib13/TwoStageVAE}.","[0, 0, 0, 1, 0, 0]",[],B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,Propose une méthode VAE en deux étapes pour générer des échantillons de haute qualité et éviter le flou.
"Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. The code for our model is available at \url{https://github.com/daib13/TwoStageVAE}.","[0, 0, 0, 1, 0, 0]",[],B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,Ce document analyse les VAE gaussiens.
"Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. The code for our model is available at \url{https://github.com/daib13/TwoStageVAE}.","[0, 0, 0, 1, 0, 0]",[],B1e0X3C9tQ,Diagnosing and Enhancing VAE Models,"L'article fournit un certain nombre de résultats théoriques sur les Auto-Encodeurs Variationnels Gaussiens ""vanille"", qui sont ensuite utilisés pour construire un nouvel algorithme appelé ""VAE à 2 étapes""."
"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters.The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search $8\times$.   Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).","[0, 0, 1, 0, 0, 0, 0]",[],H1zriGeCZ,Hyperparameter optimization: a spectral approach,Un algorithme de réglage des hyperparamètres utilisant l'analyse de Fourier discrète et la détection comprimée.
"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters.The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search $8\times$.   Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).","[0, 0, 1, 0, 0, 0, 0]",[],H1zriGeCZ,Hyperparameter optimization: a spectral approach,"Étudie le problème de l'optimisation des hyperparamètres en supposant que la fonction inconnue peut être approximée, en montrant que la minimisation approximative peut être effectuée sur l'hypercube booléen."
"We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters.The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  We also outperform Random Search $8\times$.   Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively).","[0, 0, 1, 0, 0, 0, 0]",[],H1zriGeCZ,Hyperparameter optimization: a spectral approach,L'article explore l'optimisation des hyperparamètres en supposant une structure dans la fonction inconnue qui relie les hyperparamètres à la précision de la classification.
"Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data.Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable.In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator.With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings.We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.","[0, 0, 1, 0, 0, 0]",[],Byt3oJ-0W,Learning Latent Permutations with Gumbel-Sinkhorn Networks,"Une nouvelle méthode d'inférence de permutations par descente de gradient, avec des applications à l'inférence de correspondances latentes et à l'apprentissage supervisé de permutations avec des réseaux neuronaux."
"Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data.Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable.In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator.With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings.We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.","[0, 0, 1, 0, 0, 0]",[],Byt3oJ-0W,Learning Latent Permutations with Gumbel-Sinkhorn Networks,L'article utilise une approximation finie de l'opérateur Sinkhorn pour décrire comment construire un réseau neuronal pour l'apprentissage à partir de données de formation à valeur de permutation. 
"Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data.Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable.In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator.With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings.We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms.","[0, 0, 1, 0, 0, 0]",[],Byt3oJ-0W,Learning Latent Permutations with Gumbel-Sinkhorn Networks,L'article propose une nouvelle méthode d'approximation du poids maximal discret pour l'apprentissage des permutations latentes.
"Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources.However, existing quantization methods often represent all weights and activations with the same precision (bit-width).In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths.We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization.Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet.Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.","[0, 0, 0, 1, 0, 0]",[],BJGVX3CqYm,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,Un nouveau cadre de recherche d'architecture neuronale différentiable pour la quantification mixte des ConvNets.
"Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources.However, existing quantization methods often represent all weights and activations with the same precision (bit-width).In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths.We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization.Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet.Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.","[0, 0, 0, 1, 0, 0]",[],BJGVX3CqYm,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,"Les auteurs présentent une nouvelle méthode de recherche d'architecture neuronale qui sélectionne la quantification de précision des poids à chaque couche du réseau neuronal, et l'utilisent dans le contexte de la compression de réseau."
"Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources.However, existing quantization methods often represent all weights and activations with the same precision (bit-width).In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths.We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization.Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet.Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models.","[0, 0, 0, 1, 0, 0]",[],BJGVX3CqYm,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,L'article présente une nouvelle approche de la quantification du réseau en quantifiant différentes couches avec des largeurs de bits différentes et introduit un nouveau cadre de recherche d'architecture neuronale différentiable.
"The top-$k$ error is a common measure of performance in machine learning and computer vision.In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.The widely used cross-entropy is a special case of our family.Evaluating our smooth loss functions is computationally challenging: a na{\""i}ve algorithm would require $\mathcal{O}(\binom{n}{k})$ operations, where $n$ is the number of classes.Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\mathcal{O}(k n)$.Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hk5elxbRW,Smooth Loss Functions for Deep Top-k Classification,Fonction de perte lisse pour la minimisation de l'erreur top-k
"The top-$k$ error is a common measure of performance in machine learning and computer vision.In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.The widely used cross-entropy is a special case of our family.Evaluating our smooth loss functions is computationally challenging: a na{\""i}ve algorithm would require $\mathcal{O}(\binom{n}{k})$ operations, where $n$ is the number of classes.Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\mathcal{O}(k n)$.Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hk5elxbRW,Smooth Loss Functions for Deep Top-k Classification,Propose d'utiliser la perte top-k avec des modèles profonds pour résoudre le problème de la confusion des classes avec des classes similaires présentes ou absentes de l'ensemble de données d'entraînement.
"The top-$k$ error is a common measure of performance in machine learning and computer vision.In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.The widely used cross-entropy is a special case of our family.Evaluating our smooth loss functions is computationally challenging: a na{\""i}ve algorithm would require $\mathcal{O}(\binom{n}{k})$ operations, where $n$ is the number of classes.Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\mathcal{O}(k n)$.Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hk5elxbRW,Smooth Loss Functions for Deep Top-k Classification,Lisse les pertes top-k.
"The top-$k$ error is a common measure of performance in machine learning and computer vision.In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.The widely used cross-entropy is a special case of our family.Evaluating our smooth loss functions is computationally challenging: a na{\""i}ve algorithm would require $\mathcal{O}(\binom{n}{k})$ operations, where $n$ is the number of classes.Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\mathcal{O}(k n)$.Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hk5elxbRW,Smooth Loss Functions for Deep Top-k Classification,"Cet article introduit une fonction de perte de substitution lisse pour le SVM top-k, dans le but de brancher le SVM aux réseaux neuronaux profonds."
"Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties.To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property.We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP).In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.","[0, 1, 0, 0, 0]",[],BklKFo09YX,Mol-CycleGAN - a generative model for molecular optimization,Nous présentons Mol-CycleGAN - un nouveau modèle génératif pour l'optimisation des molécules afin d'améliorer la conception des médicaments.
"Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties.To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property.We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP).In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.","[0, 1, 0, 0, 0]",[],BklKFo09YX,Mol-CycleGAN - a generative model for molecular optimization,L'article présente une approche d'optimisation des propriétés moléculaires basée sur l'application des CycleGAN aux auto-codeurs variationnels pour les molécules et utilise un VAE spécifique au domaine appelé Junction Tree VAE (JT-VAE).
"Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties.To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property.We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP).In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.","[0, 1, 0, 0, 0]",[],BklKFo09YX,Mol-CycleGAN - a generative model for molecular optimization,"Cet article utilise un autoencodeur variationnel pour apprendre une fonction de traduction, de l'ensemble des molécules sans la propriété concernée à l'ensemble des molécules avec la propriété. "
"Knowledge distillation is a potential solution for model compression.The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one.Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network.However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered.To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks.Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates.In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification.Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],rJFOptp6Z,Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,Nous prenons la reconnaissance des visages comme point de rupture et proposons la distillation de modèles avec transfert de connaissances de la classification des visages à l'alignement et à la vérification.
"Knowledge distillation is a potential solution for model compression.The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one.Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network.However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered.To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks.Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates.In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification.Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],rJFOptp6Z,Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,Cet article propose de transférer le classificateur du modèle de classification des visages à la tâche d'alignement et de vérification.
"Knowledge distillation is a potential solution for model compression.The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one.Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network.However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered.To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks.Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates.In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification.Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],rJFOptp6Z,Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification,Le manuscrit présente des expériences sur la distillation des connaissances d'un modèle de classification des visages vers des modèles d'étudiants pour l'alignement et la vérification des visages.
"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.Unlike data augmentation-based improvements, our method does not increase training times significantly.","[0, 1, 0, 0, 0]",[],ryCM8zWRb,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,Amélioration de 35% des recommandations basées sur la session avec des RNN (GRU4Rec) en utilisant des fonctions de perte et un échantillonnage nouvellement conçus.
"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.Unlike data augmentation-based improvements, our method does not increase training times significantly.","[0, 1, 0, 0, 0]",[],ryCM8zWRb,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,Cet article analyse les fonctions de perte existantes pour les recommandations basées sur les sessions et propose deux nouvelles fonctions de perte qui ajoutent une pondération aux fonctions de perte existantes basées sur le classement.
"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.Unlike data augmentation-based improvements, our method does not increase training times significantly.","[0, 1, 0, 0, 0]",[],ryCM8zWRb,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"présente des modifications par rapport aux travaux antérieurs pour la recommandation basée sur la session en utilisant le RNN en pondérant les exemples négatifs par leur ""pertinence""."
"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.Unlike data augmentation-based improvements, our method does not increase training times significantly.","[0, 1, 0, 0, 0]",[],ryCM8zWRb,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"Cet article discute des problèmes d'optimisation des fonctions de perte dans GRU4Rec, propose des astuces d'optimisation et suggère une version améliorée."
"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks.Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task.We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task.Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks.We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.","[0, 0, 1, 0, 0, 0]",[],rJUBryZ0W,Lifelong Learning by Adjusting Priors,"Nous développons une approche d'apprentissage tout au long de la vie pour l'apprentissage par transfert basée sur la théorie PAC-Bayes, dans laquelle les prieurs sont ajustés au fur et à mesure que de nouvelles tâches sont rencontrées, facilitant ainsi l'apprentissage de nouvelles tâches."
"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks.Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task.We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task.Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks.We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.","[0, 0, 1, 0, 0, 0]",[],rJUBryZ0W,Lifelong Learning by Adjusting Priors,"Une nouvelle limite de risque PAC-Bayes qui sert de fonction objective pour l'apprentissage automatique multi-tâches, et un algorithme pour minimiser une version simplifiée de cette fonction objective."
"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks.Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task.We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task.Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks.We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples.","[0, 0, 1, 0, 0, 0]",[],rJUBryZ0W,Lifelong Learning by Adjusting Priors,"Étend les limites PAC-Bayes existantes à l'apprentissage multi-tâches, afin de permettre l'adaptation de l'antériorité à différentes tâches."
"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models.While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs).In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam.As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD.Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits.By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.","[0, 0, 0, 1, 0, 0]",[],HJSA_e1AW,Normalized Direction-preserving Adam,"Une version adaptée d'Adam pour l'entraînement de DNNs, qui comble le fossé de généralisation entre Adam et SGD."
"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models.While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs).In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam.As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD.Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits.By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.","[0, 0, 0, 1, 0, 0]",[],HJSA_e1AW,Normalized Direction-preserving Adam,Propose une variante de l'algorithme d'optimisation ADAM qui normalise les poids de chaque unité cachée en utilisant la normalisation par lots.
"Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models.While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs).In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam.As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD.Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits.By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others.","[0, 0, 0, 1, 0, 0]",[],HJSA_e1AW,Normalized Direction-preserving Adam,Extension de l'algorithme d'optimisation d'Adam pour préserver la direction de mise à jour en adaptant le taux d'apprentissage pour les poids entrants dans une unité cachée en utilisant conjointement la norme L2 du vecteur gradient.
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.However, autonomously learning effective sets of options is still a major challenge in the field.In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.","[0, 0, 0, 0, 1, 0, 0, 0]",[],Bk8ZcAxR-,Eigenoption Discovery through the Deep Successor Representation,"Nous montrons comment nous pouvons utiliser la représentation successeur pour découvrir des options propres dans des domaines stochastiques, à partir de pixels bruts. Les options propres sont des options apprises pour naviguer dans les dimensions latentes d'une représentation apprise."
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.However, autonomously learning effective sets of options is still a major challenge in the field.In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.","[0, 0, 0, 0, 1, 0, 0, 0]",[],Bk8ZcAxR-,Eigenoption Discovery through the Deep Successor Representation,Étend l'idée des options propres aux domaines avec des transitions stochastiques et où les caractéristiques de l'état sont apprises.
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.However, autonomously learning effective sets of options is still a major challenge in the field.In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.","[0, 0, 0, 0, 1, 0, 0, 0]",[],Bk8ZcAxR-,Eigenoption Discovery through the Deep Successor Representation,Montre l'équivalence entre les fonctions de valeur proto et les représentations de successeur et dérive l'idée d'options propres comme mécanisme de découverte d'options.
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.However, autonomously learning effective sets of options is still a major challenge in the field.In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.","[0, 0, 0, 0, 1, 0, 0, 0]",[],Bk8ZcAxR-,Eigenoption Discovery through the Deep Successor Representation,"Cet article fait suite aux travaux antérieurs de Machado et al. (2017) montrant comment les fonctions de proto-valeur peuvent être utilisées pour définir des options appelées "" options propres ""."
"One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.In addition, we present a tighter upper bound that leverages network coefficients.We test both on trained networks.The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.The refined upper bound is particularly stronger on networks with narrow layers.  ","[0, 1, 0, 0, 0, 0, 0, 0]",[],B1MAJhR5YX,Empirical Bounds on Linear Regions of Deep Rectifier Networks,"Nous fournissons des limites supérieures améliorées pour le nombre de régions linéaires utilisées dans l'expressivité du réseau, et un algorithme très efficace (par rapport au comptage exact) pour obtenir des limites inférieures probabilistes sur le nombre réel de régions linéaires."
"One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.In addition, we present a tighter upper bound that leverages network coefficients.We test both on trained networks.The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.The refined upper bound is particularly stronger on networks with narrow layers.  ","[0, 1, 0, 0, 0, 0, 0, 0]",[],B1MAJhR5YX,Empirical Bounds on Linear Regions of Deep Rectifier Networks,Contribue à l'étude du nombre de régions linéaires dans les réseaux neuronaux RELU en utilisant un algorithme de comptage probabiliste approximatif et une analyse.
"One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.In addition, we present a tighter upper bound that leverages network coefficients.We test both on trained networks.The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.The refined upper bound is particularly stronger on networks with narrow layers.  ","[0, 1, 0, 0, 0, 0, 0, 0]",[],B1MAJhR5YX,Empirical Bounds on Linear Regions of Deep Rectifier Networks,S'appuie sur des travaux antérieurs étudiant le comptage des régions linéaires dans les réseaux neuronaux profonds et améliore la limite supérieure proposée précédemment en modifiant la contrainte de dimensionnalité.
"One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.In addition, we present a tighter upper bound that leverages network coefficients.We test both on trained networks.The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.The refined upper bound is particularly stronger on networks with narrow layers.  ","[0, 1, 0, 0, 0, 0, 0, 0]",[],B1MAJhR5YX,Empirical Bounds on Linear Regions of Deep Rectifier Networks,"L'article traite de l'expressivité d'un réseau neuronal linéaire par morceaux, caractérisée par le nombre de régions linéaires de la fonction modélisée, et tire parti des algorithmes probabilistes pour calculer les limites plus rapidement et prouver des limites plus strictes."
"The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision.This critical faculty allows us to understand highly complex visual scenes.Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene.Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory.We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory.We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees.The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer.By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction.Our model obtains highly competitive classification performance on MNIST and CIFAR-10.As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation.Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism).Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1fbosCcYm,A Biologically Inspired Visual Working Memory for Deep Networks,Une mémoire de travail d'inspiration biologique pouvant être intégrée dans des modèles d'attention visuelle récurrente pour des performances de pointe
"The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision.This critical faculty allows us to understand highly complex visual scenes.Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene.Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory.We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory.We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees.The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer.By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction.Our model obtains highly competitive classification performance on MNIST and CIFAR-10.As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation.Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism).Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1fbosCcYm,A Biologically Inspired Visual Working Memory for Deep Networks,Introduit une nouvelle architecture de réseau inspirée de la mémoire de travail visuelle attentive et l'applique à des tâches de classification et l'utilise comme modèle génératif
"The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision.This critical faculty allows us to understand highly complex visual scenes.Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene.Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory.We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory.We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees.The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer.By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction.Our model obtains highly competitive classification performance on MNIST and CIFAR-10.As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation.Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism).Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1fbosCcYm,A Biologically Inspired Visual Working Memory for Deep Networks,L'article ajoute au modèle d'attention récurrent un nouveau modèle de mémoire de travail de Hebb-Rosenblatt et obtient des résultats compétitifs sur MNIST.
"Generative models have been successfully applied to image style transfer and domain translation.However, there is still a wide gap in the quality of results when learning such tasks on musical audio.Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.This allows a faster and more stable training, along with a controllable latent space encoder.By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],HJgOl3AqY7,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,"L'article utilise l'encodage automatique variationnel et le conditionnement de réseau pour le transfert de timbres musicaux, nous développons et généralisons notre architecture pour les transferts d'instruments entre plusieurs, avec des visualisations et des évaluations."
"Generative models have been successfully applied to image style transfer and domain translation.However, there is still a wide gap in the quality of results when learning such tasks on musical audio.Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.This allows a faster and more stable training, along with a controllable latent space encoder.By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],HJgOl3AqY7,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,propose un auto-encodeur variationnel modulé pour effectuer le transfert de timbres musicaux en remplaçant le critère de traduction contradictoire habituel par un écart moyen maximal.
"Generative models have been successfully applied to image style transfer and domain translation.However, there is still a wide gap in the quality of results when learning such tasks on musical audio.Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.This allows a faster and more stable training, along with a controllable latent space encoder.By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],HJgOl3AqY7,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,Décrit un modèle many-to-many pour le transfert de timbre musical qui s'appuie sur les développements récents en matière de transfert de domaine et de style.
"Generative models have been successfully applied to image style transfer and domain translation.However, there is still a wide gap in the quality of results when learning such tasks on musical audio.Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.This allows a faster and more stable training, along with a controllable latent space encoder.By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],HJgOl3AqY7,Modulated Variational Auto-Encoders for Many-to-Many Musical Timbre Transfer,Propose un modèle hybride basé sur le VAE pour effectuer le transfert de timbre sur des enregistrements d'instruments de musique.
"We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.","[0, 1, 0, 0, 0, 0, 0]",[],HJx54i05tX,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training","Nous étudions le comportement des autoencodeurs multicouches de vanille liés à des poids sous l'hypothèse de poids aléatoires. Grâce à une caractérisation exacte dans la limite des grandes dimensions, notre analyse révèle des phénomènes intéressants de transition de phase."
"We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.","[0, 1, 0, 0, 0, 0, 0]",[],HJx54i05tX,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training",Une analyse théorique des auto-codeurs avec poids liés entre codeur et décodeur (weight-tied) via l'analyse du champ moyen.
"We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.","[0, 1, 0, 0, 0, 0, 0]",[],HJx54i05tX,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training","Analyse les performances des auto-encodeurs liés pondérés en s'appuyant sur les progrès récents réalisés dans l'analyse des problèmes de statistiques à haute dimension et, en particulier, sur l'algorithme de passage de messages."
"We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs “approximate inference” as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability.","[0, 1, 0, 0, 0, 0, 0]",[],HJx54i05tX,"On Random Deep Weight-Tied Autoencoders: Exact Asymptotic Analysis, Phase Transitions, and Implications to Training","Cet article étudie les auto-codeurs sous plusieurs hypothèses, et souligne que ce modèle d'auto-codeur aléatoire peut être analysé de manière élégante et rigoureuse avec des équations unidimensionnelles."
"Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) andkernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE).CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.","[0, 0, 1, 0, 0]",[],rkgwuiA9F7,Cramer-Wold AutoEncoder,"Inspirés par des travaux antérieurs sur les autoencodeurs Sliced-Wasserstein (SWAE) et le lissage à noyau, nous construisons un nouveau modèle génératif : l'autoencodeur Cramer-Wold (CWAE)."
"Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) andkernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE).CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.","[0, 0, 1, 0, 0]",[],rkgwuiA9F7,Cramer-Wold AutoEncoder,Cet article propose une variante du WAE basée sur une nouvelle distance statistique entre la distribution des données codées et la distribution antérieure latente.
"Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) andkernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE).CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.","[0, 0, 1, 0, 0]",[],rkgwuiA9F7,Cramer-Wold AutoEncoder,Introduit une variation des AudoEncoders de Wasserstein qui est une nouvelle architecture d'auto-encodeur régularisé qui propose un choix spécifique de la pénalité de divergence.
"Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) andkernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE).CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE.","[0, 0, 1, 0, 0]",[],rkgwuiA9F7,Cramer-Wold AutoEncoder,"Cet article propose un auto-codeur de Cramer-Wold, qui utilise la distance de Cramer-Wold entre deux distributions sur la base du théorème de Cramer-Wold."
"We propose a rejection sampling scheme using the discriminator of a GAN toapproximately correct errors in the GAN generator distribution.We show thatunder quite strict assumptions, this will allow us to recover the data distributionexactly.We then examine where those strict assumptions break down and design apractical algorithm—called Discriminator Rejection Sampling (DRS)—that can beused on real data-sets.Finally, we demonstrate the efficacy of DRS on a mixture ofGaussians and on the state of the art SAGAN model.On ImageNet, we train animproved baseline that increases the best published Inception Score from 52.52 to62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79.We then useDRS to further improve on this baseline, improving the Inception Score to 76.08and the FID to 13.75.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1GkToR5tm,Discriminator Rejection Sampling,Nous utilisons un discriminateur GAN pour réaliser un schéma d'échantillonnage de rejet approximatif sur la sortie du générateur GAN.
"We propose a rejection sampling scheme using the discriminator of a GAN toapproximately correct errors in the GAN generator distribution.We show thatunder quite strict assumptions, this will allow us to recover the data distributionexactly.We then examine where those strict assumptions break down and design apractical algorithm—called Discriminator Rejection Sampling (DRS)—that can beused on real data-sets.Finally, we demonstrate the efficacy of DRS on a mixture ofGaussians and on the state of the art SAGAN model.On ImageNet, we train animproved baseline that increases the best published Inception Score from 52.52 to62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79.We then useDRS to further improve on this baseline, improving the Inception Score to 76.08and the FID to 13.75.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1GkToR5tm,Discriminator Rejection Sampling, Propose un algorithme d'échantillonnage par rejet pour l'échantillonnage du générateur GAN.
"We propose a rejection sampling scheme using the discriminator of a GAN toapproximately correct errors in the GAN generator distribution.We show thatunder quite strict assumptions, this will allow us to recover the data distributionexactly.We then examine where those strict assumptions break down and design apractical algorithm—called Discriminator Rejection Sampling (DRS)—that can beused on real data-sets.Finally, we demonstrate the efficacy of DRS on a mixture ofGaussians and on the state of the art SAGAN model.On ImageNet, we train animproved baseline that increases the best published Inception Score from 52.52 to62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79.We then useDRS to further improve on this baseline, improving the Inception Score to 76.08and the FID to 13.75.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1GkToR5tm,Discriminator Rejection Sampling,"Cet article a proposé un schéma d'échantillonnage de rejet post-traitement pour les GAN, nommé Discriminator Rejection Sampling, pour aider à filtrer les â€˜bonsâ€™ échantillons du générateur de GANsâ€™."
"The quality of the features used in visual recognition is of fundamental importance for the overall system.For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition.Visual features have recently been extracted from trained convolutional neural networks.Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process.In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.This methodology is evaluated on different datasets and compared with state-of-the-art approaches.","[0, 0, 1, 0, 0, 0]",[],SyGT_6yCZ,Simple Fast Convolutional Feature Learning,Une méthode simple et rapide pour extraire les caractéristiques visuelles des réseaux de neurones convolutifs
"The quality of the features used in visual recognition is of fundamental importance for the overall system.For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition.Visual features have recently been extracted from trained convolutional neural networks.Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process.In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.This methodology is evaluated on different datasets and compared with state-of-the-art approaches.","[0, 0, 1, 0, 0, 0]",[],SyGT_6yCZ,Simple Fast Convolutional Feature Learning,propose un moyen rapide d'apprendre des caractéristiques convolutionnelles qui peuvent ensuite être utilisées avec n'importe quel classificateur en utilisant un nombre réduit d'épocs d'entraînement et des délais spécifiques de la vitesse d'apprentissage.
"The quality of the features used in visual recognition is of fundamental importance for the overall system.For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition.Visual features have recently been extracted from trained convolutional neural networks.Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process.In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.This methodology is evaluated on different datasets and compared with state-of-the-art approaches.","[0, 0, 1, 0, 0, 0]",[],SyGT_6yCZ,Simple Fast Convolutional Feature Learning,Utilisez un schéma de décroissance du taux d'apprentissage qui est fixe par rapport au nombre d'époques utilisées dans la formation et extrayez la sortie de l'avant-dernière couche comme caractéristiques pour former un classificateur conventionnel.
"We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BJej72AqF7,A Max-Affine Spline Perspective of Recurrent Neural Networks,Nous fournissons de nouvelles perspectives et interprétations des RNN du point de vue des opérateurs spline max-affine.
"We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BJej72AqF7,A Max-Affine Spline Perspective of Recurrent Neural Networks,Réécrit les équations du RNN d'Elman en termes d'opérateurs spline max-affine.
"We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BJej72AqF7,A Max-Affine Spline Perspective of Recurrent Neural Networks,Fournir une nouvelle approche pour comprendre les RNN utilisant des opérateurs spline max-affine (MASO) en les réécrivant avec des activations affines et convexes par morceaux MASO.
"We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BJej72AqF7,A Max-Affine Spline Perspective of Recurrent Neural Networks,"Les auteurs s'appuient sur l'interpétation par l'opérateur spline max-affine d'une classe importante de réseaux profonds, en se concentrant sur les réseaux neuronaux récurrents qui utilisent le bruit dans l'état caché initial comme régularisation."
"Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. Transducing text to logical forms which can be operated on is a brittle and error-prone process. Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly. These issues are addressed by Neural Theorem Provers (NTPs) (Rocktäschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog’s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations. In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namelya) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, andb) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.The proposed method is able to extract rules and provide explanations—involving both textual patterns and KB relations—from large KBs and text corpora.We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzmzn0ctX,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,"Nous adaptons les Projecteurs de Théorèmes Neuraux à de grands ensembles de données, améliorons le processus d'apprentissage des règles, et l'étendons pour raisonner conjointement sur du texte et des bases de connaissances."
"Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. Transducing text to logical forms which can be operated on is a brittle and error-prone process. Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly. These issues are addressed by Neural Theorem Provers (NTPs) (Rocktäschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog’s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations. In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namelya) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, andb) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.The proposed method is able to extract rules and provide explanations—involving both textual patterns and KB relations—from large KBs and text corpora.We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzmzn0ctX,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,propose une extension du système de vérificateurs de théorèmes neuronaux qui répond aux principaux problèmes de ce modèle en réduisant la complexité temporelle et spatiale de celui-ci
"Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. Transducing text to logical forms which can be operated on is a brittle and error-prone process. Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly. These issues are addressed by Neural Theorem Provers (NTPs) (Rocktäschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog’s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations. In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namelya) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, andb) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.The proposed method is able to extract rules and provide explanations—involving both textual patterns and KB relations—from large KBs and text corpora.We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzmzn0ctX,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,met à l'échelle les PNT en utilisant la recherche approximative du plus proche voisin sur les faits et les règles pendant l'unification et suggère de paramétrer les prédicats en utilisant l'attention sur les prédicats connus
"Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. Transducing text to logical forms which can be operated on is a brittle and error-prone process. Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly. These issues are addressed by Neural Theorem Provers (NTPs) (Rocktäschel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog’s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations. In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namelya) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, andb) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.The proposed method is able to extract rules and provide explanations—involving both textual patterns and KB relations—from large KBs and text corpora.We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzmzn0ctX,Scalable Neural Theorem Proving on Knowledge Bases and Natural Language,améliore l'approche du vérificateur de théorèmes neuronal proposée précédemment en utilisant la recherche du plus proche voisin.
"We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data.Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyFaiGbCW,Generalization of Learning using Reservoir Computing,"Généralisation des relations apprises entre des paires d'images à l'aide d'un petit nombre de données d'entraînement à des types d'images inédites en utilisant un modèle de systèmes dynamiques explicables, le Reservoir Computing, et une technique d'apprentissage biologiquement plausible basée sur des analogies."
"We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data.Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyFaiGbCW,Generalization of Learning using Reservoir Computing,"Revendique les résultats de la ""combinaison de transformations"" dans le contexte de la RC en utilisant un réseau d'écho-état avec des acctivations tanh standard, à la différence que les poids récurrents ne sont pas formés."
"We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data.Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyFaiGbCW,Generalization of Learning using Reservoir Computing,Nouvelle méthode de classification de différentes distorsions des données MNIST
"We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data.Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyFaiGbCW,Generalization of Learning using Reservoir Computing,L'article utilise un réseau à état d'écho pour apprendre à classer les transformations d'images entre paires d'images dans l'une des cinq classes.
"We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset.Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.","[1, 0, 0, 0, 0]",[],H1xAH2RqK7,Generative Adversarial Models for Learning Private and Fair Representations,"Nous présentons Generative Adversarial Privacy and Fairness (GAPF), un cadre axé sur les données pour l'apprentissage de représentations privées et équitables avec des garanties certifiées de confidentialité et d'équité."
"We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset.Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.","[1, 0, 0, 0, 0]",[],H1xAH2RqK7,Generative Adversarial Models for Learning Private and Fair Representations,Cet article utilise un modèle GAN pour donner un aperçu des travaux liés à l'apprentissage par représentation privée/équitable (PRL).
"We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset.Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.","[1, 0, 0, 0, 0]",[],H1xAH2RqK7,Generative Adversarial Models for Learning Private and Fair Representations,Cet article présente une approche basée sur les adversaires pour des représentations privées et équitables par une distorsion apprise des données qui minimise la dépendance aux variables sensibles alors que le degré de distorsion est contraint.
"We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.GAPF leverages recent advances in adversarial learning to allow a data holder to learn ""universal"" representations that decouple a set of sensitive attributes from the rest of the dataset.Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries.","[1, 0, 0, 0, 0]",[],H1xAH2RqK7,Generative Adversarial Models for Learning Private and Fair Representations,Les auteurs décrivent un cadre permettant d'apprendre une représentation de parité démographique qui peut être utilisée pour entraîner certains classificateurs.
"Current machine learning algorithms can be easily fooled by adversarial examples.One possible solution path is to make models that use confidence thresholding to avoid making mistakes.Such models refuse to make a prediction when they are not confident of their answer.We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability.We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.Experiments show the attack attains good results in practice.We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,Nous présentons des mesures et une attaque optimale pour évaluer les modèles qui se défendent contre les exemples adverses en utilisant le seuillage de confiance.
"Current machine learning algorithms can be easily fooled by adversarial examples.One possible solution path is to make models that use confidence thresholding to avoid making mistakes.Such models refuse to make a prediction when they are not confident of their answer.We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability.We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.Experiments show the attack attains good results in practice.We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,"Cet article présente une famille d'attaques contre les algorithmes de seuillage de confiance, en se concentrant principalement sur les méthodologies d'évaluation."
"Current machine learning algorithms can be easily fooled by adversarial examples.One possible solution path is to make models that use confidence thresholding to avoid making mistakes.Such models refuse to make a prediction when they are not confident of their answer.We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability.We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.Experiments show the attack attains good results in practice.We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,Propose une méthode d'évaluation des modèles de défense par seuillage de confiance et une approche pour générer des exemples adverses en choisissant la mauvaise classe avec le plus de confiance lors d'attaques ciblées.
"Current machine learning algorithms can be easily fooled by adversarial examples.One possible solution path is to make models that use confidence thresholding to avoid making mistakes.Such models refuse to make a prediction when they are not confident of their answer.We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability.We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.Experiments show the attack attains good results in practice.We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1g0piA9tQ,Evaluation Methodology for Attacks Against Confidence Thresholding Models,L'article présente une méthodologie d'évaluation des attaques contre les méthodes de seuillage de confiance et propose un nouveau type d'attaque.
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.Each task provides only binary rewards indicating whether or not the goal is achieved.Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.Extensive experiments demonstrate that this method leads to faster converge and improved task performance.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Sklsm20ctX,Competitive experience replay,une nouvelle méthode d'apprentissage avec récompense clairsemée utilisant le ré-étiquetage de la récompense contradictoire
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.Each task provides only binary rewards indicating whether or not the goal is achieved.Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.Extensive experiments demonstrate that this method leads to faster converge and improved task performance.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Sklsm20ctX,Competitive experience replay,propose d'utiliser un cadre multi-agents compétitif pour encourager l'exploration et montre que CER + HER > HER ~ CER
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.Each task provides only binary rewards indicating whether or not the goal is achieved.Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.Extensive experiments demonstrate that this method leads to faster converge and improved task performance.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Sklsm20ctX,Competitive experience replay,Proposer une nouvelle méthode d'apprentissage à partir de récompenses éparses dans des contextes d'apprentissage par renforcement sans modèle et densifier les récompenses.
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.Each task provides only binary rewards indicating whether or not the goal is achieved.Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.Extensive experiments demonstrate that this method leads to faster converge and improved task performance.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Sklsm20ctX,Competitive experience replay,"Pour résoudre les problèmes de récompenses éparses et encourager l'exploration dans les algorithmes de RL, les auteurs proposent une stratégie de ré-étiquetage appelée Competitive Experience Reply (CER)."
"This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.The model is formulated as a conditional generative model with two levels of hierarchical latent variables.The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.This amounts to using a Gaussian mixture model (GMM) for the latent distribution.Extensive evaluation demonstrates its ability to control the aforementioned attributes.In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.","[0, 0, 0, 1, 0, 0, 0]",[],rygkk305YQ,Hierarchical Generative Modeling for Controllable Speech Synthesis,"La construction d'un modèle TTS avec des VAE à mélange gaussien permet un contrôle fin du style d'élocution, des conditions de bruit, etc."
"This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.The model is formulated as a conditional generative model with two levels of hierarchical latent variables.The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.This amounts to using a Gaussian mixture model (GMM) for the latent distribution.Extensive evaluation demonstrates its ability to control the aforementioned attributes.In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.","[0, 0, 0, 1, 0, 0, 0]",[],rygkk305YQ,Hierarchical Generative Modeling for Controllable Speech Synthesis,Décrit le modèle GAN conditionné pour générer des spectres Mel conditionnés par le locuteur en augmentant l'espace z correspondant à l'identification.
"This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.The model is formulated as a conditional generative model with two levels of hierarchical latent variables.The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.This amounts to using a Gaussian mixture model (GMM) for the latent distribution.Extensive evaluation demonstrates its ability to control the aforementioned attributes.In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.","[0, 0, 0, 1, 0, 0, 0]",[],rygkk305YQ,Hierarchical Generative Modeling for Controllable Speech Synthesis,"Cet article propose un modèle de variable latente à deux couches pour obtenir une représentation latente démêlée, facilitant ainsi le contrôle fin de divers attributs."
"This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.The model is formulated as a conditional generative model with two levels of hierarchical latent variables.The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.This amounts to using a Gaussian mixture model (GMM) for the latent distribution.Extensive evaluation demonstrates its ability to control the aforementioned attributes.In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.","[0, 0, 0, 1, 0, 0, 0]",[],rygkk305YQ,Hierarchical Generative Modeling for Controllable Speech Synthesis,"Cet article propose un modèle qui peut contrôler les attributs non annotés tels que le style de parole, l'accent, le bruit de fond, etc."
"Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.We identify a fundamental problem due to soft attention in these models as a cause.To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.","[1, 0, 0, 0, 0]",[],B12Js_yRb,Learning to Count Objects in Natural Images for Visual Question Answering,Permettre aux modèles de réponse aux questions visuelles de compter en traitant les propositions d'objets qui se chevauchent.
"Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.We identify a fundamental problem due to soft attention in these models as a cause.To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.","[1, 0, 0, 0, 0]",[],B12Js_yRb,Learning to Count Objects in Natural Images for Visual Question Answering,Cet article propose une architecture de réseau conçue à la main sur un graphe de propositions d'objets pour effectuer une suppression douce non maximale afin d'obtenir le nombre d'objets.
"Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.We identify a fundamental problem due to soft attention in these models as a cause.To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.","[1, 0, 0, 0, 0]",[],B12Js_yRb,Learning to Count Objects in Natural Images for Visual Question Answering,Se concentre sur un problème de comptage dans la réponse à une question visuelle en utilisant le mécanisme d'attention et propose un composant de comptage différentiable qui compte explicitement le nombre d'objets.
"Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.We identify a fundamental problem due to soft attention in these models as a cause.To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%.","[1, 0, 0, 0, 0]",[],B12Js_yRb,Learning to Count Objects in Natural Images for Visual Question Answering,"Cet article aborde le problème du comptage des objets dans les réponses aux questions visuelles, il propose plusieurs heuristiques pour trouver le comptage correct."
"We propose a simple and robust training-free approach for building sentence representations.Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.We model the semantic meaning of a word in a sentence based on two aspects.One is its relatedness to the word vector subspace already spanned by its contextual words.The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.This approach requires zero training and zero parameters, along with efficient inference performance.We evaluate our approach on 11 downstream NLP tasks.Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rJedbn0ctQ,Zero-training Sentence Embedding via Orthogonal Basis,Une approche simple et sans apprentissage pour l'incorporation de phrases avec des performances compétitives par rapport aux modèles sophistiqués nécessitant une grande quantité de données d'apprentissage ou un temps d'apprentissage prolongé.
"We propose a simple and robust training-free approach for building sentence representations.Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.We model the semantic meaning of a word in a sentence based on two aspects.One is its relatedness to the word vector subspace already spanned by its contextual words.The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.This approach requires zero training and zero parameters, along with efficient inference performance.We evaluate our approach on 11 downstream NLP tasks.Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rJedbn0ctQ,Zero-training Sentence Embedding via Orthogonal Basis,Présentation d'une nouvelle méthode sans formation pour générer l'intégration des phrases avec une analyse systématique.
"We propose a simple and robust training-free approach for building sentence representations.Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.We model the semantic meaning of a word in a sentence based on two aspects.One is its relatedness to the word vector subspace already spanned by its contextual words.The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.This approach requires zero training and zero parameters, along with efficient inference performance.We evaluate our approach on 11 downstream NLP tasks.Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rJedbn0ctQ,Zero-training Sentence Embedding via Orthogonal Basis,"propose une nouvelle méthode basée sur la géométrie pour l'incorporation de phrases à partir de vecteurs d'incorporation de mots en quantifiant la nouveauté, l'importance et l'unicité de corpus de chaque mot"
"We propose a simple and robust training-free approach for building sentence representations.Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.We model the semantic meaning of a word in a sentence based on two aspects.One is its relatedness to the word vector subspace already spanned by its contextual words.The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.This approach requires zero training and zero parameters, along with efficient inference performance.We evaluate our approach on 11 downstream NLP tasks.Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rJedbn0ctQ,Zero-training Sentence Embedding via Orthogonal Basis,Cet article explore l'intégration de phrases basée sur la décomposition orthogonale de l'espace couvert par les intégrations de mots.
"In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJcSzz-CZ,Meta-Learning for Semi-Supervised Few-Shot Classification,Nous proposons de nouvelles extensions des réseaux prototypiques qui sont augmentés de la capacité d'utiliser des exemples non étiquetés lors de la production de prototypes.
"In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJcSzz-CZ,Meta-Learning for Semi-Supervised Few-Shot Classification,Cet article est une extension d'un réseau prototypique qui envisage d'utiliser les exemples non étiquetés disponibles pour aider à former chaque épisode.
"In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJcSzz-CZ,Meta-Learning for Semi-Supervised Few-Shot Classification,Étudie le problème de la classification semi-supervisée de quelques images en étendant les réseaux prototypiques à l'apprentissage semi-supervisé avec des exemples de classes de distracteurs.
"In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJcSzz-CZ,Meta-Learning for Semi-Supervised Few-Shot Classification,"Étend le réseau prototypique au cadre semi-supervisé en mettant à jour les prototypes à l'aide de pseudo-étiquettes attribuées, en traitant les distracteurs et en évaluant les échantillons en fonction de la distance aux prototypes originaux."
"We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space.We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations.We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean.We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.","[1, 0, 0, 0, 0]",[],SyMhLo0qKQ,Distribution-Interpolation Trade off in Generative Models,Nous prouvons théoriquement que les interpolations linéaires ne sont pas adaptées à l'analyse des modèles génératifs implicites entraînés. 
"We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space.We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations.We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean.We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.","[1, 0, 0, 0, 0]",[],SyMhLo0qKQ,Distribution-Interpolation Trade off in Generative Models,"étudie le problème du moment où l'interpolant linéaire entre deux variables aléatoires suit la même distribution, lié à la distribution préalable d'un modèle génératif implicite"
"We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space.We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations.We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean.We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions.","[1, 0, 0, 0, 0]",[],SyMhLo0qKQ,Distribution-Interpolation Trade off in Generative Models,Ce travail demande comment interpoler dans l'espace latent étant donné un modèle de variable latente.
"Deep neural networks (DNN) have shown promising performance in computer vision.In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations.The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rk1FQA0pW,End-to-End Abnormality Detection in Medical Imaging,Détection d'un nodule pulmonaire à partir de données de projection plutôt que d'images.
"Deep neural networks (DNN) have shown promising performance in computer vision.In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations.The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rk1FQA0pW,End-to-End Abnormality Detection in Medical Imaging,Les DNN sont utilisés pour la détection des nodules pulmonaires par patchs dans les données de projection CT.
"Deep neural networks (DNN) have shown promising performance in computer vision.In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations.The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rk1FQA0pW,End-to-End Abnormality Detection in Medical Imaging,Modélisation conjointe de la reconstruction de la tomographie assistée par ordinateur et de la détection des lésions pulmonaires par l'apprentissage du mappage du sinogramme brut aux sorties de détection de bout en bout
"Deep neural networks (DNN) have shown promising performance in computer vision.In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations.The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rk1FQA0pW,End-to-End Abnormality Detection in Medical Imaging,Présente une formation de bout en bout d'une architecture CNN qui combine le traitement du signal d'une image CT et l'analyse d'image.
"Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to “learn to navigate” and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments.Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal.In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments.Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal.However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BkiIkBJ0b,Do Deep Reinforcement Learning Algorithms really Learn to Navigate?,Nous évaluons quantitativement et qualitativement les méthodes de navigation basées sur l'apprentissage par renforcement profond dans diverses conditions pour répondre à la question de savoir dans quelle mesure elles sont proches de remplacer les planificateurs de chemin et les algorithmes de cartographie classiques.
"Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to “learn to navigate” and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments.Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal.In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments.Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal.However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BkiIkBJ0b,Do Deep Reinforcement Learning Algorithms really Learn to Navigate?,Évaluer un modèle RL profond sur des labyrinthes d'entraînement en mesurant la latence répétée vers le but et la comparaison avec le chemin le plus court.
"In many robotic applications, it is crucial to maintain a belief about the state of a system, like the location of a robot or the pose of an object.These state estimates serve as input for planning and decision making and provide feedback during task execution. Recursive Bayesian Filtering algorithms address the state estimation problem,but they require a model of the process dynamics and the sensory observations as well as noise estimates that quantify the accuracy of these models. Recently, multiple works have demonstrated that the process and sensor models can be learned by end-to-end training through differentiable versions of Recursive Filtering methods.However, even if the predictive models are known, finding suitable noise models remains challenging.Therefore, many practical applications rely on very simplistic noise models. Our hypothesis is that end-to-end training through differentiable Bayesian Filters enables us to learn more complex heteroscedastic noise models forthe system dynamics.We evaluate learning such models with different types of filtering algorithms and on two different robotic tasks.Our experiments show that especially for sampling-based filters like the Particle Filter, learning heteroscedastic noise models can drastically improve the tracking performance in comparison to using constant noise models.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BylBns0qtX,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,Nous évaluons l'apprentissage de modèles de bruit hétéroscédastiques à l'aide de différents filtres de Bayes différentiables.
"In many robotic applications, it is crucial to maintain a belief about the state of a system, like the location of a robot or the pose of an object.These state estimates serve as input for planning and decision making and provide feedback during task execution. Recursive Bayesian Filtering algorithms address the state estimation problem,but they require a model of the process dynamics and the sensory observations as well as noise estimates that quantify the accuracy of these models. Recently, multiple works have demonstrated that the process and sensor models can be learned by end-to-end training through differentiable versions of Recursive Filtering methods.However, even if the predictive models are known, finding suitable noise models remains challenging.Therefore, many practical applications rely on very simplistic noise models. Our hypothesis is that end-to-end training through differentiable Bayesian Filters enables us to learn more complex heteroscedastic noise models forthe system dynamics.We evaluate learning such models with different types of filtering algorithms and on two different robotic tasks.Our experiments show that especially for sampling-based filters like the Particle Filter, learning heteroscedastic noise models can drastically improve the tracking performance in comparison to using constant noise models.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BylBns0qtX,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,propose d'apprendre des modèles de bruit hétéroscédastiques à partir de données en optimisant la probabilité de prédiction de bout en bout par le biais de filtres bayésiens différentiables et de deux versions différentes du filtre de Kalman non centré.
"In many robotic applications, it is crucial to maintain a belief about the state of a system, like the location of a robot or the pose of an object.These state estimates serve as input for planning and decision making and provide feedback during task execution. Recursive Bayesian Filtering algorithms address the state estimation problem,but they require a model of the process dynamics and the sensory observations as well as noise estimates that quantify the accuracy of these models. Recently, multiple works have demonstrated that the process and sensor models can be learned by end-to-end training through differentiable versions of Recursive Filtering methods.However, even if the predictive models are known, finding suitable noise models remains challenging.Therefore, many practical applications rely on very simplistic noise models. Our hypothesis is that end-to-end training through differentiable Bayesian Filters enables us to learn more complex heteroscedastic noise models forthe system dynamics.We evaluate learning such models with different types of filtering algorithms and on two different robotic tasks.Our experiments show that especially for sampling-based filters like the Particle Filter, learning heteroscedastic noise models can drastically improve the tracking performance in comparison to using constant noise models.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BylBns0qtX,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,Revoit les filtres de Bayes et évalue l'avantage d'entraîner les modèles d'observation et de bruit de processus tout en gardant tous les autres modèles fixes.
"In many robotic applications, it is crucial to maintain a belief about the state of a system, like the location of a robot or the pose of an object.These state estimates serve as input for planning and decision making and provide feedback during task execution. Recursive Bayesian Filtering algorithms address the state estimation problem,but they require a model of the process dynamics and the sensory observations as well as noise estimates that quantify the accuracy of these models. Recently, multiple works have demonstrated that the process and sensor models can be learned by end-to-end training through differentiable versions of Recursive Filtering methods.However, even if the predictive models are known, finding suitable noise models remains challenging.Therefore, many practical applications rely on very simplistic noise models. Our hypothesis is that end-to-end training through differentiable Bayesian Filters enables us to learn more complex heteroscedastic noise models forthe system dynamics.We evaluate learning such models with different types of filtering algorithms and on two different robotic tasks.Our experiments show that especially for sampling-based filters like the Particle Filter, learning heteroscedastic noise models can drastically improve the tracking performance in comparison to using constant noise models.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],BylBns0qtX,On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters,Cet article présente une méthode pour apprendre et utiliser le bruit dépendant de l'état et de l'observation dans les algorithmes traditionnels de filtrage bayésien. L'approche consiste à construire un modèle de réseau neuronal qui prend en entrée les données d'observation brutes et produit une représentation compacte et une covariance diagonale associée.
"Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning.These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data.However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning.In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes.DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections.These connections are added based on a node's relationship to its ancestors and descendants.A weighting scheme is further used to weigh their contribution depending on the distance to the node.Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkgs0oAqFQ,Rethinking Knowledge Graph Propagation for Zero-Shot Learning,Nous repensons la manière dont l'information peut être exploitée plus efficacement dans le graphe de connaissances afin d'améliorer les performances dans la tâche d'apprentissage à zéro coup et proposons un module de propagation de graphe dense (DGP) à cette fin.
"Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning.These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data.However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning.In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes.DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections.These connections are added based on a node's relationship to its ancestors and descendants.A weighting scheme is further used to weigh their contribution depending on the distance to the node.Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkgs0oAqFQ,Rethinking Knowledge Graph Propagation for Zero-Shot Learning,"Les auteurs proposent une solution au problème du lissage excessif dans les réseaux de convulsions graphiques en permettant une propagation dense entre tous les nœuds apparentés, pondérée par la distance mutuelle."
"Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning.These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data.However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning.In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes.DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections.These connections are added based on a node's relationship to its ancestors and descendants.A weighting scheme is further used to weigh their contribution depending on the distance to the node.Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkgs0oAqFQ,Rethinking Knowledge Graph Propagation for Zero-Shot Learning,propose un nouveau réseau de neurones convolutifs à graphes pour résoudre le problème de la classification à zéro coup en utilisant les structures relationnelles entre les classes comme entrée des réseaux convolutifs à graphes pour apprendre des classificateurs de classes non vues.
"In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.","[0, 1, 0, 0, 0]",[],H1xpe2C5Km,Trace-back along capsules and its application on semantic segmentation  		,"Une segmentation sémantique basée sur les capsules, dans laquelle les probabilités des étiquettes de classe sont retracées dans le pipeline des capsules. "
"In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.","[0, 1, 0, 0, 0]",[],H1xpe2C5Km,Trace-back along capsules and its application on semantic segmentation  		,Les auteurs présentent un mécanisme de traçage pour associer le niveau le plus bas des Capsules à leurs classes respectives.
"In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.","[0, 1, 0, 0, 0]",[],H1xpe2C5Km,Trace-back along capsules and its application on semantic segmentation  		,propose une couche de traçage pour les réseaux de capsules afin de réaliser une segmentation sémantique et utilise explicitement la relation partie-entière dans les couches de capsules
"In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.","[0, 1, 0, 0, 0]",[],H1xpe2C5Km,Trace-back along capsules and its application on semantic segmentation  		,Propose une méthode de traçage basée sur le concept CapsNet de Sabour pour effectuer une segmentation sémantique en parallèle à la classification.
"Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.","[0, 0, 0, 0, 0, 1, 0]",[],SkMON20ctX,On the Trajectory of Stochastic Gradient Descent in the Information Plane,"Nous considérons le SGD comme une trajectoire dans l'espace des mesures de probabilité, nous montrons son lien avec les processus de Markov, nous proposons un modèle de Markov simple de l'apprentissage du SGD, et nous le comparons expérimentalement au SGD utilisant des quantités théoriques d'information. "
"Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.","[0, 0, 0, 0, 0, 1, 0]",[],SkMON20ctX,On the Trajectory of Stochastic Gradient Descent in the Information Plane,construit une chaîne de Markov qui suit un chemin court dans la métrique TV sur P et montre que les trajectoires de SGD et \alpha-SMLC ont une entropie conditionnelle similaire
"Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.","[0, 0, 0, 0, 0, 1, 0]",[],SkMON20ctX,On the Trajectory of Stochastic Gradient Descent in the Information Plane,Etude de la trajectoire de H(\hat{y}) par rapport à H(\hat{y}|y) sur le plan d'information pour les méthodes de descente de gradient stochastique pour l'entraînement des réseaux de neurones
"Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error.","[0, 0, 0, 0, 0, 1, 0]",[],SkMON20ctX,On the Trajectory of Stochastic Gradient Descent in the Information Plane,"Décrit le SGD du point de vue de la distribution p(y',y) où y est le vrai label de classe (éventuellement corrompu) et y' une prédiction du modèle."
"Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling.However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition.This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes.Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.","[0, 0, 1, 0, 0]",[],HkeoOo09YX,Meta-Learning For Stochastic Gradient MCMC,Cet article propose une méthode pour automatiser la conception de la proposition MCMC à gradient stochastique en utilisant une approche de méta apprentissage. 
"Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling.However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition.This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes.Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.","[0, 0, 1, 0, 0]",[],HkeoOo09YX,Meta-Learning For Stochastic Gradient MCMC,Présente une approche de méta-apprentissage pour concevoir automatiquement l'échantillonneur MCMC basé sur la dynamique hamiltonienne afin qu'il se mélange plus rapidement sur des problèmes similaires aux problèmes d'entraînement.
"Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling.However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition.This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes.Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures.","[0, 0, 1, 0, 0]",[],HkeoOo09YX,Meta-Learning For Stochastic Gradient MCMC,Paramétrage des matrices de diffusion et de curl par des réseaux neuronaux et méta-apprentissage et optimisation d'un algorithme sg-mcmc. 
"We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature.Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions.We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function.We then provide a set of systematic experiments that explore its hyper-parameters.We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned.We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.","[0, 1, 0, 0, 0, 0]",[],ryzm6BATZ,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,Les techniques d'évaluation de la qualité des images améliorent la formation et l'évaluation des réseaux adversariaux génératifs basés sur l'énergie.
"We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature.Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions.We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function.We then provide a set of systematic experiments that explore its hyper-parameters.We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned.We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.","[0, 1, 0, 0, 0, 0]",[],ryzm6BATZ,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,propose une formulation basée sur l'énergie pour le modeal BEGAN et le modifie pour inclure un terme basé sur l'évaluation de la qualité de l'image
"We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature.Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions.We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function.We then provide a set of systematic experiments that explore its hyper-parameters.We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned.We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.","[0, 1, 0, 0, 0, 0]",[],ryzm6BATZ,Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks,"Propose de nouvelles fonctions d'énergie dans le cadre de BEGAN (cadre GAN à équilibre frontal), notamment le score l_1, le score de similarité de gradient de magnitude et le score de chrominance."
"Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.Its performance depends crucially on a damping coefficient.Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives.Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.","[0, 0, 0, 1, 0, 0, 0]",[],Syxt5oC5YQ,Aggregated Momentum: Stability Through Passive Damping,"Nous présentons une variante simple de l'optimisation du momentum qui est capable de surpasser le momentum classique, Nesterov et Adam sur des tâches d'apprentissage profond avec un réglage minimal des hyperparamètres."
"Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.Its performance depends crucially on a damping coefficient.Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives.Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.","[0, 0, 0, 1, 0, 0, 0]",[],Syxt5oC5YQ,Aggregated Momentum: Stability Through Passive Damping,"Introduit une variante de la quantité de mouvement qui agrège plusieurs vitesses avec des coefficients d'amortissement différents, ce qui réduit considérablement l'oscillation."
"Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.Its performance depends crucially on a damping coefficient.Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives.Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.","[0, 0, 0, 1, 0, 0, 0]",[],Syxt5oC5YQ,Aggregated Momentum: Stability Through Passive Damping,Proposition d'une méthode de momentum agrégé pour l'optimisation basée sur le gradient en utilisant plusieurs vecteurs de vitesse avec différents facteurs d'amortissement au lieu d'un seul vecteur de vitesse pour améliorer la stabilité.
"Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.Its performance depends crucially on a damping coefficient.Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives.Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning.","[0, 0, 0, 1, 0, 0, 0]",[],Syxt5oC5YQ,Aggregated Momentum: Stability Through Passive Damping,Les auteurs combinent plusieurs étapes de mise à jour pour obtenir un momentum agrégé et démontrent que cette méthode est plus stable que les autres méthodes de momentum.
"Recurrent Neural Networks architectures excel at processing sequences bymodelling dependencies over different timescales.The recently introducedRecurrent Weighted Average (RWA) unit captures long term dependenciesfar better than an LSTM on several challenging tasks.The RWA achievesthis by applying attention to each input and computing a weighted averageover the full history of its computations.Unfortunately, the RWA cannotchange the attention it has assigned to previous timesteps, and so struggleswith carrying out consecutive tasks or tasks with changing requirements.We present the Recurrent Discounted Attention (RDA) unit that builds onthe RWA by additionally allowing the discounting of the past.We empirically compare our model to RWA, LSTM and GRU units onseveral challenging tasks.On tasks with a single output the RWA, RDA andGRU units learn much quicker than the LSTM and with better performance.On the multiple sequence copy task our RDA unit learns the task threetimes as quickly as the LSTM or GRU units while the RWA fails to learn atall.On the Wikipedia character prediction task the LSTM performs bestbut it followed closely by our RDA unit.Overall our RDA unit performswell and is sample efficient on a large variety of sequence tasks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJ78bJZCZ,Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,Nous introduisons l'unité actualisée récurrente qui applique l'attention à une séquence de n'importe quelle longueur en temps linéaire.
"Recurrent Neural Networks architectures excel at processing sequences bymodelling dependencies over different timescales.The recently introducedRecurrent Weighted Average (RWA) unit captures long term dependenciesfar better than an LSTM on several challenging tasks.The RWA achievesthis by applying attention to each input and computing a weighted averageover the full history of its computations.Unfortunately, the RWA cannotchange the attention it has assigned to previous timesteps, and so struggleswith carrying out consecutive tasks or tasks with changing requirements.We present the Recurrent Discounted Attention (RDA) unit that builds onthe RWA by additionally allowing the discounting of the past.We empirically compare our model to RWA, LSTM and GRU units onseveral challenging tasks.On tasks with a single output the RWA, RDA andGRU units learn much quicker than the LSTM and with better performance.On the multiple sequence copy task our RDA unit learns the task threetimes as quickly as the LSTM or GRU units while the RWA fails to learn atall.On the Wikipedia character prediction task the LSTM performs bestbut it followed closely by our RDA unit.Overall our RDA unit performswell and is sample efficient on a large variety of sequence tasks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJ78bJZCZ,Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,"Cet article propose l'attention récurrente actualisée (RDA), une extension de la moyenne pondérée récurrente (RWA) en ajoutant un facteur d'actualisation."
"Recurrent Neural Networks architectures excel at processing sequences bymodelling dependencies over different timescales.The recently introducedRecurrent Weighted Average (RWA) unit captures long term dependenciesfar better than an LSTM on several challenging tasks.The RWA achievesthis by applying attention to each input and computing a weighted averageover the full history of its computations.Unfortunately, the RWA cannotchange the attention it has assigned to previous timesteps, and so struggleswith carrying out consecutive tasks or tasks with changing requirements.We present the Recurrent Discounted Attention (RDA) unit that builds onthe RWA by additionally allowing the discounting of the past.We empirically compare our model to RWA, LSTM and GRU units onseveral challenging tasks.On tasks with a single output the RWA, RDA andGRU units learn much quicker than the LSTM and with better performance.On the multiple sequence copy task our RDA unit learns the task threetimes as quickly as the LSTM or GRU units while the RWA fails to learn atall.On the Wikipedia character prediction task the LSTM performs bestbut it followed closely by our RDA unit.Overall our RDA unit performswell and is sample efficient on a large variety of sequence tasks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJ78bJZCZ,Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit,Étend la moyenne de poids récurrente pour surmonter les limites de la méthode originale tout en conservant ses avantages et propose la méthode d'utilisation des réseaux Elman comme RNN de base.
"Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging.In this paper, we introduce variance layers, a different kind of stochastic layers.Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance.It means that each object is represented by a zero-mean distribution in the space of the activations.We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks.We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors.We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.","[0, 0, 1, 0, 0, 0, 0]",[],B1GAUs0cKQ,Variance Networks: When Expectation Does Not Meet Your Expectations,"Il est possible d'apprendre une distribution gaussienne centrée sur zéro sur les poids d'un réseau neuronal en apprenant uniquement les variances, et cela fonctionne étonnamment bien."
"Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging.In this paper, we introduce variance layers, a different kind of stochastic layers.Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance.It means that each object is represented by a zero-mean distribution in the space of the activations.We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks.We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors.We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.","[0, 0, 1, 0, 0, 0, 0]",[],B1GAUs0cKQ,Variance Networks: When Expectation Does Not Meet Your Expectations,"Cet article étudie les effets de la moyenne du postérieur variationnel et propose une couche de variance, qui utilise uniquement la variance pour stocker l'information."
"Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging.In this paper, we introduce variance layers, a different kind of stochastic layers.Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance.It means that each object is represented by a zero-mean distribution in the space of the activations.We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks.We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors.We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned.","[0, 0, 1, 0, 0, 0, 0]",[],B1GAUs0cKQ,Variance Networks: When Expectation Does Not Meet Your Expectations,étudie les réseaux neuronaux de variance qui se rapprochent du postérieur des réseaux neuronaux bayésiens avec des distributions gaussiennes à moyenne nulle.
"Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data.At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks.In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work.At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective.Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI.In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.","[0, 0, 0, 0, 1, 0]",[],SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,"Nous créons un réseau de réseaux de convolution graphique, en alimentant chacun une puissance différente de la matrice d'adjacence, en combinant toute leur représentation dans un sous-réseau de classification, ce qui permet d'atteindre l'état de l'art en matière de classification semi-supervisée des nœuds."
"Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data.At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks.In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work.At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective.Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI.In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.","[0, 0, 0, 0, 1, 0]",[],SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,Propose un nouveau réseau de GCN avec deux approches : une couche entièrement connectée au-dessus des caractéristiques empilées et un mécanisme d'attention qui utilise un poids scalaire par GCN.
"Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data.At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks.In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work.At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective.Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI.In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.","[0, 0, 0, 0, 1, 0]",[],SkaPsfZ0W,Network of Graph Convolutional Networks Trained on Random Walks,présente un réseau de réseaux convolutifs de graphes qui utilise des statistiques de marche aléatoire pour extraire des informations des voisins proches et éloignés dans le graphe.
Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy.However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy.In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network.Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches.Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.,"[0, 0, 0, 1, 0, 0]",[],SJtChcgAW,Cheap DNN Pruning with Performance Guarantees ,Un algorithme d'élagage rapide pour les couches DNN entièrement connectées avec une analyse théorique de la dégradation de l'erreur de généralisation.
Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy.However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy.In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network.Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches.Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.,"[0, 0, 0, 1, 0, 0]",[],SJtChcgAW,Cheap DNN Pruning with Performance Guarantees ,Présente un algorithme d'élagage bon marché pour les couches denses des DNN.
Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy.However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy.In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network.Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches.Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding.,"[0, 0, 0, 1, 0, 0]",[],SJtChcgAW,Cheap DNN Pruning with Performance Guarantees ,Propose une solution au problème de l'élagage des DNN en posant la fonction objectif de Net-trim comme une fonction Différence de convexité (DC).
"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years.It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing.In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage.Our model is simple but extremely effective in terms of video sequence labeling.The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","[0, 0, 0, 0, 1]",[],r1nzLmWAb,Video Action Segmentation with Hybrid Temporal Networks,Nous proposons un nouveau réseau temporel hybride qui atteint des performances de pointe pour la segmentation d'actions vidéo sur trois ensembles de données publiques.
"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years.It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing.In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage.Our model is simple but extremely effective in terms of video sequence labeling.The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","[0, 0, 0, 0, 1]",[],r1nzLmWAb,Video Action Segmentation with Hybrid Temporal Networks,"Aborde le problème de la segmentation d'actions dans des vidéos longues, jusqu'à 10 minutes, en utilisant une architecture d'encodeur-décodeur convolutif temporel."
"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years.It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing.In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage.Our model is simple but extremely effective in terms of video sequence labeling.The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","[0, 0, 0, 0, 1]",[],r1nzLmWAb,Video Action Segmentation with Hybrid Temporal Networks,Propose une combinaison de réseaux convolutifs et récurrents temporels pour la segmentation d'actions vidéo.
"Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative.It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher.Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved.Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications.Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance.In this work, we argue that feature is the most important knowledge from teacher.It is sufficient for student to just learn good features regardless of the target task.From this discovery, we further present an efficient learning strategy to mimic features stage by stage.Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,Cet article propose de transférer les connaissances d'un modèle profond à un modèle peu profond en imitant les caractéristiques étape par étape.
"Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative.It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher.Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved.Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications.Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance.In this work, we argue that feature is the most important knowledge from teacher.It is sufficient for student to just learn good features regardless of the target task.From this discovery, we further present an efficient learning strategy to mimic features stage by stage.Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,Explique une méthode de transfert de connaissances étape par étape en utilisant différentes structures de réseaux.
"Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative.It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher.Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved.Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications.Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance.In this work, we argue that feature is the most important knowledge from teacher.It is sufficient for student to just learn good features regardless of the target task.From this discovery, we further present an efficient learning strategy to mimic features stage by stage.Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJegl2C9K7,Feature Matters: A Stage-by-Stage Approach for Task Independent Knowledge Transfer,Cet article propose de diviser un réseau en plusieurs parties et de distiller chaque partie séquentiellement afin d'améliorer les performances de la distillation dans les réseaux profonds d'enseignants.
"We augment adversarial training (AT) with worst case adversarial training(WCAT) which improves adversarial robustness by 11% over the current state-of-the-art result in the `2-norm on CIFAR-10.We interpret adversarial training asTotal Variation Regularization, which is a fundamental tool in mathematical im-age processing, and WCAT as Lipschitz regularization, which appears in ImageInpainting.We obtain verifiable worst and average case robustness guarantees,based on the expected and maximum values of the norm of the gradient of theloss.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HkxAisC9FQ,Improved robustness to adversarial examples using Lipschitz regularization of the loss,"Des améliorations de la robustesse des adversaires, ainsi que des garanties de robustesse démontrables, sont obtenues en augmentant l'entraînement des adversaires avec une régularisation Lipschitz traçable."
"We augment adversarial training (AT) with worst case adversarial training(WCAT) which improves adversarial robustness by 11% over the current state-of-the-art result in the `2-norm on CIFAR-10.We interpret adversarial training asTotal Variation Regularization, which is a fundamental tool in mathematical im-age processing, and WCAT as Lipschitz regularization, which appears in ImageInpainting.We obtain verifiable worst and average case robustness guarantees,based on the expected and maximum values of the norm of the gradient of theloss.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HkxAisC9FQ,Improved robustness to adversarial examples using Lipschitz regularization of the loss,Étude de l'augmentation de la perte d'apprentissage par un terme supplémentaire de régularisation du gradient afin d'améliorer la robustesse des modèles face à des exemples contradictoires.
"We augment adversarial training (AT) with worst case adversarial training(WCAT) which improves adversarial robustness by 11% over the current state-of-the-art result in the `2-norm on CIFAR-10.We interpret adversarial training asTotal Variation Regularization, which is a fundamental tool in mathematical im-age processing, and WCAT as Lipschitz regularization, which appears in ImageInpainting.We obtain verifiable worst and average case robustness guarantees,based on the expected and maximum values of the norm of the gradient of theloss.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HkxAisC9FQ,Improved robustness to adversarial examples using Lipschitz regularization of the loss,Utilise une astuce pour simplifier la perte de l'adversaire par une perte dans laquelle la perturbation de l'adversaire apparaît sous une forme fermée.
"The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{\textit{passage, question}\} pair and select one of the $n$ given options.The current state of the art model for this task first computes a query-aware representation for the passage and then \textit{selects} the option which has the maximum similarity with this representation.However, when humans perform this task they do not just focus on option selection but use a combination of \textit{elimination} and \textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).This process could be repeated multiple times till the reader is finally ready to select the correct option.We propose \textit{ElimiNet}, a neural network based model which tries to mimic this process.Specifically, it has gates which decide whether an option can be eliminated given the \{\textit{document, question}\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.Further we show that taking an ensemble of our \textit{elimination-selection} based method with a \textit{selection} based method gives us an improvement of 7\% (relative) over the best reported performance on this dataset.    ","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1bgpzZAZ,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,Un modèle combinant élimination et sélection pour répondre à des questions à choix multiples
"The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{\textit{passage, question}\} pair and select one of the $n$ given options.The current state of the art model for this task first computes a query-aware representation for the passage and then \textit{selects} the option which has the maximum similarity with this representation.However, when humans perform this task they do not just focus on option selection but use a combination of \textit{elimination} and \textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).This process could be repeated multiple times till the reader is finally ready to select the correct option.We propose \textit{ElimiNet}, a neural network based model which tries to mimic this process.Specifically, it has gates which decide whether an option can be eliminated given the \{\textit{document, question}\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.Further we show that taking an ensemble of our \textit{elimination-selection} based method with a \textit{selection} based method gives us an improvement of 7\% (relative) over the best reported performance on this dataset.    ","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1bgpzZAZ,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,Donne une élaboration sur le lecteur d'attention graduée ajoutant des portes basées sur l'élimination des réponses dans la compréhension de lecture à choix multiple.
"The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{\textit{passage, question}\} pair and select one of the $n$ given options.The current state of the art model for this task first computes a query-aware representation for the passage and then \textit{selects} the option which has the maximum similarity with this representation.However, when humans perform this task they do not just focus on option selection but use a combination of \textit{elimination} and \textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).This process could be repeated multiple times till the reader is finally ready to select the correct option.We propose \textit{ElimiNet}, a neural network based model which tries to mimic this process.Specifically, it has gates which decide whether an option can be eliminated given the \{\textit{document, question}\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.Further we show that taking an ensemble of our \textit{elimination-selection} based method with a \textit{selection} based method gives us an improvement of 7\% (relative) over the best reported performance on this dataset.    ","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1bgpzZAZ,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,"Cet article propose l'utilisation d'une porte d'élimination dans les architectures de modèles pour les tâches de compréhension de la lecture, mais n'obtient pas de résultats à la pointe de la technologie."
"The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{\textit{passage, question}\} pair and select one of the $n$ given options.The current state of the art model for this task first computes a query-aware representation for the passage and then \textit{selects} the option which has the maximum similarity with this representation.However, when humans perform this task they do not just focus on option selection but use a combination of \textit{elimination} and \textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).This process could be repeated multiple times till the reader is finally ready to select the correct option.We propose \textit{ElimiNet}, a neural network based model which tries to mimic this process.Specifically, it has gates which decide whether an option can be eliminated given the \{\textit{document, question}\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.Further we show that taking an ensemble of our \textit{elimination-selection} based method with a \textit{selection} based method gives us an improvement of 7\% (relative) over the best reported performance on this dataset.    ","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1bgpzZAZ,ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions,Cet article propose un nouveau modèle de compréhension de lecture à choix multiples basé sur l'idée que certaines options devraient être éliminées pour déduire de meilleures représentations du passage/de la question.
"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy.We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  ","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rkl6As0cF7,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,Nous avons proposé un nouveau cadre de raisonnement récursif probabiliste (PR2) pour les tâches d'apprentissage par renforcement profond multi-agents.
"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy.We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  ","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rkl6As0cF7,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,propose une nouvelle approche pour une formation entièrement décentralisée dans l'apprentissage par renforcement multi-agent.
"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy.We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  ","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rkl6As0cF7,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,"Aborde le problème de doter les agents RL de capacités de raisonnement récursif dans un cadre multi-agents, en se basant sur l'hypothèse que le raisonnement récursif leur est bénéfique pour converger vers des équilibres non triviaux."
"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy.We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  ","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rkl6As0cF7,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,"L'article présente une méthode de formation décentralisée pour l'apprentissage par renforcement multi-agent, où les agents déduisent les politiques des autres agents et utilisent les modèles déduits pour prendre des décisions. "
"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkEfPeZRb,Variance-based Gradient Compression for Efficient Distributed Deep Learning,Un nouvel algorithme pour réduire le surcoût de communication de l'apprentissage profond distribué en distinguant les gradients â€˜unambiguâ€™.
"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkEfPeZRb,Variance-based Gradient Compression for Efficient Distributed Deep Learning,Proposition d'une méthode de compression du gradient basée sur la variance pour réduire les frais de communication de l'apprentissage profond distribué.
"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkEfPeZRb,Variance-based Gradient Compression for Efficient Distributed Deep Learning,propose une nouvelle méthode de compression des mises à jour du gradient pour les SGD distribués afin d'accélérer l'exécution globale.
"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkEfPeZRb,Variance-based Gradient Compression for Efficient Distributed Deep Learning,Présente la méthode de compression du gradient basée sur la variance pour une formation distribuée efficace des réseaux neuronaux et la mesure de l'ambiguïté.
"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics.Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion.We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","[1, 0, 0, 0]",[],rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,Une nouvelle technique non supervisée d'adaptation au domaine profond qui unifie efficacement l'alignement des corrélations et la minimisation de l'entropie.
"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics.Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion.We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","[1, 0, 0, 0]",[],rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,"Améliore l'approche d'alignement par corrélation pour l'adaptation au domaine en remplaçant la distance euclidienne par la distance géodésique log-euclidienne entre deux matrices de covariance, et en sélectionnant automatiquement le coût d'équilibrage par l'entropie sur le domaine cible."
"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics.Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion.We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","[1, 0, 0, 0]",[],rJWechg0Z,Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation,"Proposition d'alignement de corrélation à entropie minimale, un algorithme d'adaptation de domaine non supervisé qui relie les méthodes de minimisation d'entropie et d'alignement de corrélation."
"Catastrophic interference has been a major roadblock in the research of continual learning.Here we propose a variant of the back-propagation algorithm, ""Conceptor-Aided Backprop"" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks.Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting.CAB extends these results to deep feedforward networks.On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","[0, 1, 0, 0, 0]",[],B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,"Nous proposons une variante de l'algorithme de rétropropagation, dans laquelle les gradients sont protégés par des conceptors contre la dégradation des tâches apprises précédemment."
"Catastrophic interference has been a major roadblock in the research of continual learning.Here we propose a variant of the back-propagation algorithm, ""Conceptor-Aided Backprop"" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks.Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting.CAB extends these results to deep feedforward networks.On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","[0, 1, 0, 0, 0]",[],B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,"Cet article applique la notion de concepteurs, une forme de régularisateur, pour empêcher l'oubli dans l'apprentissage continu dans la formation de réseaux neuronaux sur des tâches séquentielles."
"Catastrophic interference has been a major roadblock in the research of continual learning.Here we propose a variant of the back-propagation algorithm, ""Conceptor-Aided Backprop"" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks.Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting.CAB extends these results to deep feedforward networks.On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","[0, 1, 0, 0, 0]",[],B1al7jg0b,Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation,"Présente une méthode d'apprentissage de nouvelles tâches, sans interférence avec les tâches précédentes, à l'aide de concepteurs."
"Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task.Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks.In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora.Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering.It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering.On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses.The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.","[1, 0, 0, 0, 0, 0, 0]",[],H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,Analysez la raison pour laquelle les modèles génératifs à réponse neuronale préfèrent les réponses universelles ; proposez une méthode pour l'éviter.
"Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task.Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks.In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora.Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering.It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering.On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses.The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.","[1, 0, 0, 0, 0, 0, 0]",[],H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,Étudie le problème des réponses universelles dont souffrent les modèles de génération neuronale Seq2Seq.
"Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task.Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks.In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora.Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering.It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering.On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses.The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.","[1, 0, 0, 0, 0, 0, 0]",[],H1eqviAqYX,Why Do Neural Response Generation Models Prefer Universal Replies?,L'article cherche à améliorer la tâche de génération de réponses neuronales en minimisant les réponses communes par une modification de la fonction de perte et en présentant les réponses communes/universelles pendant la phase de formation.
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.An interactive online demo of our model is available at http://code2seq.org.Our code, data and trained models are available at http://github.com/tech-srl/code2seq.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1gKYo09tX,code2seq: Generating Sequences from Structured Representations of Code,Nous tirons parti de la structure syntaxique du code source pour générer des séquences en langage naturel.
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.An interactive online demo of our model is available at http://code2seq.org.Our code, data and trained models are available at http://github.com/tech-srl/code2seq.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1gKYo09tX,code2seq: Generating Sequences from Structured Representations of Code,Présente une méthode pour générer des séquences à partir d'un code en analysant et en produisant un arbre syntaxique.
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.An interactive online demo of our model is available at http://code2seq.org.Our code, data and trained models are available at http://github.com/tech-srl/code2seq.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1gKYo09tX,code2seq: Generating Sequences from Structured Representations of Code,Cet article présente un codage basé sur l'AST pour le code de programmation et montre son efficacité dans les tâches de résumé de code extrême et de sous-titrage de code.
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.An interactive online demo of our model is available at http://code2seq.org.Our code, data and trained models are available at http://github.com/tech-srl/code2seq.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1gKYo09tX,code2seq: Generating Sequences from Structured Representations of Code,Cet article présente un nouveau modèle code-séquence qui exploite la structure syntaxique des langages de programmation pour coder des extraits de code source et les décoder ensuite en langage naturel.
"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","[1, 0, 0, 0, 0, 0]",[],rJe7FW-Cb,A Painless Attention Mechanism for Convolutional Neural Networks,Nous améliorons les CNN avec un nouveau mécanisme d'attention pour une reconnaissance fine. Des performances supérieures sont obtenues sur 5 jeux de données.
"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","[1, 0, 0, 0, 0, 0]",[],rJe7FW-Cb,A Painless Attention Mechanism for Convolutional Neural Networks,Décrit un nouveau mécanisme attentionnel appliqué à la reconnaissance à grain fin qui améliore constamment la précision de reconnaissance de la ligne de base.
"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","[1, 0, 0, 0, 0, 0]",[],rJe7FW-Cb,A Painless Attention Mechanism for Convolutional Neural Networks,Cet article propose un mécanisme d'attention à action directe pour la classification d'images à grain fin.
"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","[1, 0, 0, 0, 0, 0]",[],rJe7FW-Cb,A Painless Attention Mechanism for Convolutional Neural Networks,Cet article présente un mécanisme d'attention intéressant pour la classification d'images à grain fin.
"Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator.We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances.We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator.Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin.Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.","[0, 0, 0, 1, 0, 0]",[],rkQsMCJCb,Generative Adversarial Networks using Adaptive Convolution,Nous remplaçons les convolutions normales par des convolutions adaptatives pour améliorer le générateur de GANs.
"Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator.We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances.We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator.Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin.Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.","[0, 0, 0, 1, 0, 0]",[],rkQsMCJCb,Generative Adversarial Networks using Adaptive Convolution,propose de remplacer les convolutions dans le générateur par un bloc de convolution adaptatif qui apprend à générer les poids de convolution et les biais des opérations de suréchantillonnage de manière adaptative par emplacement de pixel.
"Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator.We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances.We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator.Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin.Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.","[0, 0, 0, 1, 0, 0]",[],rkQsMCJCb,Generative Adversarial Networks using Adaptive Convolution,"Utilise la convolution adaptative dans le contexte des GAN avec un bloc appelé AdaConvBlock qui remplace la convolution ordinaire, ce qui donne plus de contexte local par poids de noyau de sorte qu'il peut générer des objets localement flexibles."
"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkr1UDeC-,Large scale distributed neural network training through online distillation,Nous réalisons des expériences à grande échelle pour montrer qu'une simple variante en ligne de la distillation peut nous aider à étendre la formation de réseaux neuronaux distribués à un plus grand nombre de machines.
"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkr1UDeC-,Large scale distributed neural network training through online distillation,Proposition d'une méthode permettant d'étendre la formation distribuée au-delà des limites actuelles de la descente de gradient stochastique en mini-batch.
"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkr1UDeC-,Large scale distributed neural network training through online distillation,"Proposition d'une méthode de distillation en ligne appelée co-distillation, appliquée à l'échelle, où deux modèles différents sont entraînés à faire correspondre les prédictions de l'autre modèle en plus de minimiser sa propre perte."
"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rkr1UDeC-,Large scale distributed neural network training through online distillation,Une technique de distillation en ligne est introduite pour accélérer les algorithmes traditionnels de formation de réseaux neuronaux distribués à grande échelle.
"Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis.Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings.In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set.We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data.Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem.We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.","[0, 0, 1, 0, 0, 0, 0]",[],r1saNM-RW,Small Coresets to Represent Large Training Data for Support Vector Machines,Nous présentons un algorithme pour accélérer l'entraînement des SVM sur des ensembles de données massifs en construisant des représentations compactes qui fournissent une inférence efficace et approximative.
"Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis.Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings.In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set.We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data.Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem.We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.","[0, 0, 1, 0, 0, 0, 0]",[],r1saNM-RW,Small Coresets to Represent Large Training Data for Support Vector Machines,étudie l'approche du coreset pour les SVM et vise à échantillonner un petit ensemble de points pondérés de telle sorte que la fonction de perte sur les points se rapproche de manière prouvable de celle sur l'ensemble du jeu de données.
"Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis.Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings.In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set.We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data.Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem.We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.","[0, 0, 1, 0, 0, 0, 0]",[],r1saNM-RW,Small Coresets to Represent Large Training Data for Support Vector Machines,L'article propose une construction de Coreset basée sur l'échantillonnage d'importance pour représenter de grandes données d'entraînement pour les SVM.
"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates.Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors.For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions.We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension.We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","[1, 0, 0, 0, 0]",[],HyxjwgbRZ,Convergence rate of sign stochastic gradient descent for non-convex functions,"Nous prouvons un taux de convergence non convexe pour la méthode du gradient stochastique du signe. L'algorithme a des liens avec des algorithmes comme Adam et Rprop, ainsi que des schémas de quantification du gradient utilisés dans l'apprentissage automatique distribué."
"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates.Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors.For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions.We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension.We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","[1, 0, 0, 0, 0]",[],HyxjwgbRZ,Convergence rate of sign stochastic gradient descent for non-convex functions,Fourni une analyse de convergence de l'algorithme Sign SGD pour les cas non-covexes
"The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates.Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors.For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions.We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension.We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information.","[1, 0, 0, 0, 0]",[],HyxjwgbRZ,Convergence rate of sign stochastic gradient descent for non-convex functions,L'article explore un algorithme qui utilise le signe des gradients au lieu des gradients réels pour former des modèles profonds.
"Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection.Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe.While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries. To this end, we introduce a transparent middleware layer for neural network acceleration.The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices.In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks.We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively.","[0, 0, 0, 1, 0, 0, 0]",[],rkf5hnNDj7,Towards Transparent Neural Network Acceleration,"Nous présentons un intergiciel transparent pour l'accélération des réseaux neuronaux, avec son propre moteur de compilation, qui atteint une vitesse de 11,8 fois sur les CPU et de 2,3 fois sur les GPU."
"Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection.Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe.While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries. To this end, we introduce a transparent middleware layer for neural network acceleration.The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices.In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks.We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively.","[0, 0, 0, 1, 0, 0, 0]",[],rkf5hnNDj7,Towards Transparent Neural Network Acceleration,Cet article propose une couche intergicielle transparente pour l'accélération des réseaux neuronaux et obtient quelques résultats d'accélération sur des architectures de base de CPU et de GPU.
"Performance of neural networks can be significantly improved by encoding known invariance for particular tasks.Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation.In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance.We call our new method the Conic Convolution and DFT Network (CFNet).We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection.Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.","[0, 0, 1, 0, 0, 0]",[],BJepX2A9tX,Rotation Equivariant Networks via Conic Convolution and the DFT,Nous proposons une convolution conique et la 2D-DFT pour encoder l'équivariance de rotation dans un réseau neuronal.
"Performance of neural networks can be significantly improved by encoding known invariance for particular tasks.Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation.In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance.We call our new method the Conic Convolution and DFT Network (CFNet).We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection.Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.","[0, 0, 1, 0, 0, 0]",[],BJepX2A9tX,Rotation Equivariant Networks via Conic Convolution and the DFT,"Dans le contexte de la classification d'images, l'article propose une architecture de réseau neuronal convolutif avec des cartes de caractéristiques équivoques en rotation qui sont finalement rendues invariantes en rotation en utilisant la magnitude de la transformée de Fourier discrète (DFT) 2D."
"Performance of neural networks can be significantly improved by encoding known invariance for particular tasks.Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation.In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance.We call our new method the Conic Convolution and DFT Network (CFNet).We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection.Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications.","[0, 0, 1, 0, 0, 0]",[],BJepX2A9tX,Rotation Equivariant Networks via Conic Convolution and the DFT,Les auteurs proposent un réseau neuronal invariant en rotation en combinant convolution conique et 2D-DFT.
"The problem of visual metamerism is defined as finding a family of perceptuallyindistinguishable, yet physically different images.In this paper, we propose ourNeuroFovea metamer model, a foveated generative model that is based on a mixtureof peripheral representations and style transfer forward-pass algorithms.Ourgradient-descent free model is parametrized by a foveated VGG19 encoder-decoderwhich allows us to encode images in high dimensional space and interpolatebetween the content and texture information with adaptive instance normalizationanywhere in the visual field.Our contributions include:1) A framework forcomputing metamers that resembles a noisy communication system via a foveatedfeed-forward encoder-decoder network – We observe that metamerism arises as abyproduct of noisy perturbations that partially lie in the perceptual null space;2)A perceptual optimization scheme as a solution to the hyperparametric nature ofour metamer model that requires tuning of the image-texture tradeoff coefficientseverywhere in the visual field which are a consequence of internal noise;3) AnABX psychophysical evaluation of our metamers where we also find that the rateof growth of the receptive fields in our model match V1 for reference metamersand V2 between synthesized samples.Our model also renders metamers at roughlya second, presenting a ×1000 speed-up compared to the previous work, which nowallows for tractable data-driven metamer experiments.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzbG20cFQ,Towards Metamerism via Foveated Style Transfer,"Nous présentons une nouvelle structure de type ""feed-forward"" pour générer des métamères visuels."
"The problem of visual metamerism is defined as finding a family of perceptuallyindistinguishable, yet physically different images.In this paper, we propose ourNeuroFovea metamer model, a foveated generative model that is based on a mixtureof peripheral representations and style transfer forward-pass algorithms.Ourgradient-descent free model is parametrized by a foveated VGG19 encoder-decoderwhich allows us to encode images in high dimensional space and interpolatebetween the content and texture information with adaptive instance normalizationanywhere in the visual field.Our contributions include:1) A framework forcomputing metamers that resembles a noisy communication system via a foveatedfeed-forward encoder-decoder network – We observe that metamerism arises as abyproduct of noisy perturbations that partially lie in the perceptual null space;2)A perceptual optimization scheme as a solution to the hyperparametric nature ofour metamer model that requires tuning of the image-texture tradeoff coefficientseverywhere in the visual field which are a consequence of internal noise;3) AnABX psychophysical evaluation of our metamers where we also find that the rateof growth of the receptive fields in our model match V1 for reference metamersand V2 between synthesized samples.Our model also renders metamers at roughlya second, presenting a ×1000 speed-up compared to the previous work, which nowallows for tractable data-driven metamer experiments.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzbG20cFQ,Towards Metamerism via Foveated Style Transfer,propose un modèle NeuroFovea pour la génération de métamères de point de fixation en utilisant une approche de transfert de style via une architecture de style Encoder-Decoder.
"The problem of visual metamerism is defined as finding a family of perceptuallyindistinguishable, yet physically different images.In this paper, we propose ourNeuroFovea metamer model, a foveated generative model that is based on a mixtureof peripheral representations and style transfer forward-pass algorithms.Ourgradient-descent free model is parametrized by a foveated VGG19 encoder-decoderwhich allows us to encode images in high dimensional space and interpolatebetween the content and texture information with adaptive instance normalizationanywhere in the visual field.Our contributions include:1) A framework forcomputing metamers that resembles a noisy communication system via a foveatedfeed-forward encoder-decoder network – We observe that metamerism arises as abyproduct of noisy perturbations that partially lie in the perceptual null space;2)A perceptual optimization scheme as a solution to the hyperparametric nature ofour metamer model that requires tuning of the image-texture tradeoff coefficientseverywhere in the visual field which are a consequence of internal noise;3) AnABX psychophysical evaluation of our metamers where we also find that the rateof growth of the receptive fields in our model match V1 for reference metamersand V2 between synthesized samples.Our model also renders metamers at roughlya second, presenting a ×1000 speed-up compared to the previous work, which nowallows for tractable data-driven metamer experiments.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzbG20cFQ,Towards Metamerism via Foveated Style Transfer,Une analyse du métamérisme et un modèle capable de produire rapidement des métamères utiles pour la psychophysique expérimentale et d'autres domaines.
"The problem of visual metamerism is defined as finding a family of perceptuallyindistinguishable, yet physically different images.In this paper, we propose ourNeuroFovea metamer model, a foveated generative model that is based on a mixtureof peripheral representations and style transfer forward-pass algorithms.Ourgradient-descent free model is parametrized by a foveated VGG19 encoder-decoderwhich allows us to encode images in high dimensional space and interpolatebetween the content and texture information with adaptive instance normalizationanywhere in the visual field.Our contributions include:1) A framework forcomputing metamers that resembles a noisy communication system via a foveatedfeed-forward encoder-decoder network – We observe that metamerism arises as abyproduct of noisy perturbations that partially lie in the perceptual null space;2)A perceptual optimization scheme as a solution to the hyperparametric nature ofour metamer model that requires tuning of the image-texture tradeoff coefficientseverywhere in the visual field which are a consequence of internal noise;3) AnABX psychophysical evaluation of our metamers where we also find that the rateof growth of the receptive fields in our model match V1 for reference metamersand V2 between synthesized samples.Our model also renders metamers at roughlya second, presenting a ×1000 speed-up compared to the previous work, which nowallows for tractable data-driven metamer experiments.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJzbG20cFQ,Towards Metamerism via Foveated Style Transfer,"L'article propose une mÃ©thode rapide pour gÃ©nÃ©rer des mÃ©tamÃ?res visuels â€"" des images physiquement diffÃ©rentes qui ne peuvent pas Ãªtre distinguÃ©es d'un original â€"" par le biais d'un transfert de style fovÃ©ral, rapide et arbitraire."
"Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks.Towards explaining this phenomenon, we adopt a margin-based perspective.We establish:1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks.In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees.The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees.We validate this gap between the two approaches empirically.Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization.We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJGtFoC5Fm,On the Margin Theory of Feedforward Neural Networks,Nous montrons que l'entraînement des réseaux relu feedforward avec un régularisateur faible donne une marge maximale et nous analysons les implications de ce résultat.
"Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks.Towards explaining this phenomenon, we adopt a margin-based perspective.We establish:1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks.In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees.The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees.We validate this gap between the two approaches empirically.Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization.We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJGtFoC5Fm,On the Margin Theory of Feedforward Neural Networks,étudie la théorie de la marge pour les ensembles neuronaux et montre que la marge maximale augmente de façon monotone avec la taille du réseau
"Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks.Towards explaining this phenomenon, we adopt a margin-based perspective.We establish:1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks.In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees.The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees.We validate this gap between the two approaches empirically.Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization.We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJGtFoC5Fm,On the Margin Theory of Feedforward Neural Networks,"Cet article étudie le biais implicite des minimiseurs d'une perte d'entropie croisée régularisée d'un réseau à deux couches avec des activations ReLU, en obtenant une limite supérieure de généralisation qui n'augmente pas avec la taille du réseau."
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.","[0, 0, 0, 1]",[],H1Dy---0Z,Distributed Prioritized Experience Replay,"Une architecture distribuée pour l'apprentissage par renforcement profond à l'échelle, utilisant la génération de données en parallèle pour améliorer l'état de l'art sur le benchmark Arcade Learning Environment en une fraction du temps d'entraînement de l'horloge murale des approches précédentes."
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.","[0, 0, 0, 1]",[],H1Dy---0Z,Distributed Prioritized Experience Replay,"Examine un système Deep RL distirbué dans lequel les expériences, plutôt que les gradients, sont partagées entre les travaux parallèles et l'apprenant centralisé."
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.","[0, 0, 0, 1]",[],H1Dy---0Z,Distributed Prioritized Experience Replay,"Une approche parallèle de l'apprentissage du DQN, basée sur l'idée que plusieurs acteurs collectent des données en parallèle tandis qu'un seul apprenant entraîne le modèle à partir d'expériences échantillonnées dans la mémoire centrale."
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.","[0, 0, 0, 1]",[],H1Dy---0Z,Distributed Prioritized Experience Replay,"Cet article propose une architecture distribuée pour l'apprentissage par renforcement profond à l'échelle, en se concentrant sur l'ajout de la parallélisation dans l'algorithme de l'acteur dans le cadre de la répétition d'expérience priorisée."
"Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly.In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed.As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform.Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization.We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.","[0, 1, 0, 0, 0, 0]",[],S1fHmlbCW,Neural Networks for irregularly observed continuous-time Stochastic Processes,Architectures neuronales fournissant des représentations de signaux observés de manière irrégulière qui permettent de manière prouvée la reconstruction du signal.
"Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly.In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed.As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform.Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization.We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.","[0, 1, 0, 0, 0, 0]",[],S1fHmlbCW,Neural Networks for irregularly observed continuous-time Stochastic Processes,"prouve que les réseaux neuronaux convolutifs avec la fonction d'activation Leaky ReLU sont des cadres non linéaires, avec des résultats similaires pour les séries chronologiques non uniformément échantillonnées"
"Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly.In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed.As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform.Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization.We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting.","[0, 1, 0, 0, 0, 0]",[],S1fHmlbCW,Neural Networks for irregularly observed continuous-time Stochastic Processes,Cet article considère les réseaux neuronaux sur les séries temporelles et montre que les premiers filtres convolutifs peuvent être choisis pour représenter une transformée en ondelettes discrète.
"Most state-of-the-art neural machine translation systems, despite being differentin architectural skeletons (e.g., recurrence, convolutional), share an indispensablefeature: the Attention.However, most existing attention methods are token-basedand ignore the importance of phrasal alignments, the key ingredient for the successof phrase-based statistical machine translation.In this paper, we proposenovel phrase-based attention methods to model n-grams of tokens as attentionentities.We incorporate our phrase-based attentions into the recently proposedTransformer network, and demonstrate that our approach yields improvements of1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translationtasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks on WMT newstest2014 using WMT’16 training data.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1xN5oA5tm,Phrase-Based Attentions,"Mécanismes d'attention basés sur les phrases pour attribuer l'attention sur les phrases, en réalisant des alignements d'attention de jeton à phrase, de phrase à jeton, de phrase à phrase, en plus des attentions existantes de jeton à jeton."
"Most state-of-the-art neural machine translation systems, despite being differentin architectural skeletons (e.g., recurrence, convolutional), share an indispensablefeature: the Attention.However, most existing attention methods are token-basedand ignore the importance of phrasal alignments, the key ingredient for the successof phrase-based statistical machine translation.In this paper, we proposenovel phrase-based attention methods to model n-grams of tokens as attentionentities.We incorporate our phrase-based attentions into the recently proposedTransformer network, and demonstrate that our approach yields improvements of1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translationtasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks on WMT newstest2014 using WMT’16 training data.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1xN5oA5tm,Phrase-Based Attentions,L'article présente un mécanisme d'attention qui calcule une somme pondérée non seulement sur des tokens uniques mais aussi sur des ngrammes (phrases).
"Intuitively, unfamiliarity should lead to lack of confidence.In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training.Unlike domain adaptation methods, we cannot gather an ""unexpected dataset"" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected.We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score.Experimentally, we investigate the overconfidence problem and evaluate our solution by creating ""familiar"" and ""novel"" test splits, where ""familiar"" are identically distributed with training and ""novel"" are not.We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods.In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing.Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.","[0, 1, 0, 0, 0, 0, 0, 0]",[],S1giro05t7,Reducing Overconfident Errors outside the Known Distribution,"Les réseaux profonds sont plus susceptibles de se tromper en toute confiance lorsqu'ils sont testés sur des données inattendues. Nous proposons une méthodologie expérimentale pour étudier le problème, et deux méthodes pour réduire les erreurs de confiance sur des distributions d'entrée inconnues."
"Intuitively, unfamiliarity should lead to lack of confidence.In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training.Unlike domain adaptation methods, we cannot gather an ""unexpected dataset"" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected.We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score.Experimentally, we investigate the overconfidence problem and evaluate our solution by creating ""familiar"" and ""novel"" test splits, where ""familiar"" are identically distributed with training and ""novel"" are not.We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods.In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing.Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.","[0, 1, 0, 0, 0, 0, 0, 0]",[],S1giro05t7,Reducing Overconfident Errors outside the Known Distribution,"Propose deux idées pour réduire les prédictions erronées trop confiantes : la ""distillation G"" de l'ensemble d'am avec des données supplémentaires non supervisées et la réduction de la confiance en la nouveauté à l'aide d'un détecteur de nouveauté."
"Intuitively, unfamiliarity should lead to lack of confidence.In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training.Unlike domain adaptation methods, we cannot gather an ""unexpected dataset"" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected.We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score.Experimentally, we investigate the overconfidence problem and evaluate our solution by creating ""familiar"" and ""novel"" test splits, where ""familiar"" are identically distributed with training and ""novel"" are not.We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods.In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing.Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data.","[0, 1, 0, 0, 0, 0, 0, 0]",[],S1giro05t7,Reducing Overconfident Errors outside the Known Distribution,"Les auteurs proposent deux méthodes pour estimer la confiance de la classification sur de nouvelles distributions de données non vues. La première idée consiste à utiliser des méthodes d'ensemble comme approche de base pour aider à identifier les cas incertains, puis à utiliser des méthodes de distillation pour réduire l'ensemble en un seul modèle imitant le comportement de l'ensemble. La deuxième idée est d'utiliser un classificateur détecteur de nouveauté et de pondérer la sortie du réseau par le score de nouveauté."
"Progress in deep learning is slowed by the days or weeks it takes to train large models.The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources.In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available.Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products.We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps.At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\%.Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\%.Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rkLyJl-0-,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,Nous décrivons un algorithme d'optimisation pratique pour les réseaux neuronaux profonds qui fonctionne plus rapidement et génère de meilleurs modèles par rapport aux algorithmes largement utilisés.
"Progress in deep learning is slowed by the days or weeks it takes to train large models.The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources.In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available.Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products.We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps.At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\%.Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\%.Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rkLyJl-0-,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,propose un nouvel algorithme qui prétend utiliser le hessien de manière implicite et qui s'inspire des séries de puissance.
"Progress in deep learning is slowed by the days or weeks it takes to train large models.The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources.In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available.Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products.We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps.At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\%.Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\%.Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rkLyJl-0-,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,Présente un nouvel algorithme d'ordre 2 qui utilise implicitement l'information sur la courbure et montre l'intuition derrière les schémas d'approximation dans les algorithmes et valide l'heuristique dans diverses expériences.
"Recent work has shown that performing inference with fast, very-low-bitwidth(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurateresults.However, although 2-bit approximated networks have been shown tobe quite accurate, 1 bit approximations, which are twice as fast, have restrictivelylow accuracy.We propose a method to train models whose weights are a mixtureof bitwidths, that allows us to more finely tune the accuracy/speed trade-off.Wepresent the “middle-out” criterion for determining the bitwidth for each value, andshow how to integrate it into training models with a desired mixture of bitwidths.We evaluate several architectures and binarization techniques on the ImageNetdataset.We show that our heterogeneous bitwidth approximation achieves superlinearscaling of accuracy with bitwidth.Using an average of only 1.4 bits, we areable to outperform state-of-the-art 2-bit architectures.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJDV5YxCW,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,Nous introduisons l'approximation par largeur de bit fractionnelle et montrons qu'elle présente des avantages significatifs.
"Recent work has shown that performing inference with fast, very-low-bitwidth(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurateresults.However, although 2-bit approximated networks have been shown tobe quite accurate, 1 bit approximations, which are twice as fast, have restrictivelylow accuracy.We propose a method to train models whose weights are a mixtureof bitwidths, that allows us to more finely tune the accuracy/speed trade-off.Wepresent the “middle-out” criterion for determining the bitwidth for each value, andshow how to integrate it into training models with a desired mixture of bitwidths.We evaluate several architectures and binarization techniques on the ImageNetdataset.We show that our heterogeneous bitwidth approximation achieves superlinearscaling of accuracy with bitwidth.Using an average of only 1.4 bits, we areable to outperform state-of-the-art 2-bit architectures.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJDV5YxCW,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,Propose une méthode pour faire varier le degré de quantification dans un réseau neuronal pendant la phase de propagation vers l'avant.
"Recent work has shown that performing inference with fast, very-low-bitwidth(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurateresults.However, although 2-bit approximated networks have been shown tobe quite accurate, 1 bit approximations, which are twice as fast, have restrictivelylow accuracy.We propose a method to train models whose weights are a mixtureof bitwidths, that allows us to more finely tune the accuracy/speed trade-off.Wepresent the “middle-out” criterion for determining the bitwidth for each value, andshow how to integrate it into training models with a desired mixture of bitwidths.We evaluate several architectures and binarization techniques on the ImageNetdataset.We show that our heterogeneous bitwidth approximation achieves superlinearscaling of accuracy with bitwidth.Using an average of only 1.4 bits, we areable to outperform state-of-the-art 2-bit architectures.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJDV5YxCW,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,Maintenir la précision d'un mot net de 2bits tout en utilisant des poids de moins de 2bits
"Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model.We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.","[0, 0, 1, 0]",[],BJxRVnC5Fm,Mean Replacement Pruning  ,Le remplacement de la moyenne est une méthode efficace pour améliorer la perte après élagage et les fonctions de notation basées sur l'approximation de Taylor fonctionnent mieux avec les valeurs absolues. 
"Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model.We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.","[0, 0, 1, 0]",[],BJxRVnC5Fm,Mean Replacement Pruning  ,"Propose une amélioration simple des méthodes d'élagage des unités utilisant le ""remplacement moyen""."
"Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model.We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training.","[0, 0, 1, 0]",[],BJxRVnC5Fm,Mean Replacement Pruning  ,Cet article présente une stratégie d'élagage par remplacement de la moyenne et utilise le développement de Taylor à valeur absolue comme fonction de notation pour l'élagage.
"Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective.We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information.Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior.For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis.We demonstrate our method’s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.","[0, 0, 1, 0, 0]",[],BJe0Gn0cY7,Preventing Posterior Collapse with delta-VAEs, Évitez l'effondrement postérieur en limitant le taux.
"Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective.We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information.Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior.For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis.We demonstrate our method’s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.","[0, 0, 1, 0, 0]",[],BJe0Gn0cY7,Preventing Posterior Collapse with delta-VAEs,Présente une approche visant à empêcher l'effondrement de la postériorité dans les VAE en limitant la famille de l'approximation variationnelle de la postériorité.
"Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective.We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information.Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior.For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis.We demonstrate our method’s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.","[0, 0, 1, 0, 0]",[],BJe0Gn0cY7,Preventing Posterior Collapse with delta-VAEs,Cet article introduit une contrainte sur la famille des a posteriori variationnels de sorte que le terme KL peut être contrôlé pour lutter contre l'effondrement des a posteriori dans les modèles génératifs profonds tels que les VAE.
"Mini-batch gradient descent and its variants are commonly used in deep learning.The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient.However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network.Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds.We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification.Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data.Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes.We also identified an interesting phenomenon, batch size boom.The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SybqeKgA-,On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,"Nous avons développé un momentum adaptatif par lot qui peut atteindre une perte plus faible par rapport aux méthodes par mini-batch après avoir analysé les mêmes époques de données, et il est plus robuste contre une grande taille de pas."
"Mini-batch gradient descent and its variants are commonly used in deep learning.The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient.However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network.Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds.We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification.Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data.Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes.We also identified an interesting phenomenon, batch size boom.The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SybqeKgA-,On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,"Cet article aborde le problème du réglage automatique de la taille des lots pendant la formation d'apprentissage profond, et prétend étendre le SGD adaptatif par lot à la dynamique adaptative et adopter les algorithmes aux problèmes de réseaux neuronaux complexes."
"Mini-batch gradient descent and its variants are commonly used in deep learning.The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient.However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network.Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds.We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification.Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data.Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes.We also identified an interesting phenomenon, batch size boom.The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SybqeKgA-,On Batch Adaptive Training for Deep Learning: Lower Loss and Larger Step Size,L'article propose de généraliser un algorithme qui exécute le SGD avec des tailles de lot adaptatives en ajoutant un momentum à la fonction d'utilité.
"Deep learning models for graphs have advanced the state of the art on many tasks.Despite their recent success, little is known about their robustness.We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings.Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information.Our attacks do not assume any knowledge about or access to the target classifiers.","[0, 0, 1, 0, 0, 0, 0]",[],Bylnx209YX,Adversarial Attacks on Graph Neural Networks via Meta Learning,Nous utilisons les méta-gradients pour attaquer la procédure de formation des réseaux neuronaux profonds pour les graphes.
"Deep learning models for graphs have advanced the state of the art on many tasks.Despite their recent success, little is known about their robustness.We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings.Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information.Our attacks do not assume any knowledge about or access to the target classifiers.","[0, 0, 1, 0, 0, 0, 0]",[],Bylnx209YX,Adversarial Attacks on Graph Neural Networks via Meta Learning,Etudie le problème de l'apprentissage d'un meilleur paramètre de graphe empoisonné qui peut maximiser la perte d'un réseau neuronal de graphe. 
"Deep learning models for graphs have advanced the state of the art on many tasks.Despite their recent success, little is known about their robustness.We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings.Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information.Our attacks do not assume any knowledge about or access to the target classifiers.","[0, 0, 1, 0, 0, 0, 0]",[],Bylnx209YX,Adversarial Attacks on Graph Neural Networks via Meta Learning,"Un algorithme pour modifier la structure du graphe en ajoutant/supprimant des arêtes de manière à dégrader la performance globale de la classification des nœuds, et l'idée d'utiliser le méta-apprentissage pour résoudre le problème d'optimisation à deux niveaux."
"Numerous models for grounded language understanding have been recently proposed, including(i) generic models that can be easily adapted to any given task and(ii) intuitively appealing modular models that require background knowledge to be instantiated.We compare both types of models in how much they lend themselves to a particular form of systematic generalization.Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them.Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected.We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization.We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization.Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],HkezXnA9YX,Systematic Generalization: What Is Required and Can It Be Learned?,Nous montrons que les modèles structurés modulaires sont les meilleurs en termes de généralisation systématique et que leurs versions de bout en bout ne généralisent pas aussi bien.
"Numerous models for grounded language understanding have been recently proposed, including(i) generic models that can be easily adapted to any given task and(ii) intuitively appealing modular models that require background knowledge to be instantiated.We compare both types of models in how much they lend themselves to a particular form of systematic generalization.Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them.Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected.We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization.We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization.Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],HkezXnA9YX,Systematic Generalization: What Is Required and Can It Be Learned?,Cet article évalue la généralisation systémique entre les réseaux neuronaux modulaires et les modèles génériques par l'introduction d'un nouvel ensemble de données de raisonnement spatial.
"Numerous models for grounded language understanding have been recently proposed, including(i) generic models that can be easily adapted to any given task and(ii) intuitively appealing modular models that require background knowledge to be instantiated.We compare both types of models in how much they lend themselves to a particular form of systematic generalization.Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them.Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected.We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization.We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization.Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],HkezXnA9YX,Systematic Generalization: What Is Required and Can It Be Learned?,"Une évaluation empirique ciblée de la généralisation dans les modèles de raisonnement visuel, axée sur le problème de la reconnaissance de triples (objet, relation, objet) dans des scènes synthétiques comportant des lettres et des chiffres."
"The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them.Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments.Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions.Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary.Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.","[0, 1, 0, 0, 0, 0]",[],rJlEojAqFm,Relational Forward Models for Multi-Agent Learning,"Les modèles relationnels prospectifs pour l'apprentissage multi-agents font des prédictions précises du comportement futur des agents, ils produisent des représentations interprétables et peuvent être utilisés à l'intérieur des agents."
"The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them.Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments.Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions.Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary.Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.","[0, 1, 0, 0, 0, 0]",[],rJlEojAqFm,Relational Forward Models for Multi-Agent Learning,"Une façon de réduire la variance dans l'apprentissage sans modèle en ayant un modèle explicite, qui utilise une architecture de type réseau de convoyage de graphes, des actions que les autres agents prendront. "
"The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them.Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments.Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions.Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary.Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial.","[0, 1, 0, 0, 0, 0]",[],rJlEojAqFm,Relational Forward Models for Multi-Agent Learning,"Prédire le comportement des multi-agents à l'aide d'un modèle relationnel prospectif avec une composante récurrente, en surpassant deux lignes de base et deux ablations."
"We show that gradient descent on an unregularized logistic regressionproblem, for almost all separable datasets, converges to the same direction as the max-margin solution.The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss.Furthermore,we show this convergence is very slow, and only logarithmic in theconvergence of the loss itself.This can help explain the benefitof continuing to optimize the logistic or cross-entropy loss evenafter the training error is zero and the training loss is extremelysmall, and, as we show, even if the validation loss increases.Ourmethodology can also aid in understanding implicit regularizationin more complex models and with other optimization methods.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,La solution normalisée de la descente du gradient sur la régression logistique (ou une perte décroissante similaire) converge lentement vers la solution de la marge max L2 sur des données séparables.
"We show that gradient descent on an unregularized logistic regressionproblem, for almost all separable datasets, converges to the same direction as the max-margin solution.The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss.Furthermore,we show this convergence is very slow, and only logarithmic in theconvergence of the loss itself.This can help explain the benefitof continuing to optimize the logistic or cross-entropy loss evenafter the training error is zero and the training loss is extremelysmall, and, as we show, even if the validation loss increases.Ourmethodology can also aid in understanding implicit regularizationin more complex models and with other optimization methods.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,L'article offre une preuve formelle que la descente du gradient sur la perte logistique converge très lentement vers la solution SVM dure dans le cas où les données sont linéairement séparables. 
"We show that gradient descent on an unregularized logistic regressionproblem, for almost all separable datasets, converges to the same direction as the max-margin solution.The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss.Furthermore,we show this convergence is very slow, and only logarithmic in theconvergence of the loss itself.This can help explain the benefitof continuing to optimize the logistic or cross-entropy loss evenafter the training error is zero and the training loss is extremelysmall, and, as we show, even if the validation loss increases.Ourmethodology can also aid in understanding implicit regularizationin more complex models and with other optimization methods.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1q7n9gAb,The Implicit Bias of Gradient Descent on Separable Data,"Cet article se concentre sur la caractérisation du comportement de la minimisation de la perte logarithmique sur des données linéairement séparables, et montre que la perte logarithmique, minimisée par descente de gradient, conduit à la convergence vers la solution de marge maximale."
"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier.Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals.To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image.Then we introduce two techniques for improving our networks' out-of-sample performance.The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable.The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJEjjoR9K7,Learning Robust Representations by Projecting Superficial Statistics Out,"En s'appuyant sur des travaux antérieurs sur la généralisation des domaines, nous espérons produire un classificateur qui se généralisera à des domaines inconnus, même si les identifiants de domaine ne sont pas disponibles pendant la formation."
"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier.Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals.To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image.Then we introduce two techniques for improving our networks' out-of-sample performance.The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable.The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJEjjoR9K7,Learning Robust Representations by Projecting Superficial Statistics Out,Une approche de généralisation de domaine pour révéler l'information sémantique basée sur un schéma de projection linéaire des couches de sortie du CNN et du NGLCM.
"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier.Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals.To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image.Then we introduce two techniques for improving our networks' out-of-sample performance.The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable.The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJEjjoR9K7,Learning Robust Representations by Projecting Superficial Statistics Out,L'article propose une approche non supervisée pour identifier les caractéristiques d'image qui ne sont pas significatives pour les tâches de classification d'image.
"In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors.Our approach alternates between two threads.In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles.In the second, we minimize the approximated detection score by searching for the optimal camouflage.Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.","[1, 0, 0, 0, 0, 0]",[],SJgEl3A5tm,CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild,Nous proposons une méthode d'apprentissage du camouflage physique des véhicules pour attaquer de manière adversative les détecteurs d'objets dans la nature. Nous trouvons notre camouflage efficace et transférable.
"In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors.Our approach alternates between two threads.In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles.In the second, we minimize the approximated detection score by searching for the optimal camouflage.Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.","[1, 0, 0, 0, 0, 0]",[],SJgEl3A5tm,CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild,"Les auteurs étudient le problème de l'apprentissage d'un motif de camouflage qui, appliqué à un véhicule simulé, empêchera un détecteur d'objets de le détecter."
"In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors.Our approach alternates between two threads.In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles.In the second, we minimize the approximated detection score by searching for the optimal camouflage.Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors.","[1, 0, 0, 0, 0, 0]",[],SJgEl3A5tm,CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild,Cet article vise l'apprentissage contradictoire pour la détection des voitures interférentes en apprenant des motifs de camouflage.
"As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows.Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data.On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.","[0, 0, 0, 1, 0, 0]",[],rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,Nous combinons des arbres de décision différentiables avec des autoencodeurs variationnels supervisés pour améliorer l'interprétabilité de la classification. 
"As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows.Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data.On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.","[0, 0, 0, 1, 0, 0]",[],rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,"Cet article propose un modèle hybride d'un autoencodeur variationnel composé d'un arbre de décision différentiable et d'un schéma d'apprentissage associé. Les expériences démontrent les performances de classification de l'arbre, les performances de la vraisemblance logarithmique négative et l'interprétabilité de l'espace latent."
"As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows.Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data.On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture.","[0, 0, 0, 1, 0, 0]",[],rJhR_pxCZ,Interpretable Classification via Supervised Variational Autoencoders and Differentiable Decision Trees,L'article tente de construire un classificateur interprétable et précis en combinant un VAE supervisé et un arbre de décision différentiable.
"We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples.We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class.To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available.Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights.Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.","[1, 0, 0, 0, 0, 0]",[],rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,Une approche pratique et garantie pour l'entraînement de classifieurs efficaces en présence de décalages d'étiquettes entre les ensembles de données source et cible.
"We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples.We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class.To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available.Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights.Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.","[1, 0, 0, 0, 0, 0]",[],rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,Les auteurs proposent un nouvel algorithme pour améliorer la stabilité de la procédure d'estimation de la pondération de l'importance des classes avec une procédure en deux étapes.
"We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples.We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class.To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available.Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights.Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods.","[1, 0, 0, 0, 0, 0]",[],rJl0r3R9KX,Regularized Learning for  Domain Adaptation under Label Shifts,"Les auteurs considèrent le problème de l'apprentissage en cas de décalage d'étiquettes, où les proportions d'étiquettes diffèrent alors que les conditionnels sont égaux, et proposent un estimateur amélioré avec régularisation."
"The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset.This is because the real visual world has a few classes that are common while others are rare.Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset.To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes.To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.The proposed method is simple and easy-to-implement in existing deep learning frameworks.Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.","[0, 0, 0, 0, 0, 0, 1]",[],Bk9nkMa4G,Bayesian Embeddings for Long-Tailed Datasets,Approche visant à améliorer la précision de la classification sur les classes de la queue.
"The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset.This is because the real visual world has a few classes that are common while others are rare.Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset.To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes.To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.The proposed method is simple and easy-to-implement in existing deep learning frameworks.Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.","[0, 0, 0, 0, 0, 0, 1]",[],Bk9nkMa4G,Bayesian Embeddings for Long-Tailed Datasets,L'objectif principal de cet article est d'apprendre un classificateur ConvNet qui est plus performant pour les classes situées dans la queue de la distribution d'occurrence des classes.
"The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset.This is because the real visual world has a few classes that are common while others are rare.Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset.To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes.To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.The proposed method is simple and easy-to-implement in existing deep learning frameworks.Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples.","[0, 0, 0, 0, 0, 0, 1]",[],Bk9nkMa4G,Bayesian Embeddings for Long-Tailed Datasets,"Proposition d'un cadre bayésien avec un modèle de mélange gaussien pour résoudre un problème dans les applications de classification, à savoir que le nombre de données d'entraînement provenant de différentes classes n'est pas équilibré."
"As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents.Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior.However, this topic has been relatively under-explored in the reinforcement learning community.In this work we present a method for synthesizing states of interest for a trained agent.Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary.Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system.To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods.We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator.Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BJf9k305Fm,Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning,Nous présentons une méthode permettant de synthétiser des états d'intérêt pour les agents d'apprentissage par renforcement afin d'analyser leur comportement. 
"As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents.Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior.However, this topic has been relatively under-explored in the reinforcement learning community.In this work we present a method for synthesizing states of interest for a trained agent.Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary.Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system.To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods.We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator.Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BJf9k305Fm,Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning,Cet article propose un modèle génératif d'observations visuelles en RL qui est capable de générer des observations d'intérêt.
"As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents.Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior.However, this topic has been relatively under-explored in the reinforcement learning community.In this work we present a method for synthesizing states of interest for a trained agent.Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary.Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system.To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods.We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator.Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BJf9k305Fm,Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning,Une approche pour visualiser les états d'intérêt qui implique un auto-codeur variationnel qui apprend à reconstruire l'espace d'état et une étape d'optimisation qui trouve des paramètres de conditionnement pour générer des images synthétiques.
"We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples.We show that such deep abstaining classifiers can:(i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;(ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and(iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes.We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.","[1, 0, 0, 0, 0, 0, 0]",[],rJxF73R9tX,Knows When it Doesn’t Know: Deep Abstaining Classifiers,Un réseau neuronal profond d'abstention formé avec une nouvelle fonction de perte qui apprend des représentations pour savoir quand s'abstenir permettant un apprentissage robuste en présence de différents types de bruit.
"We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples.We show that such deep abstaining classifiers can:(i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;(ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and(iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes.We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.","[1, 0, 0, 0, 0, 0, 0]",[],rJxF73R9tX,Knows When it Doesn’t Know: Deep Abstaining Classifiers,"Une nouvelle fonction de perte pour l'entraînement d'un réseau neuronal profond qui peut s'abstenir, avec des performances examinées sous différents angles : en existence de bruit structuré, en existence de bruit non structuré, et détection de monde ouvert."
"We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples.We show that such deep abstaining classifiers can:(i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;(ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and(iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes.We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise.","[1, 0, 0, 0, 0, 0, 0]",[],rJxF73R9tX,Knows When it Doesn’t Know: Deep Abstaining Classifiers,"Ce manuscrit introduit des classificateurs profonds d'abstention qui modifient la perte d'entropie croisée multiclasse avec une perte d'abstention, qui est ensuite appliquée aux tâches de classification d'images perturbées."
"Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability.In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence.This approach can be easily applied to both linear and nonlinear function approximators. HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.","[0, 0, 1, 0, 0]",[],rylbWhC5Ym,HR-TD: A Regularized TD Method to Avoid Over-Generalization,"Une technique de régularisation pour l'apprentissage TD qui évite la sur-généralisation temporelle, en particulier dans les réseaux profonds."
"Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability.In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence.This approach can be easily applied to both linear and nonlinear function approximators. HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.","[0, 0, 1, 0, 0]",[],rylbWhC5Ym,HR-TD: A Regularized TD Method to Avoid Over-Generalization,Une variation de l'apprentissage par différence temporelle pour le cas de l'approximation de fonction qui tente de résoudre le problème de la sur-généralisation à travers les états temporellement successifs.
"Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability.In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence.This approach can be easily applied to both linear and nonlinear function approximators. HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance.","[0, 0, 1, 0, 0]",[],rylbWhC5Ym,HR-TD: A Regularized TD Method to Avoid Over-Generalization,"L'article présente HR-TD, une variante de l'algorithme TD(0), destinée à améliorer le problème de sur-généralisation dans les TD classiques."
"We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters.Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation.As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters.We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation.Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.","[0, 0, 0, 0, 0, 1]",[],Bkl-43C9FQ,Spherical CNNs on Unstructured Grids,"Nous présentons un nouveau noyau CNN pour les grilles non structurées pour les signaux sphériques, et montrons un gain significatif de précision et d'efficacité des paramètres sur des tâches telles que la classification 3D et la segmentation d'images omnidirectionnelles."
"We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters.Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation.As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters.We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation.Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.","[0, 0, 0, 0, 0, 1]",[],Bkl-43C9FQ,Spherical CNNs on Unstructured Grids,Une méthode efficace permettant l'apprentissage profond sur des données sphériques qui atteint des chiffres compétitifs/de pointe avec beaucoup moins de paramètres que les approches populaires.
"We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters.Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation.As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters.We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation.Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.","[0, 0, 0, 0, 0, 1]",[],Bkl-43C9FQ,Spherical CNNs on Unstructured Grids,L'article propose un nouveau noyau convolutionnel pour le CNN sur les grilles non structurées et formule la convolution par une combinaison linéaire d'opérateurs différentiels.
"Prediction is arguably one of the most basic functions of an intelligent system.In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult.However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up.To exploit this, we decouple visual prediction from a rigid notion of time.While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable ""bottleneck"" frames no matter when they occur.We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks.Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.","[0, 0, 0, 0, 0, 0, 1]",[],SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,"Dans les tâches de prédiction visuelle, laisser votre modèle prédictif choisir les moments à prédire a deux effets : (i) améliorer la qualité de la prédiction, et (ii) conduire à des prédictions sémantiquement cohérentes ""d'état de goulot d'étranglement"", qui sont utiles pour la planification."
"Prediction is arguably one of the most basic functions of an intelligent system.In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult.However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up.To exploit this, we decouple visual prediction from a rigid notion of time.While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable ""bottleneck"" frames no matter when they occur.We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks.Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.","[0, 0, 0, 0, 0, 0, 1]",[],SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,"Procédé sur la prédiction d'images dans une vidéo, l'approche comprenant que la prédiction de la cible est flottante, résolue par un minimum sur l'erreur de prédiction."
"Prediction is arguably one of the most basic functions of an intelligent system.In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult.However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up.To exploit this, we decouple visual prediction from a rigid notion of time.While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable ""bottleneck"" frames no matter when they occur.We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks.Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.","[0, 0, 0, 0, 0, 0, 1]",[],SyzVb3CcFX,Time-Agnostic Prediction: Predicting Predictable Video Frames,"Reformule la tâche de prédiction/interpolation vidéo de manière à ce qu'un prédicteur ne soit pas obligé de générer des images à des intervalles de temps fixes, mais qu'il soit entraîné à générer des images qui se produisent à n'importe quel moment dans le futur."
"In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly.We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process.First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes.Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location.Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior.We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.","[0, 1, 0, 0, 0, 0]",[],ryBnUWb0b,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,"Nous avons utilisé un LSTM pour détecter quand un smartphone entre dans un bâtiment. Ensuite, nous prédisons le niveau de l'étage de l'appareil en utilisant les données des capteurs à bord du smartphone."
"In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly.We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process.First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes.Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location.Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior.We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.","[0, 1, 0, 0, 0, 0]",[],ryBnUWb0b,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,L'article présente un système permettant d'estimer le niveau d'un étage par le biais des données des capteurs de leur appareil mobile en utilisant un LSTM et les changements de la pression barométrique.
"In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly.We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process.First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes.Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location.Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior.We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.","[0, 1, 0, 0, 0, 0]",[],ryBnUWb0b,Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data,Proposition d'une méthode en deux étapes pour déterminer à quel étage se trouve un téléphone mobile à l'intérieur d'un grand immeuble.
"Sparse reward is one of the most challenging problems in reinforcement learning (RL).Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals.Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation.We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation.We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn.We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons.We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.","[0, 0, 0, 0, 0, 1, 0]",[],HyM8V2A9Km,ACTRCE: Augmenting Experience via Teacher’s Advice,Combiner la représentation des objectifs du langage avec des reconstitutions d'expériences a posteriori.
"Sparse reward is one of the most challenging problems in reinforcement learning (RL).Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals.Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation.We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation.We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn.We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons.We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.","[0, 0, 0, 0, 0, 1, 0]",[],HyM8V2A9Km,ACTRCE: Augmenting Experience via Teacher’s Advice,"Cet article considère l'hypothèse implicite dans le retour d'expérience a posteriori, à savoir qu'il est possible d'accéder à une correspondance entre les états et les objectifs, et propose une représentation des objectifs en langage naturel."
"Sparse reward is one of the most challenging problems in reinforcement learning (RL).Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals.Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation.We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation.We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn.We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons.We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.","[0, 0, 0, 0, 0, 1, 0]",[],HyM8V2A9Km,ACTRCE: Augmenting Experience via Teacher’s Advice,Cette soumission utilise le cadre Hindsight Experience Replay avec des objectifs en langage naturel pour améliorer l'efficacité de l'échantillonnage des modèles de suivi des instructions.
"Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval.Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected.Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features.Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances.This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances.Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost.This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.","[0, 0, 0, 0, 0, 1, 0]",[],B1e0KsRcYQ,Efficient Codebook and Factorization for Second Order Representation Learning,Nous proposons un codebook conjoint et un schéma de factorisation pour améliorer la mise en commun du second ordre.
"Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval.Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected.Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features.Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances.This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances.Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost.This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.","[0, 0, 0, 0, 0, 1, 0]",[],B1e0KsRcYQ,Efficient Codebook and Factorization for Second Order Representation Learning,Cet article présente un moyen de combiner les représentations factorisées du second ordre existantes avec une affectation difficile de type codebook.
"Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval.Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected.Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features.Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances.This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances.Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost.This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.","[0, 0, 0, 0, 0, 1, 0]",[],B1e0KsRcYQ,Efficient Codebook and Factorization for Second Order Representation Learning,"Proposition d'une nouvelle représentation bilinéaire basée sur un modèle de livre de codes, et une formulation efficace dans laquelle les projections basées sur le livre de codes sont factorisées via une projection partagée pour réduire davantage la taille des paramètres."
"Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.Such models often outperform their simpler counterparts significantly.However, their performance relies on the availability of large amounts of labeled data, which are rarely available.To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.Finally, we discuss the optimal setting for applying weak supervision using this methodology.","[0, 0, 0, 0, 1, 0, 0]",[],rygDeZqap7,Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction,"Nous proposons et appliquons une méthodologie de méta-apprentissage basée sur la supervision faible, pour combiner l'apprentissage semi-supervisé et d'ensemble sur la tâche d'extraction de relations biomédicales."
"Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.Such models often outperform their simpler counterparts significantly.However, their performance relies on the availability of large amounts of labeled data, which are rarely available.To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.Finally, we discuss the optimal setting for applying weak supervision using this methodology.","[0, 0, 0, 0, 1, 0, 0]",[],rygDeZqap7,Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction,"Une méthode semi-supervisée pour la classification des relations, qui forme plusieurs apprenants de base en utilisant un petit ensemble de données étiquetées et applique certains d'entre eux pour annoter des exemples non étiquetés pour l'apprentissage semi-supervisé."
"Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.Such models often outperform their simpler counterparts significantly.However, their performance relies on the availability of large amounts of labeled data, which are rarely available.To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.Finally, we discuss the optimal setting for applying weak supervision using this methodology.","[0, 0, 0, 0, 1, 0, 0]",[],rygDeZqap7,Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction,"Cet article aborde le problème de la génération de données d'entraînement pour l'extraction de relations biologiques, et utilise les prédictions de données étiquetées par des classificateurs faibles comme données d'entraînement supplémentaires pour un algorithme de méta apprentissage."
"Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.Such models often outperform their simpler counterparts significantly.However, their performance relies on the availability of large amounts of labeled data, which are rarely available.To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.Finally, we discuss the optimal setting for applying weak supervision using this methodology.","[0, 0, 0, 0, 1, 0, 0]",[],rygDeZqap7,Semi-supervised Ensemble Learning with Weak Supervision for Biomedical Relationship Extraction,Cet article propose une combinaison d'apprentissage semi-supervisé et d'apprentissage d'ensemble pour l'extraction d'informations. Des expériences ont été menées sur une tâche d'extraction de relations biomédicales.
"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations.CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations.Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly.Our approach offers two major advantages:(i) for each prediction, valid instance-specific explanations are generated with no computational overhead and(ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings.We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations.Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","[0, 1, 0, 0, 0, 0, 0, 0]",[],HJUOHGWRb,Contextual Explanation Networks,Une classe de réseaux qui génèrent des modèles simples à la volée (appelés explications) qui agissent comme un régularisateur et permettent des diagnostics et une interprétabilité cohérents des modèles.
"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations.CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations.Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly.Our approach offers two major advantages:(i) for each prediction, valid instance-specific explanations are generated with no computational overhead and(ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings.We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations.Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","[0, 1, 0, 0, 0, 0, 0, 0]",[],HJUOHGWRb,Contextual Explanation Networks,"Les auteurs affirment que l'art antérieur intègre directement les réseaux neuronaux dans les modèles graphiques en tant que composants, ce qui rend les modèles ininterprétables."
"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations.CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations.Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly.Our approach offers two major advantages:(i) for each prediction, valid instance-specific explanations are generated with no computational overhead and(ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings.We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations.Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","[0, 1, 0, 0, 0, 0, 0, 0]",[],HJUOHGWRb,Contextual Explanation Networks,Proposition d'une combinaison de réseaux neuronaux et de modèles graphiques en utilisant un réseau neuronal profond pour prédire les paramètres d'un modèle graphique.
"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations.Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.However, GAIL requires a huge number of interactions with environment during training.We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.To this end, we propose a model free, off-policy IL algorithm for continuous control.The keys of our algorithm are two folds:1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations.Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJ3fy0k0Z,Deterministic Policy Imitation Gradient Algorithm,"Nous proposons un algorithme d'apprentissage par imitation sans modèle qui est capable de réduire le nombre d'interactions avec l'environnement par rapport à l'algorithme d'apprentissage par imitation le plus avancé, à savoir GAIL."
"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations.Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.However, GAIL requires a huge number of interactions with environment during training.We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.To this end, we propose a model free, off-policy IL algorithm for continuous control.The keys of our algorithm are two folds:1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations.Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJ3fy0k0Z,Deterministic Policy Imitation Gradient Algorithm,"Propose d'étendre l'algorithme déterministe de gradient de politique pour apprendre à partir de démonstrations, tout en le combinant avec un type d'estimation de densité de l'expert."
"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations.Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.However, GAIL requires a huge number of interactions with environment during training.We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.To this end, we propose a model free, off-policy IL algorithm for continuous control.The keys of our algorithm are two folds:1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations.Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJ3fy0k0Z,Deterministic Policy Imitation Gradient Algorithm,Cet article considère le problème de l'apprentissage par imitation sans modèle et propose une extension de l'algorithme génératif d'apprentissage par imitation adversariale en remplaçant la politique stochastique de l'apprenant par une politique déterministe.
"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations.Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.However, GAIL requires a huge number of interactions with environment during training.We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.To this end, we propose a model free, off-policy IL algorithm for continuous control.The keys of our algorithm are two folds:1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations.Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],rJ3fy0k0Z,Deterministic Policy Imitation Gradient Algorithm,"L'article combine l'IRL, la formation contradictoire et les idées des gradients de politique déterministe dans le but de réduire la complexité de l'échantillon."
"Convolution acts as a local feature extractor in convolutional neural networks (CNNs).However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.The outputs are the weighted sum of these filters’ outputs, extraction of both vertex features and strength of correlation between vertices.Itcan be used with both directed and undirected graphs.The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a numberof data sets.As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H113pWZRb,Topology Adaptive Graph Convolutional  Networks,Graphique CNN à faible complexité de calcul (sans approximation) avec une meilleure précision de classification.
"Convolution acts as a local feature extractor in convolutional neural networks (CNNs).However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.The outputs are the weighted sum of these filters’ outputs, extraction of both vertex features and strength of correlation between vertices.Itcan be used with both directed and undirected graphs.The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a numberof data sets.As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H113pWZRb,Topology Adaptive Graph Convolutional  Networks,Propose une nouvelle approche CNN pour la classification des graphes en utilisant un filtre basé sur des marches sortantes de longueur croissante pour incorporer des informations provenant de sommets plus éloignés en une étape de propagation.
"Convolution acts as a local feature extractor in convolutional neural networks (CNNs).However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.The outputs are the weighted sum of these filters’ outputs, extraction of both vertex features and strength of correlation between vertices.Itcan be used with both directed and undirected graphs.The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a numberof data sets.As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H113pWZRb,Topology Adaptive Graph Convolutional  Networks,"Proposition d'une nouvelle architecture de réseau neuronal pour la classification semi-supervisée de graphes, basée sur des filtres polynomiaux de graphes et leur utilisation sur des couches successives de réseau neuronal avec des fonctions d'activation ReLU."
"Convolution acts as a local feature extractor in convolutional neural networks (CNNs).However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.The outputs are the weighted sum of these filters’ outputs, extraction of both vertex features and strength of correlation between vertices.Itcan be used with both directed and undirected graphs.The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a numberof data sets.As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H113pWZRb,Topology Adaptive Graph Convolutional  Networks,L'article présente le GCN adaptatif à la topologie pour généraliser les réseaux convolutifs aux données structurées par des graphes.
"Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.Across several benchmark data sets, we find that:(i) certain examples are forgotten with high frequency, and some not at all;(ii) a data set's (un)forgettable examples generalize across neural architectures; and(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.","[0, 0, 0, 0, 0, 0, 1]",[],BJlxm30cKm,An Empirical Study of Example Forgetting during Deep Neural Network Learning,Nous montrons que l'oubli catastrophique se produit dans ce qui est considéré comme une tâche unique et nous constatons que les exemples qui ne sont pas sujets à l'oubli peuvent être retirés de l'ensemble d'entraînement sans perte de généralisation.
"Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.Across several benchmark data sets, we find that:(i) certain examples are forgotten with high frequency, and some not at all;(ii) a data set's (un)forgettable examples generalize across neural architectures; and(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.","[0, 0, 0, 0, 0, 0, 1]",[],BJlxm30cKm,An Empirical Study of Example Forgetting during Deep Neural Network Learning,"Étudie le comportement d'oubli des exemples de formation pendant le SGD, et montre qu'il existe des ""exemples de soutien"" dans la formation des réseaux neuronaux à travers différentes architectures de réseau."
"Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.Across several benchmark data sets, we find that:(i) certain examples are forgotten with high frequency, and some not at all;(ii) a data set's (un)forgettable examples generalize across neural architectures; and(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.","[0, 0, 0, 0, 0, 0, 1]",[],BJlxm30cKm,An Empirical Study of Example Forgetting during Deep Neural Network Learning,"Cet article analyse la mesure dans laquelle les réseaux apprennent à classer correctement des exemples spécifiques, puis oublient ces exemples au cours de la formation."
"Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.Across several benchmark data sets, we find that:(i) certain examples are forgotten with high frequency, and some not at all;(ii) a data set's (un)forgettable examples generalize across neural architectures; and(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.","[0, 0, 0, 0, 0, 0, 1]",[],BJlxm30cKm,An Empirical Study of Example Forgetting during Deep Neural Network Learning,L'article étudie si certains exemples dans la formation des réseaux neuronaux sont plus difficiles à apprendre que d'autres. Ces exemples sont oubliés et réappris plusieurs fois au cours de l'apprentissage.
"Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1g6XnCcKQ,Object-Contrastive Networks: Unsupervised Object Representations,Une approche non supervisée pour apprendre des représentations démêlées d'objets entièrement à partir de vidéos monoculaires non étiquetées.
"Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1g6XnCcKQ,Object-Contrastive Networks: Unsupervised Object Representations,Conçoit une représentation des caractéristiques à partir de séquences vidéo capturées dans une scène depuis différents points de vue.
"Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1g6XnCcKQ,Object-Contrastive Networks: Unsupervised Object Representations,Proposition d'une méthode d'apprentissage de représentation non supervisée pour les entrées visuelles qui incorpore une approche d'apprentissage métrique rapprochant les paires de parcelles d'images les plus proches dans l'espace d'intégration tout en écartant les autres paires.
"Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1g6XnCcKQ,Object-Contrastive Networks: Unsupervised Object Representations,"Cet article explore l'apprentissage auto-supervisé des représentations d'objets, avec l'idée principale d'encourager les objets ayant des caractéristiques similaires à être davantage â€˜attraitsâ€™ les uns des autres."
"Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.","[0, 0, 0, 1, 0]",[],r1lFYoRcFm,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,"Nous entraînons avec des récompenses de vecteurs alignés sur l'état un agent prédisant les changements d'état à partir de distributions d'actions, en utilisant une nouvelle technique d'apprentissage par renforcement inspirée de la régression quantile."
"Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.","[0, 0, 0, 1, 0]",[],r1lFYoRcFm,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,Présente un algorithme qui vise à accélérer l'apprentissage par renforcement dans les situations où la récompense est alignée avec l'espace d'état. 
"Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.","[0, 0, 0, 1, 0]",[],r1lFYoRcFm,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,"Cet article traite de la RL dans l'espace d'action continu, en utilisant une politique reparamétrée et un nouvel objectif de formation basé sur les vecteurs."
"Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards.","[0, 0, 0, 1, 0]",[],r1lFYoRcFm,Quantile Regression Reinforcement Learning with State Aligned Vector Rewards,"Ce travail propose de mélanger la RL distributionnelle avec un réseau chargé de modéliser l'évolution du monde en termes de quantiles, en revendiquant des améliorations de l'efficacité de l'échantillon."
"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","[1, 0, 0, 0]",[],BJvWjcgAZ,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,"Nous proposons la mise à jour arrière épisodique, un nouvel algorithme d'apprentissage par renforcement profond qui échantillonne les transitions épisode par épisode et met à jour les valeurs de manière récursive et à rebours pour obtenir un apprentissage rapide et stable."
"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","[1, 0, 0, 0]",[],BJvWjcgAZ,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,Propose un nouveau DQN où les cibles sont calculées sur un épisode complet par une mise à jour en arrière (de la fin au début) pour une propagation plus rapide des récompenses à la fin de l'épisode.
"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","[1, 0, 0, 0]",[],BJvWjcgAZ,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,Les auteurs proposent de modifier l'algorithme DQN en appliquant l'opérateur max de Bellman de manière récursive sur une trajectoire avec une certaine décroissance pour éviter l'accumulation d'erreurs avec le max imbriqué.
"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","[1, 0, 0, 0]",[],BJvWjcgAZ,Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,"Dans les réseaux à Q profond, la mise à jour des valeurs de Q commence à la fin de l'épisode afin de faciliter la propagation rapide des récompenses le long de l'épisode."
"Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc.Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events.This leads to the nonidentifiability of the event times’ distribution parameters, which makes the problem significantly more challenging.In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks.Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks.Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0]",[],HkjL6MiTb,Siamese Survival Analysis with Competing Risks,"Dans ce travail, nous présentons une nouvelle architecture de réseau neuronal profond siamois capable d'apprendre efficacement des données en présence de multiples événements indésirables."
"Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc.Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events.This leads to the nonidentifiability of the event times’ distribution parameters, which makes the problem significantly more challenging.In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks.Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks.Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0]",[],HkjL6MiTb,Siamese Survival Analysis with Competing Risks,Cet article introduit les réseaux neuronaux siamois dans le cadre des risques concurrents en optimisant l'indice c directement.
"Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc.Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events.This leads to the nonidentifiability of the event times’ distribution parameters, which makes the problem significantly more challenging.In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks.Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks.Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0]",[],HkjL6MiTb,Siamese Survival Analysis with Competing Risks,Les auteurs abordent les problèmes d'estimation du risque dans un contexte d'analyse de survie avec des risques concurrents et proposent d'optimiser directement l'indice de discrimination dépendant du temps en utilisant un réseau de survie siamois.
"The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables.However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables.We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query.We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction.Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary.We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model.We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning.","[0, 0, 1, 0, 0, 0, 0]",[],BkUDW_lCb,Pointing Out SQL Queries From Text,Nous présentons un modèle de réseau de pointeurs basé sur le type ainsi qu'une méthode de perte basée sur la valeur pour entraîner efficacement un modèle neuronal afin de traduire le langage naturel en SQL.
"The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables.However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables.We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query.We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction.Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary.We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model.We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning.","[0, 0, 1, 0, 0, 0, 0]",[],BkUDW_lCb,Pointing Out SQL Queries From Text,L'article prétend développer une nouvelle méthode pour convertir les requêtes en langage naturel en SQL en utilisant une grammaire pour guider le décodage et en utilisant une nouvelle fonction de perte pour le mécanisme de pointeur/copie.
"To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space.Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.Python code for reproducible research is publicly available.","[0, 0, 0, 1, 0]",[],S1lg0jAcYm,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Un estimateur de gradient sans biais et à faible variance pour les modèles de variables latentes discrètes.
"To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space.Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.Python code for reproducible research is publicly available.","[0, 0, 0, 1, 0]",[],S1lg0jAcYm,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Propose une nouvelle technique de réduction de la variance à utiliser lors du calcul d'un gradient de perte attendue où l'attente est par rapport à des variables aléatoires binaires indépendantes.
"To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space.Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.Python code for reproducible research is publicly available.","[0, 0, 0, 1, 0]",[],S1lg0jAcYm,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Un algorithme combinant Rao-Blackwellization et nombres aléatoires communs pour réduire la variance de l'estimateur du gradient de la fonction de score dans le cas particulier des réseaux binaires stochastiques.
"To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric ""self-control"" baseline function together with the REINFORCE estimator in that augmented space.Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.Python code for reproducible research is publicly available.","[0, 0, 0, 1, 0]",[],S1lg0jAcYm,ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks,Un estimateur sans biais et à faible variance augment-REINFORCE-merge (ARM) pour le calcul et la rétropropagation des gradients dans les réseaux neuronaux binaires.
"Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.To overcome this communication bottleneck recent works propose to reduce the communication frequency.An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.This scheme shows promising results in practice, but eluded thorough theoretical analysis.    We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.This also holds for asynchronous implementations.Local SGD can also be used for large scale training of deep learning models.The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1g2JnRcFX,Local SGD Converges Fast and Communicates Little,Nous prouvons que la SGD locale parallèle atteint une vitesse linéaire avec beaucoup moins de communication que la SGD parallèle par mini-lots.
"Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.To overcome this communication bottleneck recent works propose to reduce the communication frequency.An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.This scheme shows promising results in practice, but eluded thorough theoretical analysis.    We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.This also holds for asynchronous implementations.Local SGD can also be used for large scale training of deep learning models.The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1g2JnRcFX,Local SGD Converges Fast and Communicates Little,"Fournit une preuve de convergence pour le SGD local, et prouve que le SGD local peut fournir les mêmes gains de vitesse que le minibatch, mais peut être capable de communiquer beaucoup moins."
"Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.To overcome this communication bottleneck recent works propose to reduce the communication frequency.An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.This scheme shows promising results in practice, but eluded thorough theoretical analysis.    We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.This also holds for asynchronous implementations.Local SGD can also be used for large scale training of deep learning models.The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1g2JnRcFX,Local SGD Converges Fast and Communicates Little,Cet article présente une analyse de la SGD locale et des limites sur la fréquence à laquelle les estimateurs obtenus en exécutant la SGD doivent être moyennés afin de produire des accélérations de parallélisation linéaires.
"Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.To overcome this communication bottleneck recent works propose to reduce the communication frequency.An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.This scheme shows promising results in practice, but eluded thorough theoretical analysis.    We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.This also holds for asynchronous implementations.Local SGD can also be used for large scale training of deep learning models.The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1g2JnRcFX,Local SGD Converges Fast and Communicates Little,"Les auteurs analysent l'algorithme SGD local, où $K$ chaînes parallèles de SGD sont exécutées, et les itérés sont occasionnellement synchronisés entre les machines en faisant la moyenne"
"Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems.The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams.Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions.We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions.To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation.We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded.We provide theoretical guarantees concerning the convergence of the proposed learning algorithm.To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters.Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems.Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,Perception compacte du processus dynamique
"Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems.The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams.Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions.We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions.To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation.We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded.We provide theoretical guarantees concerning the convergence of the proposed learning algorithm.To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters.Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems.Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,Etudie le problème de la représentation compacte du modèle d'un système dynamique complexe tout en préservant l'information en utilisant une méthode de goulot d'information.
"Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems.The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams.Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions.We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions.To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation.We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded.We provide theoretical guarantees concerning the convergence of the proposed learning algorithm.To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters.Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems.Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rJgTciR9tm,Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy,Cet article étudie la dynamique linéaire gaussienne et propose un algorithme pour calculer la hiérarchie des goulets d'information (IBH).
"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly.As the density of the connection increases, the number of paths through which the gradient flows can be increased.It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time.Larger gradients, however, can also cause exploding gradient problem.To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows.We describe the relation between the attention gate and the gradient flows by approximation.The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","[1, 0, 0, 0, 0, 0, 0]",[],rJVruWZRW,Dense Recurrent Neural Network with Attention Gate,RNN dense qui possède des connexions complètes de chaque état caché à plusieurs états cachés précédents de toutes les couches directement.
"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly.As the density of the connection increases, the number of paths through which the gradient flows can be increased.It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time.Larger gradients, however, can also cause exploding gradient problem.To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows.We describe the relation between the attention gate and the gradient flows by approximation.The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","[1, 0, 0, 0, 0, 0, 0]",[],rJVruWZRW,Dense Recurrent Neural Network with Attention Gate,"Propose une nouvelle architecture RNN qui modélise mieux les dépendances à long terme, peut apprendre la représentation multi-échelle des données séquentielles et contourne le problème des gradients en utilisant des unités de déclenchement paramétrées."
"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly.As the density of the connection increases, the number of paths through which the gradient flows can be increased.It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time.Larger gradients, however, can also cause exploding gradient problem.To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows.We describe the relation between the attention gate and the gradient flows by approximation.The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","[1, 0, 0, 0, 0, 0, 0]",[],rJVruWZRW,Dense Recurrent Neural Network with Attention Gate,"Cet article propose une architecture RNN dense entièrement connectée avec des connexions gated à chaque couche et des connexions aux couches précédentes, et ses résultats sur la tâche de modélisation au niveau du personnage de la PTB."
"We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs).In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose.Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code.Corresponding samples from the real dataset consist of two distinct photographs of the same subject.In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person.We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training.Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.","[1, 0, 0, 0, 0, 0, 0]",[],S1nQvfgA-,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"Les SD-GAN démêlent les codes latents en fonction des points communs connus dans un ensemble de données (par exemple, des photographies représentant la même personne)."
"We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs).In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose.Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code.Corresponding samples from the real dataset consist of two distinct photographs of the same subject.In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person.We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training.Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.","[1, 0, 0, 0, 0, 0, 0]",[],S1nQvfgA-,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,Cet article étudie le problème de la génération d'images contrôlées et propose un algorithme qui produit une paire d'images ayant la même identité.
"We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs).In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose.Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code.Corresponding samples from the real dataset consist of two distinct photographs of the same subject.In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person.We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training.Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.","[1, 0, 0, 0, 0, 0, 0]",[],S1nQvfgA-,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"Cet article propose le SD-GAN, une méthode d'entraînement des GAN permettant de démêler les informations d'identité et de non-identité dans le vecteur latent d'entrée Z."
"The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains.While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017].In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains.In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods.Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions.Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.","[0, 0, 0, 0, 1, 0, 0]",[],S1lNELLKuN,AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows,"Nous proposons un cadre d'apprentissage pour les traductions interdomaines qui est exactement cohérent avec les cycles et qui peut être appris par apprentissage contradictoire, par estimation du maximum de vraisemblance ou par une méthode hybride."
"The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains.While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017].In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains.In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods.Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions.Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.","[0, 0, 0, 0, 1, 0, 0]",[],S1lNELLKuN,AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows,"Propose AlignFlow, un moyen efficace de mettre en œuvre le principe de cohérence des cycles en utilisant des flux inversibles."
"The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains.While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017].In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains.In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods.Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions.Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets.","[0, 0, 0, 0, 1, 0, 0]",[],S1lNELLKuN,AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows,Modèles de flux pour la traduction d'image à image non appariée
"Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.","[0, 0, 0, 1, 0, 0]",[],B1CQGfZ0b,Learning to select examples for program synthesis,"Dans un contexte de synthèse de programme où l'entrée est un ensemble d'exemples, nous réduisons le coût en calculant un sous-ensemble d'exemples représentatifs."
"Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.","[0, 0, 0, 1, 0, 0]",[],B1CQGfZ0b,Learning to select examples for program synthesis,Propose une méthode d'identification d'exemples représentatifs pour la synthèse de programmes afin d'augmenter l'évolutivité des solutions de programmation par contraintes existantes.
"Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.","[0, 0, 0, 1, 0, 0]",[],B1CQGfZ0b,Learning to select examples for program synthesis,Une méthode pour choisir un sous-ensemble d'exemples sur lequel exécuter un solveur de contraintes afin de résoudre des problèmes de synthèse de programmes.
"Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver.","[0, 0, 0, 1, 0, 0]",[],B1CQGfZ0b,Learning to select examples for program synthesis,Cet article propose une méthode pour accélérer les synthétiseurs de programmes à usage général.
"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","[0, 0, 0, 1, 0, 0]",[],SkJKHMW0Z,Recurrent Relational Networks for complex relational reasoning,"Nous présentons les réseaux relationnels récurrents, un module de réseau neuronal puissant et général pour le raisonnement relationnel, et l'utilisons pour résoudre 96,6 % des Sudokus les plus difficiles et 19/20 des tâches BaBi."
"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","[0, 0, 0, 1, 0, 0]",[],SkJKHMW0Z,Recurrent Relational Networks for complex relational reasoning,Introduction d'un réseau relationnel récurrent (RRN) qui peut être ajouté à n'importe quel réseau neuronal pour ajouter une capacité de raisonnement relationnel.
"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","[0, 0, 0, 1, 0, 0]",[],SkJKHMW0Z,Recurrent Relational Networks for complex relational reasoning,Présentation d'un réseau neuronal profond pour la prédiction structurée qui obtient des performances de pointe sur les puzzles Soduku et la tâche BaBi.
"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","[0, 0, 0, 1, 0, 0]",[],SkJKHMW0Z,Recurrent Relational Networks for complex relational reasoning,Cet article décrit une méthode appelée réseau relationnel pour ajouter une capacité de raisonnement relationnel aux réseaux neuronaux profonds.
"Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification.In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors.These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data.Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent.Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.","[0, 0, 0, 0, 0, 1]",[],B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,"Trois classes de prieurs sont tout ce dont vous avez besoin pour entraîner des modèles profonds à partir de données U uniquement, alors que deux ne devraient pas suffire."
"Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification.In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors.These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data.Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent.Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.","[0, 0, 0, 0, 0, 1]",[],B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,Propose un estimateur sans biais qui permet d'entraîner des modèles avec une faible supervision sur deux ensembles de données non étiquetées avec des prieurs de classe connus et discute des propriétés théoriques des estimateurs.
"Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification.In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors.These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data.Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent.Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.","[0, 0, 0, 0, 0, 1]",[],B1xWcj0qYm,On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data,"Une méthodologie pour l'entraînement de n'importe quel classificateur binaire à partir de données non étiquetées seulement, et une méthode de minimisation du risque empirique pour deux ensembles de données non étiquetées où les priorités de classe sont données."
"Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions.We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain.Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering.This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features).The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification.Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts.This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.","[0, 0, 0, 1, 0, 0, 0]",[],SkfMWhAqYQ,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,"L'agrégation des preuves de classe à partir de nombreux petits patchs d'image suffit à résoudre ImageNet, donne des modèles plus interprétables et peut expliquer certains aspects de la prise de décision des DNN populaires."
"Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions.We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain.Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering.This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features).The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification.Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts.This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.","[0, 0, 0, 1, 0, 0, 0]",[],SkfMWhAqYQ,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,Cet article propose une architecture de réseau neuronal nouvelle et compacte qui utilise les informations contenues dans les caractéristiques des sacs de mots. L'algorithme proposé n'utilise que les informations du patch de manière indépendante et effectue un vote majoritaire en utilisant des patchs classés de manière indépendante.
"Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.This method introduces a new domain within bioinformatics and personalized medicine – somatic whole genome mutation calling for liquid biopsy.","[0, 0, 0, 1, 0, 0, 0]",[],H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy," Les méthodes actuelles de mutation somatique ne fonctionnent pas avec les biopsies liquides (c'est-à-dire le séquençage à faible couverture), nous appliquons une architecture CNN à une représentation unique d'une lecture et de son ailgnment, nous montrons une amélioration significative par rapport aux méthodes précédentes dans le cadre de la faible fréquence."
"Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.This method introduces a new domain within bioinformatics and personalized medicine – somatic whole genome mutation calling for liquid biopsy.","[0, 0, 0, 1, 0, 0, 0]",[],H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,Propose une solution basée sur CNN appelée Kittyhawk pour l'appel de mutation somatique à des fréquences alléliques ultra basses.
"Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.This method introduces a new domain within bioinformatics and personalized medicine – somatic whole genome mutation calling for liquid biopsy.","[0, 0, 0, 1, 0, 0, 0]",[],H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,Un nouvel algorithme pour détecter les mutations cancéreuses à partir du séquençage de l'ADN libre de cellules qui identifiera le contexte de séquence qui caractérise les erreurs de séquençage des véritables mutations.
"Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.This method introduces a new domain within bioinformatics and personalized medicine – somatic whole genome mutation calling for liquid biopsy.","[0, 0, 0, 1, 0, 0, 0]",[],H1DkN7ZCZ,Deep learning mutation prediction enables early stage lung cancer detection in liquid biopsy,"Cet article propose un cadre d'apprentissage profond pour prédire les mutations somatiques à des fréquences extrêmement faibles, ce qui se produit lors de la détection de tumeurs à partir d'ADN libre."
"This paper presents the formal release of {\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts.What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval.To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.","[1, 0, 0, 0]",[],SylxCx5pTQ,MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts,L'article présente un nouveau corpus de référence (gold-standard corpus) de littérature scientifique biomédicale annotée manuellement avec des mentions de concepts UMLS.
"This paper presents the formal release of {\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts.What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval.To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.","[1, 0, 0, 0]",[],SylxCx5pTQ,MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts,"Détaille la construction d'un ensemble de données annotées manuellement, couvrant des concepts biomédicaux, qui est plus grand et couvert par une ontologie plus vaste que les ensembles de données précédents."
"This paper presents the formal release of {\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts.What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval.To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.","[1, 0, 0, 0]",[],SylxCx5pTQ,MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts,"Cet article utilise MedMentions, un modèle semi-Markov de TaggerOne pour la reconnaissance et la liaison de concepts de bout en bout sur un ensemble de résumés Pubmed pour étiqueter les articles avec des concepts/entités biomédicaux."
"In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.A clustering network transforms the data into another space and then selects one of the clusters.Next, the autoencoder associated with this cluster is used to reconstruct the data-point.The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJg_fnRqF7,Deep clustering based on a mixture of autoencoders,"Nous proposons une méthode de clustering profond où, au lieu d'un centroïde, chaque cluster est représenté par un autoencodeur."
"In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.A clustering network transforms the data into another space and then selects one of the clusters.Next, the autoencoder associated with this cluster is used to reconstruct the data-point.The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJg_fnRqF7,Deep clustering based on a mixture of autoencoders,"Présente le clustering profond basé sur un mélange d'autoencodeurs, où les points de données sont alloués à un cluster en fonction de l'erreur de représentation si le réseau autoencodeur était utilisé pour le représenter."
"In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.A clustering network transforms the data into another space and then selects one of the clusters.Next, the autoencoder associated with this cluster is used to reconstruct the data-point.The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJg_fnRqF7,Deep clustering based on a mixture of autoencoders,Une approche de regroupement profond qui utilise un cadre d'autoencodeur pour apprendre une intégration à faible dimension des données simultanément au regroupement des données à l'aide d'un réseau neuronal profond.
"In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.A clustering network transforms the data into another space and then selects one of the clusters.Next, the autoencoder associated with this cluster is used to reconstruct the data-point.The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJg_fnRqF7,Deep clustering based on a mixture of autoencoders,"Une méthode de clustering profond qui représente chaque cluster avec différents auto-encodeurs, fonctionne de bout en bout et peut également être utilisée pour regrouper de nouvelles données entrantes sans refaire toute la procédure de clustering."
"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SJA7xfb0b,Sobolev GAN,Nous définissons une nouvelle métrique de probabilité intégrale (IPM de Sobolev) et montrons comment elle peut être utilisée pour l'entraînement de GANs pour la génération de textes et l'apprentissage semi-supervisé.
"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SJA7xfb0b,Sobolev GAN,"Suggère un nouveau schéma de régularisation pour les GANs basé sur une norme de Sobolev, mesurant les déviations entre les normes L2 des dérivés."
"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SJA7xfb0b,Sobolev GAN,"Les auteurs proposent un autre type de GAN utilisant la configuration typique d'un GAN mais avec une classe de fonctions différente, et produisent une recette pour l'entraînement des GANs avec ce type de classe de fonctions."
"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SJA7xfb0b,Sobolev GAN,L'article propose une pénalité de gradient différente pour les critiques GAN qui oblige la norme quadratique attendue du gradient à être égale à 1.
"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.We term our method Mixture Generative Adversarial Nets (MGAN).We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem.By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkmu5b0a-,MGAN: Training Generative Adversarial Nets with Multiple Generators,Nous proposons une nouvelle approche pour entraîner les GAN avec un mélange de générateurs afin de surmonter le problème de l'effondrement des modes.
"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.We term our method Mixture Generative Adversarial Nets (MGAN).We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem.By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkmu5b0a-,MGAN: Training Generative Adversarial Nets with Multiple Generators,Aborder le problème de l'effondrement des modes dans les GAN en utilisant une distribution de mélange sous contrainte pour le générateur et un classificateur auxiliaire qui prédit la composante de mélange source.
"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.We term our method Mixture Generative Adversarial Nets (MGAN).We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem.By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkmu5b0a-,MGAN: Training Generative Adversarial Nets with Multiple Generators,L'article propose un mélange de générateurs pour entraîner les GAN sans coût de calcul supplémentaire.
"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.We term our method Mixture Generative Adversarial Nets (MGAN).We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem.By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkmu5b0a-,MGAN: Training Generative Adversarial Nets with Multiple Generators,"Les auteurs présentent l'utilisation du MGAN, qui vise à surmonter le problème de l'effondrement du modèle par des générateurs de mélange, et qui permet d'obtenir des résultats de pointe."
"Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts.We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty.Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English.The platform also provides a hand-crafted bot agent, which simulates a human teacher.  We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels.We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJeXCo0cYX,BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,Nous présentons la plateforme BabyAI pour étudier l'efficacité des données de l'apprentissage des langues avec un humain dans la boucle.
"Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts.We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty.Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English.The platform also provides a hand-crafted bot agent, which simulates a human teacher.  We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels.We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJeXCo0cYX,BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,Présente une plateforme de recherche avec un robot dans la boucle pour apprendre à exécuter des instructions linguistiques dans lesquelles le langage a des structures de composition.
"Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts.We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty.Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English.The platform also provides a hand-crafted bot agent, which simulates a human teacher.  We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels.We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJeXCo0cYX,BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,"Présentation d'une plateforme d'apprentissage des langues fondée sur des bases solides, qui remplace tout humain dans la boucle par un enseignant heuristique et utilise une langue synthétique cartographiée dans un monde quadrillé en 2D."
"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance.However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both.In this paper, we propose a novel algorithm that jointly learns and compresses a neural network.The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty.We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints.Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches.","[0, 0, 0, 0, 0, 1]",[],HkinqfbAb,Automatic Parameter Tying in Neural Networks,Un antécédent k-means combiné à une régularisation L1 donne des résultats de compression de pointe.
"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance.However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both.In this paper, we propose a novel algorithm that jointly learns and compresses a neural network.The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty.We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints.Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches.","[0, 0, 0, 0, 0, 1]",[],HkinqfbAb,Automatic Parameter Tying in Neural Networks,Cet article explore la compression et la liaison des paramètres souples des DNNs/CNNs.
"The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success.The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem.We show that naive application of the SVRG technique and related approaches fail, and explore why.","[0, 1, 0]",[],B1MIBs05F7,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,La méthode SVRG échoue dans les problèmes modernes d'apprentissage profond.
"The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success.The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem.We show that naive application of the SVRG technique and related approaches fail, and explore why.","[0, 1, 0]",[],B1MIBs05F7,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,"Cet article présente une analyse des méthodes de style SVRG, montrant que l'abandon, la norme de lot, l'augmentation des données (culture aléatoire/rotation/translations) ont tendance à augmenter le biais et/ou la variance des mises à jour."
"The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success.The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem.We show that naive application of the SVRG technique and related approaches fail, and explore why.","[0, 1, 0]",[],B1MIBs05F7,On the Ineffectiveness of Variance Reduced Optimization for Deep Learning,Cet article étudie l'applicabilité de la SVGD aux réseaux neuronaux modernes et montre que l'application naïve de la SVGD échoue généralement.
"The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks.One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces.In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks.We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.","[0, 0, 1, 0, 0]",[],rkeSiiA5Fm,Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution,Une méthode pour appliquer l'apprentissage profond à des surfaces 3D en utilisant leurs descripteurs sphériques et la convolution anisotropique alt-az sur 2-sphères.
"The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks.One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces.In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks.We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.","[0, 0, 1, 0, 0]",[],rkeSiiA5Fm,Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution,Présente un schéma de convolution anisotrope polaire sur une sphère unitaire en remplaçant la translation du filtre par sa rotation.
"The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks.One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces.In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks.We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval.","[0, 0, 1, 0, 0]",[],rkeSiiA5Fm,Deep Learning 3D Shapes Using Alt-az Anisotropic 2-Sphere Convolution,Cet article explore l'apprentissage profond des formes 3D en utilisant la convolution anisotropique alt-az à 2 sphères.
"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs.For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size.In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters.We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights.Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","[0, 0, 0, 1, 0, 0]",[],BySRH6CpW,Learning Discrete Weights Using the Local Reparameterization Trick,Entraînement de réseaux binaires/ternaires par reparamétrisation locale avec l'approximation CLT
"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs.For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size.In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters.We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights.Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","[0, 0, 0, 1, 0, 0]",[],BySRH6CpW,Learning Discrete Weights Using the Local Reparameterization Trick,Entraîne les réseaux de distribution de poids binaires et ternaires en utilisant la rétropropagation pour échantillonner les pré-activations des neurones avec l'astuce de reparamétrisation.
"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs.For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size.In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters.We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights.Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","[0, 0, 0, 1, 0, 0]",[],BySRH6CpW,Learning Discrete Weights Using the Local Reparameterization Trick,"Cet article propose d'utiliser des paramètres stochastiques en combinaison avec l'astuce de reparamétrisation locale pour entraîner des réseaux neuronaux avec des poids binaires ou ternaires, ce qui conduit à des résultats de pointe."
"We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\%$ WER and $4.5\%$ WER, respectively.","[1, 0, 0, 0, 0]",[],rkMW1hRqKX,Optimal Completion Distillation for Sequence Learning,La distillation de complétion optimale (OCD) est une procédure d'entraînement pour l'optimisation des modèles de séquence à séquence basée sur la distance d'édition qui permet d'atteindre l'état de l'art dans les tâches de reconnaissance de la parole de bout en bout.
"We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\%$ WER and $4.5\%$ WER, respectively.","[1, 0, 0, 0, 0]",[],rkMW1hRqKX,Optimal Completion Distillation for Sequence Learning,Approche alternative à l'entraînement des modèles seq2seq utilisant un programme dynamique pour calculer les continuations optimales des préfixes prédits
"We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\%$ WER and $4.5\%$ WER, respectively.","[1, 0, 0, 0, 0]",[],rkMW1hRqKX,Optimal Completion Distillation for Sequence Learning,Un algorithme d'apprentissage pour les modèles auto-régressifs qui ne nécessite pas de pré-entraînement MLE et peut directement optimiser à partir de l'échantillonnage.
"We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\%$ WER and $4.5\%$ WER, respectively.","[1, 0, 0, 0, 0]",[],rkMW1hRqKX,Optimal Completion Distillation for Sequence Learning,L'article examine un défaut des modèles de séquence à séquence entraînés en utilisant l'estimation du maximum de vraisemblance et propose une approche basée sur les distances d'édition et l'utilisation implicite de séquences d'étiquettes données pendant l'entraînement.
"As an emerging field, federated learning has recently attracted considerable attention.Compared to distributed learning in the datacenter setting, federated learninghas more strict constraints on computate efficiency of the learned model and communicationcost during the training process.In this work, we propose an efficientfederated learning framework based on variational dropout.Our approach is ableto jointly learn a sparse model while reducing the amount of gradients exchangedduring the iterative training process.We demonstrate the superior performanceof our approach on achieving significant model compression and communicationreduction ratios with no accuracy loss.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BkeAf2CqY7,Efficient Federated Learning via Variational Dropout,un modèle conjoint et une méthode de sparsification du gradient pour l'apprentissage fédéré
"As an emerging field, federated learning has recently attracted considerable attention.Compared to distributed learning in the datacenter setting, federated learninghas more strict constraints on computate efficiency of the learned model and communicationcost during the training process.In this work, we propose an efficientfederated learning framework based on variational dropout.Our approach is ableto jointly learn a sparse model while reducing the amount of gradients exchangedduring the iterative training process.We demonstrate the superior performanceof our approach on achieving significant model compression and communicationreduction ratios with no accuracy loss.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BkeAf2CqY7,Efficient Federated Learning via Variational Dropout,"Applique le dropout variationnel pour réduire le coût de communication de la formation distribuée des réseaux neuronaux, et fait des expériences sur les jeux de données mnist, cifar10 et svhn. "
"As an emerging field, federated learning has recently attracted considerable attention.Compared to distributed learning in the datacenter setting, federated learninghas more strict constraints on computate efficiency of the learned model and communicationcost during the training process.In this work, we propose an efficientfederated learning framework based on variational dropout.Our approach is ableto jointly learn a sparse model while reducing the amount of gradients exchangedduring the iterative training process.We demonstrate the superior performanceof our approach on achieving significant model compression and communicationreduction ratios with no accuracy loss.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BkeAf2CqY7,Efficient Federated Learning via Variational Dropout,Les auteurs proposent un algorithme qui réduit les coûts de communication dans l'apprentissage fédéré en envoyant des gradients épars du dispositif au serveur et inversement.
"As an emerging field, federated learning has recently attracted considerable attention.Compared to distributed learning in the datacenter setting, federated learninghas more strict constraints on computate efficiency of the learned model and communicationcost during the training process.In this work, we propose an efficientfederated learning framework based on variational dropout.Our approach is ableto jointly learn a sparse model while reducing the amount of gradients exchangedduring the iterative training process.We demonstrate the superior performanceof our approach on achieving significant model compression and communicationreduction ratios with no accuracy loss.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BkeAf2CqY7,Efficient Federated Learning via Variational Dropout,Combine l'algorithme d'optimisation distribuée avec le dropout variationnel pour sparifier les gradients envoyés au serveur maître par les apprenants locaux.
"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"".We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","[1, 0, 0, 0, 0, 0]",[],SksY3deAW,Learning Deep ResNet Blocks Sequentially using Boosting Theory,Nous prouvons une théorie de boosting multiclasse pour les architectures ResNet qui crée simultanément une nouvelle technique de boosting multiclasse et fournit un nouvel algorithme pour les architectures de style ResNet.
"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"".We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","[1, 0, 0, 0, 0, 0]",[],SksY3deAW,Learning Deep ResNet Blocks Sequentially using Boosting Theory,"Présente un algorithme de type boosting pour la formation de réseaux résiduels profonds, une analyse de convergence pour l'erreur de formation et une analyse de la capacité de généralisation."
"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"".We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","[1, 0, 0, 0, 0, 0]",[],SksY3deAW,Learning Deep ResNet Blocks Sequentially using Boosting Theory,Une méthode d'apprentissage pour ResNet utilisant le cadre de boosting qui décompose l'apprentissage de réseaux complexes et utilise moins de coûts de calcul.
"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"".We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","[1, 0, 0, 0, 0, 0]",[],SksY3deAW,Learning Deep ResNet Blocks Sequentially using Boosting Theory,"Les auteurs proposent le ResNet profond comme algorithme de boosting, et affirment qu'il est plus efficace que la rétropropagation standard de bout en bout."
"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \sigma(Bx) + \xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\xi$ is a noise vector.We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. 	Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:	1.All local minima of $G$ are also global minima.2.All global minima of $G$ correspond to the ground truth parameters.3.The value and gradient of $G$ can be estimated using samples.	With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters.We also prove finite sample complexity results and validate the results by simulations.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkwHObbRZ,Learning One-hidden-layer Neural Networks with Landscape Design,L'article analyse le paysage d'optimisation des réseaux neuronaux à une couche cachée et conçoit un nouvel objectif qui ne présente pas de minimum local parasite. 
"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \sigma(Bx) + \xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\xi$ is a noise vector.We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. 	Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:	1.All local minima of $G$ are also global minima.2.All global minima of $G$ correspond to the ground truth parameters.3.The value and gradient of $G$ can be estimated using samples.	With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters.We also prove finite sample complexity results and validate the results by simulations.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkwHObbRZ,Learning One-hidden-layer Neural Networks with Landscape Design,"Cet article étudie le problème de l'apprentissage des réseaux neuronaux à une couche cachée, établit un lien entre la perte de population par les moindres carrés et les polynômes d'Hermite, et propose une nouvelle fonction de perte."
"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \sigma(Bx) + \xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\xi$ is a noise vector.We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. 	Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:	1.All local minima of $G$ are also global minima.2.All global minima of $G$ correspond to the ground truth parameters.3.The value and gradient of $G$ can be estimated using samples.	With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters.We also prove finite sample complexity results and validate the results by simulations.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkwHObbRZ,Learning One-hidden-layer Neural Networks with Landscape Design,Une méthode de factorisation tensorielle pour l'apprentissage d'un réseau neuronal à une couche cachée.
"Open information extraction (OIE) systems extract relations and their  arguments from natural language text in an unsupervised manner.The resulting  extractions are a valuable resource for downstream tasks such as knowledge  base construction, open question answering, or event schema induction.In this  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was  extracted from the text of English Wikipedia.OPIEC complements the available  OIE resources: It is the largest OIE corpus publicly available to date (over  340M triples) and contains valuable metadata such as provenance information,  confidence scores, linguistic annotations, and semantic annotations including  spatial and temporal information.We analyze the OPIEC corpus by comparing its  content with knowledge bases such as DBpedia or YAGO, which are also based on  Wikipedia.We found that most of the facts between entities present in OPIEC  cannot be found in DBpedia and/or YAGO, that OIE facts   often differ in the level of specificity compared to knowledge base facts, and  that OIE open relations are generally highly polysemous.We believe that the  OPIEC corpus is a valuable resource for future research on automated knowledge  base construction.","[1, 0, 0, 0, 0, 0, 0]",[],HJxeGb5pTm,OPIEC: An Open Information Extraction Corpus,Un corpus ouvert d'extraction d'information et son analyse approfondie
"Open information extraction (OIE) systems extract relations and their  arguments from natural language text in an unsupervised manner.The resulting  extractions are a valuable resource for downstream tasks such as knowledge  base construction, open question answering, or event schema induction.In this  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was  extracted from the text of English Wikipedia.OPIEC complements the available  OIE resources: It is the largest OIE corpus publicly available to date (over  340M triples) and contains valuable metadata such as provenance information,  confidence scores, linguistic annotations, and semantic annotations including  spatial and temporal information.We analyze the OPIEC corpus by comparing its  content with knowledge bases such as DBpedia or YAGO, which are also based on  Wikipedia.We found that most of the facts between entities present in OPIEC  cannot be found in DBpedia and/or YAGO, that OIE facts   often differ in the level of specificity compared to knowledge base facts, and  that OIE open relations are generally highly polysemous.We believe that the  OPIEC corpus is a valuable resource for future research on automated knowledge  base construction.","[1, 0, 0, 0, 0, 0, 0]",[],HJxeGb5pTm,OPIEC: An Open Information Extraction Corpus,Construit un nouveau corpus pour l'extraction d'informations qui est plus grand que les corpus publics précédents et contient des informations qui n'existent pas dans les corpus actuels.
"Open information extraction (OIE) systems extract relations and their  arguments from natural language text in an unsupervised manner.The resulting  extractions are a valuable resource for downstream tasks such as knowledge  base construction, open question answering, or event schema induction.In this  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was  extracted from the text of English Wikipedia.OPIEC complements the available  OIE resources: It is the largest OIE corpus publicly available to date (over  340M triples) and contains valuable metadata such as provenance information,  confidence scores, linguistic annotations, and semantic annotations including  spatial and temporal information.We analyze the OPIEC corpus by comparing its  content with knowledge bases such as DBpedia or YAGO, which are also based on  Wikipedia.We found that most of the facts between entities present in OPIEC  cannot be found in DBpedia and/or YAGO, that OIE facts   often differ in the level of specificity compared to knowledge base facts, and  that OIE open relations are generally highly polysemous.We believe that the  OPIEC corpus is a valuable resource for future research on automated knowledge  base construction.","[1, 0, 0, 0, 0, 0, 0]",[],HJxeGb5pTm,OPIEC: An Open Information Extraction Corpus,Présente un ensemble de données de triples open-IE qui ont été collectées à partir de Wikipedia à l'aide d'un système d'extraction récent. 
"Open information extraction (OIE) systems extract relations and their  arguments from natural language text in an unsupervised manner.The resulting  extractions are a valuable resource for downstream tasks such as knowledge  base construction, open question answering, or event schema induction.In this  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was  extracted from the text of English Wikipedia.OPIEC complements the available  OIE resources: It is the largest OIE corpus publicly available to date (over  340M triples) and contains valuable metadata such as provenance information,  confidence scores, linguistic annotations, and semantic annotations including  spatial and temporal information.We analyze the OPIEC corpus by comparing its  content with knowledge bases such as DBpedia or YAGO, which are also based on  Wikipedia.We found that most of the facts between entities present in OPIEC  cannot be found in DBpedia and/or YAGO, that OIE facts   often differ in the level of specificity compared to knowledge base facts, and  that OIE open relations are generally highly polysemous.We believe that the  OPIEC corpus is a valuable resource for future research on automated knowledge  base construction.","[1, 0, 0, 0, 0, 0, 0]",[],HJxeGb5pTm,OPIEC: An Open Information Extraction Corpus,L'article décrit la création d'un corpus Open IE sur la Wikipédia anglaise par une méthode automatique.
"The process of designing neural architectures requires expert knowledge and extensive trial and error.While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","[0, 0, 0, 1, 0, 0]",[],SkOb1Fl0Z,A Flexible Approach to Automated RNN Architecture Generation,"Nous définissons un DSL flexible pour la génération d'architectures RNN qui autorise des RNN de taille et de complexité variables et proposons une fonction de classement qui représente les RNN sous forme de réseaux neuronaux récursifs, en simulant leurs performances pour décider des architectures les plus prometteuses."
"The process of designing neural architectures requires expert knowledge and extensive trial and error.While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","[0, 0, 0, 1, 0, 0]",[],SkOb1Fl0Z,A Flexible Approach to Automated RNN Architecture Generation,Présente une nouvelle méthode pour générer des architectures RNNs en utilisant un langage spécifique au domaine pour deux types de générateurs (aléatoire et basé sur RL) ainsi qu'une fonction de classement et un évaluateur.
"The process of designing neural architectures requires expert knowledge and extensive trial and error.While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","[0, 0, 0, 1, 0, 0]",[],SkOb1Fl0Z,A Flexible Approach to Automated RNN Architecture Generation,Cet article présente la recherche de bonnes architectures de cellules RNN comme un problème d'optimisation en boîte noire où les exemples sont représentés comme un arbre d'opérateurs et notés sur la base de fonctions apprises ou générées par un agent RL.
"The process of designing neural architectures requires expert knowledge and extensive trial and error.While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.","[0, 0, 0, 1, 0, 0]",[],SkOb1Fl0Z,A Flexible Approach to Automated RNN Architecture Generation,Cet article étudie une stratégie de méta-apprentissage pour la recherche automatique d'architecture dans le contexte des RNN en utilisant un DSL qui spécifie les opérations récurrentes des RNN.
"Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics.Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously.In this work, we develop a new method termed as ``""WAGE"" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers.To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation.Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization.Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.","[0, 1, 0, 0, 0, 0]",[],HJGXzmspb,Training and Inference with Integers in Deep Neural Networks,Nous appliquons la formation et l'inférence avec seulement des nombres entiers de faible largeur dans les DNNs.
"Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics.Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously.In this work, we develop a new method termed as ``""WAGE"" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers.To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation.Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization.Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.","[0, 1, 0, 0, 0, 0]",[],HJGXzmspb,Training and Inference with Integers in Deep Neural Networks,Une méthode appelée WAGE qui quantifie tous les opérandes et opérateurs dans un réseau neuronal afin de réduire le nombre de bits pour la représentation dans un réseau.
"Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics.Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously.In this work, we develop a new method termed as ``""WAGE"" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers.To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation.Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization.Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.","[0, 1, 0, 0, 0, 0]",[],HJGXzmspb,Training and Inference with Integers in Deep Neural Networks,"Les auteurs proposent de discrétiser les poids, les activations, les gradients et les erreurs des réseaux neuronaux, tant au moment de la formation que du test."
"Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters.Their deployment exerts computational, storage and energy demands, particularly on embedded platforms.Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy.Such retraining is not feasible in some contexts.In this paper, we explore the sparsification of CNNs by proposing three model-independent methods.Our methods are applied on-the-fly and require no retraining.We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy.Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkz1YD0vjm,Fast On-the-fly Retraining-free Sparsification of Convolutional Neural Networks,"Dans cet article, nous développons des méthodes de sparification rapide sans réentraînement qui peuvent être déployées pour la sparification à la volée des CNN dans de nombreux contextes industriels."
"Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters.Their deployment exerts computational, storage and energy demands, particularly on embedded platforms.Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy.Such retraining is not feasible in some contexts.In this paper, we explore the sparsification of CNNs by proposing three model-independent methods.Our methods are applied on-the-fly and require no retraining.We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy.Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkz1YD0vjm,Fast On-the-fly Retraining-free Sparsification of Convolutional Neural Networks,Cet article propose des approches pour l'élagage des CNN sans recyclage en introduisant trois schémas pour déterminer les seuils des poids d'élagage.
"Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters.Their deployment exerts computational, storage and energy demands, particularly on embedded platforms.Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy.Such retraining is not feasible in some contexts.In this paper, we explore the sparsification of CNNs by proposing three model-independent methods.Our methods are applied on-the-fly and require no retraining.We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy.Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkz1YD0vjm,Fast On-the-fly Retraining-free Sparsification of Convolutional Neural Networks,Cet article décrit une méthode de sparsification des CNN sans réapprentissage.
"Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels.Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance.In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions.Results of the statistical tests show that the proposed method is better than classical method and similar with the others.These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.","[0, 1, 0, 0, 0, 0]",[],SJ1fQYlCZ,Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,"Nous proposons que la formation avec des ensembles croissants, étape par étape, offre une optimisation pour les réseaux neuronaux."
"Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels.Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance.In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions.Results of the statistical tests show that the proposed method is better than classical method and similar with the others.These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.","[0, 1, 0, 0, 0, 0]",[],SJ1fQYlCZ,Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,Les auteurs comparent l'apprentissage par programme à l'apprentissage dans un ordre aléatoire avec des étapes qui ajoutent un nouvel échantillon d'exemples à l'ensemble précédemment construit au hasard.
"Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels.Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance.In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions.Results of the statistical tests show that the proposed method is better than classical method and similar with the others.These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods.","[0, 1, 0, 0, 0, 0]",[],SJ1fQYlCZ,Training with Growing Sets: A Simple Alternative to Curriculum Learning and Self Paced Learning,"Cet article étudie l'influence de l'ordre dans le curriculum et l'apprentissage autogéré, et montre que, dans une certaine mesure, l'ordre des instances de formation n'est pas important."
"We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\vb \in B$ contain all the information that exists in samples $\va\in A$ and some additional information.For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information.When mapping a sample $\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\vb\in B$.Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. Our solution employs a single two-pathway encoder and a single decoder for both domains.The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$.The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term.Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains.We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BylE1205Fm,Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer,"Une méthode de traduction d'image à image qui ajoute à une image le contenu d'une autre, créant ainsi une nouvelle image."
"We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\vb \in B$ contain all the information that exists in samples $\va\in A$ and some additional information.For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information.When mapping a sample $\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\vb\in B$.Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. Our solution employs a single two-pathway encoder and a single decoder for both domains.The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$.The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term.Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains.We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],BylE1205Fm,Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer,"Ce document aborde la tâche du transfert de contenu, avec la noblesse étant sur la perte."
"Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules.In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format.The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes.Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.","[0, 0, 0, 1]",[],H1gR5iR5FX,Analysing Mathematical Reasoning Abilities of Neural Models,"Un ensemble de données pour tester le raisonnement mathématique (et la généralisation algébrique), et des résultats sur les modèles actuels de séquence à séquence."
"Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules.In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format.The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes.Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.","[0, 0, 0, 1]",[],H1gR5iR5FX,Analysing Mathematical Reasoning Abilities of Neural Models,"Présente un nouvel ensemble de données synthétiques pour évaluer la capacité de raisonnement mathématique des modèles de séquence à séquence, et l'utilise pour évaluer plusieurs modèles."
"Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules.In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format.The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes.Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.","[0, 0, 0, 1]",[],H1gR5iR5FX,Analysing Mathematical Reasoning Abilities of Neural Models,Modèle pour la résolution de problèmes mathématiques de base.
"Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.Commonly, the convolution operators couple features from all channels.For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],H1eRIoA5Y7,Low-Cost Parameterizations of Deep Convolutional Neural Networks,Cet article présente des paramétrages efficaces et économiques des réseaux de neurones convolutifs motivés par des équations différentielles partielles. 
"Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.Commonly, the convolution operators couple features from all channels.For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],H1eRIoA5Y7,Low-Cost Parameterizations of Deep Convolutional Neural Networks,"Présente quatre alternatives ""à faible coût"" à l'opération de convolution standard qui peuvent être utilisées à la place de cette dernière afin de réduire leur complexité de calcul."
"Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.Commonly, the convolution operators couple features from all channels.For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],H1eRIoA5Y7,Low-Cost Parameterizations of Deep Convolutional Neural Networks,"Cet article présente des méthodes permettant de réduire le coût de calcul des implémentations CNN, et présente de nouvelles paramétrisations des architectures de type CNN qui limitent le couplage des paramètres."
"Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.Commonly, the convolution operators couple features from all channels.For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],H1eRIoA5Y7,Low-Cost Parameterizations of Deep Convolutional Neural Networks,L'article propose une perspective basée sur les EDP pour comprendre et paramétrer les CNN.
"In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.","[0, 0, 0, 0, 1]",[],rkemqsC9Fm,Information Theoretic lower bounds on negative log likelihood,Utiliser la théorie du taux de distorsion pour déterminer dans quelle mesure un modèle de variable latente peut être amélioré.
"In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.","[0, 0, 0, 0, 1]",[],rkemqsC9Fm,Information Theoretic lower bounds on negative log likelihood,Aborde les problèmes d'optimisation de l'antériorité dans le modèle de variable latente et la sélection de la fonction de vraisemblance en proposant des critères basés sur une limite inférieure de la log-vraisemblance négative.
"In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.","[0, 0, 0, 0, 1]",[],rkemqsC9Fm,Information Theoretic lower bounds on negative log likelihood,Présente un théorème qui donne une limite inférieure à la log-vraisemblance négative de la distorsion de taux pour la modélisation à variables latentes.
"In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling.","[0, 0, 0, 0, 1]",[],rkemqsC9Fm,Information Theoretic lower bounds on negative log likelihood,Les auteurs soutiennent que la théorie de la distorsion du taux pour la compression avec perte fournit une boîte à outils naturelle pour étudier les modèles de variables latentes propose une limite inférieure.
"Backprop is the primary learning algorithm used in many machine learning algorithms.In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints.One set of constraints is to avoid weights that lead to saturated units.The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt.Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow.Here we investigate backpropagating error terms only linearly.That is, we ignore the saturation that arise by ensuring gradients always flow.We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear.In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed.Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],ByfPDyrYim,Linear Backprop in non-linear networks,Nous ignorons les non-linéarités et ne calculons pas les gradients dans le passage en arrière afin d'économiser le calcul et de garantir que les gradients circulent toujours. 
"Backprop is the primary learning algorithm used in many machine learning algorithms.In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints.One set of constraints is to avoid weights that lead to saturated units.The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt.Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow.Here we investigate backpropagating error terms only linearly.That is, we ignore the saturation that arise by ensuring gradients always flow.We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear.In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed.Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],ByfPDyrYim,Linear Backprop in non-linear networks,L'auteur a proposé des algorithmes de rétropropagation linéaire pour assurer le flux des gradients pour toutes les parties pendant la rétropropagation.
"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks.There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts.Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.","[0, 0, 0, 0, 1]",[],HkGGfhC5Y7,Towards a better understanding of Vector Quantized Autoencoders,Comprendre l'auto-codeur discret VQ-VAE de manière systématique en utilisant EM et l'utiliser pour concevoir un modèle de traduction non-autogressif correspondant à une base autorégressive forte.
"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks.There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts.Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.","[0, 0, 0, 0, 1]",[],HkGGfhC5Y7,Towards a better understanding of Vector Quantized Autoencoders,Cet article introduit une nouvelle façon d'interpréter le VQ-VAE et propose un nouvel algorithme de formation basé sur le regroupement EM doux.
"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks.There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts.Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.","[0, 0, 0, 0, 1]",[],HkGGfhC5Y7,Towards a better understanding of Vector Quantized Autoencoders,L'article présente un point de vue alternatif sur la procédure de formation pour le VQ-VAE en utilisant l'algorithme EM doux.
"Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution.In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies.In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution.We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks.In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations.And our ODNmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.","[0, 0, 1, 0, 0, 0, 0]",[],HygcvsAcFX,Optimal margin Distribution Network,"Cet article présente un réseau neuronal profond intégrant une fonction de perte en ce qui concerne la distribution optimale de la marge, qui atténue le problème de l'ajustement excessif sur le plan théorique et empirique."
"Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution.In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies.In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution.We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks.In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations.And our ODNmodel also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data.","[0, 0, 1, 0, 0, 0, 0]",[],HygcvsAcFX,Optimal margin Distribution Network,Présente une limite PAC-Bayes pour une perte de marge
"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.We propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","[0, 0, 0, 0, 0, 1, 0]",[],HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,Nous cherchons à comprendre les représentations apprises dans les réseaux comprimés par le biais d'un régime expérimental que nous appelons le triage des réseaux profonds.
"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.We propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","[0, 0, 0, 0, 0, 1, 0]",[],HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,Compare diverses méthodes d'initialisation et d'entraînement pour transférer les connaissances d'un réseau VGG à un réseau d'étudiants plus petit en remplaçant les blocs de couches par des couches uniques.
"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.We propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","[0, 0, 0, 0, 0, 1, 0]",[],HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,Ce document présente cinq méthodes pour effectuer le triage ou la compression des couches de blocs pour les réseaux profonds.
"Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.We propose a suite of triage methods and compare them on problem spaces of varying complexity.  We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally.","[0, 0, 0, 0, 0, 1, 0]",[],HJWpQCa7z,Deep Net Triage: Assessing The Criticality of Network Layers by Structural Compression,L'article propose une méthode de compression d'un bloc de couches dans un NN qui évalue plusieurs sous-approches différentes.
"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.","[0, 0, 0, 0, 1, 0, 0]",[],H1A5ztj3b,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,La preuve empirique d'un nouveau phénomène nécessite de nouveaux éclairages théoriques et est pertinente pour les discussions actives dans la littérature sur le SGD et la généralisation de la compréhension.
"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.","[0, 0, 0, 0, 1, 0, 0]",[],H1A5ztj3b,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,L'article traite d'un phénomène selon lequel la formation de réseaux neuronaux dans des contextes très spécifiques peut tirer un grand profit d'un programme comprenant des taux d'apprentissage élevés.
"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.","[0, 0, 0, 0, 1, 0, 0]",[],H1A5ztj3b,Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates,"Les auteurs analysent la formation des réseaux résiduels en utilisant de grands taux d'apprentissage cycliques, et démontrent une convergence rapide avec les taux d'apprentissage cycliques et des preuves que les grands taux d'apprentissage agissent comme une régularisation."
"Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks.Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers.To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks.The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way.Furthermore, we examine the practical implications of this construction for standard, finite-width networks.In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand.We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.","[0, 0, 0, 1, 0, 0, 0, 0]",[],SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,"Nous proposons une méthode pour la construction de réseaux infiniment larges arbitrairement profonds, sur la base de laquelle nous dérivons un nouveau schéma d'initialisation des poids pour les réseaux finis et démontrons sa performance compétitive."
"Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks.Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers.To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks.The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way.Furthermore, we examine the practical implications of this construction for standard, finite-width networks.In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand.We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.","[0, 0, 0, 1, 0, 0, 0, 0]",[],SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,"Propose une approche d'initialisation des poids pour permettre des réseaux infiniment profonds et de largeur infinie, avec des résultats expérimentaux sur de petits ensembles de données."
"Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks.Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers.To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks.The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way.Furthermore, we examine the practical implications of this construction for standard, finite-width networks.In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand.We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets.","[0, 0, 0, 1, 0, 0, 0, 0]",[],SkGT6sRcFX,Infinitely Deep Infinite-Width Networks,Propose des réseaux neuronaux profonds de largeur infinie.
"Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds.Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations.Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities.The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network.To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.","[0, 0, 0, 0, 0, 1, 0]",[],Syl3_2JCZ,A Self-Organizing Memory Network,Nous avons dérivé des règles d'apprentissage de la plasticité synaptique biologiquement plausibles pour un réseau neuronal récurrent permettant de stocker des représentations de stimulus. 
"Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds.Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations.Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities.The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network.To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.","[0, 0, 0, 0, 0, 1, 0]",[],Syl3_2JCZ,A Self-Organizing Memory Network,Modèle de réseau neuronal constitué de neurones connectés de manière récurrente et d'une ou plusieurs redoutes qui vise à conserver une certaine sortie dans le temps.
"Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds.Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations.Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities.The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network.To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli.","[0, 0, 0, 0, 0, 1, 0]",[],Syl3_2JCZ,A Self-Organizing Memory Network,"Cet article présente un mécanisme de mémoire auto-organisée dans un modèle neuronal, et introduit une fonction objective qui minimise les changements dans le signal à mémoriser."
"Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models.While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice.In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions’ immediate neighborhood.To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself.To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse.This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice.Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJYQLb-RW,On the limitations of first order approximation in GAN dynamics,"Pour comprendre l'entraînement du GAN, nous définissons une dynamique simple du GAN, et montrons les différences quantitatives entre les mises à jour optimales et de premier ordre dans ce modèle."
"Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models.While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice.In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions’ immediate neighborhood.To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself.To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse.This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice.Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJYQLb-RW,On the limitations of first order approximation in GAN dynamics,"Les auteurs étudient l'impact des GAN dans des contextes où, à chaque itération, le discriminateur s'entraîne jusqu'à convergence et le générateur se met à jour avec des étapes de gradient, ou lorsque quelques étapes de gradient sont effectuées pour le discriminateur et le générateur."
"Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models.While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice.In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions’ immediate neighborhood.To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself.To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse.This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice.Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJYQLb-RW,On the limitations of first order approximation in GAN dynamics,Cet article étudie la dynamique de l'apprentissage contradictoire de GANs sur un modèle de mélange gaussien.
"The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network.However, it is expensive and time-consuming to manually label sufficient amount of training data.Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task.While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting.We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule.GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training.Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset.In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain.We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1xL_iR9Km,GradMix: Multi-source Transfer across Domains and Tasks,Nous proposons une méthode basée sur le gradient pour transférer des connaissances provenant de sources multiples dans différents domaines et tâches.
"The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network.However, it is expensive and time-consuming to manually label sufficient amount of training data.Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task.While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting.We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule.GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training.Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset.In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain.We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1xL_iR9Km,GradMix: Multi-source Transfer across Domains and Tasks,Cet article propose de combiner les gradients des domaines sources pour faciliter l'apprentissage dans le domaine cible. 
"Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates.In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis.We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions.We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models.We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference.Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling.Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.","[0, 0, 0, 0, 0, 0, 1]",[],SJVmjjR9FX,Variational Bayesian Phylogenetic Inference,"La première formulation bayésienne variationnelle de l'inférence phylogénétique, un problème d'inférence difficile sur des structures dont les composantes discrètes et continues sont entremêlées."
"Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates.In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis.We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions.We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models.We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference.Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling.Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.","[0, 0, 0, 0, 0, 0, 1]",[],SJVmjjR9FX,Variational Bayesian Phylogenetic Inference,Explore une solution d'inférence approximative au problème de l'inférence bayésienne des arbres phylogénétiques en tirant parti des réseaux bayésiens subsplit récemment proposés et des estimateurs de gradient modernes pour VI.
"Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates.In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis.We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions.We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models.We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference.Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling.Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.","[0, 0, 0, 0, 0, 0, 1]",[],SJVmjjR9FX,Variational Bayesian Phylogenetic Inference,Propose une approche variationnelle de l'inférence bayésienne postérieure dans les arbres phylogénétiques.
"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressivemodels for raw audio waveform generation.As an example, we proposea hybrid model that combines an autoregressive network named WaveNet and aconventional LSTM model to address speech synthesis.Instead of generatingone sample per time-step, the proposed HybridNet generates multiple samples pertime-step by exploiting the long-term memory utilization property of LSTMs.Inthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.HybridNet achieves a 3.83 subjective 5-scale mean opinion score onUS English, largely outperforming the same size WaveNet in terms of naturalnessand provide 2x speed up at inference.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJoXrxZAZ,HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,Il s'agit d'une architecture neuronale hybride permettant d'accélérer le modèle autorégressif. 
"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressivemodels for raw audio waveform generation.As an example, we proposea hybrid model that combines an autoregressive network named WaveNet and aconventional LSTM model to address speech synthesis.Instead of generatingone sample per time-step, the proposed HybridNet generates multiple samples pertime-step by exploiting the long-term memory utilization property of LSTMs.Inthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.HybridNet achieves a 3.83 subjective 5-scale mean opinion score onUS English, largely outperforming the same size WaveNet in terms of naturalnessand provide 2x speed up at inference.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJoXrxZAZ,HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,"Il en ressort que pour augmenter la taille du modèle sans augmenter le temps d'inférence pour la prédiction séquentielle, il faut utiliser un modèle qui prédit plusieurs pas de temps à la fois."
"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressivemodels for raw audio waveform generation.As an example, we proposea hybrid model that combines an autoregressive network named WaveNet and aconventional LSTM model to address speech synthesis.Instead of generatingone sample per time-step, the proposed HybridNet generates multiple samples pertime-step by exploiting the long-term memory utilization property of LSTMs.Inthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.HybridNet achieves a 3.83 subjective 5-scale mean opinion score onUS English, largely outperforming the same size WaveNet in terms of naturalnessand provide 2x speed up at inference.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJoXrxZAZ,HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models,"Cet article présente HybridNet, un système neuronal de synthèse de la parole et d'autres sons qui combine le modèle WaveNet avec un LSTM dans le but d'offrir un modèle permettant une génération audio plus rapide en temps d'inférence."
"Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them.In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations.We interpret the model through average visualizations of this reduced set of features.Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features.In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations.Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.","[0, 1, 0, 0, 0, 0, 0]",[],H1ziPjC5Fm,Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks,Interprétation en identifiant les caractéristiques apprises par le modèle qui servent d'indicateurs pour la tâche d'intérêt. Expliquer les décisions du modèle en mettant en évidence la réponse de ces caractéristiques dans les données de test. Évaluer objectivement les explications à l'aide d'un ensemble de données contrôlé.
"Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them.In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations.We interpret the model through average visualizations of this reduced set of features.Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features.In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations.Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.","[0, 1, 0, 0, 0, 0, 0]",[],H1ziPjC5Fm,Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks,Cet article propose une méthode pour produire des explications visuelles pour les sorties des réseaux neuronaux profonds et publie un nouvel ensemble de données synthétiques.
"Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them.In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations.We interpret the model through average visualizations of this reduced set of features.Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features.In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations.Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.","[0, 1, 0, 0, 0, 0, 0]",[],H1ziPjC5Fm,Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks,"Une méthode pour les réseaux neuronaux profonds qui identifie automatiquement les caractéristiques pertinentes de l'ensemble des classes, soutenant l'interprétation et l'explication sans dépendre d'annotations supplémentaires."
"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data.Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem.Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations.This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations.We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","[1, 0, 0, 0, 0]",[],rJvJXZb0W,An efficient framework for learning sentence representations,Un cadre pour l'apprentissage efficace de représentations de phrases de haute qualité.
"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data.Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem.Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations.This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations.We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","[1, 0, 0, 0, 0]",[],rJvJXZb0W,An efficient framework for learning sentence representations,"Propose un algorithme plus rapide pour l'apprentissage de représentations de phrases de type SkipThought à partir de corpus de phrases ordonnées, qui remplace le décodeur au niveau des mots par une perte de classification contrastive."
"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data.Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem.Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations.This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations.We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","[1, 0, 0, 0, 0]",[],rJvJXZb0W,An efficient framework for learning sentence representations,Cet article propose un cadre pour l'apprentissage non supervisé des représentations de phrases en maximisant un modèle de la probabilité des vraies phrases contextuelles par rapport aux phrases candidates aléatoires.
"Many regularization methods have been proposed to prevent overfitting in neural networks.Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian.However, this method cannot be generalized to regular neural network architectures.We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.Unlike in previous literature, it can be applied to any general neural network.We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification.We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.","[0, 0, 0, 1, 0, 0, 0]",[],SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,Nous dérivons une pénalité de norme sur la sortie du réseau neuronal du point de vue du goulot d'étranglement de l'information.
"Many regularization methods have been proposed to prevent overfitting in neural networks.Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian.However, this method cannot be generalized to regular neural network architectures.We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.Unlike in previous literature, it can be applied to any general neural network.We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification.We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.","[0, 0, 0, 1, 0, 0, 0]",[],SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,"Met en avant la pénalité de la norme d'activation, une régularisation de type L_2 sur les activations, en la dérivant du principe du goulot d'étranglement de l'information."
"Many regularization methods have been proposed to prevent overfitting in neural networks.Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian.However, this method cannot be generalized to regular neural network architectures.We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.Unlike in previous literature, it can be applied to any general neural network.We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification.We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.","[0, 0, 0, 1, 0, 0, 0]",[],SySpa-Z0Z,From Information Bottleneck To Activation Norm Penalty,Cet article crée une correspondance entre les pénalités de la norme d'activation et le cadre du goulot d'étranglement de l'information en utilisant le cadre du décrochage variationnel.
"Unsupervised learning of timeseries data is a challenging problem in machine learning.Here, we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.Then it jointly optimizes the clustering objective and the dimensionality reduction objective.Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.Several similarity metrics are considered and compared.  To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.In each case, we show that our algorithm outperforms traditional methods.This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SJFM0ZWCb,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,"Une méthode entièrement non supervisée, pour intégrer naturellement la réduction de la dimensionnalité et le regroupement temporel dans un cadre unique d'apprentissage de bout en bout."
"Unsupervised learning of timeseries data is a challenging problem in machine learning.Here, we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.Then it jointly optimizes the clustering objective and the dimensionality reduction objective.Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.Several similarity metrics are considered and compared.  To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.In each case, we show that our algorithm outperforms traditional methods.This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SJFM0ZWCb,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,Propose un algorithme qui intègre un auto-codeur avec le regroupement de données de séries temporelles en utilisant une structure de réseau qui convient aux données de séries temporelles.
"Unsupervised learning of timeseries data is a challenging problem in machine learning.Here, we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.Then it jointly optimizes the clustering objective and the dimensionality reduction objective.Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.Several similarity metrics are considered and compared.  To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.In each case, we show that our algorithm outperforms traditional methods.This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SJFM0ZWCb,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,"Un algorithme pour effectuer conjointement une réduction de la dimensionnalité et un regroupement temporel dans un contexte d'apprentissage profond, en utilisant un autoencodeur et un objectif de regroupement."
"Unsupervised learning of timeseries data is a challenging problem in machine learning.Here, we propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.Then it jointly optimizes the clustering objective and the dimensionality reduction objective.Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.Several similarity metrics are considered and compared.  To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.In each case, we show that our algorithm outperforms traditional methods.This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SJFM0ZWCb,Deep Temporal Clustering: Fully unsupervised learning of time-domain features,"Les auteurs ont proposé une méthode non supervisée de regroupement de séries temporelles construite avec des réseaux neuronaux profonds et équipée d'un encodeur-décodeur et d'un mode de regroupement pour raccourcir les séries temporelles, extraire les caractéristiques temporelles locales et obtenir les représentations encodées."
"We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied.MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training.In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning.It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain.MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes.While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem.We design different training strategies of MahiNet for supervised learning and meta-learning.Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem.In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJlcV2Actm,MahiNet: A Neural Network for Many-Class Few-Shot Learning with Class Hierarchy,Un réseau de neurones à mémoire augmentée qui résout le problème du nombre limité de classes en tirant parti de la hiérarchie des classes dans l'apprentissage supervisé et le méta-apprentissage.
"We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied.MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training.In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning.It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain.MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes.While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem.We design different training strategies of MahiNet for supervised learning and meta-learning.Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem.In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJlcV2Actm,MahiNet: A Neural Network for Many-Class Few-Shot Learning with Class Hierarchy,Cet article présente des méthodes pour ajouter un biais inductif à un classificateur par le biais d'une prédiction grossière à fine le long d'une hiérarchie de classes et l'apprentissage d'un classificateur KNN basé sur la mémoire qui garde la trace des instances mal étiquetées pendant l'apprentissage.
"We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied.MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training.In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning.It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain.MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes.While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem.We design different training strategies of MahiNet for supervised learning and meta-learning.Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem.In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJlcV2Actm,MahiNet: A Neural Network for Many-Class Few-Shot Learning with Class Hierarchy,Cet article formule le problème de la classification de plusieurs classes à partir d'une perspective d'apprentissage supervisé et d'une perspective de méta-apprentissage.
"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.In this work, we focus on learning a representation that would be useful in a clustering task.We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","[0, 0, 1, 0]",[],S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,Un nouveau composant de perte qui force le réseau à apprendre une représentation qui est bien adaptée au regroupement pendant la formation pour une tâche de classification.
"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.In this work, we focus on learning a representation that would be useful in a clustering task.We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","[0, 0, 1, 0]",[],S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,Cet article propose deux termes de régularisation basés sur une perte charnière composée sur la divergence KL entre deux arguments d'entrée normalisés par softmax afin d'encourager l'apprentissage de représentations démêlées.
"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.In this work, we focus on learning a representation that would be useful in a clustering task.We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","[0, 0, 1, 0]",[],S17mtzbRb,Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels,Proposition de deux régularisateurs destinés à rendre les représentations apprises dans l'avant-dernière couche d'un classificateur plus conformes à la structure inhérente des données.
"In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.We study the interaction between nearest neighbor algorithms and neural networks in more detail.We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.","[0, 0, 0, 1, 0, 0]",[],SkrHeXbCW,Learning Representations for Faster Similarity Search,Nous montrons comment obtenir de bonnes représentations du point de vue de la recherche de similiarité.
"In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.We study the interaction between nearest neighbor algorithms and neural networks in more detail.We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.","[0, 0, 0, 1, 0, 0]",[],SkrHeXbCW,Learning Representations for Faster Similarity Search,Etudie l'impact du changement de la partie classification d'image au-dessus du DNN sur la capacité d'indexer les descripteurs avec un algorithme LSH ou kd-tree.
"In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.We study the interaction between nearest neighbor algorithms and neural networks in more detail.We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x.","[0, 0, 0, 1, 0, 0]",[],SkrHeXbCW,Learning Representations for Faster Similarity Search,Propose d'utiliser la perte d'entropie croisée softmax pour apprendre un réseau qui tente de réduire les angles entre les entrées et les vecteurs de classe correspondants dans un cadre supervisé en utilisant.
"Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.","[0, 1, 0, 0, 0, 0]",[],HkxjYoCqKX,Relaxed Quantization for Discretized Neural Networks,Nous présentons une technique qui permet l'apprentissage de réseaux neuronaux quantifiés basé sur le gradient.
"Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.","[0, 1, 0, 0, 0, 0]",[],HkxjYoCqKX,Relaxed Quantization for Discretized Neural Networks,Propose une méthode unifiée et générale d'entraînement des réseaux neuronaux avec des poids et des activations synaptiques quantifiés à précision réduite.
"Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.","[0, 1, 0, 0, 0, 0]",[],HkxjYoCqKX,Relaxed Quantization for Discretized Neural Networks,Une nouvelle approche de la quantification des activations qui est à la pointe de l'art ou compétitive sur plusieurs problèmes d'images réelles.
"Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.","[0, 1, 0, 0, 0, 0]",[],HkxjYoCqKX,Relaxed Quantization for Discretized Neural Networks,Procédé d'apprentissage de réseaux neuronaux avec des poids et des activations quantifiés par quantification stochastique des valeurs et remplacement de la distribution catégorielle résultante par une relaxation continue.
"In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations.In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models.Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations.We show how they can be easily implemented on top of existing models.Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators.In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.","[0, 0, 0, 0, 1, 0]",[],SyKoKWbC-,Distributional Adversarial Networks,"Nous montrons que le problème d'effondrement des modes dans les GANs peut s'expliquer par un manque de partage d'informations entre les observations d'un lot d'entraînement, et nous proposons un cadre basé sur la distribution pour le partage global d'informations entre les gradients qui conduit à un entraînement contradictoire plus stable et plus efficace."
"In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations.In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models.Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations.We show how they can be easily implemented on top of existing models.Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators.In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.","[0, 0, 0, 0, 1, 0]",[],SyKoKWbC-,Distributional Adversarial Networks,Propose de remplacer les discriminateurs à échantillon unique dans la formation adversariale par des discriminateurs qui opèrent explicitement sur des distributions d'exemples.
"In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations.In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models.Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations.We show how they can be easily implemented on top of existing models.Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators.In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art.","[0, 0, 0, 0, 1, 0]",[],SyKoKWbC-,Distributional Adversarial Networks,Théorie sur les tests à deux échantillons et la MMD et comment les intégrer avantageusement dans le cadre du GAN.
"Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization.Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula.However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task.In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model.Our framework is trained end to end and is fully data-driven.Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.","[0, 0, 0, 1, 0, 0]",[],rJg_NjCqtX,CHEMICAL NAMES STANDARDIZATION USING NEURAL SEQUENCE TO SEQUENCE MODEL,Nous avons conçu un cadre de bout en bout utilisant le modèle de séquence à séquence pour effectuer la normalisation des noms chimiques.
"Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization.Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula.However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task.In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model.Our framework is trained end to end and is fully data-driven.Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.","[0, 0, 0, 1, 0, 0]",[],rJg_NjCqtX,CHEMICAL NAMES STANDARDIZATION USING NEURAL SEQUENCE TO SEQUENCE MODEL,Standardise les noms non systématiques dans l'extraction d'informations chimiques en créant un corpus parallèle de noms non systématiques et systématiques et en construisant un modèle seq2seq.
"Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization.Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula.However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task.In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model.Our framework is trained end to end and is fully data-driven.Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result.","[0, 0, 0, 1, 0, 0]",[],rJg_NjCqtX,CHEMICAL NAMES STANDARDIZATION USING NEURAL SEQUENCE TO SEQUENCE MODEL,Ce travail présente une méthode permettant de traduire les noms non systématiques des composés chimiques en leurs équivalents systématiques en utilisant une combinaison de mécanismes
"The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss.This was found to correlate with a good final generalization performance.  In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD.At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions).To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD.Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.","[0, 0, 0, 0, 0, 1, 0]",[],SkgEaj05t7,On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,"Le SGD est orienté au début de la formation vers une région dans laquelle son pas est trop grand par rapport à la courbure, ce qui a un impact sur le reste de la formation. "
"The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss.This was found to correlate with a good final generalization performance.  In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD.At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions).To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD.Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.","[0, 0, 0, 0, 0, 1, 0]",[],SkgEaj05t7,On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,Analyse la relation entre la convergence/généralisation et la mise à jour sur les plus grands vecteurs propres du Hessien des pertes empiriques des DNNs.
"The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss.This was found to correlate with a good final generalization performance.  In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD.At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions).To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD.Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.","[0, 0, 0, 0, 0, 1, 0]",[],SkgEaj05t7,On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,Ce travail étudie la relation entre la taille de l'étape SGD et la courbure de la surface de perte.
"We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems.Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution.Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state.This allows the agent to learn a simple stochastic policy that has an easy to compute expected return.In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.","[1, 0, 0, 0, 0]",[],SJgf6Z-0W,Predicting Multiple Actions for Stochastic Continuous Control,"Nous présentons un nouvel algorithme d'apprentissage par renforcement, qui prédit des actions multiples et en tire des échantillons."
"We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems.Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution.Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state.This allows the agent to learn a simple stochastic policy that has an easy to compute expected return.In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.","[1, 0, 0, 0, 0]",[],SJgf6Z-0W,Predicting Multiple Actions for Stochastic Continuous Control,"Ce travail introduit un mélange uniforme de politiques déterministes, et constate que cette paramétrisation des politiques stochastiques surpasse DDPG sur plusieurs repères de gymnastique OpenAI."
"We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems.Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution.Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state.This allows the agent to learn a simple stochastic policy that has an easy to compute expected return.In all experiments, this facilitates better exploration of the state space during training and converges to a better policy.","[1, 0, 0, 0, 0]",[],SJgf6Z-0W,Predicting Multiple Actions for Stochastic Continuous Control,"Les auteurs étudient une méthode permettant d'améliorer les performances des réseaux formés avec DDPG, et montrent une amélioration des performances sur un grand nombre d'environnements de contrôle continu standard."
"Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks.DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse.However, like other CNN models, DenseNets also face overfitting problem if not severer.Existing dropout method can be applied but not as effective due to the introduced nonlinear connections.In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps.To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability.The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections.Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.","[0, 0, 0, 0, 0, 1, 0, 0]",[],r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,"Conscients des inconvénients de l'application du dropout original sur DenseNet, nous concevons la méthode de dropout à partir de trois aspects, dont l'idée pourrait également être appliquée à d'autres modèles CNN."
"Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks.DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse.However, like other CNN models, DenseNets also face overfitting problem if not severer.Existing dropout method can be applied but not as effective due to the introduced nonlinear connections.In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps.To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability.The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections.Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.","[0, 0, 0, 0, 0, 1, 0, 0]",[],r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,Application de différentes structures et planifications de décrochage binaire dans le but spécifique de régulariser l'architecture DenseNet.
"Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks.DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse.However, like other CNN models, DenseNets also face overfitting problem if not severer.Existing dropout method can be applied but not as effective due to the introduced nonlinear connections.In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps.To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability.The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections.Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.","[0, 0, 0, 0, 0, 1, 0, 0]",[],r1gOe209t7,Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout,Propose une technique de pré-dropout pour densenet qui implémente le dropout avant la fonction d'activation non-linéaire.
"While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models.Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples.Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.","[0, 0, 1, 0]",[],HJeOMhA5K7,Human-Guided Column Networks: Augmenting Deep Learning with Advice,Guider les modèles profonds conscients des relations vers un meilleur apprentissage avec des connaissances humaines.
"While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models.Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples.Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.","[0, 0, 1, 0]",[],HJeOMhA5K7,Human-Guided Column Networks: Augmenting Deep Learning with Advice,Ce travail propose une variante du réseau de colonnes basée sur l'injection de conseils humains en modifiant les calculs dans le réseau.
"While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models.Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples.Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence.","[0, 0, 1, 0]",[],HJeOMhA5K7,Human-Guided Column Networks: Augmenting Deep Learning with Advice,"Une méthode pour incorporer des conseils humains à l'apprentissage profond en étendant Column Network, un réseau neuronal à graphes pour la classification collective."
"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent.However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations.Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors.In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved.Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations.Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks.Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","[0, 0, 1, 0, 0, 0, 0, 0]",[],B1IDRdeCW,The High-Dimensional Geometry of Binary Neural Networks,Les récents succès des réseaux de neurones binaires peuvent être compris à partir de la géométrie des vecteurs binaires de haute dimension.
"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent.However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations.Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors.In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved.Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations.Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks.Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","[0, 0, 1, 0, 0, 0, 0, 0]",[],B1IDRdeCW,The High-Dimensional Geometry of Binary Neural Networks,Étudie numériquement et théoriquement les raisons du succès empirique des réseaux neuronaux binarisés.
"Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent.However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations.Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors.In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved.Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations.Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks.Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.","[0, 0, 1, 0, 0, 0, 0, 0]",[],B1IDRdeCW,The High-Dimensional Geometry of Binary Neural Networks,Ce document analyse l'efficacité des réseaux neuronaux binaires et explique pourquoi la binarisation permet de préserver les performances du modèle.
"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR).In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator.Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries.In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","[0, 0, 1, 0, 0]",[],SyqAPeWAZ,CNNs as Inverse Problem Solvers and Double Network Superresolution,"Après avoir prouvé qu'un neurone agit comme un résolveur de problème inverse pour la superrésolution et qu'un réseau de neurones est garanti pour fournir une solution, nous avons proposé une architecture de réseau double qui est plus rapide que l'état de l'art."
"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR).In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator.Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries.In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","[0, 0, 1, 0, 0]",[],SyqAPeWAZ,CNNs as Inverse Problem Solvers and Double Network Superresolution,Discute de l'utilisation des réseaux neuronaux pour la super-résolution.
"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR).In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator.Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries.In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","[0, 0, 1, 0, 0]",[],SyqAPeWAZ,CNNs as Inverse Problem Solvers and Double Network Superresolution,"Une nouvelle architecture pour la résolution de tâches de super-résolution d'images, et une analyse visant à établir une connexion entre les CNN pour la résolution de super-résolution et la résolution de problèmes inverses régularisés de faible densité."
"We consider the learning of algorithmic tasks by mere observation of input-outputpairs.Rather than studying this as a black-box discrete regression problem withno assumption whatsoever on the input-output mapping, we concentrate on tasksthat are amenable to the principle of divide and conquer, and study what are itsimplications in terms of learning.This principle creates a powerful inductive bias that we leverage with neuralarchitectures that are defined recursively and dynamically, by learning two scale-invariant atomic operations: how to split a given input into smaller sets, and howto merge two partially solved tasks into a larger partial solution.Our model can betrained in weakly supervised environments, namely by just observing input-outputpairs, and in even weaker environments, using a non-differentiable reward signal.Moreover, thanks to the dynamic aspect of our architecture, we can incorporatethe computational complexity as a regularization term that can be optimized bybackpropagation.We demonstrate the flexibility and efficiency of the Divide-and-Conquer Network on several combinatorial and geometric tasks: convex hull,clustering, knapsack and euclidean TSP.Thanks to the dynamic programmingnature of our model, we show significant improvements in terms of generalizationerror and computational complexity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1jscMbAW,Divide and Conquer Networks,Modèle dynamique qui apprend des stratégies de division et de conquête par supervision faible.
"We consider the learning of algorithmic tasks by mere observation of input-outputpairs.Rather than studying this as a black-box discrete regression problem withno assumption whatsoever on the input-output mapping, we concentrate on tasksthat are amenable to the principle of divide and conquer, and study what are itsimplications in terms of learning.This principle creates a powerful inductive bias that we leverage with neuralarchitectures that are defined recursively and dynamically, by learning two scale-invariant atomic operations: how to split a given input into smaller sets, and howto merge two partially solved tasks into a larger partial solution.Our model can betrained in weakly supervised environments, namely by just observing input-outputpairs, and in even weaker environments, using a non-differentiable reward signal.Moreover, thanks to the dynamic aspect of our architecture, we can incorporatethe computational complexity as a regularization term that can be optimized bybackpropagation.We demonstrate the flexibility and efficiency of the Divide-and-Conquer Network on several combinatorial and geometric tasks: convex hull,clustering, knapsack and euclidean TSP.Thanks to the dynamic programmingnature of our model, we show significant improvements in terms of generalizationerror and computational complexity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1jscMbAW,Divide and Conquer Networks,Propose d'ajouter un nouveau biais inductif à l'architecture des réseaux neuronaux en utilisant une stratégie de division et de conquête.
"We consider the learning of algorithmic tasks by mere observation of input-outputpairs.Rather than studying this as a black-box discrete regression problem withno assumption whatsoever on the input-output mapping, we concentrate on tasksthat are amenable to the principle of divide and conquer, and study what are itsimplications in terms of learning.This principle creates a powerful inductive bias that we leverage with neuralarchitectures that are defined recursively and dynamically, by learning two scale-invariant atomic operations: how to split a given input into smaller sets, and howto merge two partially solved tasks into a larger partial solution.Our model can betrained in weakly supervised environments, namely by just observing input-outputpairs, and in even weaker environments, using a non-differentiable reward signal.Moreover, thanks to the dynamic aspect of our architecture, we can incorporatethe computational complexity as a regularization term that can be optimized bybackpropagation.We demonstrate the flexibility and efficiency of the Divide-and-Conquer Network on several combinatorial and geometric tasks: convex hull,clustering, knapsack and euclidean TSP.Thanks to the dynamic programmingnature of our model, we show significant improvements in terms of generalizationerror and computational complexity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1jscMbAW,Divide and Conquer Networks,"Cet article étudie les problèmes qui peuvent être résolus à l'aide d'une approche de programmation dynamique, et propose une architecture de réseau neuronal pour résoudre ces problèmes qui battent les lignes de base de séquence à séquence."
"We consider the learning of algorithmic tasks by mere observation of input-outputpairs.Rather than studying this as a black-box discrete regression problem withno assumption whatsoever on the input-output mapping, we concentrate on tasksthat are amenable to the principle of divide and conquer, and study what are itsimplications in terms of learning.This principle creates a powerful inductive bias that we leverage with neuralarchitectures that are defined recursively and dynamically, by learning two scale-invariant atomic operations: how to split a given input into smaller sets, and howto merge two partially solved tasks into a larger partial solution.Our model can betrained in weakly supervised environments, namely by just observing input-outputpairs, and in even weaker environments, using a non-differentiable reward signal.Moreover, thanks to the dynamic aspect of our architecture, we can incorporatethe computational complexity as a regularization term that can be optimized bybackpropagation.We demonstrate the flexibility and efficiency of the Divide-and-Conquer Network on several combinatorial and geometric tasks: convex hull,clustering, knapsack and euclidean TSP.Thanks to the dynamic programmingnature of our model, we show significant improvements in terms of generalizationerror and computational complexity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1jscMbAW,Divide and Conquer Networks,L'article propose une architecture de réseau unique qui peut apprendre des stratégies de division et de conquête pour résoudre des tâches algorithmiques.
"Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\boldsymbol{\gamma}$ for expectation-based objectives $\mathbb{E}_{q_{\boldsymbol{\gamma}} (\boldsymbol{y})} [f (\boldsymbol{y}) ]$.Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.","[0, 0, 0, 0, 1]",[],ryf6Fs09YX,GO Gradient for Expectation-Based Objectives,"un gradient de type Rep pour les distributions continues/discrètes non reparamétrisables ; généralisé ensuite aux modèles probabilistes profonds, donnant lieu à une rétro-propagation statistique"
"Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\boldsymbol{\gamma}$ for expectation-based objectives $\mathbb{E}_{q_{\boldsymbol{\gamma}} (\boldsymbol{y})} [f (\boldsymbol{y}) ]$.Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.","[0, 0, 0, 0, 1]",[],ryf6Fs09YX,GO Gradient for Expectation-Based Objectives,"Présente un estimateur de gradient pour les objectifs basés sur l'espérance qui est sans biais, a une faible variance et s'applique aux variables aléatoires continues et discrètes."
"Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\boldsymbol{\gamma}$ for expectation-based objectives $\mathbb{E}_{q_{\boldsymbol{\gamma}} (\boldsymbol{y})} [f (\boldsymbol{y}) ]$.Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.","[0, 0, 0, 0, 1]",[],ryf6Fs09YX,GO Gradient for Expectation-Based Objectives,"Une méthode améliorée pour calculer les dérivés de l'espérance, et un nouvel estimateur à gradient de faible variance qui permet l'apprentissage de modèles génératifs dans lesquels les observations ou les variables latentes sont discrètes."
"Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\boldsymbol{\gamma}$ for expectation-based objectives $\mathbb{E}_{q_{\boldsymbol{\gamma}} (\boldsymbol{y})} [f (\boldsymbol{y}) ]$.Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables.","[0, 0, 0, 0, 1]",[],ryf6Fs09YX,GO Gradient for Expectation-Based Objectives,Conçoit un gradient de faible variance pour des distributions associées à des variables aléatoires continues ou discrètes.
"Quantum computers promise significant advantages over classical computers for a number of different applications.We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.","[0, 1, 0, 0, 0]",[],SyxvSiCcFQ,Neural Network Cost Landscapes as Quantum States,Nous montrons que les paysages des paramètres et des coûts des hyperparamètres des NN peuvent être générés sous forme d'états quantiques à l'aide d'un seul circuit quantique et qu'ils peuvent être utilisés pour la formation et la méta-formation.
"Quantum computers promise significant advantages over classical computers for a number of different applications.We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.","[0, 1, 0, 0, 0]",[],SyxvSiCcFQ,Neural Network Cost Landscapes as Quantum States,Décrit une méthode permettant de quantifier un cadre d'apprentissage profond en considérant la forme à deux états d'une sphère de Bloch/qubit et en créant un réseau neuronal binaire quantique.
"Quantum computers promise significant advantages over classical computers for a number of different applications.We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.","[0, 1, 0, 0, 0]",[],SyxvSiCcFQ,Neural Network Cost Landscapes as Quantum States,"Cet article propose l'amplification d'amplitude quantique, un nouvel algorithme pour la formation et la sélection de modèles dans les réseaux de neurones binaires."
"Quantum computers promise significant advantages over classical computers for a number of different applications.We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network.","[0, 1, 0, 0, 0]",[],SyxvSiCcFQ,Neural Network Cost Landscapes as Quantum States,"Propose une nouvelle idée de sortie d'un état quantique qui représente un paysage complet des coûts de tous les paramètres pour un réseau neuronal binaire donné, en construisant un réseau neuronal binaire quantique (QBNN)."
"Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations.These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others.We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness.Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.","[1, 0, 0, 0, 0]",[],BygANhA9tQ,Cost-Sensitive Robustness against Adversarial Examples,Une méthode générale pour l'entraînement d'un classificateur robuste certifié sensible aux coûts contre les perturbations adverses
"Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations.These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others.We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness.Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.","[1, 0, 0, 0, 0]",[],BygANhA9tQ,Cost-Sensitive Robustness against Adversarial Examples,Calcule et intègre les coûts des attaques adverses dans l'objectif d'optimisation afin d'obtenir un modèle qui soit sensible aux coûts et robuste contre les attaques adverses. 
"Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations.These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others.We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness.Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.","[1, 0, 0, 0, 0]",[],BygANhA9tQ,Cost-Sensitive Robustness against Adversarial Examples,S'appuie sur les travaux semestriels de Dalvi et al. et étend l'approche de la robustesse certifiable avec une matrice de coûts qui spécifie pour chaque paire de classes source-cible si le modèle doit être robuste aux exemples adverses.
"Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain.However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina.Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response.This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed.Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina.The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input.Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.","[0, 0, 0, 0, 0, 0, 1]",[],HJhIM0xAW,Learning a neural response metric for retinal prosthesis,Utilisation de triplets pour apprendre une métrique de comparaison des réponses neuronales et améliorer les performances d'une prothèse.
"Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain.However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina.Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response.This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed.Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina.The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input.Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.","[0, 0, 0, 0, 0, 0, 1]",[],HJhIM0xAW,Learning a neural response metric for retinal prosthesis,"Les auteurs développent de nouvelles métriques de distance de train d'épis, incluant des réseaux neuronaux et des métriques quadratiques. Ils montrent que ces métriques sont plus performantes que la distance naïve de Hamming et qu'elles capturent implicitement une certaine structure dans le code neuronal."
"Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain.However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina.Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response.This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed.Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina.The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input.Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis.","[0, 0, 0, 0, 0, 0, 1]",[],HJhIM0xAW,Learning a neural response metric for retinal prosthesis,"En vue d'améliorer les prothèses neuronales, les auteurs proposent d'apprendre une métrique entre les réponses neuronales en optimisant une forme quadratique ou un réseau neuronal profond."
"We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem.The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth).Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic.We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries.In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention.Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.","[0, 0, 0, 0, 1, 0, 0]",[],r5vnRRwrgX,QCue: Queries and Cues for Computer-Facilitated Mind-Mapping,Cet article présente une méthode permettant de générer des questions (indices) et des requêtes (suggestions) afin d'aider les utilisateurs à réaliser des cartes mentales.
"We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem.The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth).Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic.We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries.In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention.Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.","[0, 0, 0, 0, 1, 0, 0]",[],r5vnRRwrgX,QCue: Queries and Cues for Computer-Facilitated Mind-Mapping,Présente un outil d'aide à la cartographie de l'esprit grâce à des suggestions de contexte liées aux nœuds existants et à des questions qui développent des branches moins développées.
"We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem.The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth).Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic.We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries.In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention.Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback.","[0, 0, 0, 0, 1, 0, 0]",[],r5vnRRwrgX,QCue: Queries and Cues for Computer-Facilitated Mind-Mapping,"Cet article présente une approche pour aider les personnes à effectuer des tâches de mindmapping, en concevant une interface et des caractéristiques algorithmiques pour soutenir le mindmapping, et contribue à une étude d'évaluation."
"The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.The source code of our method will be made publicly available.","[0, 1, 0, 0, 0, 0, 0]",[],rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,Détection d'échantillons hors distribution à l'aide de statistiques de caractéristiques d'ordre inférieur sans qu'il soit nécessaire de modifier le DNN sous-jacent.
"The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.The source code of our method will be made publicly available.","[0, 1, 0, 0, 0, 0, 0]",[],rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,Présente un algorithme pour détecter les échantillons hors distribution en utilisant l'estimation courante de la moyenne et de la variance dans les couches BatchNorm pour construire des représentations de caractéristiques qui sont ensuite introduites dans un classificateur linéaire.
"The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.The source code of our method will be made publicly available.","[0, 1, 0, 0, 0, 0, 0]",[],rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,Une approche pour détecter les échantillons hors distribution dans laquelle les auteurs proposent d'utiliser la régression logistique sur les statistiques simples de chaque couche de normalisation de lot du CNN.
"The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.The source code of our method will be made publicly available.","[0, 1, 0, 0, 0, 0, 0]",[],rkgpCoRctm,Detecting Out-Of-Distribution Samples Using Low-Order Deep Features Statistics,L'article suggère d'utiliser les scores Z pour comparer les échantillons d'ID et d'OOD afin d'évaluer ce que les réseaux profonds essaient de faire.
"Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security.However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts.In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder.In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries.We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.","[0, 0, 1, 0, 0]",[],ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,Nous proposons une nouvelle méthode nommée Maximal Divergence Sequential Auto-Encoder qui exploite la représentation de l'AutoEncoder variationnel pour la détection des vulnérabilités du code binaire.
"Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security.However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts.In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder.In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries.We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.","[0, 0, 1, 0, 0]",[],ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,Cet article propose une architecture basée sur un autoencodeur variationnel pour les incorporations de code dans le cadre de la détection des vulnérabilités des logiciels binaires. Les incorporations apprises sont plus efficaces pour distinguer les codes binaires vulnérables et non vulnérables que les codes de base.
"Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security.However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts.In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder.In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries.We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest.","[0, 0, 1, 0, 0]",[],ByloIiCqYQ,Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability Detection,Cet article propose un modèle d'extraction automatique de caractéristiques pour la détection de vulnérabilités à l'aide d'une technique d'apprentissage profond. 
"Modern neural architectures critically rely on attention for mapping structured inputs to sequences.In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output.Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output.Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.","[0, 0, 0, 0, 1, 0]",[],BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,Le calcul de l'attention sur la base de la distribution postérieure conduit à une attention plus significative et à de meilleures performances.
"Modern neural architectures critically rely on attention for mapping structured inputs to sequences.In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output.Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output.Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.","[0, 0, 0, 0, 1, 0]",[],BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,"Cet article propose un modèle de séquence à séquence où l'attention est traitée comme une variable latente, et dérive de nouvelles procédures d'inférence pour ce modèle, obtenant des améliorations dans les tâches de traduction automatique et de génération d'inflexions morphologiques."
"Modern neural architectures critically rely on attention for mapping structured inputs to sequences.In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  First, the position where attention is marginalized is changed from the input to the output.Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output.Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models.","[0, 0, 0, 0, 1, 0]",[],BkltNhC9FX,Posterior Attention Models for Sequence to Sequence Learning,Cet article présente un nouveau modèle d'attention postérieure pour les problèmes seq2seq.
"The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms.Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation.This is achieved by either eliminating components from the model, or penalizing complexity during training.While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions.In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it.We will show that using this technique, we can achieve competitive results.","[0, 0, 1, 0, 0, 0, 0]",[],By0ANxbRW,DNN Model Compression Under Accuracy Constraints,Compression des modèles DNN entraînés en minimisant leur complexité tout en limitant leur perte.
"The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms.Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation.This is achieved by either eliminating components from the model, or penalizing complexity during training.While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions.In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it.We will show that using this technique, we can achieve competitive results.","[0, 0, 1, 0, 0, 0, 0]",[],By0ANxbRW,DNN Model Compression Under Accuracy Constraints,Cet article propose une méthode de compression des réseaux neuronaux profonds sous contraintes de précision.
"The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms.Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation.This is achieved by either eliminating components from the model, or penalizing complexity during training.While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions.In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it.We will show that using this technique, we can achieve competitive results.","[0, 0, 1, 0, 0, 0, 0]",[],By0ANxbRW,DNN Model Compression Under Accuracy Constraints,Cet article présente une méthode d'encodage k-means contrainte par la valeur de perte pour la compression de réseau et développe un algorithme itératif pour l'optimisation du modèle.
"Deep neural networks are able to solve tasks across a variety of domains and modalities of data.Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes.In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs.The computed ""attention masks"" support improved interpretability by highlighting which input attributes are critical in determining output.We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing.The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","[1, 0, 0, 0, 0, 0, 0]",[],SJ60SbW0b,Modeling Latent Attention Within Neural Networks,Nous développons une technique permettant de visualiser les mécanismes d'attention dans des réseaux neuronaux arbitraires. 
"Deep neural networks are able to solve tasks across a variety of domains and modalities of data.Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes.In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs.The computed ""attention masks"" support improved interpretability by highlighting which input attributes are critical in determining output.We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing.The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","[1, 0, 0, 0, 0, 0, 0]",[],SJ60SbW0b,Modeling Latent Attention Within Neural Networks,Propose d'apprendre un réseau d'attention latente qui peut aider à visualiser la structure interne d'un réseau neuronal profond.
"Deep neural networks are able to solve tasks across a variety of domains and modalities of data.Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes.In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs.The computed ""attention masks"" support improved interpretability by highlighting which input attributes are critical in determining output.We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing.The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","[1, 0, 0, 0, 0, 0, 0]",[],SJ60SbW0b,Modeling Latent Attention Within Neural Networks,Les auteurs de cet article proposent un schéma de visualisation de boîtes noires basé sur les données. 
"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","[0, 0, 0, 1, 0]",[],HkcTe-bR-,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design,Nous étudions une variété d'algorithmes RL pour la génération de molécules et définissons de nouveaux benchmarks (qui seront publiés sous la forme d'un Gym OpenAI). Nous avons constaté que le PPO et un algorithme MLE d'ascension de colline fonctionnent le mieux.
"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","[0, 0, 0, 1, 0]",[],HkcTe-bR-,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design,"Examine l'évaluation des modèles pour la génération de molécules en proposant 19 points de référence, en élargissant les petits ensembles de données à un grand ensemble de données normalisées et en explorant comment appliquer les techniques de RL pour la conception de molécules."
"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","[0, 0, 0, 1, 0]",[],HkcTe-bR-,Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design,"Cet article montre que les méthodes RL les plus sophistiquées sont moins efficaces que la simple technique d'ascension de collines, l'OPP faisant exception, lors de la modélisation et de la synthèse de molécules."
"Analogical reasoning has been a principal focus of various waves of AI research.Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience.Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data.We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model.The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features.Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.","[0, 0, 0, 0, 1, 0]",[],SylLYsCcFm,Learning to Make Analogies by Contrasting Abstract Relational Structure,La capacité la plus robuste de raisonnement analogique est induite lorsque les réseaux apprennent des analogies en contrastant des structures relationnelles abstraites dans leurs domaines d'entrée.
"Analogical reasoning has been a principal focus of various waves of AI research.Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience.Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data.We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model.The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features.Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.","[0, 0, 0, 0, 1, 0]",[],SylLYsCcFm,Learning to Make Analogies by Contrasting Abstract Relational Structure,"L'article étudie la capacité d'un réseau neuronal à apprendre l'analogie, en montrant qu'un réseau neuronal simple est capable de résoudre certains problèmes d'analogie."
"Analogical reasoning has been a principal focus of various waves of AI research.Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience.Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data.We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model.The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features.Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures.","[0, 0, 0, 0, 1, 0]",[],SylLYsCcFm,Learning to Make Analogies by Contrasting Abstract Relational Structure,"Cet article décrit une approche pour former des réseaux de neurones pour des tâches de raisonnement analogique, en considérant spécifiquement l'analogie visuelle et les analogies symboliques."
"Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding.Much progress has been made to build conversation models using techniques such as sequence2sequence modeling.One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals.Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome.However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model.In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents.Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged.We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJXyS7bRb,A Goal-oriented Neural Conversation Model by Self-Play,Un modèle de conversation neuronale orienté vers le but par l'autoproduction
"Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding.Much progress has been made to build conversation models using techniques such as sequence2sequence modeling.One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals.Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome.However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model.In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents.Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged.We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJXyS7bRb,A Goal-oriented Neural Conversation Model by Self-Play,"Un modèle de jeu autonome pour la génération de dialogues orientés vers les objectifs, visant à renforcer le couplage entre la récompense de la tâche et le modèle de langage."
"Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding.Much progress has been made to build conversation models using techniques such as sequence2sequence modeling.One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals.Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome.However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model.In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents.Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged.We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJXyS7bRb,A Goal-oriented Neural Conversation Model by Self-Play,Cet article décrit une méthode d'amélioration d'un système de dialogue orienté vers un objectif en utilisant le selfplay. 
"Search engine users nowadays heavily depend on query completion and correction to shape their queries. Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database. In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.  We show how to address two main challenges that renders this method practical for large-scale deployment: 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms).Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.","[1, 0, 0, 0, 0, 0, 0]",[],By3VrbbAb,Realtime query completion via deep language models,complétion de requêtes de recherche en temps réel à l'aide de modèles de langage LSTM au niveau des caractères
"Search engine users nowadays heavily depend on query completion and correction to shape their queries. Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database. In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.  We show how to address two main challenges that renders this method practical for large-scale deployment: 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms).Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.","[1, 0, 0, 0, 0, 0, 0]",[],By3VrbbAb,Realtime query completion via deep language models,"Cet article présente des méthodes de complétion de requêtes qui incluent la correction des préfixes, ainsi que certains détails d'ingénierie pour répondre à des exigences particulières de latence sur un CPU."
"Search engine users nowadays heavily depend on query completion and correction to shape their queries. Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database. In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix.  We show how to address two main challenges that renders this method practical for large-scale deployment: 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms).Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.","[1, 0, 0, 0, 0, 0, 0]",[],By3VrbbAb,Realtime query completion via deep language models,Les auteurs proposent un algorithme pour résoudre le problème de complétion des requêtes avec correction des erreurs. Ils adoptent une modélisation basée sur les RNN au niveau des caractères et optimisent la partie inférence pour atteindre les objectifs en temps réel.
"RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear.Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants.In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways.First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10.Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \beta_1.We show that at very high values of the momentum parameter (\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses.On the other hand, NAG can sometimes do better when ADAM's \beta_1 is set to the most commonly used value: \beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkgd0iA9FQ,Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration,"Dans cet article, nous prouvons la convergence vers la criticité de RMSProp (stochastique et déterministe) et de ADAM déterministe pour des objectifs lisses non convexes et nous démontrons une sensibilité intéressante de beta_1 pour ADAM sur les autoencodeurs. "
"RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear.Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants.In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways.First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10.Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \beta_1.We show that at very high values of the momentum parameter (\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses.On the other hand, NAG can sometimes do better when ADAM's \beta_1 is set to the most commonly used value: \beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkgd0iA9FQ,Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration,Cet article présente une analyse de convergence de RMSProp et ADAM dans le cas de fonctions lisses non convexes.
"Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems.We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model.The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners.We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network.To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel.Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer.As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed.We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm.Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyI6s40a-,Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,Il est essentiel de concevoir des mécanismes de défense non supervisés contre les attaques adverses pour garantir la généralisation de la défense. 
"Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems.We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model.The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners.We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network.To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel.Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer.As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed.We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm.Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyI6s40a-,Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,Cet article présente une méthode de détection des exemples contradictoires dans un contexte de classification par apprentissage profond.
"Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems.We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model.The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners.We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network.To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel.Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer.As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed.We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm.Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyI6s40a-,Towards Safe Deep Learning: Unsupervised Defense Against Generic Adversarial Attacks,Cet article présente une méthode non supervisée de détection des exemples défavorables de réseaux neuronaux.
"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures.However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet).Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size).As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs.These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task.In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms.We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set.Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization.On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters.On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency.We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,Recherche d'une architecture neuronale sans proxy pour l'apprentissage direct d'architectures sur une tâche cible à grande échelle (ImageNet) tout en réduisant le coût au même niveau que la formation normale.
"Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures.However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet).Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size).As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs.These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task.In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms.We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set.Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization.On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6× fewer parameters.On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2× faster with measured GPU latency.We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HylVB3AqYm,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,"Cet article aborde le problème de la recherche d'architecture, et cherche spécifiquement à le faire sans avoir à s'entraîner sur des tâches ""proxy"" où le problème est simplifié par une optimisation, une complexité architecturale ou une taille d'ensemble de données plus limitées."
"With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}.Our method first assumes there exists a underlying higher-order distribution $\mathcal{P}(z)$, which generated label-wise distribution $\mathcal{P}(y)$over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However, we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore, we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Throughcomprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,Un nouveau cadre basé sur l'inférence variationnelle pour la détection de hors distribution
"With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}.Our method first assumes there exists a underlying higher-order distribution $\mathcal{P}(z)$, which generated label-wise distribution $\mathcal{P}(y)$over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However, we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore, we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Throughcomprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,Décrit une approche probabiliste pour quantifier l'incertitude dans les tâches de classification DNN qui surpasse les autres méthodes SOTA dans la tâche de détection de la non-répartition.
"With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}.Our method first assumes there exists a underlying higher-order distribution $\mathcal{P}(z)$, which generated label-wise distribution $\mathcal{P}(y)$over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However, we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore, we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Throughcomprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,"Un nouveau cadre pour la détection de la non-répartition, basé sur l'inférence variaitonale et une distribution de Dirichlet antérieure, qui présente les résultats de l'état de l'art sur plusieurs ensembles de données."
"With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}.Our method first assumes there exists a underlying higher-order distribution $\mathcal{P}(z)$, which generated label-wise distribution $\mathcal{P}(y)$over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However, we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore, we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Throughcomprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],ByxmXnA9FQ,A Variational Dirichlet Framework for Out-of-Distribution Detection,Une détection de la distribution hors norme via une nouvelle méthode d'approximation de la distribution de confiance de la probabilité de classification utilisant l'inférence variationnelle de la distribution de Dirichlet.
"Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.In this work, we address the problem of learning an agent’s action space purely from visual observation.We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.Project website: https://daniilidis-group.github.io/learned_action_spaces","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],SylPMnR9Ym,Learning what you can do before doing anything,Nous apprenons une représentation de l'espace d'action d'un agent à partir d'observations visuelles pures. Nous utilisons une approche récurrente à variables latentes avec une nouvelle perte de composabilité.
"Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.In this work, we address the problem of learning an agent’s action space purely from visual observation.We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.Project website: https://daniilidis-group.github.io/learned_action_spaces","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],SylPMnR9Ym,Learning what you can do before doing anything,Propose un modèle compositionnel à variables latentes pour apprendre des modèles qui prédisent ce qui va se passer ensuite dans des scénarios où les étiquettes d'action ne sont pas disponibles en abondance.
"Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.In this work, we address the problem of learning an agent’s action space purely from visual observation.We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.Project website: https://daniilidis-group.github.io/learned_action_spaces","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],SylPMnR9Ym,Learning what you can do before doing anything,"Une approche basée sur l'IB variationnelle pour apprendre des représentations d'actions directement à partir de vidéos d'actions en cours, ce qui permet d'obtenir une meilleure efficacité des méthodes d'apprentissage ultérieures tout en nécessitant une quantité moindre de vidéos d'étiquettes d'actions."
"Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.In this work, we address the problem of learning an agent’s action space purely from visual observation.We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.Project website: https://daniilidis-group.github.io/learned_action_spaces","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],SylPMnR9Ym,Learning what you can do before doing anything,Cet article propose une approche de la prédiction vidéo qui trouve de manière autonome un espace d'action codant les différences entre les images suivantes.
"When autonomous agents interact in the same environment, they must often cooperate to achieve their goals.One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it.However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement.Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols.More general methods usually require human input or domain-specific data, and so do not scale.To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning.Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven.We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory.Additionally, we investigate how the physical location of agents influences negotiation outcomes.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJG0ojCcFm,Negotiating Team Formation Using Deep Reinforcement Learning,L'apprentissage par renforcement peut être utilisé pour entraîner les agents à négocier la formation d'équipes dans de nombreux protocoles de négociation.
"When autonomous agents interact in the same environment, they must often cooperate to achieve their goals.One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it.However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement.Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols.More general methods usually require human input or domain-specific data, and so do not scale.To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning.Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven.We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory.Additionally, we investigate how the physical location of agents influences negotiation outcomes.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJG0ojCcFm,Negotiating Team Formation Using Deep Reinforcement Learning,"Cet article étudie la RL profonde multi-agents dans des contextes où tous les agents doivent coopérer pour accomplir une tâche (par exemple, recherche et sauvetage, jeux vidéo multi-joueurs), et utilise des jeux simples de vote pondéré coopératif pour étudier l'efficacité de la RL profonde et pour comparer les solutions trouvées par la RL profonde à une solution équitable."
"When autonomous agents interact in the same environment, they must often cooperate to achieve their goals.One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it.However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement.Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols.More general methods usually require human input or domain-specific data, and so do not scale.To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning.Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven.We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory.Additionally, we investigate how the physical location of agents influences negotiation outcomes.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJG0ojCcFm,Negotiating Team Formation Using Deep Reinforcement Learning,Une approche d'apprentissage par renforcement pour négocier des coalitions dans des contextes de théorie des jeux coopératifs qui peut être utilisée dans les cas où des simulations d'entraînement illimitées sont disponibles.
"Neural machine translation (NMT) models learn representations containing substantial linguistic information.However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons.We develop unsupervised methods for discovering important neurons in NMT models.Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision.We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena.Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.","[0, 0, 1, 0, 0, 0]",[],H1z-PsR5KX,Identifying and Controlling Important Neurons in Neural Machine Translation,"Méthodes non supervisées pour la recherche, l'analyse et le contrôle des neurones importants en NMT"
"Neural machine translation (NMT) models learn representations containing substantial linguistic information.However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons.We develop unsupervised methods for discovering important neurons in NMT models.Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision.We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena.Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.","[0, 0, 1, 0, 0, 0]",[],H1z-PsR5KX,Identifying and Controlling Important Neurons in Neural Machine Translation,Cet article présente des approches non supervisées pour découvrir les neurones importants dans les systèmes neuronaux de traduction automatique et analyse les propriétés linguistiques contrôlées par ces neurones.
"Neural machine translation (NMT) models learn representations containing substantial linguistic information.However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons.We develop unsupervised methods for discovering important neurons in NMT models.Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision.We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena.Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.","[0, 0, 1, 0, 0, 0]",[],H1z-PsR5KX,Identifying and Controlling Important Neurons in Neural Machine Translation,Méthodes non supervisées de classement des neurones dans la traduction automatique où les neurones importants sont ainsi identifiés et utilisés pour contrôler la sortie de la TA.
"Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task.Hence, both environment and task specific knowledge are entangled into one framework.However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes.Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units.The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task.The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.","[0, 0, 0, 1, 0, 0]",[],B1mvVm-C-,Universal Agent for Disentangling Environments and Tasks,Nous proposons un cadre DRL qui dissocie les connaissances spécifiques à la tâche et à l'environnement.
"Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task.Hence, both environment and task specific knowledge are entangled into one framework.However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes.Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units.The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task.The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.","[0, 0, 0, 1, 0, 0]",[],B1mvVm-C-,Universal Agent for Disentangling Environments and Tasks,Les auteurs proposent de décomposer l'apprentissage par renforcement en une fonction PATH et une fonction GOAL.
"Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task.Hence, both environment and task specific knowledge are entangled into one framework.However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes.Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units.The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task.The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.","[0, 0, 0, 1, 0, 0]",[],B1mvVm-C-,Universal Agent for Disentangling Environments and Tasks,"Une architecture modulaire visant à séparer les connaissances spécifiques à l'environnement et les connaissances spécifiques à la tâche en différents modules, au même titre que l'A3C standard pour un large éventail de tâches."
"Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics.We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.Our method learns the depth and orientation of scene points visible in images.Our model can then predict the structure of a scene from various, previously unseen view points.It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry.We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images.We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet.Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJeem3C9F7,Pix2Scene: Learning Implicit 3D Representations from Images,pix2scene : une approche générative profonde pour la modélisation implicite des propriétés géométriques d'une scène 3D à partir d'images
"Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics.We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.Our method learns the depth and orientation of scene points visible in images.Our model can then predict the structure of a scene from various, previously unseen view points.It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry.We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images.We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet.Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJeem3C9F7,Pix2Scene: Learning Implicit 3D Representations from Images,"Exploration de l'explication de scènes avec des surfels dans un modèle de reconnaissance neuronal, et démonstration des résultats sur la reconstruction et la synthèse d'images, et la rotation de formes mentales."
"Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics.We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.Our method learns the depth and orientation of scene points visible in images.Our model can then predict the structure of a scene from various, previously unseen view points.It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry.We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images.We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet.Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.","[0, 1, 0, 0, 0, 0, 0, 0]",[],BJeem3C9F7,Pix2Scene: Learning Implicit 3D Representations from Images,Les auteurs présentent une méthode permettant de créer un modèle de scène 3D à partir d'une image 2D et d'une pose de caméra en utilisant un modèle auto-superficiel.
"Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning.Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any.Despite this, how to accurately learn generic relation representations from word representations remains unclear.We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction.Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.","[0, 0, 0, 1, 0, 0]",[],r1e3WW5aTX,Learning Relation Representations from Word Representations,L'identification des relations qui relient les mots est importante pour diverses tâches de TAL. Nous modélisons la représentation des relations comme un problème d'apprentissage supervisé et apprenons des opérateurs paramétrés qui font correspondre des encastrements de mots pré-entraînés à des représentations de relations.
"Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning.Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any.Despite this, how to accurately learn generic relation representations from word representations remains unclear.We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction.Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.","[0, 0, 0, 1, 0, 0]",[],r1e3WW5aTX,Learning Relation Representations from Word Representations,Cet article présente une nouvelle méthode pour représenter les relations lexicales sous forme de vecteurs en utilisant uniquement des encastrements de mots pré-entraînés et une nouvelle fonction de perte opérant sur des paires de paires de mots.
"Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning.Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any.Despite this, how to accurately learn generic relation representations from word representations remains unclear.We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction.Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words.","[0, 0, 0, 1, 0, 0]",[],r1e3WW5aTX,Learning Relation Representations from Word Representations,Une nouvelle solution au problème de la composition des relations lorsque vous disposez déjà d'embeddings de mots/entités pré-entraînés et que vous souhaitez uniquement apprendre à composer des représentations de relations.
"Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction.However, optimizing RNNs is known to be harder compared to feed-forward neural networks.A number of techniques have been proposed in literature to address this problem.In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal.Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust.We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout.We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2.We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJyVzQ-C-,Fraternal Dropout,Nous proposons d'entraîner deux copies identiques d'un réseau neuronal récurrent (qui partagent des paramètres) avec des masques d'abandon différents tout en minimisant la différence entre leurs prédictions (pré-softmax).
"Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction.However, optimizing RNNs is known to be harder compared to feed-forward neural networks.A number of techniques have been proposed in literature to address this problem.In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal.Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust.We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout.We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2.We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SJyVzQ-C-,Fraternal Dropout,"Présente le décrochage fraternel comme une amélioration par rapport au décrochage linéaire par espérance en termes de convergence, et démontre l'utilité du décrochage fraternel sur un certain nombre de tâches et d'ensembles de données."
"We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.Our method specifically targets the space-time representation of physical surfaces from liquid simulations.Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.Our representation makes it possible to rapidly generate the desired implicit surfaces.We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyeGBj09Fm,Generating Liquid Simulations with Deformation-aware Neural Networks,Apprentissage de la pondération et des déformations des ensembles de données spatio-temporelles pour des approximations très efficaces du comportement des liquides.
"We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.Our method specifically targets the space-time representation of physical surfaces from liquid simulations.Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.Our representation makes it possible to rapidly generate the desired implicit surfaces.We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyeGBj09Fm,Generating Liquid Simulations with Deformation-aware Neural Networks,Un modèle basé sur un réseau neuronal est utilisé pour interpoler des simulations pour de nouvelles conditions de scène à partir de surfaces implicites 4D densément enregistrées pour une scène structurée.
"We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.Our method specifically targets the space-time representation of physical surfaces from liquid simulations.Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.Our representation makes it possible to rapidly generate the desired implicit surfaces.We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyeGBj09Fm,Generating Liquid Simulations with Deformation-aware Neural Networks,Cet article présente une approche d'apprentissage profond couplé pour générer des données réalistes de simulation de liquide qui peuvent être utiles pour des applications d'aide à la décision en temps réel.
"We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.Our method specifically targets the space-time representation of physical surfaces from liquid simulations.Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.Our representation makes it possible to rapidly generate the desired implicit surfaces.We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyeGBj09Fm,Generating Liquid Simulations with Deformation-aware Neural Networks,Cet article présente une approche d'apprentissage profond pour la simulation physique qui combine deux réseaux pour synthétiser des données 4D représentant des simulations physiques 3D.
"This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.Thus the network is aware not of the specific color object, but its colorfulness.The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.An additional annotation was done which labeled whether the car shown was red or non-red.  The networks were evaluated by their performance on the classification task.With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.We further split the test data in red and non-red cars and did a similar evaluation.It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.The limits of these networks are also discussed.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkoCeqgR-,On the Construction and Evaluation of Color Invariant Networks,Nous construisons et évaluons des réseaux neuronaux invariants en fonction de la couleur sur un nouvel ensemble de données réalistes.
"This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.Thus the network is aware not of the specific color object, but its colorfulness.The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.An additional annotation was done which labeled whether the car shown was red or non-red.  The networks were evaluated by their performance on the classification task.With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.We further split the test data in red and non-red cars and did a similar evaluation.It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.The limits of these networks are also discussed.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkoCeqgR-,On the Construction and Evaluation of Color Invariant Networks,Propose une méthode pour rendre les réseaux de neurones pour la reconnaissance d'images invariants en couleur et l'évalue sur le jeu de données cifar 10.
"This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.Thus the network is aware not of the specific color object, but its colorfulness.The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.An additional annotation was done which labeled whether the car shown was red or non-red.  The networks were evaluated by their performance on the classification task.With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.We further split the test data in red and non-red cars and did a similar evaluation.It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.The limits of these networks are also discussed.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkoCeqgR-,On the Construction and Evaluation of Color Invariant Networks,Les auteurs étudient une couche d'entrée modifiée qui permet d'obtenir des réseaux invariants en couleur et montrent que certaines couches d'entrée invariantes en couleur peuvent améliorer la précision pour des images de test dont la distribution des couleurs est différente de celle des images d'apprentissage.
"This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.Thus the network is aware not of the specific color object, but its colorfulness.The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.An additional annotation was done which labeled whether the car shown was red or non-red.  The networks were evaluated by their performance on the classification task.With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.We further split the test data in red and non-red cars and did a similar evaluation.It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.The limits of these networks are also discussed.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BkoCeqgR-,On the Construction and Evaluation of Color Invariant Networks,"Les auteurs testent un CNN sur des images dont les canaux de couleur ont été modifiés pour être invariants aux permutations, les performances n'étant pas trop dégradées. "
"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well.Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","[0, 0, 1, 0, 0, 0]",[],HkNGsseC-,On the Expressive Power of Overlapping Architectures of Deep Learning,Nous analysons comment le degré de chevauchement entre les champs réceptifs d'un réseau convolutif affecte son pouvoir expressif.
"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well.Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","[0, 0, 1, 0, 0, 0]",[],HkNGsseC-,On the Expressive Power of Overlapping Architectures of Deep Learning,"L'article étudie le pouvoir expressif fourni par le ""chevauchement"" dans les couches de convolution des DNN en considérant des activations linéaires avec mise en commun des produits."
"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well.Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.","[0, 0, 1, 0, 0, 0]",[],HkNGsseC-,On the Expressive Power of Overlapping Architectures of Deep Learning,Cet article analyse l'expressivité des circuits arithmétiques convolutifs et montre qu'un nombre exponentiellement grand de ConvAC non chevauchants est nécessaire pour approximer le tenseur de grille d'un ConvAC chevauchant.
"We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HylTXn0qYX,Efficiently testing local optimality and escaping saddles for ReLU networks,Un algorithme théorique pour tester l'optimalité locale et extraire les directions de descente aux points non différentiables des risques empiriques des réseaux ReLU à une couche cachée.
"We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HylTXn0qYX,Efficiently testing local optimality and escaping saddles for ReLU networks,Propose un algorithme pour vérifier si un point donné est un point stationnaire généralisé du second ordre.
"We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HylTXn0qYX,Efficiently testing local optimality and escaping saddles for ReLU networks,"Un algorithme théorique, impliquant la résolution de programmes quadratiques convexes et non convexes, pour vérifier l'optimalité locale et échapper aux selles lors de l'entraînement de réseaux ReLU à deux couches."
"We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HylTXn0qYX,Efficiently testing local optimality and escaping saddles for ReLU networks,L'auteur propose une méthode pour vérifier si un point est un point stationnaire ou non et ensuite classer les points stationnaires comme étant des points stationnaires locaux ou de second ordre.
"We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).","[0, 0, 0, 0, 1]",[],BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,Une nouvelle perte basée sur des négatifs relativement durs qui permet d'obtenir des performances de pointe dans la recherche de légendes d'images.
"We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1).","[0, 0, 0, 0, 1]",[],BkTQ8UckG,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,Apprentissage de l'intégration conjointe de phrases et d'images à l'aide de la perte de triplets appliquée aux négatifs les plus durs au lieu de faire la moyenne de tous les triplets.
"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks.It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem.We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively.DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations.In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","[1, 0, 0, 0, 0, 0]",[],B1D6ty-A-,Training Autoencoders by Alternating Minimization,Nous utilisons le principe de minimisation alternée pour fournir une nouvelle technique efficace d'entraînement des autoencodeurs profonds.
"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks.It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem.We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively.DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations.In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","[1, 0, 0, 0, 0, 0]",[],B1D6ty-A-,Training Autoencoders by Alternating Minimization,Cadre de minimisation alternée pour l'entraînement des réseaux auto-codeurs et codeurs-décodeurs
"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks.It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem.We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively.DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations.In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","[1, 0, 0, 0, 0, 0]",[],B1D6ty-A-,Training Autoencoders by Alternating Minimization,"Les auteurs explorent une approche d'optimisation alternée pour l'entraînement des codeurs automatiques, en traitant chaque couche comme un modèle linéaire généralisé, et suggèrent d'utiliser le GD normalisé stochastique comme algorithme de minimisation dans chaque phase."
"We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.","[1, 0, 0, 0]",[],ByzoVi0cFQ,Transfer Learning for Estimating Causal Effects Using Neural Networks,Apprentissage par transfert pour l'estimation des effets causaux à l'aide de réseaux neuronaux.
"We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.","[1, 0, 0, 0]",[],ByzoVi0cFQ,Transfer Learning for Estimating Causal Effects Using Neural Networks,"Développe des algorithmes pour estimer l'effet de traitement moyen conditionnel par ensemble de données auxiliaires dans différents environnements, à la fois avec et sans apprenant de base."
"We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.","[1, 0, 0, 0]",[],ByzoVi0cFQ,Transfer Learning for Estimating Causal Effects Using Neural Networks,"Les auteurs proposent des méthodes pour aborder une nouvelle tâche d'apprentissage par transfert pour l'estimation de la fonction CATE, et les évaluent en utilisant un cadre synthétique et un ensemble de données expérimentales du monde réel."
"We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data.","[1, 0, 0, 0]",[],ByzoVi0cFQ,Transfer Learning for Estimating Causal Effects Using Neural Networks,Utilisation de la régression par réseaux neuronaux et comparaison des cadres d'apprentissage par transfert pour estimer un effet de traitement moyen conditionnel sous des hypothèses d'ignorabilité stricte
"Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or ""motifs"", are thought to be building blocks of neural representations and information processing.We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology.Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction.Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used.An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance.In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.","[0, 1, 0, 0, 0, 0]",[],SkloDjAqYm,LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos,"Nous présentons LeMoNADe, une méthode de détection de motifs appris de bout en bout fonctionnant directement sur des vidéos d'imagerie calcique."
"Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or ""motifs"", are thought to be building blocks of neural representations and information processing.We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology.Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction.Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used.An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance.In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.","[0, 1, 0, 0, 0, 0]",[],SkloDjAqYm,LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos,"Cet article propose un modèle de type VAE pour l'identification de motifs à partir de vidéos d'imagerie calcique, qui s'appuie sur des variables de Bernouli et nécessite une astuce Gumbel-softmax pour l'inférence."
"A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning.However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set.Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks.For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime.Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set.The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills.The videos related to our experiments are available at: https://sites.google.com/view/deepdj","[1, 0, 0, 0, 0, 0, 0, 0]",[],rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,Nous proposons un cadre pour apprendre une bonne politique par apprentissage par imitation à partir d'un ensemble de démonstrations bruyantes via le méta-entraînement d'un évaluateur d'adéquation des démonstrations.
"A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning.However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set.Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks.For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime.Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set.The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills.The videos related to our experiments are available at: https://sites.google.com/view/deepdj","[1, 0, 0, 0, 0, 0, 0, 0]",[],rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,"Contribue à un algorithme basé sur MAML pour l'apprentissage par imitation qui détermine automatiquement si les démonstrations fournies sont ""appropriées""."
"A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning.However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set.Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks.For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime.Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set.The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills.The videos related to our experiments are available at: https://sites.google.com/view/deepdj","[1, 0, 0, 0, 0, 0, 0, 0]",[],rkxkHnA5tX,Learning from Noisy Demonstration Sets via Meta-Learned Suitability Assessor,"Procédé d'apprentissage par imitation à partir d'un ensemble de démonstrations comprenant des comportements inutiles, qui sélectionne les démonstrations utiles en fonction des gains de performance qu'elles procurent au moment du méta-apprentissage."
"We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions.We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph.We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young.We preserve the dependency structure between the labels with a given causal graph.We devise a two-stage procedure for learning a CiGM over the labels and the image.First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels.Later, we combine this with a conditional GAN to generate images conditioned on the binary labels.We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN.We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels.The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image.We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJE-4xW0W,CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,"Nous introduisons des modèles génératifs implicites causaux, qui peuvent échantillonner à partir de distributions conditionnelles et interventionnelles, et proposons également deux nouveaux GAN conditionnels que nous utilisons pour les entraîner."
"We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions.We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph.We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young.We preserve the dependency structure between the labels with a given causal graph.We devise a two-stage procedure for learning a CiGM over the labels and the image.First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels.Later, we combine this with a conditional GAN to generate images conditioned on the binary labels.We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN.We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels.The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image.We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJE-4xW0W,CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,"Procédé de combinaison d'un graphe occasionnel, décrivant la structure de dépendance des étiquettes, avec deux architectures GAN conditionnelles qui génèrent des images en conditionnant l'étiquette binaire."
"We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions.We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph.We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young.We preserve the dependency structure between the labels with a given causal graph.We devise a two-stage procedure for learning a CiGM over the labels and the image.First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels.Later, we combine this with a conditional GAN to generate images conditioned on the binary labels.We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN.We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels.The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image.We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJE-4xW0W,CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training,"Les auteurs abordent la question de l'apprentissage d'un modèle causal entre les variables de l'image et l'image elle-même à partir de données d'observation, lorsqu'il existe une structure causale entre les étiquettes de l'image."
"Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive.In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention.Severalrecent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why.In this study, we provide a theoretical justification to this property by viewingNCE as a low-rank matrix approximation.Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models.It also uncovers a surprising negative correlation between self-normalization andperplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1-IBSgMz, A Matrix Approximation View of NCE that Justifies Self-Normalization,Nous prouvons que le NCE est auto-normalisé et le démontrons sur des ensembles de données.
"Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive.In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention.Severalrecent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why.In this study, we provide a theoretical justification to this property by viewingNCE as a low-rank matrix approximation.Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models.It also uncovers a surprising negative correlation between self-normalization andperplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1-IBSgMz, A Matrix Approximation View of NCE that Justifies Self-Normalization,Présente une preuve de l'auto-normalisation de NCE en tant que résultat d'être une approximation de matrice de bas-rang de la matrice de probabilités conditionnelles normalisées.
"Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive.In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention.Severalrecent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why.In this study, we provide a theoretical justification to this property by viewingNCE as a low-rank matrix approximation.Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models.It also uncovers a surprising negative correlation between self-normalization andperplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1-IBSgMz, A Matrix Approximation View of NCE that Justifies Self-Normalization,Cet article examine le problème des modèles auto-normalisants et explique le mécanisme d'auto-normalisation en interprétant les NCE en termes de factorisation matricielle.
"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach.We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus.Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","[0, 0, 0, 0, 0, 1, 0]",[],By5SY2gA-,Towards Building Affect sensitive Word Distributions,L'enrichissement des incorporations de mots avec des informations sur les affects améliore leurs performances dans les tâches de prédiction des sentiments.
"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach.We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus.Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","[0, 0, 0, 0, 0, 1, 0]",[],By5SY2gA-,Towards Building Affect sensitive Word Distributions,Propose d'utiliser des lexiques affectés pour améliorer les incorporations de mots afin de surpasser les standards Word2vec et Glove.
"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach.We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus.Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","[0, 0, 0, 0, 0, 1, 0]",[],By5SY2gA-,Towards Building Affect sensitive Word Distributions,Cet article propose d'intégrer des informations provenant d'une ressource sémantique quantifiant l'affect des mots dans un algorithme d'incorporation de mots basé sur le texte afin que les modèles de langage reflètent mieux les phénomènes sémantiques et pragmatiques.
"Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach.We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus.Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication.","[0, 0, 0, 0, 0, 1, 0]",[],By5SY2gA-,Towards Building Affect sensitive Word Distributions,Cet article introduit des modifications aux fonctions de perte word2vec et GloVe afin d'incorporer des lexiques d'affects pour faciliter l'apprentissage d'incorporations de mots sensibles aux affects.
"Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks.Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset.However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained.To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph.Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs.","[0, 0, 0, 1, 0]",[],SygxYoC5FX,BIGSAGE: unsupervised inductive representation learning of graph via bi-attended sampling and global-biased aggregating,"Pour l'intégration non supervisée et inductive des réseaux, nous proposons une nouvelle approche pour explorer les voisins les plus pertinents et préserver les connaissances acquises précédemment sur les nœuds en utilisant l'architecture de bi-attention et en introduisant un biais global, respectivement."
"Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks.Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset.However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained.To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph.Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs.","[0, 0, 0, 1, 0]",[],SygxYoC5FX,BIGSAGE: unsupervised inductive representation learning of graph via bi-attended sampling and global-biased aggregating,Ce document propose une extension de GraphSAGE en utilisant une matrice de biais d'intégration globale dans les fonctions d'agrégation locales et une méthode d'échantillonnage des nœuds intéressants.
"Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors.We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors.Both theoretical and empirical evidence are provided to support this argument:(a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;(b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients.We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.","[0, 0, 1, 0, 0, 0, 0]",[],B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,Nous soutenons que la généralisation de l'intégration des graphes linéaires n'est pas due à la contrainte de dimensionnalité mais plutôt à la petite norme des vecteurs d'intégration.
"Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors.We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors.Both theoretical and empirical evidence are provided to support this argument:(a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;(b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients.We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.","[0, 0, 1, 0, 0, 0, 0]",[],B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,Les auteurs montrent que l'erreur de généralisation des méthodes d'intégration de graphes linéaires est limitée par la norme des vecteurs d'intégration plutôt que par des contraintes de dimensionnalité.
"Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors.We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors.Both theoretical and empirical evidence are provided to support this argument:(a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;(b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients.We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice.","[0, 0, 1, 0, 0, 0, 0]",[],B1e9csRcFm,The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration,Les auteurs proposent une limite théorique sur la performance de généralisation de l'apprentissage d'intégrations de graphes et soutiennent que la norme des coordonnées détermine le succès de la représentation apprise.
"Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning.We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step.We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover.Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE.We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers.Code is immediately available.","[0, 1, 0, 0, 0, 0]",[],S1fUpoR5FQ,Quasi-hyperbolic momentum and Adam for deep learning,Mélangez SGD et momentum (ou faites quelque chose de similaire avec Adam) pour un grand profit.
"Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning.We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step.We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover.Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE.We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers.Code is immediately available.","[0, 1, 0, 0, 0, 0]",[],S1fUpoR5FQ,Quasi-hyperbolic momentum and Adam for deep learning,"L'article propose des modifications simples à SGD et Adam, appelées variantes QH, qui peuvent récupérer la méthode parentale et une foule d'autres astuces d'optimisation."
"Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning.We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step.We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover.Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE.We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers.Code is immediately available.","[0, 1, 0, 0, 0, 0]",[],S1fUpoR5FQ,Quasi-hyperbolic momentum and Adam for deep learning,"Une variante du momentum classique qui prend une moyenne pondérée du momentum et de la mise à jour du gradient, et une évaluation de ses relations avec d'autres schémas d'optimisation basés sur le momentum."
"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks.A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function.lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads.Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice.Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets.Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns.We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HkpRBFxRb,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,Une nouvelle façon de généraliser les rendements lambda en permettant à l'agent RL de décider du poids qu'il veut accorder à chacun des rendements à n étapes.
"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks.A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function.lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads.Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice.Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets.Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns.We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HkpRBFxRb,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,"Étend l'algorithme A3C avec des retours lambda, et propose une approche pour apprendre les poids des retours."
"Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks.A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function.lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads.Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice.Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets.Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns.We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HkpRBFxRb,Learning to Mix n-Step Returns: Generalizing Lambda-Returns for Deep Reinforcement Learning,"Les auteurs présentent les retours autodidactiques basés sur la confiance, une méthode d'apprentissage profond RL permettant d'ajuster les poids d'un vecteur d'admissibilité dans l'estimation de valeur de type TD(lambda) afin de favoriser des estimations plus stables de l'état."
"Current end-to-end deep learning driving models have two problems: (1) Poorgeneralization ability of unobserved driving environment when diversity of train-ing driving dataset is limited (2) Lack of accident explanation ability when drivingmodels don’t work as expected.To tackle these two problems, rooted on the be-lieve that knowledge of associated easy task is benificial for addressing difficulttask, we proposed a new driving model which is composed of perception modulefor see and think and driving module for behave, and trained it with multi-taskperception-related basic knowledge and driving knowledge stepwisely. Specifi-cally segmentation map and depth map (pixel level understanding of images) wereconsidered as what & where and how far knowledge for tackling easier driving-related perception problems before generating final control commands for difficultdriving task.The results of experiments demonstrated the effectiveness of multi-task perception knowledge for better generalization and accident explanation abil-ity.With our method the average sucess rate of finishing most difficult navigationtasks in untrained city of CoRL test surpassed current benchmark method for 15percent in trained weather and 20 percent in untrained weathers.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B14rPj0qY7,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,nous avons proposé un nouveau modèle de conduite autonome composé d'un module de perception pour voir et penser et d'un module de conduite pour se comporter afin d'acquérir une meilleure capacité de généralisation et d'explication des accidents.
"Current end-to-end deep learning driving models have two problems: (1) Poorgeneralization ability of unobserved driving environment when diversity of train-ing driving dataset is limited (2) Lack of accident explanation ability when drivingmodels don’t work as expected.To tackle these two problems, rooted on the be-lieve that knowledge of associated easy task is benificial for addressing difficulttask, we proposed a new driving model which is composed of perception modulefor see and think and driving module for behave, and trained it with multi-taskperception-related basic knowledge and driving knowledge stepwisely. Specifi-cally segmentation map and depth map (pixel level understanding of images) wereconsidered as what & where and how far knowledge for tackling easier driving-related perception problems before generating final control commands for difficultdriving task.The results of experiments demonstrated the effectiveness of multi-task perception knowledge for better generalization and accident explanation abil-ity.With our method the average sucess rate of finishing most difficult navigationtasks in untrained city of CoRL test surpassed current benchmark method for 15percent in trained weather and 20 percent in untrained weathers.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B14rPj0qY7,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,Présente une architecture d'apprentissage multitâche pour l'estimation des cartes de profondeur et de segmentation et la prédiction de la conduite à l'aide d'un module de perception et d'un module de décision de conduite.
"Current end-to-end deep learning driving models have two problems: (1) Poorgeneralization ability of unobserved driving environment when diversity of train-ing driving dataset is limited (2) Lack of accident explanation ability when drivingmodels don’t work as expected.To tackle these two problems, rooted on the be-lieve that knowledge of associated easy task is benificial for addressing difficulttask, we proposed a new driving model which is composed of perception modulefor see and think and driving module for behave, and trained it with multi-taskperception-related basic knowledge and driving knowledge stepwisely. Specifi-cally segmentation map and depth map (pixel level understanding of images) wereconsidered as what & where and how far knowledge for tackling easier driving-related perception problems before generating final control commands for difficultdriving task.The results of experiments demonstrated the effectiveness of multi-task perception knowledge for better generalization and accident explanation abil-ity.With our method the average sucess rate of finishing most difficult navigationtasks in untrained city of CoRL test surpassed current benchmark method for 15percent in trained weather and 20 percent in untrained weathers.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B14rPj0qY7,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,"Une méthode pour une architecture modifiée de bout en bout qui a une meilleure capacité de généralisation et d'explication, qui est plus robuste à un cadre de test différent, et qui a une sortie de décodeur qui peut aider à déboguer le modèle."
"Current end-to-end deep learning driving models have two problems: (1) Poorgeneralization ability of unobserved driving environment when diversity of train-ing driving dataset is limited (2) Lack of accident explanation ability when drivingmodels don’t work as expected.To tackle these two problems, rooted on the be-lieve that knowledge of associated easy task is benificial for addressing difficulttask, we proposed a new driving model which is composed of perception modulefor see and think and driving module for behave, and trained it with multi-taskperception-related basic knowledge and driving knowledge stepwisely. Specifi-cally segmentation map and depth map (pixel level understanding of images) wereconsidered as what & where and how far knowledge for tackling easier driving-related perception problems before generating final control commands for difficultdriving task.The results of experiments demonstrated the effectiveness of multi-task perception knowledge for better generalization and accident explanation abil-ity.With our method the average sucess rate of finishing most difficult navigationtasks in untrained city of CoRL test surpassed current benchmark method for 15percent in trained weather and 20 percent in untrained weathers.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B14rPj0qY7,RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY,Les auteurs présentent un réseau de neurones convolutifs multi-tâches pour la conduite de bout en bout et fournissent des évaluations avec le simulateur open source CARLA montrant une meilleure performance de généralisation dans de nouvelles conditions de conduite que les lignes de base.
"Recently there has been a surge of interest in designing graph embedding methods.Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements.In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs.MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph.It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns.The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them.We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs.Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification.MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJeKCi0qYX,MILE: A Multi-Level Framework for Scalable Graph Embedding,Un cadre générique pour adapter les techniques existantes d'intégration de graphes à de grands graphes.
"Recently there has been a surge of interest in designing graph embedding methods.Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements.In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs.MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph.It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns.The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them.We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs.Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification.MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJeKCi0qYX,MILE: A Multi-Level Framework for Scalable Graph Embedding,Cet article propose un cadre d'intégration multi-niveaux à appliquer en plus des méthodes d'intégration de réseau existantes afin de s'adapter plus rapidement aux réseaux à grande échelle.
"Recently there has been a surge of interest in designing graph embedding methods.Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements.In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs.MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph.It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns.The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them.We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs.Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification.MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJeKCi0qYX,MILE: A Multi-Level Framework for Scalable Graph Embedding,Les auteurs proposent un cadre en trois étapes pour l'intégration de graphes à grande échelle avec une qualité d'intégration améliorée.
"Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries.Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data.With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure.To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner.We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space.The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)","[1, 0, 0, 0, 0, 0, 0]",[],BJgd7m0xRZ,Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,Une nouvelle méthode pour augmenter la résistance des OCSVM contre les attaques d'intégrité ciblées par des transformations non linéaires sélectives des données vers des dimensions inférieures.
"Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries.Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data.With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure.To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner.We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space.The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)","[1, 0, 0, 0, 0, 0, 0]",[],BJgd7m0xRZ,Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,Les auteurs proposent une défense contre les attaques contre la sécurité des détecteurs d'anomalies basés sur des SVM à une classe.
"Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries.Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data.With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure.To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner.We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space.The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)","[1, 0, 0, 0, 0, 0, 0]",[],BJgd7m0xRZ,Unsupervised Adversarial Anomaly  Detection using One-Class Support Vector Machines,Cet article explore la façon dont les projections aléatoires peuvent être utilisées pour rendre l'OCSVM robuste aux données d'entraînement perturbées de façon adversariale.
"In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively.We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation.Previously, the Information Bottleneck (IB) framework (\cite{Tishby99}) extracts relevant information for a target variable.Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance.We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  ","[0, 0, 0, 1, 0, 0]",[],ByED-X-0W,Parametric Information Bottleneck to Optimize Stochastic Neural Networks,Apprendre une meilleure représentation des réseaux neuronaux grâce au principe du goulot d'étranglement de l'information.
"In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively.We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation.Previously, the Information Bottleneck (IB) framework (\cite{Tishby99}) extracts relevant information for a target variable.Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance.We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  ","[0, 0, 0, 1, 0, 0]",[],ByED-X-0W,Parametric Information Bottleneck to Optimize Stochastic Neural Networks,"Propose une méthode d'apprentissage basée sur le cadre du goulot d'étranglement de l'information, où les couches cachées des réseaux profonds compriment l'entrée X tout en conservant suffisamment d'informations pour prédire la sortie Y."
"In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively.We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation.Previously, the Information Bottleneck (IB) framework (\cite{Tishby99}) extracts relevant information for a target variable.Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance.We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  ","[0, 0, 0, 1, 0, 0]",[],ByED-X-0W,Parametric Information Bottleneck to Optimize Stochastic Neural Networks,Cet article présente une nouvelle méthode de formation des réseaux neuronaux stochastiques suivant un cadre de pertinence/compression de l'information similaire au goulot d'étranglement de l'information.
"The maximum mean discrepancy (MMD) between two probability measures Pand Q is a metric that is zero if and only if all moments of the two measuresare equal, making it an appealing statistic for two-sample tests.Given i.i.d. samplesfrom P and Q, Gretton et al. (2012) show that we can construct an unbiasedestimator for the square of the MMD between the two distributions.If P is adistribution of interest and Q is the distribution implied by a generative neuralnetwork with stochastic inputs, we can use this estimator to train our neural network.However, in practice we do not always have i.i.d. samples from our targetof interest.Data sets often exhibit biases—for example, under-representation ofcertain demographics—and if we ignore this fact our machine learning algorithmswill propagate these biases.Alternatively, it may be useful to assume our data hasbeen gathered via a biased sample selection mechanism in order to manipulateproperties of the estimating distribution Q.In this paper, we construct an estimator for the MMD between P and Q when weonly have access to P via some biased sample selection mechanism, and suggestmethods for estimating this sample selection mechanism when it is not alreadyknown.We show that this estimator can be used to train generative neural networkson a biased data sample, to give a simulator that reverses the effect of thatbias.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,"Nous proposons un estimateur de l'écart moyen maximal, approprié lorsqu'une distribution cible n'est accessible que par une procédure de sélection d'échantillon biaisée, et nous montrons qu'il peut être utilisé dans un réseau génératif pour corriger ce biais."
"The maximum mean discrepancy (MMD) between two probability measures Pand Q is a metric that is zero if and only if all moments of the two measuresare equal, making it an appealing statistic for two-sample tests.Given i.i.d. samplesfrom P and Q, Gretton et al. (2012) show that we can construct an unbiasedestimator for the square of the MMD between the two distributions.If P is adistribution of interest and Q is the distribution implied by a generative neuralnetwork with stochastic inputs, we can use this estimator to train our neural network.However, in practice we do not always have i.i.d. samples from our targetof interest.Data sets often exhibit biases—for example, under-representation ofcertain demographics—and if we ignore this fact our machine learning algorithmswill propagate these biases.Alternatively, it may be useful to assume our data hasbeen gathered via a biased sample selection mechanism in order to manipulateproperties of the estimating distribution Q.In this paper, we construct an estimator for the MMD between P and Q when weonly have access to P via some biased sample selection mechanism, and suggestmethods for estimating this sample selection mechanism when it is not alreadyknown.We show that this estimator can be used to train generative neural networkson a biased data sample, to give a simulator that reverses the effect of thatbias.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,Propose un estimateur pondéré par l'importance de la MMD pour estimer la MMD entre des distributions basées sur des échantillons biaisés selon un schéma inconnu connu ou estimé.
"The maximum mean discrepancy (MMD) between two probability measures Pand Q is a metric that is zero if and only if all moments of the two measuresare equal, making it an appealing statistic for two-sample tests.Given i.i.d. samplesfrom P and Q, Gretton et al. (2012) show that we can construct an unbiasedestimator for the square of the MMD between the two distributions.If P is adistribution of interest and Q is the distribution implied by a generative neuralnetwork with stochastic inputs, we can use this estimator to train our neural network.However, in practice we do not always have i.i.d. samples from our targetof interest.Data sets often exhibit biases—for example, under-representation ofcertain demographics—and if we ignore this fact our machine learning algorithmswill propagate these biases.Alternatively, it may be useful to assume our data hasbeen gathered via a biased sample selection mechanism in order to manipulateproperties of the estimating distribution Q.In this paper, we construct an estimator for the MMD between P and Q when weonly have access to P via some biased sample selection mechanism, and suggestmethods for estimating this sample selection mechanism when it is not alreadyknown.We show that this estimator can be used to train generative neural networkson a biased data sample, to give a simulator that reverses the effect of thatbias.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,Les auteurs abordent le problème du biais de sélection de l'échantillon dans les MMD-GAN et proposent une estimation de la MMD entre deux distributions en utilisant la divergence moyenne maximale pondérée.
"The maximum mean discrepancy (MMD) between two probability measures Pand Q is a metric that is zero if and only if all moments of the two measuresare equal, making it an appealing statistic for two-sample tests.Given i.i.d. samplesfrom P and Q, Gretton et al. (2012) show that we can construct an unbiasedestimator for the square of the MMD between the two distributions.If P is adistribution of interest and Q is the distribution implied by a generative neuralnetwork with stochastic inputs, we can use this estimator to train our neural network.However, in practice we do not always have i.i.d. samples from our targetof interest.Data sets often exhibit biases—for example, under-representation ofcertain demographics—and if we ignore this fact our machine learning algorithmswill propagate these biases.Alternatively, it may be useful to assume our data hasbeen gathered via a biased sample selection mechanism in order to manipulateproperties of the estimating distribution Q.In this paper, we construct an estimator for the MMD between P and Q when weonly have access to P via some biased sample selection mechanism, and suggestmethods for estimating this sample selection mechanism when it is not alreadyknown.We show that this estimator can be used to train generative neural networkson a biased data sample, to give a simulator that reverses the effect of thatbias.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyuWNMZ0W,Directing Generative Networks with Weighted Maximum Mean Discrepancy,Cet article présente une modification de l'objectif utilisé pour entraîner les réseaux génératifs avec un adversaire MMD. 
"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm.Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive.We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution.We apply our method to a wide range of Atari Arcade Learning Environments.Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","[0, 1, 0, 0, 0]",[],Bk6qQGWRb,Efficient Exploration through Bayesian   Deep Q-Networks,Utilisation de la régression bayésienne pour estimer la postériorité des fonctions Q et déploiement de l'échantillonnage de Thompson comme stratégie d'exploration ciblée avec un compromis efficace entre l'exploration et l'exploitation.
"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm.Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive.We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution.We apply our method to a wide range of Atari Arcade Learning Environments.Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","[0, 1, 0, 0, 0]",[],Bk6qQGWRb,Efficient Exploration through Bayesian   Deep Q-Networks,Les auteurs proposent un nouvel algorithme d'exploration en RL profond où ils appliquent une régression linéaire bayésienne avec les caractéristiques de la dernière couche d'un réseau DQN pour estimer la fonction Q de chaque action.
"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm.Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive.We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution.We apply our method to a wide range of Atari Arcade Learning Environments.Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","[0, 1, 0, 0, 0]",[],Bk6qQGWRb,Efficient Exploration through Bayesian   Deep Q-Networks,Les auteurs décrivent comment utiliser les réseaux neuronaux bayésiens avec l'échantillonnage de Thompson pour une exploration efficace dans le cadre du q-learning et proposent une approche qui surpasse les approches d'exploration epsilon-greedy.
"In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN.The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out.Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer.Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning.While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers.By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency.We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.","[0, 1, 0, 0, 0, 0, 0]",[],B1GHb2RqYX,PolyCNN: Learning Seed Convolutional Filters,"PolyCNN n'a besoin d'apprendre qu'un seul filtre convolutif d'ensemencement à chaque couche. Il s'agit d'une variante efficace du CNN traditionnel, avec des performances équivalentes."
"In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN.The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out.Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer.Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning.While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers.By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency.We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.","[0, 1, 0, 0, 0, 0, 0]",[],B1GHb2RqYX,PolyCNN: Learning Seed Convolutional Filters,Tentatives de réduction du nombre de paramètres du modèle CNN en utilisant la transformation polynomiale des filtres pour créer un gonflement des réponses des filtres.
"In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN.The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out.Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer.Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning.While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers.By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency.We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.","[0, 1, 0, 0, 0, 0, 0]",[],B1GHb2RqYX,PolyCNN: Learning Seed Convolutional Filters,Les auteurs proposent une architecture de partage de poids pour réduire le nombre de paramètres des réseaux neuronaux convolutionnels avec filtres d'ensemencement.
"Detecting the emergence of abrupt property changes in time series is a challenging problem.Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches.However, selecting kernels is non-trivial in practice.Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms.In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model.With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications.The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.","[0, 0, 0, 0, 1, 0, 0]",[],r1GbfhRqF7,Kernel Change-point Detection with Auxiliary Deep Generative Models,"Dans cet article, nous proposons KL-CPD, un nouveau cadre d'apprentissage à noyau pour la CPD des séries temporelles qui optimise une limite inférieure de la puissance de test via un modèle génératif auxiliaire comme substitut de la distribution anormale. "
"Detecting the emergence of abrupt property changes in time series is a challenging problem.Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches.However, selecting kernels is non-trivial in practice.Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms.In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model.With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications.The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies.","[0, 0, 0, 0, 1, 0, 0]",[],r1GbfhRqF7,Kernel Change-point Detection with Auxiliary Deep Generative Models,Décrit une nouvelle approche pour optimiser le choix du noyau en vue d'une puissance d'essai accrue et montre qu'elle offre des améliorations par rapport aux autres solutions.
"Theories in cognitive psychology postulate that humans use similarity as a basisfor object categorization.However, work in image classification generally as-sumes disjoint and equally dissimilar classes to achieve super-human levels ofperformance on certain datasets.In our work, we adapt notions of similarity usingweak labels over multiple hierarchical levels to boost classification performance.Instead of pitting clustering directly against classification, we use a warm-startbased evaluation to explicitly provide value to a clustering representation by itsability to aid classification.We evaluate on CIFAR10 and a fine-grained classifi-cation dataset to show improvements in performance with the procedural additionof intermediate losses and weak labels based on multiple hierarchy levels.Further-more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-tion with intermediate losses outperforms a classification baseline by over 17% ona subset of Birdsnap dataset.Finally, we show improvement over AlexNet trainedusing ImageNet pre-trained weights as initializations which further supports our claim of the importance of similarity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1-4BLaQz,Cluster-based Warm-Start Nets,Regrouper avant de classer ; utiliser des étiquettes faibles pour améliorer la classification 
"Theories in cognitive psychology postulate that humans use similarity as a basisfor object categorization.However, work in image classification generally as-sumes disjoint and equally dissimilar classes to achieve super-human levels ofperformance on certain datasets.In our work, we adapt notions of similarity usingweak labels over multiple hierarchical levels to boost classification performance.Instead of pitting clustering directly against classification, we use a warm-startbased evaluation to explicitly provide value to a clustering representation by itsability to aid classification.We evaluate on CIFAR10 and a fine-grained classifi-cation dataset to show improvements in performance with the procedural additionof intermediate losses and weak labels based on multiple hierarchy levels.Further-more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-tion with intermediate losses outperforms a classification baseline by over 17% ona subset of Birdsnap dataset.Finally, we show improvement over AlexNet trainedusing ImageNet pre-trained weights as initializations which further supports our claim of the importance of similarity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1-4BLaQz,Cluster-based Warm-Start Nets,Propose l'utilisation d'une fonction de perte basée sur le regroupement à plusieurs niveaux d'un réseau profond ainsi que l'utilisation de la structure hiérarchique de l'espace des étiquettes pour former de meilleures représentations.
"Theories in cognitive psychology postulate that humans use similarity as a basisfor object categorization.However, work in image classification generally as-sumes disjoint and equally dissimilar classes to achieve super-human levels ofperformance on certain datasets.In our work, we adapt notions of similarity usingweak labels over multiple hierarchical levels to boost classification performance.Instead of pitting clustering directly against classification, we use a warm-startbased evaluation to explicitly provide value to a clustering representation by itsability to aid classification.We evaluate on CIFAR10 and a fine-grained classifi-cation dataset to show improvements in performance with the procedural additionof intermediate losses and weak labels based on multiple hierarchy levels.Further-more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-tion with intermediate losses outperforms a classification baseline by over 17% ona subset of Birdsnap dataset.Finally, we show improvement over AlexNet trainedusing ImageNet pre-trained weights as initializations which further supports our claim of the importance of similarity.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1-4BLaQz,Cluster-based Warm-Start Nets,Cet article utilise des informations hiérarchiques sur les étiquettes pour imposer des pertes supplémentaires aux représentations intermédiaires dans la formation des réseaux neuronaux.
"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels.However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks.In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state.We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","[0, 0, 1, 0]",[],BkCV_W-AZ,Regret Minimization for Partially Observable Deep Reinforcement Learning,"La minimisation du regret basée sur l'avantage est un nouvel algorithme d'apprentissage par renforcement profond qui est particulièrement efficace pour les tâches partiellement observables, comme la navigation à la première personne dans Doom et Minecraft."
"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels.However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks.In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state.We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","[0, 0, 1, 0]",[],BkCV_W-AZ,Regret Minimization for Partially Observable Deep Reinforcement Learning,Cet article présente les concepts de minimisation du regret contrefactuel dans le domaine de la RL profonde et un algorithme appelé ARM qui peut mieux gérer l'observabilité partielle.
"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels.However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks.In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state.We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","[0, 0, 1, 0]",[],BkCV_W-AZ,Regret Minimization for Partially Observable Deep Reinforcement Learning,L'article présente une variante de l'algorithme policy-gradient inspirée de la théorie des jeux et basée sur l'idée de minimisation du regret contre-factuel. Il affirme que cette approche peut traiter le domaine observable partiel mieux que les méthodes standard.
"Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks.Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed.To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL).TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers.Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers.Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task’s performance, particularly favourable in scenarios where some of the tasks have insufficient data.","[0, 0, 1, 0, 0, 0]",[],BJxmXhRcK7,TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING,un modèle d'apprentissage multi-tâches profond adaptant la représentation tensorielle en anneau
"Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks.Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed.To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL).TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers.Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers.Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task’s performance, particularly favourable in scenarios where some of the tasks have insufficient data.","[0, 0, 1, 0, 0, 0]",[],BJxmXhRcK7,TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING,"Une variante de la formulation de l'anneau tensoriel pour l'apprentissage multitâche en partageant certains des noyaux TT pour l'apprentissage de la ""tâche commune"" tout en apprenant des noyaux TT individuels pour chaque tâche séparée."
"Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.Each function models the distribution of the output given an input, conditioned on the context.NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.","[1, 0, 0, 0, 0, 0]",[],SkE6PjC9KX,Attentive Neural Processes,"Un modèle de régression qui apprend les distributions conditionnelles d'un processus stochastique, en incorporant l'attention dans les processus neuronaux."
"Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.Each function models the distribution of the output given an input, conditioned on the context.NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.","[1, 0, 0, 0, 0, 0]",[],SkE6PjC9KX,Attentive Neural Processes,Propose de résoudre le problème de l'underfitting dans la méthode du processus neuronal en ajoutant un mécanisme d'attention au chemin déterministe.
"Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.Each function models the distribution of the output given an input, conditioned on the context.NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.","[1, 0, 0, 0, 0, 0]",[],SkE6PjC9KX,Attentive Neural Processes,"Une extension au cadre des processus neuronaux qui ajoute un mécanisme de conditionnement basé sur l'attention, permettant au modèle de mieux capturer les dépendances dans l'ensemble de conditionnement."
"Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.Each function models the distribution of the output given an input, conditioned on the context.NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.","[1, 0, 0, 0, 0, 0]",[],SkE6PjC9KX,Attentive Neural Processes,Les auteurs étendent les processus neuronaux en incorporant l'auto-attention pour enrichir les caractéristiques des points de contexte et l'attention croisée pour produire une représentation spécifique à la requête. Ils résolvent le problème de sous-adaptation des PNA et montrent que les PNA convergent mieux et plus rapidement que les PNA.
"Deconvolutional layers have been widely used in a variety of deepmodels for up-sampling, including encoder-decoder networks forsemantic segmentation and deep generative models for unsupervisedlearning.One of the key limitations of deconvolutional operationsis that they result in the so-called checkerboard problem.This iscaused by the fact that no direct relationship exists among adjacentpixels on the output feature map.To address this problem, wepropose the pixel deconvolutional layer (PixelDCL) to establishdirect relationships among adjacent pixels on the up-sampled featuremap.Our method is based on a fresh interpretation of the regulardeconvolution operation.The resulting PixelDCL can be used toreplace any deconvolutional layer in a plug-and-play manner withoutcompromising the fully trainable capabilities of original models.The proposed PixelDCL may result in slight decrease in efficiency,but this can be overcome by an implementation trick.Experimentalresults on semantic segmentation demonstrate that PixelDCL canconsider spatial features such as edges and shapes and yields moreaccurate segmentation outputs than deconvolutional layers.When usedin image generation tasks, our PixelDCL can largely overcome thecheckerboard problem suffered by regular deconvolution operations.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1spAqUp-,Pixel Deconvolutional Networks,Résoudre le problème du damier dans la couche déconvolutionnelle en construisant des dépendances entre les pixels.
"Deconvolutional layers have been widely used in a variety of deepmodels for up-sampling, including encoder-decoder networks forsemantic segmentation and deep generative models for unsupervisedlearning.One of the key limitations of deconvolutional operationsis that they result in the so-called checkerboard problem.This iscaused by the fact that no direct relationship exists among adjacentpixels on the output feature map.To address this problem, wepropose the pixel deconvolutional layer (PixelDCL) to establishdirect relationships among adjacent pixels on the up-sampled featuremap.Our method is based on a fresh interpretation of the regulardeconvolution operation.The resulting PixelDCL can be used toreplace any deconvolutional layer in a plug-and-play manner withoutcompromising the fully trainable capabilities of original models.The proposed PixelDCL may result in slight decrease in efficiency,but this can be overcome by an implementation trick.Experimentalresults on semantic segmentation demonstrate that PixelDCL canconsider spatial features such as edges and shapes and yields moreaccurate segmentation outputs than deconvolutional layers.When usedin image generation tasks, our PixelDCL can largely overcome thecheckerboard problem suffered by regular deconvolution operations.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1spAqUp-,Pixel Deconvolutional Networks,Ce travail propose des couches de déconvolution de pixels pour les réseaux neuronaux convolutifs comme moyen d'atténuer l'effet de damier.
"Deconvolutional layers have been widely used in a variety of deepmodels for up-sampling, including encoder-decoder networks forsemantic segmentation and deep generative models for unsupervisedlearning.One of the key limitations of deconvolutional operationsis that they result in the so-called checkerboard problem.This iscaused by the fact that no direct relationship exists among adjacentpixels on the output feature map.To address this problem, wepropose the pixel deconvolutional layer (PixelDCL) to establishdirect relationships among adjacent pixels on the up-sampled featuremap.Our method is based on a fresh interpretation of the regulardeconvolution operation.The resulting PixelDCL can be used toreplace any deconvolutional layer in a plug-and-play manner withoutcompromising the fully trainable capabilities of original models.The proposed PixelDCL may result in slight decrease in efficiency,but this can be overcome by an implementation trick.Experimentalresults on semantic segmentation demonstrate that PixelDCL canconsider spatial features such as edges and shapes and yields moreaccurate segmentation outputs than deconvolutional layers.When usedin image generation tasks, our PixelDCL can largely overcome thecheckerboard problem suffered by regular deconvolution operations.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],B1spAqUp-,Pixel Deconvolutional Networks,"Une nouvelle technique pour généraliser les opérations de déconvolution utilisées dans les architectures CNN standard, qui propose de faire une prédiction séquentielle des caractéristiques des pixels adjacents, ce qui permet d'obtenir des sorties plus lisses spatialement pour les couches de déconvolution."
"In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem.To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived.After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0).The method does not require fine-tuning after quantization.Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).","[1, 0, 0, 0, 0]",[],ry-TW-WAb,Variational Network Quantization,Nous quantifions et élaguons les poids des réseaux neuronaux en utilisant l'inférence bayésienne variationnelle avec un antécédent multimodal induisant la sparsité.
"In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem.To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived.After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0).The method does not require fine-tuning after quantization.Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).","[1, 0, 0, 0, 0]",[],ry-TW-WAb,Variational Network Quantization,Propose d'utiliser un mélange de spike propto continu 1/abs comme prior pour un réseau neuronal bayésien et démontre la bonne performance avec des convnets relativement sparifiés pour minist et cifar-10.
"In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem.To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived.After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0).The method does not require fine-tuning after quantization.Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10).","[1, 0, 0, 0, 0]",[],ry-TW-WAb,Variational Network Quantization,"Cet article présente une approche bayésienne variationnelle permettant de quantifier les poids des réseaux neuronaux à des valeurs ternaires après la formation, et ce de manière raisonnée."
"Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.Extensive research work have been conducted on DNN model compression or pruning.However, most of the previous work took heuristic approaches.This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.It achieves up to 34× pruning rate for ImageNet dataset and 167× pruning rate for MNIST dataset, significantly higher than those reached by the literature work.Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rygo9iR9F7,Progressive Weight Pruning Of Deep Neural Networks Using ADMM,Nous mettons en œuvre une approche d'élagage des poids des DNN qui permet d'obtenir les taux d'élagage les plus élevés.
"Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.Extensive research work have been conducted on DNN model compression or pruning.However, most of the previous work took heuristic approaches.This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.It achieves up to 34× pruning rate for ImageNet dataset and 167× pruning rate for MNIST dataset, significantly higher than those reached by the literature work.Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rygo9iR9F7,Progressive Weight Pruning Of Deep Neural Networks Using ADMM,"Cet article se concentre sur l'élagage des poids pour la compression des réseaux neuronaux, atteignant un taux de compression de 30x pour AlexNet et VGG pour ImageNet."
"Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.Extensive research work have been conducted on DNN model compression or pruning.However, most of the previous work took heuristic approaches.This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.It achieves up to 34× pruning rate for ImageNet dataset and 167× pruning rate for MNIST dataset, significantly higher than those reached by the literature work.Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],rygo9iR9F7,Progressive Weight Pruning Of Deep Neural Networks Using ADMM,"Une technique d'élagage progressif qui impose une contrainte de sparsité structurelle sur le paramètre de poids et réécrit l'optimisation comme un cadre ADMM, obtenant une précision plus élevée que la descente de gradient projetée."
"In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network.The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network.This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate.We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.","[1, 0, 0, 0, 0]",[],r1efr3C9Ym,Interpolation-Prediction Networks for Irregularly Sampled Time Series,Cet article présente une nouvelle architecture d'apprentissage profond pour résoudre le problème de l'apprentissage supervisé avec des séries temporelles multivariées éparses et irrégulièrement échantillonnées.
"In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network.The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network.This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate.We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.","[1, 0, 0, 0, 0]",[],r1efr3C9Ym,Interpolation-Prediction Networks for Irregularly Sampled Time Series,"Propose un cadre pour faire des prédictions sur des données de séries temporelles éparses et irrégulièrement échantillonnées en utilisant un module d'interpolation qui modélise les valeurs manquantes en utilisant l'interpolation lisse, l'interpolation non lisse et l'intensité. "
"In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network.The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network.This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate.We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.","[1, 0, 0, 0, 0]",[],r1efr3C9Ym,Interpolation-Prediction Networks for Irregularly Sampled Time Series,Résout le problème de l'apprentissage supervisé avec des séries temporelles multivariées éparses et irrégulièrement échantillonnées en utilisant un réseau d'interpolation semi-paramétrique suivi d'un réseau de prédiction.
"We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.","[0, 1]",[],rJlpUiAcYX,Holographic and other Point Set Distances for Machine Learning,Fonction de perte invariante par permutation pour la prédiction d'ensembles de points.
"We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.","[0, 1]",[],rJlpUiAcYX,Holographic and other Point Set Distances for Machine Learning,Propose une nouvelle perte pour l'enregistrement des points (alignement de deux ensembles de points) avec une propriété invariante de permutation préférable. 
"We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.","[0, 1]",[],rJlpUiAcYX,Holographic and other Point Set Distances for Machine Learning,"Cet article introduit une nouvelle fonction de distance entre des ensembles de points, applique deux autres distances de permutation dans une tâche de détection d'objets de bout en bout, et montre qu'en deux dimensions, tous les minima locaux de la perte holographique sont des minima globaux."
"We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection.","[0, 1]",[],rJlpUiAcYX,Holographic and other Point Set Distances for Machine Learning,Propose des fonctions de perte invariantes par permutation qui dépendent de la distance des ensembles.
"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.Our method learns to assign graph operations to groups and to allocate those groups to available devices.The grouping and device allocations are learned jointly.The proposed method is trained with policy gradient and requires no human intervention.Experiments with widely-usedcomputer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.In addition, our approach outperforms placements by humanexperts as well as a previous state-of-the-art placement method based on deep reinforcement learning.Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hkc-TeZ0W,A Hierarchical Model for Device Placement,"Nous présentons un modèle hiérarchique pour le placement efficace, de bout en bout, de graphes de calcul sur des dispositifs matériels."
"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.Our method learns to assign graph operations to groups and to allocate those groups to available devices.The grouping and device allocations are learned jointly.The proposed method is trained with policy gradient and requires no human intervention.Experiments with widely-usedcomputer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.In addition, our approach outperforms placements by humanexperts as well as a previous state-of-the-art placement method based on deep reinforcement learning.Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hkc-TeZ0W,A Hierarchical Model for Device Placement,Propose d'apprendre conjointement des groupes d'opérateurs à colocaliser et de placer les groupes appris sur des dispositifs pour distribuer les opérations d'apprentissage profond via l'apprentissage par renforcement.
"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.Our method learns to assign graph operations to groups and to allocate those groups to available devices.The grouping and device allocations are learned jointly.The proposed method is trained with policy gradient and requires no human intervention.Experiments with widely-usedcomputer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.In addition, our approach outperforms placements by humanexperts as well as a previous state-of-the-art placement method based on deep reinforcement learning.Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hkc-TeZ0W,A Hierarchical Model for Device Placement,Les auteurs visent un réseau entièrement connecté pour remplacer l'étape de co-localisation dans une méthode d'auto-placement proposée pour accélérer le temps d'exécution d'un modèle TensorFlow.
"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.Our method learns to assign graph operations to groups and to allocate those groups to available devices.The grouping and device allocations are learned jointly.The proposed method is trained with policy gradient and requires no human intervention.Experiments with widely-usedcomputer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.In addition, our approach outperforms placements by humanexperts as well as a previous state-of-the-art placement method based on deep reinforcement learning.Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hkc-TeZ0W,A Hierarchical Model for Device Placement,Propose un algorithme de placement de périphériques pour placer les opérations de tensorflow sur des périphériques.
"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem.We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion.While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself.We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels.Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types.In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry.Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","[0, 1, 0, 0, 0, 0, 0]",[],SJLlmG-AZ,Understanding image motion with group representations ,Nous proposons une méthode d'utilisation des propriétés de groupe pour apprendre une représentation du mouvement sans étiquettes et démontrons l'utilisation de cette méthode pour représenter le mouvement 2D et 3D.
"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem.We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion.While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself.We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels.Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types.In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry.Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","[0, 1, 0, 0, 0, 0, 0]",[],SJLlmG-AZ,Understanding image motion with group representations ,Propose d'apprendre le groupe de mouvement rigide à partir d'une représentation latente de séquences d'images sans avoir besoin d'étiquettes explicites et démontre expérimentalement la méthode sur des séquences de chiffres MINST et le jeu de données KITTI.
"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem.We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion.While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself.We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels.Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types.In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry.Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","[0, 1, 0, 0, 0, 0, 0]",[],SJLlmG-AZ,Understanding image motion with group representations ,"Cet article propose une approche pour apprendre les caractéristiques du mouvement vidéo de manière non supervisée, en utilisant des contraintes pour optimiser le réseau neuronal afin de produire des caractéristiques qui peuvent être utilisées pour régresser l'odométrie."
"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases.We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS.Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters.This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity.Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","[0, 1, 0, 0, 0, 0]",[],BJjBnN9a-,Continuous Convolutional Neural Networks for Image Classification,Cet article propose une nouvelle couche convolutive qui fonctionne dans un espace de Hilbert à noyau reproducteur continu.
"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases.We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS.Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters.This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity.Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","[0, 1, 0, 0, 0, 0]",[],BJjBnN9a-,Continuous Convolutional Neural Networks for Image Classification,Projeter des exemples dans un espace de Hilbert RK et effectuer une convolution et un filtrage dans cet espace.
"This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases.We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS.Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters.This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity.Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate.","[0, 1, 0, 0, 0, 0]",[],BJjBnN9a-,Continuous Convolutional Neural Networks for Image Classification,Cet article formule une variante des réseaux neuronaux convolutifs qui modélise à la fois les activations et les filtres comme des fonctions continues composées de bases de noyaux.
"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes.Some recent studies suggest a more important role of image textures.We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict.We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies.We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet.This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.","[0, 0, 0, 0, 0, 1]",[],Bygh9j09KX,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"Les CNN entraînés par ImageNet sont biaisés vers la texture des objets (au lieu de la forme comme les humains). En surmontant cette différence majeure entre la vision humaine et la vision artificielle, on obtient de meilleures performances de détection et une robustesse inédite aux distorsions de l'image."
"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes.Some recent studies suggest a more important role of image textures.We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict.We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies.We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet.This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.","[0, 0, 0, 0, 0, 1]",[],Bygh9j09KX,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,L'utilisation d'un stylisme d'image pour augmenter les données d'entraînement des CNN entraînés par ImageNet afin que les réseaux résultants soient plus conformes aux jugements humains.
"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes.Some recent studies suggest a more important role of image textures.We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict.We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies.We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet.This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.","[0, 0, 0, 0, 0, 1]",[],Bygh9j09KX,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"Cet article étudie les CNN comme AlexNet, VGG, GoogleNet et ResNet50, montre que ces modèles sont biaisés vers la texture lorsqu'ils sont entraînés sur ImageNet, et propose un nouveau jeu de données ImageNet."
"In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities.Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances.In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs.The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.","[0, 0, 1, 0]",[],ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,Nous évaluons l'efficacité de l'exécution de tâches discriminantes auxiliaires par-dessus les statistiques de la distribution postérieure apprises par les autoencodeurs variationnels pour renforcer la dépendance du locuteur.
"In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities.Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances.In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs.The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.","[0, 0, 1, 0]",[],ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,Proposer un modèle d'autoencodeur pour apprendre une représentation pour la vérification du locuteur en utilisant des fenêtres d'analyse de courte durée.
"In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities.Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances.In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs.The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines.","[0, 0, 1, 0]",[],ryeHw1vjiQ,Variational Autoencoders with implicit priors for short-duration text-independent speaker verification,Une version modifiée du modèle d'autoencodeur variationnel qui aborde le problème de la reconnaissance du locuteur dans le contexte de segments de courte durée.
"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models.Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions.In this work, we provide yet another perspective on the IWAE bounds.We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K).In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions.Based on this analysis we develop jackknife variational inference (JVI),a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency.Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders.We also report first results on applying JVI to learning variational autoencoders.Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyZoi-WRb,Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,"L'inférence variationnelle est biaisée, débarrassons-nous-en."
"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models.Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions.In this work, we provide yet another perspective on the IWAE bounds.We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K).In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions.Based on this analysis we develop jackknife variational inference (JVI),a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency.Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders.We also report first results on applying JVI to learning variational autoencoders.Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyZoi-WRb,Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,"Présente l'inférence variationnelle jackknife, une méthode permettant de débiaiser les objectifs de Monte Carlo tels que l'auto-encodeur pondéré par l'importance."
"The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models.Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions.In this work, we provide yet another perspective on the IWAE bounds.We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K).In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions.Based on this analysis we develop jackknife variational inference (JVI),a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency.Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders.We also report first results on applying JVI to learning variational autoencoders.Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HyZoi-WRb,Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference,Les auteurs analysent le biais et la variance de la limite IWAE et dérivent une approche jacknife pour estimer les moments comme moyen de débiaiser IWAE pour les échantillons finis pondérés par l'importance.
"In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.","[0, 0, 1, 0, 0]",[],B1G6uM0WG,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,"Un cadre qui fournit une politique pour le changement de voie autonome en apprenant à prendre des décisions tactiques de haut niveau avec un apprentissage par renforcement profond, et en maintenant une intégration étroite avec un contrôleur de bas niveau pour prendre des actions de bas niveau."
"In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.","[0, 0, 1, 0, 0]",[],B1G6uM0WG,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,"Il propose une nouvelle stratégie d'apprentissage, le Q-masking, qui consiste à coupler un contrôleur de bas niveau défini avec une politique de prise de décision tactique de haut niveau."
"In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.","[0, 0, 1, 0, 0]",[],B1G6uM0WG,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,"Cet article propose une approche d'apprentissage Q profond au problème du changement de voie en utilisant le "" Q-masking "", qui réduit l'espace d'action en fonction des contraintes ou des connaissances préalables."
"In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving.","[0, 0, 1, 0, 0]",[],B1G6uM0WG,Tactical Decision Making for Lane Changing with Deep Reinforcement Learning,"Les auteurs proposent une méthode qui utilise une politique de haut niveau basée sur l'apprentissage Q, combinée à un masque contextuel dérivé des contraintes de sécurité et des contrôleurs de bas niveau, qui empêche certaines actions d'être sélectionnées dans certains états. "
"Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering.Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates.To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space.We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs.In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation.We show that NGE significantly outperforms previous methods by an order of magnitude.As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs.Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2machine.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BkgWHnR5tm,Neural Graph Evolution: Towards Efficient Automatic Robot Design,Recherche automatique de conception robotique à l'aide de réseaux de neurones à graphes
"Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering.Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates.To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space.We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs.In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation.We show that NGE significantly outperforms previous methods by an order of magnitude.As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs.Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2machine.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BkgWHnR5tm,Neural Graph Evolution: Towards Efficient Automatic Robot Design,Propose une approche pour la conception automatique de robots basée sur l'évolution des graphes neuronaux. Les expériences démontrent que l'optimisation du contrôleur et du matériel est meilleure que l'optimisation du contrôleur seul.
"Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering.Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates.To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space.We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs.In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation.We show that NGE significantly outperforms previous methods by an order of magnitude.As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs.Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2machine.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BkgWHnR5tm,Neural Graph Evolution: Towards Efficient Automatic Robot Design,"Les auteurs proposent un schéma basé sur une représentation graphique de la structure du robot, et un réseau grapho-neural comme contrôleurs pour optimiser les structures des robots, combinés à leurs contrôleurs.  "
"Deep learning on graphs has become a popular research topic with many applications.However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text.Is it possible to transfer this progress to the domain of graphs?We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once.Our method is formulated as a variational autoencoder.We evaluate on the challenging task of conditional molecule generation.","[0, 0, 0, 0, 1, 0]",[],SJlhPMWAW,GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,Nous démontrons un autoencodeur pour les graphes.
"Deep learning on graphs has become a popular research topic with many applications.However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text.Is it possible to transfer this progress to the domain of graphs?We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once.Our method is formulated as a variational autoencoder.We evaluate on the challenging task of conditional molecule generation.","[0, 0, 0, 0, 1, 0]",[],SJlhPMWAW,GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,"Apprendre à générer des graphes à l'aide de méthodes d'apprentissage profond en ""une seule fois"", en produisant directement les probabilités d'existence des nœuds et des arêtes, ainsi que les vecteurs d'attributs des nœuds."
"Deep learning on graphs has become a popular research topic with many applications.However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text.Is it possible to transfer this progress to the domain of graphs?We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once.Our method is formulated as a variational autoencoder.We evaluate on the challenging task of conditional molecule generation.","[0, 0, 0, 0, 1, 0]",[],SJlhPMWAW,GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,Un codeur automatique variationnel pour générer des graphiques
"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling.Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit.In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability.However, learning towards discrete values of the gates is generally difficult.To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation.Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks.Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJiaRbk0-,Towards Binary-Valued Gates for Robust LSTM Training ,"Nous proposons un nouvel algorithme pour l'apprentissage des LSTM en apprenant vers des portes à valeurs binaires, dont nous avons montré qu'il possède de nombreuses propriétés intéressantes."
"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling.Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit.In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability.However, learning towards discrete values of the gates is generally difficult.To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation.Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks.Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJiaRbk0-,Towards Binary-Valued Gates for Robust LSTM Training ,"Proposer une nouvelle fonction ""gate"" pour le LSTM afin d'activer les valeurs des gates vers 0 ou 1. "
"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling.Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit.In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability.However, learning towards discrete values of the gates is generally difficult.To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation.Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks.Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","[0, 0, 1, 0, 0, 0, 0, 0]",[],rJiaRbk0-,Towards Binary-Valued Gates for Robust LSTM Training ,L'article vise à pousser les portes LSTM à être binaires en employant la récente astuce Gumbel-Softmax pour obtenir une distribution catégorielle entraînable de bout en bout.
"We present a personalized recommender system using neural network for recommendingproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.It produces recommendations based on customer’s implicit feedback history suchas purchases, listens or watches.Our key contribution is to formulate recommendationproblem as a model that encodes historical behavior to predict the futurebehavior using soft data split, combining predictor and auto-encoder models.Weintroduce convolutional layer for learning the importance (time decay) of the purchasesdepending on their purchase date and demonstrate that the shape of the timedecay function can be well approximated by a parametrical function.We presentoffline experimental results showing that neural networks with two hidden layerscan capture seasonality changes, and at the same time outperform other modelingtechniques, including our recommender in production.Most importantly, wedemonstrate that our model can be scaled to all digital categories, and we observesignificant improvements in an online A/B test.We also discuss key enhancementsto the neural network model and describe our production pipeline.Finallywe open-sourced our deep learning library which supports multi-gpu model paralleltraining.This is an important feature in building neural network based recommenderswith large dimensionality of input and output data.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1lMMx1CW,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS,Amélioration des recommandations à l'aide d'une modélisation sensible au temps avec des réseaux neuronaux dans plusieurs catégories de produits sur un site Web de vente au détail
"We present a personalized recommender system using neural network for recommendingproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.It produces recommendations based on customer’s implicit feedback history suchas purchases, listens or watches.Our key contribution is to formulate recommendationproblem as a model that encodes historical behavior to predict the futurebehavior using soft data split, combining predictor and auto-encoder models.Weintroduce convolutional layer for learning the importance (time decay) of the purchasesdepending on their purchase date and demonstrate that the shape of the timedecay function can be well approximated by a parametrical function.We presentoffline experimental results showing that neural networks with two hidden layerscan capture seasonality changes, and at the same time outperform other modelingtechniques, including our recommender in production.Most importantly, wedemonstrate that our model can be scaled to all digital categories, and we observesignificant improvements in an online A/B test.We also discuss key enhancementsto the neural network model and describe our production pipeline.Finallywe open-sourced our deep learning library which supports multi-gpu model paralleltraining.This is an important feature in building neural network based recommenderswith large dimensionality of input and output data.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1lMMx1CW,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS,Ce document propose une nouvelle méthode de recommandation basée sur les réseaux neuronaux.
"We present a personalized recommender system using neural network for recommendingproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.It produces recommendations based on customer’s implicit feedback history suchas purchases, listens or watches.Our key contribution is to formulate recommendationproblem as a model that encodes historical behavior to predict the futurebehavior using soft data split, combining predictor and auto-encoder models.Weintroduce convolutional layer for learning the importance (time decay) of the purchasesdepending on their purchase date and demonstrate that the shape of the timedecay function can be well approximated by a parametrical function.We presentoffline experimental results showing that neural networks with two hidden layerscan capture seasonality changes, and at the same time outperform other modelingtechniques, including our recommender in production.Most importantly, wedemonstrate that our model can be scaled to all digital categories, and we observesignificant improvements in an online A/B test.We also discuss key enhancementsto the neural network model and describe our production pipeline.Finallywe open-sourced our deep learning library which supports multi-gpu model paralleltraining.This is an important feature in building neural network based recommenderswith large dimensionality of input and output data.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1lMMx1CW,THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS,Les auteurs décrivent une procédure de construction de leur système de recommandation de production à partir de zéro et intègrent la décroissance temporelle des achats dans le cadre d'apprentissage.
"Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration.Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hylnis0qKX,Task-GAN for Improved GAN based Image Restoration,Coupler le cadre de restauration d'image basé sur le GAN avec un autre réseau spécifique à la tâche pour générer une image réaliste tout en préservant les caractéristiques spécifiques à la tâche.
"Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration.Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hylnis0qKX,Task-GAN for Improved GAN based Image Restoration,"Une nouvelle méthode de couplage Task-GAN de l'image qui couple le GAN et un réseau spécifique à la tâche, ce qui permet d'éviter l'hallucination ou l'effondrement du mode."
"Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration.Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hylnis0qKX,Task-GAN for Improved GAN based Image Restoration,"Les auteurs proposent d'augmenter la restauration d'images basée sur le GAN avec une autre branche spécifique à la tâche, telle que les tâches de classification, pour une amélioration supplémentaire."
"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core.Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space.In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM).Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model.The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","[0, 0, 0, 1, 0, 0, 0]",[],BJJLHbb0-,Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,Un réseau neuronal profond formé de bout en bout qui exploite le modèle de mélange gaussien pour effectuer une estimation de la densité et une détection non supervisée des anomalies dans un espace à faible dimension appris par un auto-codeur profond.
"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core.Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space.In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM).Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model.The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","[0, 0, 0, 1, 0, 0, 0]",[],BJJLHbb0-,Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,"L'article présente un cadre d'apprentissage profond conjoint pour la réduction de la dimension et le regroupement, qui permet une détection compétitive des anomalies."
"Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core.Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space.In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM).Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model.The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.","[0, 0, 0, 1, 0, 0, 0]",[],BJJLHbb0-,Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection,Une nouvelle technique de détection des anomalies où les étapes de réduction de la dimension et d'estimation de la densité sont optimisées conjointement.
"Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning.In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace.We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification.Moreover, our PSN approach has the ability of end-to-end learning.In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.","[0, 0, 0, 1, 0, 0]",[],rkzfuiA9F7,Projective Subspace Networks For Few-Shot Learning,Nous avons proposé des réseaux de sous-espaces projectifs pour l'apprentissage en quelques coups et l'apprentissage semi-supervisé en quelques coups.
"Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning.In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace.We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification.Moreover, our PSN approach has the ability of end-to-end learning.In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.","[0, 0, 0, 1, 0, 0]",[],rkzfuiA9F7,Projective Subspace Networks For Few-Shot Learning,Cet article propose une nouvelle approche basée sur l'encastrement pour le problème de l'apprentissage en petit nombre et une extension de ce modèle au cadre de l'apprentissage semi-supervisé en petit nombre.
"Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning.In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace.We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification.Moreover, our PSN approach has the ability of end-to-end learning.In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts.","[0, 0, 0, 1, 0, 0]",[],rkzfuiA9F7,Projective Subspace Networks For Few-Shot Learning,"Nouvelle méthode de classification entièrement et semi-supervisée de quelques images basée sur l'apprentissage d'un encastrement général, puis sur l'apprentissage d'un sous-espace de celui-ci pour chaque classe."
"This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE).In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games.The ADM is trained in a self-supervised fashion to predict the actions taken by the agent.The learned contingency information is used as a part of the state representation for exploration purposes.We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data.Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyxGB2AcY7,Contingency-Aware Exploration in Reinforcement Learning,Nous étudions la prise en compte des contingences et les aspects contrôlables dans l'exploration et obtenons des performances de pointe sur Montezuma's Revenge sans démonstration d'expert.
"This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE).In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games.The ADM is trained in a self-supervised fashion to predict the actions taken by the agent.The learned contingency information is used as a part of the state representation for exploration purposes.We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data.Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyxGB2AcY7,Contingency-Aware Exploration in Reinforcement Learning,"Cet article étudie le problème de l'extraction d'une représentation d'état significative pour aider à l'exploration lorsqu'on est confronté à une tâche de récompense éparse, en identifiant les caractéristiques contrôlables (apprises) de l'état."
"This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE).In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games.The ADM is trained in a self-supervised fashion to predict the actions taken by the agent.The learned contingency information is used as a part of the state representation for exploration purposes.We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data.Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.","[1, 0, 0, 0, 0, 0, 0, 0]",[],HyxGB2AcY7,Contingency-Aware Exploration in Reinforcement Learning,"Cet article propose l'idée novatrice d'utiliser la conscience des contingences pour faciliter l'exploration dans les tâches d'apprentissage par renforcement à récompense éparse, en obtenant des résultats à la pointe de la technologie."
"Disentangling factors of variation has always been a challenging problem in representation learning.Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc.In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation.By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images.In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training.Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.","[0, 0, 1, 0, 0, 0, 0]",[],Syr8Qc1CW,DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images,"Nous avons proposé un algorithme supervisé, le DNA-GAN, pour démêler les attributs multiples des images."
"Disentangling factors of variation has always been a challenging problem in representation learning.Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc.In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation.By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images.In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training.Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.","[0, 0, 1, 0, 0, 0, 0]",[],Syr8Qc1CW,DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images,"Cet article étudie le problème de la génération d'images conditionnées par attributs à l'aide de réseaux adversaires génératifs, et propose de générer des images à partir d'attributs et de codes latents comme représentation de haut niveau."
"Disentangling factors of variation has always been a challenging problem in representation learning.Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc.In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation.By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images.In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training.Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.","[0, 0, 1, 0, 0, 0, 0]",[],Syr8Qc1CW,DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images,Cet article propose une nouvelle méthode pour démêler les différents attributs des images à l'aide d'un nouveau GAN à structure ADN.
"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode.We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations).This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data).We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.","[0, 1, 0, 0]",[],B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,Cet article présente une nouvelle technique de modélisation générative à variables latentes qui permet de représenter des informations globales dans une variable latente et des informations locales dans une autre variable latente.
"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode.We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations).This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data).We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.","[0, 1, 0, 0]",[],B1e4wo09K7,Invariant-equivariant representation learning for multi-class data,L'article présente un VAE qui utilise des étiquettes pour séparer la représentation apprise en une partie invariante et une partie covariante.
"Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.","[0, 0, 0, 1, 0, 0, 0, 0]",[],H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,"Nous abordons le problème de l'apprentissage actif comme un problème de sélection de l'ensemble de base et nous montrons que cette approche est particulièrement utile dans le cadre de l'apprentissage actif par lots, ce qui est crucial lors de la formation des CNN."
"Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.","[0, 0, 0, 1, 0, 0, 0, 0]",[],H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,Les auteurs proposent un algorithme d'apprentissage actif agnostique pour la classification multi-classes.
"Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.","[0, 0, 0, 1, 0, 0, 0, 0]",[],H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,L'article propose un algorithme d'apprentissage actif en mode batch pour le CNN en tant que problème de noyau qui surpasse l'échantillonnage aléatoire et l'échantillonnage d'incertitude.
"Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.","[0, 0, 0, 1, 0, 0, 0, 0]",[],H1aIuk-RW,Active Learning for Convolutional Neural Networks: A Core-Set Approach,"étudie l'apprentissage actif pour les réseaux neuronaux convolutifs, formule le problème de l'apprentissage actif en tant que sélection de l'ensemble de base et présente une nouvelle stratégie."
"Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP).This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps.We introduce a simple stochastic algorithm (\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem.Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed.Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them.Our algorithm\footnote{Our code is available at https://github.com/bhargav104/h-detach.} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better.We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,Un algorithme simple pour améliorer l'optimisation et la gestion des dépendances à long terme dans les LSTM.
"Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP).This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps.We introduce a simple stochastic algorithm (\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem.Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed.Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them.Our algorithm\footnote{Our code is available at https://github.com/bhargav104/h-detach.} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better.We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,"L'article présente un algorithme stochastique simple appelé h-detach, spécifique à l'optimisation des LSTM et destiné à résoudre ce problème."
"Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP).This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps.We introduce a simple stochastic algorithm (\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem.Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed.Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them.Our algorithm\footnote{Our code is available at https://github.com/bhargav104/h-detach.} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better.We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ryf7ioRqFX,h-detach: Modifying the LSTM Gradient Towards Better Optimization,"propose une modification simple du processus d'apprentissage du LSTM pour faciliter la propogation du gradient le long des états de la cellule, ou le ""chemin temporel linéaire""."
"Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision.However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes.Further, they are also vulnerable to adversarial examples.We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set.We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization.As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples.To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness.We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Skgge3R9FQ,Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation,Un entraînement adéquat des CNN avec la classe dustbin augmente leur robustesse aux attaques adverses et leur capacité à traiter des échantillons hors distribution.
"Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision.However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes.Further, they are also vulnerable to adversarial examples.We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set.We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization.As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples.To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness.We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Skgge3R9FQ,Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation,Cet article propose d'ajouter un label supplémentaire pour la détection des échantillons OOD et des exemples adverses dans les modèles CNN.
"Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision.However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes.Further, they are also vulnerable to adversarial examples.We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set.We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization.As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples.To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness.We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Skgge3R9FQ,Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation,L'article propose une classe supplémentaire qui incorpore des images de distribution naturelle et des images interpolées pour les échantillons adverses et de distribution dans les CNN.
"Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization.Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit.Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity.Although these techniques have been proven successful in terms of results, they seem to waste capacity.In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity.In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.","[0, 0, 0, 0, 1, 0]",[],ByJWeR1AW,Data augmentation instead of explicit regularization,"Dans un réseau neuronal convolutif profond formé avec un niveau suffisant d'augmentation des données, optimisé par SGD, les régularisateurs explicites (décroissance des poids et abandon) pourraient ne pas apporter d'amélioration supplémentaire de la généralisation."
"Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization.Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit.Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity.Although these techniques have been proven successful in terms of results, they seem to waste capacity.In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity.In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.","[0, 0, 0, 0, 1, 0]",[],ByJWeR1AW,Data augmentation instead of explicit regularization,"Cet article propose l'augmentation des données comme alternative aux techniques de régularisation couramment utilisées, et montre que pour quelques modèles/tâches de référence, les mêmes performances de généralisation peuvent être obtenues en utilisant uniquement l'augmentation des données."
"Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization.Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit.Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity.Although these techniques have been proven successful in terms of results, they seem to waste capacity.In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity.In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.","[0, 0, 0, 0, 1, 0]",[],ByJWeR1AW,Data augmentation instead of explicit regularization,"Cet article présente une étude systématique de l'augmentation des données dans la classification d'images avec des réseaux neuronaux profonds, suggérant que l'augmentation des données peut répliquer certains régularisateurs communs comme la décroissance du poids et l'abandon."
"Text editing on mobile devices can be a tedious process.To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing.In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo.Variations of our gestures exist for one and two hands.We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions.Our results showed that Gedit’s gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.","[0, 0, 1, 0, 0, 0, 0]",[],CZ938F7zVF,Gedit: Keyboard Gestures for Mobile Text Editing,"Dans ce travail, nous présentons Gedit, un système de gestes sur le clavier pour une édition de texte mobile pratique."
"Text editing on mobile devices can be a tedious process.To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing.In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo.Variations of our gestures exist for one and two hands.We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions.Our results showed that Gedit’s gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.","[0, 0, 1, 0, 0, 0, 0]",[],CZ938F7zVF,Gedit: Keyboard Gestures for Mobile Text Editing,Rapporte la conception et l'évaluation des techniques d'interaction de Gedit.
"Text editing on mobile devices can be a tedious process.To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing.In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo.Variations of our gestures exist for one and two hands.We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions.Our results showed that Gedit’s gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants.","[0, 0, 1, 0, 0, 0, 0]",[],CZ938F7zVF,Gedit: Keyboard Gestures for Mobile Text Editing,Présente un nouvel ensemble de gestes tactiles permettant d'effectuer une transition transparente entre la saisie et l'édition de texte sur les appareils mobiles.
"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored.This paper attempts to provide an alternative understanding from the perspective of maximum entropy.We first derive two feature conditions that softmax regression strictly apply maximum entropy principle.DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle.The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","[0, 0, 0, 0, 1, 0]",[],r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,Nous prouvons que le DNN est une solution approximée de manière récursive au principe d'entropie maximale.
"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored.This paper attempts to provide an alternative understanding from the perspective of maximum entropy.We first derive two feature conditions that softmax regression strictly apply maximum entropy principle.DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle.The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","[0, 0, 0, 0, 1, 0]",[],r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,Présente une dérivation qui lie un DNN à l'application récursive de l'ajustement du modèle d'entropie maximale.
"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored.This paper attempts to provide an alternative understanding from the perspective of maximum entropy.We first derive two feature conditions that softmax regression strictly apply maximum entropy principle.DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle.The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","[0, 0, 0, 0, 1, 0]",[],r1kj4ACp-,Understanding Deep Learning Generalization by Maximum Entropy,Cet article vise à fournir une vue de l'apprentissage profond du point de vue du principe d'entropie maximale.
" As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.","[0, 1, 0, 0]",[],SyNvti09KQ,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,"Nous présentons une nouvelle approche de l'apprentissage par renforcement qui exploite une fonction de récompense intrinsèque indépendante de la tâche, entraînée sur des mesures de pouls périphériques qui sont corrélées aux réponses du système nerveux autonome humain. "
" As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.","[0, 1, 0, 0]",[],SyNvti09KQ,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,Propose un cadre d'apprentissage par renforcement basé sur la réaction émotionnelle humaine dans le contexte de la conduite autonome.
" As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.","[0, 1, 0, 0]",[],SyNvti09KQ,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,"Les auteurs proposent d'utiliser des signaux, tels que les réponses viscérales autonomes de base qui influencent la prise de décision, dans le cadre du RL en augmentant les fonctions de récompense du RL avec un modèle appris directement des réponses du système nerveux humain."
" As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.","[0, 1, 0, 0]",[],SyNvti09KQ,Visceral Machines: Risk-Aversion in  Reinforcement Learning with Intrinsic Physiological Rewards,propose d'utiliser les signaux physiologiques pour améliorer les performances des algorithmes d'apprentissage par renforcement et construire une fonction de récompense intrinsèque moins éparse en mesurant l'amplitude du pouls cardiaque
"Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.Are CNNs robust or fragile to label noise?Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.","[0, 0, 1, 0, 0, 0, 0]",[],H1xmqiAqFm,Investigating CNNs' Learning Representation under label noise,"Les CNN sont-ils robustes ou fragiles au bruit des étiquettes ? En pratique, robustes."
"Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.Are CNNs robust or fragile to label noise?Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.","[0, 0, 1, 0, 0, 0, 0]",[],H1xmqiAqFm,Investigating CNNs' Learning Representation under label noise,Les auteurs ont testé la robustesse des CNN au bruit des étiquettes en utilisant l'arbre ImageNet 1k de WordNet.
"Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.Are CNNs robust or fragile to label noise?Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.","[0, 0, 1, 0, 0, 0, 0]",[],H1xmqiAqFm,Investigating CNNs' Learning Representation under label noise,Une analyse de la performance des modèles de réseaux neuronaux convolutifs lorsqu'un bruit dépendant de la classe et un bruit indépendant de la classe sont introduits.
"Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.Are CNNs robust or fragile to label noise?Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations.","[0, 0, 1, 0, 0, 0, 0]",[],H1xmqiAqFm,Investigating CNNs' Learning Representation under label noise,Démontre que les CNN sont plus robustes au bruit des étiquettes pertinentes pour la classe et soutient que le bruit du monde réel devrait être pertinent pour la classe.
"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.","[0, 0, 1, 0]",[],H1xQVn09FX,GANSynth: Adversarial Neural Audio Synthesis,Synthèse audio de haute qualité avec les GAN
"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.","[0, 0, 1, 0]",[],H1xQVn09FX,GANSynth: Adversarial Neural Audio Synthesis,Propose une approche qui utilise le cadre GAN pour générer de l'audio en modélisant les magnitudes des logs et les fréquences instantanées avec une résolution fréquentielle suffisante dans le domaine spectral. 
"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.","[0, 0, 1, 0]",[],H1xQVn09FX,GANSynth: Adversarial Neural Audio Synthesis,"Une stratégie pour générer des échantillons audio à partir du bruit avec des GANs, avec des modifications de l'architecture et de la représentation nécessaires pour générer un audio convaincant qui contient un code latent interprétable."
"Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.","[0, 0, 1, 0]",[],H1xQVn09FX,GANSynth: Adversarial Neural Audio Synthesis,Présente une idée simple pour mieux représenter les données audio afin de pouvoir appliquer des modèles convolutifs tels que les réseaux adversariens génératifs.
"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain.We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity.Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","[0, 1, 0, 0]",[],HklZOfW0W,UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,Optimisation des graphes avec filtrage du signal dans le domaine des sommets.
"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain.We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity.Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","[0, 1, 0, 0]",[],HklZOfW0W,UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,L'article étudie l'apprentissage de la matrice d'adjacence d'un graphe avec un graphe non orienté faiblement connecté avec des poids d'arêtes non négatifs en utilisant un algorithme de descente de sous-gradients projetés.
"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain.We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity.Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","[0, 1, 0, 0]",[],HklZOfW0W,UPS: optimizing Undirected Positive Sparse graph for neural graph filtering,Développe un nouveau schéma de rétropropagation sur la matrice d'adjacence d'un graphe de réseau neuronal.
"The use of AR in an industrial context could help for the training of new operators.To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks.In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.","[0, 0, 0, 1, 0, 0, 0]",[],1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,Cet article décrit un outil de création 3D pour la RA dans les chaînes de montage de l'industrie 4.0.
"The use of AR in an industrial context could help for the training of new operators.To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks.In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.","[0, 0, 0, 1, 0, 0, 0]",[],1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,L'article traite de la manière dont les outils de création de RA soutiennent la formation des systèmes de chaîne de montage et propose une approche
"The use of AR in an industrial context could help for the training of new operators.To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks.In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.","[0, 0, 0, 1, 0, 0, 0]",[],1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,Un système de guidage en RA pour les chaînes de montage industrielles qui permet la création sur place de contenu en RA.
"The use of AR in an industrial context could help for the training of new operators.To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks.In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution.","[0, 0, 0, 1, 0, 0, 0]",[],1qdNTwXpgE,WAAT: a Workstation AR Authoring Tool for Industry 4.0,Présente un système qui permet de former plus efficacement les ouvriers d'une usine en utilisant un système de réalité augmentée. 
"Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions.In spite of its great success in applications, GAN is known to be notoriously hard to train.The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area.To resolve these issues, we need to first understand how GANs work.Herein, we take a step toward this direction by examining the dynamics of GANs.We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator.By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.The same framework also applies to multi-task learning and distributional robust learning problems.We verify our analysis on numerical examples with both synthetic and real data sets.We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,"Nous montrons qu'avec un choix approprié de pas, l'algorithme itératif du premier ordre largement utilisé pour l'entraînement des GANs converge en fait vers une solution stationnaire avec un taux sous-linéaire."
"Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions.In spite of its great success in applications, GAN is known to be notoriously hard to train.The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area.To resolve these issues, we need to first understand how GANs work.Herein, we take a step toward this direction by examining the dynamics of GANs.We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator.By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.The same framework also applies to multi-task learning and distributional robust learning problems.We verify our analysis on numerical examples with both synthetic and real data sets.We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,Cet article utilise les GAN et l'apprentissage multi-tâches pour fournir une garantie de convergence pour les algorithmes primal-dual sur certains problèmes min-max.
"Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions.In spite of its great success in applications, GAN is known to be notoriously hard to train.The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area.To resolve these issues, we need to first understand how GANs work.Herein, we take a step toward this direction by examining the dynamics of GANs.We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator.By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.The same framework also applies to multi-task learning and distributional robust learning problems.We verify our analysis on numerical examples with both synthetic and real data sets.We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rylIy3R9K7,Understand the dynamics of GANs via Primal-Dual Optimization,Analyse la dynamique d'apprentissage des GAN en formulant le problème comme un problème d'optimisation primal-dual en supposant une classe limitée de modèles.
"Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction.We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest.However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict.We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards).We call this consequentialist conditional cooperation.We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games.We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.","[0, 0, 0, 0, 0, 1, 0]",[],BkabRiQpb,Consequentialist conditional cooperation in social dilemmas with imperfect information,Nous montrons comment utiliser la RL profonde pour construire des agents capables de résoudre des dilemmes sociaux au-delà des jeux matriciels.
"Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction.We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest.However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict.We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards).We call this consequentialist conditional cooperation.We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games.We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.","[0, 0, 0, 0, 0, 1, 0]",[],BkabRiQpb,Consequentialist conditional cooperation in social dilemmas with imperfect information,Apprendre à jouer à des jeux à somme générale à deux joueurs avec état à information imparfaite
"Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction.We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest.However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict.We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards).We call this consequentialist conditional cooperation.We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games.We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.","[0, 0, 0, 0, 0, 1, 0]",[],BkabRiQpb,Consequentialist conditional cooperation in social dilemmas with imperfect information,"Spécifie une stratégie de déclenchement (CCC) et l'algorithme correspondant, démontrant la convergence vers des résultats efficaces dans les dilemmes sociaux sans que les agents aient besoin d'observer les actions des autres."
"In distributed training, the communication cost due to the transmission of gradientsor the parameters of the deep model is a major bottleneck in scaling up the numberof processing nodes.To address this issue, we propose dithered quantization forthe transmission of the stochastic gradients and show that training with DitheredQuantized Stochastic Gradients (DQSG) is similar to the training with unquantizedSGs perturbed by an independent bounded uniform noise, in contrast to the otherquantization methods where the perturbation depends on the gradients and hence,complicating the convergence analysis.We study the convergence of trainingalgorithms using DQSG and the trade off between the number of quantizationlevels and the training time.Next, we observe that there is a correlation among theSGs computed by workers that can be utilized to further reduce the communicationoverhead without any performance loss.Hence, we develop a simple yet effectivequantization scheme, nested dithered quantized SG (NDQSG), that can reduce thecommunication significantly without requiring the workers communicating extrainformation to each other.We prove that although NDQSG requires significantlyless bits, it can achieve the same quantization variance bound as DQSG.Oursimulation results confirm the effectiveness of training using DQSG and NDQSGin reducing the communication bits or the convergence time compared to theexisting methods without sacrificing the accuracy of the trained model.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJxMM2C5K7,Nested Dithered Quantization for Communication Reduction in Distributed Training,L'article propose et analyse deux schémas de quantification pour la communication des gradients stochastiques dans l'apprentissage distribué qui réduiraient les coûts de communication par rapport à l'état de l'art tout en maintenant la même précision.  
"In distributed training, the communication cost due to the transmission of gradientsor the parameters of the deep model is a major bottleneck in scaling up the numberof processing nodes.To address this issue, we propose dithered quantization forthe transmission of the stochastic gradients and show that training with DitheredQuantized Stochastic Gradients (DQSG) is similar to the training with unquantizedSGs perturbed by an independent bounded uniform noise, in contrast to the otherquantization methods where the perturbation depends on the gradients and hence,complicating the convergence analysis.We study the convergence of trainingalgorithms using DQSG and the trade off between the number of quantizationlevels and the training time.Next, we observe that there is a correlation among theSGs computed by workers that can be utilized to further reduce the communicationoverhead without any performance loss.Hence, we develop a simple yet effectivequantization scheme, nested dithered quantized SG (NDQSG), that can reduce thecommunication significantly without requiring the workers communicating extrainformation to each other.We prove that although NDQSG requires significantlyless bits, it can achieve the same quantization variance bound as DQSG.Oursimulation results confirm the effectiveness of training using DQSG and NDQSGin reducing the communication bits or the convergence time compared to theexisting methods without sacrificing the accuracy of the trained model.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJxMM2C5K7,Nested Dithered Quantization for Communication Reduction in Distributed Training,"Les auteurs proposent d'appliquer une quantification en escalier aux gradients stochastiques calculés par le processus d'apprentissage, ce qui permet d'améliorer l'erreur de quantification et d'obtenir des résultats supérieurs à ceux obtenus par les méthodes de référence. Ils proposent également un schéma imbriqué pour réduire le coût de communication."
"In distributed training, the communication cost due to the transmission of gradientsor the parameters of the deep model is a major bottleneck in scaling up the numberof processing nodes.To address this issue, we propose dithered quantization forthe transmission of the stochastic gradients and show that training with DitheredQuantized Stochastic Gradients (DQSG) is similar to the training with unquantizedSGs perturbed by an independent bounded uniform noise, in contrast to the otherquantization methods where the perturbation depends on the gradients and hence,complicating the convergence analysis.We study the convergence of trainingalgorithms using DQSG and the trade off between the number of quantizationlevels and the training time.Next, we observe that there is a correlation among theSGs computed by workers that can be utilized to further reduce the communicationoverhead without any performance loss.Hence, we develop a simple yet effectivequantization scheme, nested dithered quantized SG (NDQSG), that can reduce thecommunication significantly without requiring the workers communicating extrainformation to each other.We prove that although NDQSG requires significantlyless bits, it can achieve the same quantization variance bound as DQSG.Oursimulation results confirm the effectiveness of training using DQSG and NDQSGin reducing the communication bits or the convergence time compared to theexisting methods without sacrificing the accuracy of the trained model.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJxMM2C5K7,Nested Dithered Quantization for Communication Reduction in Distributed Training,Les auteurs établissent un lien entre la réduction de la communication dans l'optimisation distribuée et la quantification en escalier et développent deux nouveaux algorithmes d'entraînement distribués où la surcharge de communication est considérablement réduite.
"Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks.However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  Including adversarial examples during training is a popular defense mechanism against adversarial attacks.In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game.We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.","[0, 0, 0, 0, 0, 1, 0]",[],S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,Entraînement conjoint d'un réseau de génération de bruits adverses et d'un réseau de classification afin de fournir une meilleure robustesse aux attaques adverses.
"Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks.However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  Including adversarial examples during training is a popular defense mechanism against adversarial attacks.In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game.We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.","[0, 0, 0, 0, 0, 1, 0]",[],S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,"Une solution GAN pour les modèles profonds de classification, face aux attaques de type boîte blanche et boîte noire, qui produit des modèles robustes. "
"Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks.However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  Including adversarial examples during training is a popular defense mechanism against adversarial attacks.In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game.We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.","[0, 0, 0, 0, 0, 1, 0]",[],S1lIMn05F7,A Direct Approach to Robust Deep Learning Using Adversarial Networks,L'article propose un mécanisme défensif contre les attaques adverses en utilisant des GANs avec des perturbations générées utilisées comme exemples adverses et un discriminateur utilisé pour les distinguer.
"Deep learning has become the state of the art approach in many machine learning problems such as classification.It has recently been shown that deep learning is highly vulnerable to adversarial perturbations.Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians.Hence, in order to use deep learning without safety concerns a proper defense strategy is required.We propose to use ensemble methods as a defense strategy against adversarial perturbations.We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task.This makes ensemble methods an attractive defense strategy against adversarial attacks.We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkA1f3NpZ,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,Utilisation de méthodes d'ensemble comme défense contre les perturbations adverses contre les réseaux neuronaux profonds.
"Deep learning has become the state of the art approach in many machine learning problems such as classification.It has recently been shown that deep learning is highly vulnerable to adversarial perturbations.Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians.Hence, in order to use deep learning without safety concerns a proper defense strategy is required.We propose to use ensemble methods as a defense strategy against adversarial perturbations.We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task.This makes ensemble methods an attractive defense strategy against adversarial attacks.We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkA1f3NpZ,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,Cet article propose d'utiliser l'ensemblisme comme un mécanisme de défense contradictoire.
"Deep learning has become the state of the art approach in many machine learning problems such as classification.It has recently been shown that deep learning is highly vulnerable to adversarial perturbations.Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians.Hence, in order to use deep learning without safety concerns a proper defense strategy is required.We propose to use ensemble methods as a defense strategy against adversarial perturbations.We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task.This makes ensemble methods an attractive defense strategy against adversarial attacks.We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkA1f3NpZ,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,"Nous avons étudié de manière empirique la robustesse de différents ensembles de réseaux neuronaux profonds aux deux types d'attaques, FGSM et BIM, sur deux ensembles de données populaires, MNIST et CIFAR10."
"In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.","[0, 0, 0, 1, 0, 0]",[],HJ39YKiTb,Associative Conversation Model: Generating Visual Information from Textual Information,Proposition de la méthode de génération de phrases basée sur la fusion entre les informations textuelles et les informations visuelles associées aux informations textuelles.
"In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.","[0, 0, 0, 1, 0, 0]",[],HJ39YKiTb,Associative Conversation Model: Generating Visual Information from Textual Information,Ce travail décrit un modèle d'apprentissage profond pour les systèmes de dialogue qui tire parti des informations visuelles.
"In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.","[0, 0, 0, 1, 0, 0]",[],HJ39YKiTb,Associative Conversation Model: Generating Visual Information from Textual Information,Cet article propose un nouvel ensemble de données pour le dialogue ancré et fait une observation computationnelle selon laquelle il pourrait aider à raisonner sur la vision même lors d'un dialogue basé sur le texte.
"In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation.","[0, 0, 0, 1, 0, 0]",[],HJ39YKiTb,Associative Conversation Model: Generating Visual Information from Textual Information,"propose d'augmenter les approches traditionnelles de génération de phrases/dialogue basées sur le texte en incorporant des informations visuelles, en collectant un ensemble de données comprenant à la fois du texte et des images ou des vidéos associées."
"Feedforward convolutional neural network has achieved a great success in many computer vision tasks.While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system.In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.We found that such feedback connections could enable lower layers to ``rethink"" about their representations given the top-down contextual information.We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification.We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.","[0, 1, 0, 0, 0, 0]",[],HkzyX3CcFQ,Contextual Recurrent Convolutional Model for Robust Visual Learning,nous avons proposé un nouveau réseau convolutif récurrent contextuel avec une propriété robuste d'apprentissage visuel 
"Feedforward convolutional neural network has achieved a great success in many computer vision tasks.While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system.In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.We found that such feedback connections could enable lower layers to ``rethink"" about their representations given the top-down contextual information.We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification.We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.","[0, 1, 0, 0, 0, 0]",[],HkzyX3CcFQ,Contextual Recurrent Convolutional Model for Robust Visual Learning,Cet article présente la connexion par rétroaction pour améliorer l'apprentissage des caractéristiques en incorporant des informations contextuelles.
"Feedforward convolutional neural network has achieved a great success in many computer vision tasks.While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system.In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.We found that such feedback connections could enable lower layers to ``rethink"" about their representations given the top-down contextual information.We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification.We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system.","[0, 1, 0, 0, 0, 0]",[],HkzyX3CcFQ,Contextual Recurrent Convolutional Model for Robust Visual Learning,"L'article propose d'ajouter des connexions ""récurrentes"" dans un réseau de convolution avec un mécanisme de déclenchement."
"Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains.The techniques driving these advances, however, lack a formal method to account for model uncertainty.While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty.Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure.Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.","[0, 0, 0, 0, 1, 0, 0]",[],BJlrSmbAZ,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"Nous montrons que l'entraînement d'un réseau profond à l'aide de la normalisation par lots est équivalent à une inférence approximative dans les modèles bayésiens, et nous démontrons comment cette découverte nous permet de faire des estimations utiles de l'incertitude du modèle dans les réseaux conventionnels."
"Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains.The techniques driving these advances, however, lack a formal method to account for model uncertainty.While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty.Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure.Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.","[0, 0, 0, 0, 1, 0, 0]",[],BJlrSmbAZ,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"Cet article propose d'utiliser la normalisation par lot au moment du test pour obtenir l'incertitude prédictive, et montre que la prédiction Monte Carlo au moment du test en utilisant la norme par lot est meilleure que l'abandon."
"Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains.The techniques driving these advances, however, lack a formal method to account for model uncertainty.While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty.Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure.Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance.","[0, 0, 0, 0, 1, 0, 0]",[],BJlrSmbAZ,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"Propose que la procédure de régularisation appelée normalisation par lots peut être comprise comme une inférence bayésienne approximative, qui fonctionne de manière similaire à l'abandon MC en termes d'estimations de l'incertitude qu'elle produit."
"Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  However, gradient dropping has been shown to slow convergence.  We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.We also show that gradient dropping with a local gradient update does not reduce the model's final quality.","[0, 0, 0, 0, 1]",[],BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,Nous améliorons l'élimination des gradients (une technique qui consiste à n'échanger que les gradients importants lors de la formation distribuée) en incorporant les gradients locaux lors de la mise à jour des paramètres afin de réduire la perte de qualité et d'améliorer le temps de formation.
"Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  However, gradient dropping has been shown to slow convergence.  We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.We also show that gradient dropping with a local gradient update does not reduce the model's final quality.","[0, 0, 0, 0, 1]",[],BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,Cet article propose 3 modes de combinaison des gradients locaux et globaux pour mieux utiliser un plus grand nombre de nœuds de calcul.
"Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  However, gradient dropping has been shown to slow convergence.  We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.We also show that gradient dropping with a local gradient update does not reduce the model's final quality.","[0, 0, 0, 0, 1]",[],BkeSusCcYm,Combining Global Sparse Gradients with Local Gradients,"Examine le problème de la réduction des besoins en communication pour la mise en œuvre des techniques d'optimisation distribuées, en particulier la SGD."
"    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.    In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.This approach does not require counting and, therefore, generalizes well to the Deep RL.We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration.We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1fNJhRqFX,Exploration using Distributional RL and UCB,Exploration à l'aide de la RL distributionnelle et de la variance tronquée.
"    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.    In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.This approach does not require counting and, therefore, generalizes well to the Deep RL.We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration.We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1fNJhRqFX,Exploration using Distributional RL and UCB,Présente une méthode RL pour gérer les compromis d'exploration-exploitation via des techniques UCB.
"    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.    In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.This approach does not require counting and, therefore, generalizes well to the Deep RL.We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration.We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1fNJhRqFX,Exploration using Distributional RL and UCB,"Une méthode pour utiliser la distribution apprise par la régression quantile DQN pour l'exploration, au lieu de la stratégie epsilon-greedy habituelle."
"    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.    In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.This approach does not require counting and, therefore, generalizes well to the Deep RL.We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration.We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1fNJhRqFX,Exploration using Distributional RL and UCB,Proposition de nouveaux algorithmes (QUCB et QUCB+) pour gérer le compromis d'exploration dans les bandits à plusieurs bras et plus généralement dans l'apprentissage par renforcement.
"Good representations facilitate transfer learning and few-shot learning.Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate.Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate.Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations.Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.","[0, 0, 0, 1, 0]",[],HkzL4hR9Ym,Shaping representations through communication,"Motivés par les théories du langage et de la communication, nous présentons des auto-codeurs communautaires, dans lesquels plusieurs codeurs et décodeurs apprennent collectivement des représentations structurées et réutilisables."
"Good representations facilitate transfer learning and few-shot learning.Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate.Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate.Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations.Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.","[0, 0, 0, 1, 0]",[],HkzL4hR9Ym,Shaping representations through communication,"Les auteurs abordent le problème de l'apprentissage de la représentation, visent à construire une représentation réutilisable et structurée, soutiennent que la co-adaptation entre le codeur et le décodeur dans l'EA traditionnelle produit une représentation médiocre, et introduisent des auto-encodeurs basés sur la communauté."
"Good representations facilitate transfer learning and few-shot learning.Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate.Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate.Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations.Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure.","[0, 0, 0, 1, 0]",[],HkzL4hR9Ym,Shaping representations through communication,L'article présente un cadre d'auto-codage basé sur la communauté pour traiter la co-adaptation des codeurs et des décodeurs et vise à construire de meilleures représentations.
"Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.Achieving these abilities in autonomous agents is an open problem.In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.MetaMimic can learn both(i) policies for high-fidelity one-shot imitation of diverse novel skills, and(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,"Nous présentons MetaMimic, un algorithme qui prend en entrée un ensemble de données de démonstration et produit (i) une politique d'imitation haute-fidélité à un coup et (ii) une politique de tâche inconditionnelle."
"Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.Achieving these abilities in autonomous agents is an open problem.In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.MetaMimic can learn both(i) policies for high-fidelity one-shot imitation of diverse novel skills, and(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,"L'article examine le problème de l'imitation en une seule fois avec une grande précision d'imitation, en étendant DDPGfD pour utiliser uniquement les trajectoires d'état."
"Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.Achieving these abilities in autonomous agents is an open problem.In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.MetaMimic can learn both(i) policies for high-fidelity one-shot imitation of diverse novel skills, and(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,"Cet article propose une approche pour l'imitation à un coup avec une grande précision, et aborde le problème commun de l'exploration dans l'apprentissage par imitation."
"Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.Achieving these abilities in autonomous agents is an open problem.In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.MetaMimic can learn both(i) policies for high-fidelity one-shot imitation of diverse novel skills, and(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HJMjW3RqtX,One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL,Présente une méthode RL pour l'apprentissage à partir d'une démonstration vidéo sans accès à des actions expertes.
"Normalization methods are a central building block in the deep learning toolbox.They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.","[0, 0, 0, 0, 1]",[],HyN-M2Rctm,Mode Normalization,Nous présentons une nouvelle méthode de normalisation pour les réseaux neuronaux profonds qui est robuste aux multi-modalités dans les distributions de caractéristiques intermédiaires.
"Normalization methods are a central building block in the deep learning toolbox.They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.","[0, 0, 0, 0, 1]",[],HyN-M2Rctm,Mode Normalization,Méthode de normalisation qui apprend la distribution multimodale dans l'espace des caractéristiques.
"Normalization methods are a central building block in the deep learning toolbox.They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.","[0, 0, 0, 0, 1]",[],HyN-M2Rctm,Mode Normalization,Propose une généralisation de la normalisation des lots en supposant que les statistiques des activations unitaires sur les lots et sur les dimensions spatiales ne sont pas unimodales.
"Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving.However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations.In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation.Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation.Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.","[0, 0, 1, 0, 0, 0]",[],S1gUsoR9YX,Multilingual Neural Machine Translation with Knowledge Distillation,Nous avons proposé une méthode basée sur la distillation des connaissances pour améliorer la précision de la traduction automatique neuronale multilingue.
"Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving.However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations.In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation.Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation.Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.","[0, 0, 1, 0, 0, 0]",[],S1gUsoR9YX,Multilingual Neural Machine Translation with Knowledge Distillation,"Un modèle de traduction automatique neuronal multilingue multiple qui forme d'abord des modèles distincts pour chaque paire de langues, puis effectue une distillation."
"Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving.However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations.In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation.Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation.Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.","[0, 0, 1, 0, 0, 0]",[],S1gUsoR9YX,Multilingual Neural Machine Translation with Knowledge Distillation,L'article vise à former un modèle de traduction automatique en augmentant la perte d'entropie croisée standard avec une composante de distillation basée sur des modèles d'enseignants individuels (une seule paire de langues).
"What makes humans so good at solving seemingly complex video games?  Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making.This paper investigates the role of human priors for solving video games.Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors.We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors.We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes.Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.","[0, 0, 0, 0, 0, 1, 0]",[],Hk91SGWR-,Investigating Human Priors for Playing Video Games,Nous étudions les différents types de connaissances préalables qui aident l'apprentissage humain et nous constatons que les connaissances préalables générales sur les objets jouent le rôle le plus critique pour guider le jeu humain.
"What makes humans so good at solving seemingly complex video games?  Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making.This paper investigates the role of human priors for solving video games.Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors.We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors.We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes.Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.","[0, 0, 0, 0, 0, 1, 0]",[],Hk91SGWR-,Investigating Human Priors for Playing Video Games,"Les auteurs étudient, par le biais d'expériences, les aspects des prieurs humains qui sont les plus importants pour l'apprentissage par renforcement dans les jeux vidéo."
"What makes humans so good at solving seemingly complex video games?  Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making.This paper investigates the role of human priors for solving video games.Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors.We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors.We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes.Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.","[0, 0, 0, 0, 0, 1, 0]",[],Hk91SGWR-,Investigating Human Priors for Playing Video Games,Les auteurs présentent une étude des prieurs employés par les humains dans les jeux vidéo et démontrent l'existence d'une taxonomie de caractéristiques qui affectent à des degrés divers la capacité à accomplir des tâches dans le jeu.
"Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.","[0, 0, 1, 0, 0, 0, 0]",[],HyBbjW-RW,Open Loop Hyperparameter Optimization and Determinantal Point Processes,"Poussés par le besoin de méthodes d'optimisation des hyperparamètres parallélisables et en boucle ouverte, nous proposons l'utilisation de processus ponctuels k-déterminants dans l'optimisation des hyperparamètres par recherche aléatoire."
"Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.","[0, 0, 1, 0, 0, 0, 0]",[],HyBbjW-RW,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Propose d'utiliser le k-DPP pour sélectionner les points candidats dans les recherches d'hyperparamètres.
"Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.","[0, 0, 1, 0, 0, 0, 0]",[],HyBbjW-RW,Open Loop Hyperparameter Optimization and Determinantal Point Processes,Les auteurs proposent k-DPP comme méthode en boucle ouverte pour l'optimisation des hyperparamètres et fournissent son étude empirique et sa comparaison avec d'autres méthodes.
"Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.","[0, 0, 1, 0, 0, 0, 0]",[],HyBbjW-RW,Open Loop Hyperparameter Optimization and Determinantal Point Processes,"Examine la recherche non séquentielle et non informée d'hyperparamètres à l'aide de processus ponctuels déterminants, qui sont des distributions de probabilité sur des sous-ensembles d'un ensemble de base ayant la propriété que les sous-ensembles comportant des éléments plus ""divers"" ont une probabilité plus élevée."
"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","[1, 0, 0, 0, 0]",[],rye7IMbAZ, Explicit Induction Bias for Transfer Learning with Convolutional Networks,"Dans l'apprentissage par transfert inductif, le réglage fin des réseaux convolutifs pré-entraînés est nettement plus performant que l'apprentissage à partir de zéro."
"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","[1, 0, 0, 0, 0]",[],rye7IMbAZ, Explicit Induction Bias for Transfer Learning with Convolutional Networks,Aborde le problème de l'apprentissage par transfert dans les réseaux profonds et propose d'avoir un terme de régularisation qui pénalise la divergence par rapport à l'initialisation.
"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","[1, 0, 0, 0, 0]",[],rye7IMbAZ, Explicit Induction Bias for Transfer Learning with Convolutional Networks,"propose une analyse des différentes techniques de régularisation adaptative pour l'apprentissage par transfert profond, en se concentrant spécifiquement sur l'utilisation d'une condition L@-SP."
"Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem.We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer.These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups.This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers.Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.","[0, 0, 0, 0, 1, 0]",[],HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,"Pour des raisons d'efficacité, nous nous intéressons aux réseaux neuronaux avec des couches de produits internes en diagonale de bloc."
"Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem.We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer.These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups.This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers.Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.","[0, 0, 0, 0, 1, 0]",[],HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,Cet article propose que les couches internes d'un réseau neuronal soient des blocs diagonaux et explique que les matrices de blocs diagonaux sont plus efficaces que l'élagage et que les couches de blocs diagonaux conduisent à des réseaux plus efficaces.
"Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem.We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer.These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups.This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers.Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency.","[0, 0, 0, 0, 1, 0]",[],HyI5ro0pW,Neural Networks with Block Diagonal Inner Product Layers,Remplacement des couches entièrement connectées par des couches entièrement connectées en diagonale de bloc
"One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.","[0, 1, 0, 0]",[],B1QRgziT-,Spectral Normalization for Generative Adversarial Networks,Nous proposons une nouvelle technique de normalisation des poids appelée normalisation spectrale pour stabiliser l'apprentissage du discriminateur des GANs.
"One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.","[0, 1, 0, 0]",[],B1QRgziT-,Spectral Normalization for Generative Adversarial Networks,"Cet article utilise la régularisation spectrale pour normaliser les objectifs du GAN, et le GAN qui en résulte, appelé SN-GAN, garantit essentiellement la propriété de Lipschitz du discriminateur."
"One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.","[0, 1, 0, 0]",[],B1QRgziT-,Spectral Normalization for Generative Adversarial Networks,"Cet article propose la ""normalisation spectrale"", ce qui constitue une belle avancée dans l'amélioration de l'entraînement des GANs."
"Humans acquire complex skills by exploiting previously learned skills and making transitions between them.To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .","[0, 0, 0, 0, 1, 0, 0]",[],rygrBhC5tQ,Composing Complex Skills by Learning Transition Policies,Les politiques de transition permettent aux agents de composer des compétences complexes en reliant de manière fluide des compétences primitives précédemment acquises.
"Humans acquire complex skills by exploiting previously learned skills and making transitions between them.To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .","[0, 0, 0, 0, 1, 0, 0]",[],rygrBhC5tQ,Composing Complex Skills by Learning Transition Policies,Propose un schéma de transition vers des états de stratage favorables à l'exécution d'options données dans des domaines continus. Ce schéma utilise deux processus d'apprentissage réalisés simultanément.
"Humans acquire complex skills by exploiting previously learned skills and making transitions between them.To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .","[0, 0, 0, 0, 1, 0, 0]",[],rygrBhC5tQ,Composing Complex Skills by Learning Transition Policies,Présente une méthode d'apprentissage des politiques de transition d'une tâche à l'autre dans le but d'accomplir des tâches complexes en utilisant un estimateur de proximité d'état pour récompenser la politique de transition.
"Humans acquire complex skills by exploiting previously learned skills and making transitions between them.To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition .","[0, 0, 0, 0, 1, 0, 0]",[],rygrBhC5tQ,Composing Complex Skills by Learning Transition Policies,propose un nouveau schéma d'apprentissage avec une fonction de récompense auxiliaire apprise pour optimiser les politiques de transition qui relient l'état final d'une macro-action/option précédente aux bons états d'initiation de la macro-action/option suivante.
"Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.These findings were then experimentally verified in two dimensions by means of time series prediction.","[0, 0, 0, 1, 0, 0, 0]",[],H1eiZnAqKm,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,"Nous classons les caractéristiques dynamiques qu'une et deux cellules GRU peuvent et ne peuvent pas capturer en temps continu, et nous vérifions nos résultats expérimentalement avec la prédiction de séries temporelles à k étapes. "
"Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.These findings were then experimentally verified in two dimensions by means of time series prediction.","[0, 0, 0, 1, 0, 0, 0]",[],H1eiZnAqKm,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,"Les auteurs analysent les GRU avec des tailles cachées de un et deux comme des systèmes dynamiques à temps continu, affirmant que la puissance expressive de la représentation de l'état caché peut fournir une connaissance préalable de la performance d'un GRU sur un ensemble de données donné."
"Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.These findings were then experimentally verified in two dimensions by means of time series prediction.","[0, 0, 0, 1, 0, 0, 0]",[],H1eiZnAqKm,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,"Cet article analyse les GRU du point de vue des systèmes dynamiques et montre que les GRU 2d peuvent être entraînés à adopter une variété de points fixes et peuvent se rapprocher des attracteurs linéaires, mais ne peuvent pas imiter un attracteur annulaire."
"Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.These findings were then experimentally verified in two dimensions by means of time series prediction.","[0, 0, 0, 1, 0, 0, 0]",[],H1eiZnAqKm,The Expressive Power of Gated Recurrent Units as a Continuous Dynamical System,Convertit les équations GRU en temps continu et utilise la théorie et les expériences pour étudier les réseaux GRU à 1 et 2 dimensions et présenter toutes les variétés de topologie dynamique disponibles dans ces systèmes.
"Stacked hourglass network has become an important model for Human pose estimation.The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location.The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network.In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   And a new loss function is proposed for multi-scale stacked hourglass network.  Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],HkM3vjCcF7,Multi-Scale Stacked Hourglass Network for Human Pose Estimation,"Les entrées différenciées entraînent une différenciation fonctionnelle du réseau, et l'interaction des fonctions de perte entre les réseaux peut affecter le processus d'optimisation."
"Stacked hourglass network has become an important model for Human pose estimation.The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location.The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network.In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   And a new loss function is proposed for multi-scale stacked hourglass network.  Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],HkM3vjCcF7,Multi-Scale Stacked Hourglass Network for Human Pose Estimation,Une modification du réseau original du sablier pour l'estimation de la pose unique qui apporte des améliorations par rapport à la ligne de base originale.
"Stacked hourglass network has become an important model for Human pose estimation.The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location.The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network.In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   And a new loss function is proposed for multi-scale stacked hourglass network.  Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],HkM3vjCcF7,Multi-Scale Stacked Hourglass Network for Human Pose Estimation,Les auteurs étendent un réseau en sablier empilé avec des modules inception-resnet-A et proposent une approche multi-échelle pour l'estimation de la pose humaine dans des images fixes RVB.
"We present a new unsupervised method for learning general-purpose sentence embeddings.Unlike existing methods which rely on local contexts, such as wordsinside the sentence or immediately neighboring sentences, our method selects, foreach target sentence, influential sentences in the entire document based on a documentstructure.We identify a dependency structure of sentences using metadataor text styles.Furthermore, we propose a novel out-of-vocabulary word handlingtechnique to model many domain-specific terms, which were mostly discarded byexisting sentence embedding methods.We validate our model on several tasksshowing 30% precision improvement in coreference resolution in a technical domain,and 7.5% accuracy increase in paraphrase detection compared to baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1a37GWCZ,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,"Pour former un encastrement de phrases à partir de documents techniques, notre approche tient compte de la structure du document pour trouver un contexte plus large et gérer les mots hors vocabulaire."
"We present a new unsupervised method for learning general-purpose sentence embeddings.Unlike existing methods which rely on local contexts, such as wordsinside the sentence or immediately neighboring sentences, our method selects, foreach target sentence, influential sentences in the entire document based on a documentstructure.We identify a dependency structure of sentences using metadataor text styles.Furthermore, we propose a novel out-of-vocabulary word handlingtechnique to model many domain-specific terms, which were mostly discarded byexisting sentence embedding methods.We validate our model on several tasksshowing 30% precision improvement in coreference resolution in a technical domain,and 7.5% accuracy increase in paraphrase detection compared to baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1a37GWCZ,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,Présente des idées pour améliorer l'intégration des phrases en s'appuyant sur davantage de contexte.
"We present a new unsupervised method for learning general-purpose sentence embeddings.Unlike existing methods which rely on local contexts, such as wordsinside the sentence or immediately neighboring sentences, our method selects, foreach target sentence, influential sentences in the entire document based on a documentstructure.We identify a dependency structure of sentences using metadataor text styles.Furthermore, we propose a novel out-of-vocabulary word handlingtechnique to model many domain-specific terms, which were mostly discarded byexisting sentence embedding methods.We validate our model on several tasksshowing 30% precision improvement in coreference resolution in a technical domain,and 7.5% accuracy increase in paraphrase detection compared to baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1a37GWCZ,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,Apprendre des représentations de phrases avec des informations sur les dépendances des phrases
"We present a new unsupervised method for learning general-purpose sentence embeddings.Unlike existing methods which rely on local contexts, such as wordsinside the sentence or immediately neighboring sentences, our method selects, foreach target sentence, influential sentences in the entire document based on a documentstructure.We identify a dependency structure of sentences using metadataor text styles.Furthermore, we propose a novel out-of-vocabulary word handlingtechnique to model many domain-specific terms, which were mostly discarded byexisting sentence embedding methods.We validate our model on several tasksshowing 30% precision improvement in coreference resolution in a technical domain,and 7.5% accuracy increase in paraphrase detection compared to baselines.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1a37GWCZ,UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT,Élargit l'idée de former une représentation non supervisée des phrases utilisée dans l'approche SkipThough en utilisant un ensemble plus large d'éléments pour former la représentation d'une phrase.
"Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions.It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better.However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions.Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.","[0, 0, 0, 1, 0, 0]",[],HkmaTz-0W,Visualizing the Loss Landscape of Neural Nets,"Nous explorons la structure des fonctions de perte neuronales, et l'effet des paysages de perte sur la généralisation, en utilisant une gamme de méthodes de visualisation."
"Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions.It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better.However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions.Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.","[0, 0, 0, 1, 0, 0]",[],HkmaTz-0W,Visualizing the Loss Landscape of Neural Nets,Cet article propose une méthode pour visualiser la fonction de perte d'un NN et donne un aperçu de la capacité de formation et de généralisation des NN.
"Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions.It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better.However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions.Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.","[0, 0, 0, 1, 0, 0]",[],HkmaTz-0W,Visualizing the Loss Landscape of Neural Nets,Étudie la non-convexité de la surface de perte et des chemins d'optimisation.
"Deep models are state-of-the-art for many computer vision tasks including image classification and object detection.However, it has been shown that deep models are vulnerable to adversarial examples.We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping.We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust.Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks.We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training.The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.","[0, 0, 0, 1, 0, 0, 0]",[],B1xOYoA5tQ,Multi-way Encoding for Robustness to Adversarial Attacks,"Nous démontrons qu'en exploitant un codage de sortie à plusieurs voies, plutôt que le codage à un coup largement utilisé, nous pouvons rendre les modèles profonds plus robustes aux attaques adverses."
"Deep models are state-of-the-art for many computer vision tasks including image classification and object detection.However, it has been shown that deep models are vulnerable to adversarial examples.We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping.We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust.Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks.We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training.The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.","[0, 0, 0, 1, 0, 0, 0]",[],B1xOYoA5tQ,Multi-way Encoding for Robustness to Adversarial Attacks,Cet article propose de remplacer la couche finale d'entropie croisée formée sur des étiquettes à un coup dans les classificateurs en codant chaque étiquette comme un vecteur à haute dimension et en formant le classificateur pour minimiser la distance L2 par rapport au codage de la classe correcte.
"Deep models are state-of-the-art for many computer vision tasks including image classification and object detection.However, it has been shown that deep models are vulnerable to adversarial examples.We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping.We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust.Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks.We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training.The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.","[0, 0, 0, 1, 0, 0, 0]",[],B1xOYoA5tQ,Multi-way Encoding for Robustness to Adversarial Attacks,Les auteurs proposent une nouvelle méthode de lutte contre les attaques adverses qui présente des gains significatifs par rapport aux lignes de base.
"Existing approaches to neural machine translation condition each output word on previously generated outputs.We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs.By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","[0, 1, 0, 0, 0]",[],B1l8BtlCb,Non-Autoregressive Neural Machine Translation,"Nous présentons le premier modèle NMT avec un décodage entièrement parallèle, réduisant la latence d'inférence de 10x."
"Existing approaches to neural machine translation condition each output word on previously generated outputs.We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs.By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","[0, 1, 0, 0, 0]",[],B1l8BtlCb,Non-Autoregressive Neural Machine Translation,Ce travail propose un décodeur non autorégressif pour le cadre codeur-décodeur dans lequel la décision de générer un mot ne dépend pas de la décision antérieure des mots générés.
"Existing approaches to neural machine translation condition each output word on previously generated outputs.We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs.By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","[0, 1, 0, 0, 0]",[],B1l8BtlCb,Non-Autoregressive Neural Machine Translation,Cet article décrit une approche de décodage non autorégressif pour la traduction automatique neuronale avec la possibilité d'un décodage plus parallèle qui peut entraîner une accélération significative.
"Existing approaches to neural machine translation condition each output word on previously generated outputs.We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs.By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","[0, 1, 0, 0, 0]",[],B1l8BtlCb,Non-Autoregressive Neural Machine Translation,propose l'introduction d'un ensemble de variables latentes pour représenter la fertilité de chaque mot source afin de rendre la génération de la phrase cible non autorégressive.
"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.Can we somehow end this arms race?In this work, we study this problem for neural networks with one hidden layer.We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value.Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\epsilon = 0.1$ can cause more than $35\%$ test error.","[0, 0, 0, 0, 1, 0, 0]",[],Bys4ob-Rb,Certified Defenses against Adversarial Examples ,"Nous démontrons une méthode certifiable, entraînable et évolutive de défense contre les exemples adverses."
"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.Can we somehow end this arms race?In this work, we study this problem for neural networks with one hidden layer.We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value.Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\epsilon = 0.1$ can cause more than $35\%$ test error.","[0, 0, 0, 0, 1, 0, 0]",[],Bys4ob-Rb,Certified Defenses against Adversarial Examples ,Propose une nouvelle défense contre les attaques de sécurité sur les réseaux neuronaux avec le modèle d'attaque qui produit un certificat de sécurité sur l'algorithme.
"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.Can we somehow end this arms race?In this work, we study this problem for neural networks with one hidden layer.We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value.Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\epsilon = 0.1$ can cause more than $35\%$ test error.","[0, 0, 0, 0, 1, 0, 0]",[],Bys4ob-Rb,Certified Defenses against Adversarial Examples ,Déduit une limite supérieure de la perturbation contradictoire pour les réseaux neuronaux à une couche cachée.
"We formulate an information-based optimization problem for supervised classification.For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  We propose an objective function and prove that it solves the optimization problem.Our framework allows us to learn latent features in an more interpretable form while improving the classification performance.We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.","[0, 0, 1, 0, 0]",[],BJgvg30ctX,Information Regularized Neural Networks,nous proposons un régularisateur qui améliore les performances de classification des réseaux de neurones
"We formulate an information-based optimization problem for supervised classification.For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  We propose an objective function and prove that it solves the optimization problem.Our framework allows us to learn latent features in an more interpretable form while improving the classification performance.We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.","[0, 0, 1, 0, 0]",[],BJgvg30ctX,Information Regularized Neural Networks,"les auteurs proposent d'entraîner un modèle à partir d'un point de maximisation de l'information mutuelle entre les prédictions et les véritables sorties, avec un terme de régularisation qui minimise les informations non pertinentes pendant l'apprentissage."
"We formulate an information-based optimization problem for supervised classification.For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  We propose an objective function and prove that it solves the optimization problem.Our framework allows us to learn latent features in an more interpretable form while improving the classification performance.We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models.","[0, 0, 1, 0, 0]",[],BJgvg30ctX,Information Regularized Neural Networks,"propose de décomposer les paramètres en une carte de caractéristiques inversible F et une transformation linéaire w dans la dernière couche pour maximiser l'information mutuelle I(Y, \hat{T}) tout en limitant les informations non pertinentes"
"Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.","[0, 1, 0, 0, 0]",[],BkMqUiA5KX,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,Cet article présente un nouveau cadre de modélisation générative qui évite l'effondrement des variables latentes et clarifie l'utilisation de certains facteurs ad hoc dans la formation des auto-codeurs variationnels.
"Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.","[0, 1, 0, 0, 0]",[],BkMqUiA5KX,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,L'article propose de résoudre le problème d'un codeur automatique variationnel ignorant les variables latentes.
"Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.","[0, 1, 0, 0, 0]",[],BkMqUiA5KX,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,Cet article propose d'ajouter un autoencodeur stochastique au modèle VAE original pour résoudre le problème du décodeur LSTM d'un modèle de langage qui pourrait être trop puissant pour ignorer les informations de la variable latente.
"Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.","[0, 1, 0, 0, 0]",[],BkMqUiA5KX,Improving latent variable descriptiveness by modelling rather than ad-hoc factors,"Cet article présente AutoGen, qui combine un auto-codeur variationnel génératif avec un modèle de reconstruction haute-fidélité basé sur l'auto-codeur pour mieux utiliser la représentation latente."
"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.Previous works only calibrate the conﬁdent prediction of classiﬁers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).In contrast, this paper proposes a probabilistic way of directly estimating and ﬁne-tuning the decision boundary between seen and unseen classes.In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the ﬁrst time, are introduced to uncover and ﬁne-tune the decision boundary of each domain.Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted conﬁdently.Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0]",[],H1GaLiAcY7,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective, Cet article étudie le problème de la division de domaine en segmentant les instances tirées de différentes distributions probabilistes.  
"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.Previous works only calibrate the conﬁdent prediction of classiﬁers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).In contrast, this paper proposes a probabilistic way of directly estimating and ﬁne-tuning the decision boundary between seen and unseen classes.In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the ﬁrst time, are introduced to uncover and ﬁne-tune the decision boundary of each domain.Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted conﬁdently.Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0]",[],H1GaLiAcY7,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective,Cet article traite du problème de la reconnaissance de la nouveauté dans l'apprentissage par ensemble ouvert et l'apprentissage généralisé par zéro et propose une solution possible.
"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.Previous works only calibrate the conﬁdent prediction of classiﬁers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).In contrast, this paper proposes a probabilistic way of directly estimating and ﬁne-tuning the decision boundary between seen and unseen classes.In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the ﬁrst time, are introduced to uncover and ﬁne-tune the decision boundary of each domain.Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted conﬁdently.Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0]",[],H1GaLiAcY7,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective,"Une approche de la séparation des domaines basée sur le bootstrapping pour identifier les seuils de coupure de similarité pour les classes connues, suivi d'un test de Kolmogorov-Smirnoff pour affiner les zones d'intra-distribution bootstrappées."
"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.Previous works only calibrate the conﬁdent prediction of classiﬁers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).In contrast, this paper proposes a probabilistic way of directly estimating and ﬁne-tuning the decision boundary between seen and unseen classes.In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the ﬁrst time, are introduced to uncover and ﬁne-tune the decision boundary of each domain.Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted conﬁdently.Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0]",[],H1GaLiAcY7,Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective,"propose d'introduire un nouveau domaine, le domaine incertain, afin de mieux gérer la division entre les domaines vus/non vus dans l'apprentissage par ensembles ouverts et l'apprentissage généralisé par le zéro."
"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.This potential is however not the original loss function in general.So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.Instead, they resemble closed loops with deterministic components.We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.We provide extensive empirical validation of these claims, proven in the appendix.","[0, 0, 0, 0, 1, 0, 0, 0]",[],HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","La SGD effectue implicitement une inférence variationnelle ; le bruit de gradient est fortement non isotrope, de sorte que la SGD ne converge même pas vers les points critiques de la perte originale."
"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.This potential is however not the original loss function in general.So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.Instead, they resemble closed loops with deterministic components.We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.We provide extensive empirical validation of these claims, proven in the appendix.","[0, 0, 0, 0, 1, 0, 0, 0]",[],HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",Cet article propose une analyse variationnelle du SGD en tant que processus de non-équilibre.
"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.This potential is however not the original loss function in general.So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.Instead, they resemble closed loops with deterministic components.We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.We provide extensive empirical validation of these claims, proven in the appendix.","[0, 0, 0, 0, 1, 0, 0, 0]",[],HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","Cet article traite de la fonction objective régularisée minimisée par le SGD standard dans le contexte des réseaux neuronaux, et fournit une perspective d'inférence variationnelle en utilisant l'équation de Fokker-Planck."
"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.This potential is however not the original loss function in general.So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.Instead, they resemble closed loops with deterministic components.We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.We provide extensive empirical validation of these claims, proven in the appendix.","[0, 0, 0, 0, 1, 0, 0, 0]",[],HyWrIgW0W,"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks","Développe une théorie pour étudier l'impact du bruit de gradient stochastique pour le SGD, en particulier pour les modèles de réseaux neuronaux profonds."
"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BkisuzWRW,Zero-Shot Visual Imitation,Les agents peuvent apprendre à imiter uniquement des démonstrations visuelles (sans actions) au moment du test après avoir appris de leur propre expérience sans aucune forme de supervision au moment de la formation.
"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BkisuzWRW,Zero-Shot Visual Imitation,Cet article propose une approche de l'apprentissage visuel par l'apprentissage de fonctions d'habileté paramétriques.
"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BkisuzWRW,Zero-Shot Visual Imitation,"Un article sur l'imitation d'une tâche présentée juste pendant l'inférence, où l'apprentissage est effectué de manière auto-supervisée et où, pendant la formation, l'agent explore des tâches connexes mais différentes."
"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.","[1, 0, 0, 0, 0, 0, 0, 0]",[],BkisuzWRW,Zero-Shot Visual Imitation,propose une méthode permettant de contourner le problème de la démonstration coûteuse d'un expert en utilisant l'exploration aléatoire d'un agent pour apprendre des compétences généralisables qui peuvent être appliquées sans formation préalable spécifique.
"Distributional Semantics Models(DSM) derive word space from linguistic itemsin context.Meaning is obtained by defining a distance measure between vectorscorresponding to lexical entities.Such vectors present several problems.Thiswork concentrates on quality of word embeddings, improvement of word embeddingvectors, applicability of a novel similarity metric used ‘on top’ of theword embeddings.In this paper we provide comparison between two methodsfor post process improvements to the baseline DSM vectors.The counter-fittingmethod which enforces antonymy and synonymy constraints into the Paragramvector space representations recently showed improvement in the vectors’ capabilityfor judging semantic similarity.The second method is our novel RESMmethod applied to GloVe baseline vectors.By applying the hubness reductionmethod, implementing relational knowledge into the model by retrofitting synonymsand providing a new ranking similarity definition RESM that gives maximumweight to the top vector component values we equal the results for the ESLand TOEFL sets in comparison with our calculations using the Paragram and Paragram+ Counter-fitting methods.For SIMLEX-999 gold standard since we cannotuse the RESM the results using GloVe and PPDB are significantly worse comparedto Paragram.Apparently, counter-fitting corrects hubness.The Paragramor our cosine retrofitting method are state-of-the-art results for the SIMLEX-999gold standard.They are 0.2 better for SIMLEX-999 than word2vec with sensede-conflation (that was announced to be state-of the-art method for less reliablegold standards).Apparently relational knowledge and counter-fitting is more importantfor judging semantic similarity than sense determination for words.It is tobe mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999results.The lesson is that many corrections to word embeddings are necessaryand methods with more parameters and hyperparameters perform better.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,L'article fournit une description d'une procédure pour améliorer le modèle d'espace vectoriel des mots avec une évaluation des modèles Paragram et GloVe pour les repères de similarité.
"Distributional Semantics Models(DSM) derive word space from linguistic itemsin context.Meaning is obtained by defining a distance measure between vectorscorresponding to lexical entities.Such vectors present several problems.Thiswork concentrates on quality of word embeddings, improvement of word embeddingvectors, applicability of a novel similarity metric used ‘on top’ of theword embeddings.In this paper we provide comparison between two methodsfor post process improvements to the baseline DSM vectors.The counter-fittingmethod which enforces antonymy and synonymy constraints into the Paragramvector space representations recently showed improvement in the vectors’ capabilityfor judging semantic similarity.The second method is our novel RESMmethod applied to GloVe baseline vectors.By applying the hubness reductionmethod, implementing relational knowledge into the model by retrofitting synonymsand providing a new ranking similarity definition RESM that gives maximumweight to the top vector component values we equal the results for the ESLand TOEFL sets in comparison with our calculations using the Paragram and Paragram+ Counter-fitting methods.For SIMLEX-999 gold standard since we cannotuse the RESM the results using GloVe and PPDB are significantly worse comparedto Paragram.Apparently, counter-fitting corrects hubness.The Paragramor our cosine retrofitting method are state-of-the-art results for the SIMLEX-999gold standard.They are 0.2 better for SIMLEX-999 than word2vec with sensede-conflation (that was announced to be state-of the-art method for less reliablegold standards).Apparently relational knowledge and counter-fitting is more importantfor judging semantic similarity than sense determination for words.It is tobe mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999results.The lesson is that many corrections to word embeddings are necessaryand methods with more parameters and hyperparameters perform better.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,Cet article propose un nouvel algorithme qui ajuste les vecteurs de mots de GloVe et utilise ensuite une fonction de similarité non euclidienne entre eux.
"Distributional Semantics Models(DSM) derive word space from linguistic itemsin context.Meaning is obtained by defining a distance measure between vectorscorresponding to lexical entities.Such vectors present several problems.Thiswork concentrates on quality of word embeddings, improvement of word embeddingvectors, applicability of a novel similarity metric used ‘on top’ of theword embeddings.In this paper we provide comparison between two methodsfor post process improvements to the baseline DSM vectors.The counter-fittingmethod which enforces antonymy and synonymy constraints into the Paragramvector space representations recently showed improvement in the vectors’ capabilityfor judging semantic similarity.The second method is our novel RESMmethod applied to GloVe baseline vectors.By applying the hubness reductionmethod, implementing relational knowledge into the model by retrofitting synonymsand providing a new ranking similarity definition RESM that gives maximumweight to the top vector component values we equal the results for the ESLand TOEFL sets in comparison with our calculations using the Paragram and Paragram+ Counter-fitting methods.For SIMLEX-999 gold standard since we cannotuse the RESM the results using GloVe and PPDB are significantly worse comparedto Paragram.Apparently, counter-fitting corrects hubness.The Paragramor our cosine retrofitting method are state-of-the-art results for the SIMLEX-999gold standard.They are 0.2 better for SIMLEX-999 than word2vec with sensede-conflation (that was announced to be state-of the-art method for less reliablegold standards).Apparently relational knowledge and counter-fitting is more importantfor judging semantic similarity than sense determination for words.It is tobe mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999results.The lesson is that many corrections to word embeddings are necessaryand methods with more parameters and hyperparameters perform better.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyHmGyZCZ,Comparison of Paragram and GloVe Results for Similarity Benchmarks,Les auteurs présentent des observations sur les faiblesses des modèles d'espace vectoriel existants et énumèrent une approche en 6 étapes pour affiner les vecteurs de mots existants
"Recurrent neural networks have achieved excellent performance in many applications.However, on portable devices with limited resources, the models are often too large to deploy.For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem.Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy.By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration.Both results beat the exiting quantization works with large margins.  We extend our alternating quantization to image classification tasks.In both RNNs and feedforward neural networks, the method also achieves  excellent performance.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S19dR9x0b,Alternating Multi-bit Quantization for Recurrent Neural Networks,Nous proposons une nouvelle méthode de quantification et l'appliquons à la quantification des RNN pour la compression et l'accélération.
"Recurrent neural networks have achieved excellent performance in many applications.However, on portable devices with limited resources, the models are often too large to deploy.For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem.Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy.By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration.Both results beat the exiting quantization works with large margins.  We extend our alternating quantization to image classification tasks.In both RNNs and feedforward neural networks, the method also achieves  excellent performance.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S19dR9x0b,Alternating Multi-bit Quantization for Recurrent Neural Networks,Cet article propose une méthode de quantification multi-bits pour les réseaux neuronaux récurrents.
"Recurrent neural networks have achieved excellent performance in many applications.However, on portable devices with limited resources, the models are often too large to deploy.For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem.Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy.By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration.Both results beat the exiting quantization works with large margins.  We extend our alternating quantization to image classification tasks.In both RNNs and feedforward neural networks, the method also achieves  excellent performance.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S19dR9x0b,Alternating Multi-bit Quantization for Recurrent Neural Networks,"Une technique pour quantifier les matrices de poids des réseaux neuronaux, et une procédure d'optimisation alternée pour estimer l'ensemble des k vecteurs et coefficients binaires qui représentent le mieux le vecteur original."
"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.","[0, 0, 0, 1]",[],rkGZuJb0b,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,"Nous remplaçons les couches entièrement connectées d'un réseau neuronal par l'ansatz de renormalisation de l'intrication à plusieurs échelles, un type d'opération quantique qui décrit les corrélations à longue portée. "
"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.","[0, 0, 0, 1]",[],rkGZuJb0b,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,"Dans cet article, les auteurs suggèrent d'utiliser la technique de tensorisation MERA pour compresser les réseaux neuronaux."
"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.","[0, 0, 0, 1]",[],rkGZuJb0b,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,"Une nouvelle paramétrisation des cartes linéaires pour l'utilisation des réseaux neuronaux, utilisant une factorisation hiérarchique de la carte linéaire qui réduit le nombre de paramètres tout en permettant de modéliser des interactions relativement complexes."
"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.","[0, 0, 0, 1]",[],rkGZuJb0b,Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz,Études sur la compression des couches d'anticipation à l'aide de décompositions tensorielles de faible rang et exploration d'une décomposition en arbre
"Deep learning models have outperformed traditional methods in many fields suchas natural language processing and computer vision.However, despite theirtremendous success, the methods of designing optimal Convolutional Neural Networks(CNNs) are still based on heuristics or grid search.The resulting networksobtained using these techniques are often overparametrized with huge computationaland memory requirements.This paper focuses on a structured, explainableapproach towards optimal model design that maximizes accuracy while keepingcomputational costs tractable.We propose a single-shot analysis of a trained CNNthat uses Principal Component Analysis (PCA) to determine the number of filtersthat are doing significant transformations per layer, without the need for retraining.It can be interpreted as identifying the dimensionality of the hypothesis spaceunder consideration.The proposed technique also helps estimate an optimal numberof layers by looking at the expansion of dimensions as the model gets deeper.This analysis can be used to design an optimal structure of a given network ona dataset, or help to adapt a predesigned network on a new dataset.We demonstratethese techniques by optimizing VGG and AlexNet networks on CIFAR-10,CIFAR-100 and ImageNet datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJgzJh0qtQ,A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY,Nous présentons une analyse unique d'un réseau neuronal formé pour éliminer la redondance et identifier la structure optimale du réseau.
"Deep learning models have outperformed traditional methods in many fields suchas natural language processing and computer vision.However, despite theirtremendous success, the methods of designing optimal Convolutional Neural Networks(CNNs) are still based on heuristics or grid search.The resulting networksobtained using these techniques are often overparametrized with huge computationaland memory requirements.This paper focuses on a structured, explainableapproach towards optimal model design that maximizes accuracy while keepingcomputational costs tractable.We propose a single-shot analysis of a trained CNNthat uses Principal Component Analysis (PCA) to determine the number of filtersthat are doing significant transformations per layer, without the need for retraining.It can be interpreted as identifying the dimensionality of the hypothesis spaceunder consideration.The proposed technique also helps estimate an optimal numberof layers by looking at the expansion of dimensions as the model gets deeper.This analysis can be used to design an optimal structure of a given network ona dataset, or help to adapt a predesigned network on a new dataset.We demonstratethese techniques by optimizing VGG and AlexNet networks on CIFAR-10,CIFAR-100 and ImageNet datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJgzJh0qtQ,A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY,"Cet article propose un ensemble d'heuristiques pour identifier une bonne architecture de réseau neuronal, basée sur l'ACP des activations des unités sur l'ensemble de données."
"Deep learning models have outperformed traditional methods in many fields suchas natural language processing and computer vision.However, despite theirtremendous success, the methods of designing optimal Convolutional Neural Networks(CNNs) are still based on heuristics or grid search.The resulting networksobtained using these techniques are often overparametrized with huge computationaland memory requirements.This paper focuses on a structured, explainableapproach towards optimal model design that maximizes accuracy while keepingcomputational costs tractable.We propose a single-shot analysis of a trained CNNthat uses Principal Component Analysis (PCA) to determine the number of filtersthat are doing significant transformations per layer, without the need for retraining.It can be interpreted as identifying the dimensionality of the hypothesis spaceunder consideration.The proposed technique also helps estimate an optimal numberof layers by looking at the expansion of dimensions as the model gets deeper.This analysis can be used to design an optimal structure of a given network ona dataset, or help to adapt a predesigned network on a new dataset.We demonstratethese techniques by optimizing VGG and AlexNet networks on CIFAR-10,CIFAR-100 and ImageNet datasets.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJgzJh0qtQ,A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY,Cet article présente un cadre pour l'optimisation des architectures de réseaux neuronaux par l'identification des filtres redondants entre les couches.
"Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary’s capability to conduct attacks on black-box networks.This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim ’s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique.Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim’s entire network architecture.  In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation.Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting.From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process.Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations.Our empirical security analysis represents a step toward understanding the DNNs’ vulnerability to cache side-channel attacks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rk4Wf30qKQ,Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks,"Nous réalisons la première analyse de sécurité approfondie des attaques par empreintes digitales de DNN qui exploitent les canaux latéraux du cache, ce qui représente une étape vers la compréhension de la vulnérabilité des DNN aux attaques par canaux latéraux."
"Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary’s capability to conduct attacks on black-box networks.This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim ’s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique.Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim’s entire network architecture.  In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation.Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting.From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process.Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations.Our empirical security analysis represents a step toward understanding the DNNs’ vulnerability to cache side-channel attacks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rk4Wf30qKQ,Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks,"Cet article examine le problème de la prise d'empreintes digitales des architectures de réseaux neuronaux à l'aide de canaux secondaires de cache, et discute des défenses de sécurité par l'obscurité."
"Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary’s capability to conduct attacks on black-box networks.This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim ’s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique.Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim’s entire network architecture.  In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation.Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting.From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process.Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker’s observations.Our empirical security analysis represents a step toward understanding the DNNs’ vulnerability to cache side-channel attacks.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rk4Wf30qKQ,Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks,"Cet article réalise des attaques de type ""cache side-channel"" pour extraire les attributs d'un modèle victime et en déduire son architecture. Il montre également qu'elles permettent d'obtenir une précision de classification presque parfaite."
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.","[0, 0, 0, 1, 0, 0, 0]",[],HyM7AiA5YX,Complement Objective Training,"Nous proposons la formation par objectifs complémentaires (COT), un nouveau paradigme de formation qui optimise à la fois les objectifs primaires et complémentaires pour apprendre efficacement les paramètres des réseaux neuronaux."
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.","[0, 0, 0, 1, 0, 0, 0]",[],HyM7AiA5YX,Complement Objective Training,"Envisage d'augmenter l'objectif d'entropie croisée avec une maximisation de l'objectif ""complémentaire"", qui vise à neutraliser les probabilités prédites de classes autres que les étiquettes de vérité terrain."
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.","[0, 0, 0, 1, 0, 0, 0]",[],HyM7AiA5YX,Complement Objective Training,"Les auteurs proposent un objectif secondaire pour la minimisation de softmax basé sur l'évaluation des informations recueillies auprès des classes incorrectes, ce qui conduit à une nouvelle approche de formation."
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.","[0, 0, 0, 1, 0, 0, 0]",[],HyM7AiA5YX,Complement Objective Training,Traite de l'entraînement des réseaux neuronaux pour les tâches de classification ou de génération de séquences en utilisant la perte d'entropie transversale.
We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output.We extend softmax layer with an additional constant input.The corresponding additional output is able to represent the uncertainty of the network.The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets.We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.,"[0, 0, 0, 1, 0]",[],rJxA-h05KQ,Inhibited Softmax for Uncertainty Estimation in Neural Networks,Estimation de l'incertitude en un seul passage vers l'avant sans paramètres supplémentaires pouvant être appris.
We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output.We extend softmax layer with an additional constant input.The corresponding additional output is able to represent the uncertainty of the network.The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets.We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.,"[0, 0, 0, 1, 0]",[],rJxA-h05KQ,Inhibited Softmax for Uncertainty Estimation in Neural Networks,Une nouvelle méthode de calcul des estimations de l'incertitude de sortie dans les DNN pour les problèmes de classification qui correspond aux méthodes de pointe pour l'estimation de l'incertitude et les surpasse dans les tâches de détection de la non-répartition.
We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output.We extend softmax layer with an additional constant input.The corresponding additional output is able to represent the uncertainty of the network.The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets.We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.,"[0, 0, 0, 1, 0]",[],rJxA-h05KQ,Inhibited Softmax for Uncertainty Estimation in Neural Networks,"Les auteurs présentent le softmax inhibé, une modification du softmax par l'ajout d'une activation constante qui fournit une mesure de l'incertitude. "
"When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise.These issues are especially evident in the healthcare, finance, law and government industries.Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption.This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST.Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.","[0, 0, 0, 1, 0, 0]",[],ByCPHrgCW,Deep Learning Inferences with Hybrid Homomorphic Encryption,"Nous avons créé un système riche en fonctionnalités pour l'apprentissage profond avec des entrées cryptées, produisant des sorties cryptées, préservant la confidentialité."
"When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise.These issues are especially evident in the healthcare, finance, law and government industries.Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption.This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST.Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.","[0, 0, 0, 1, 0, 0]",[],ByCPHrgCW,Deep Learning Inferences with Hybrid Homomorphic Encryption,Un cadre pour l'inférence de modèles d'apprentissage profond privés utilisant des schémas FHE qui supportent l'amorçage rapide et peuvent donc réduire le temps de calcul.
"When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise.These issues are especially evident in the healthcare, finance, law and government industries.Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption.This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST.Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.","[0, 0, 0, 1, 0, 0]",[],ByCPHrgCW,Deep Learning Inferences with Hybrid Homomorphic Encryption,L'article présente un moyen d'évaluer un réseau neuronal de manière sécurisée en utilisant le cryptage homomorphique.
"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner.Hence, they provide an opportunity to explore theorem proving with human supervision.We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem.We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","[1, 0, 0, 0, 0]",[],r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,Nous présentons un système appelé GamePad pour explorer l'application des méthodes d'apprentissage automatique à la preuve de théorèmes dans l'assistant de preuve Coq.
"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner.Hence, they provide an opportunity to explore theorem proving with human supervision.We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem.We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","[1, 0, 0, 0, 0]",[],r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,"Cet article décrit un système d'application de l'apprentissage automatique à la démonstration interactive de théorèmes, se concentre sur les tâches de prédiction de tactique et d'évaluation de position, et montre qu'un modèle neuronal surpasse un SVM pour ces deux tâches."
"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner.Hence, they provide an opportunity to explore theorem proving with human supervision.We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem.We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","[1, 0, 0, 0, 0]",[],r1xwKoR9Y7,GamePad: A Learning Environment for Theorem Proving,Propose que des techniques d'apprentissage automatique soient utilisées pour aider à construire des preuves dans le proverbe de théorème Coq.
"Deep neural networks are usually huge, which significantly limits the deployment on low-end devices.In recent years, manyweight-quantized models have  been proposed.They have small storage and fast inference, but training can still be time-consuming.This can be improved with distributed learning.To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.We show  that(i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;(ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and(iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization.Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ryM_IoAqYX,Analysis of Quantized Models,"Dans cet article, nous avons étudié l'entraînement efficace de réseaux quantifiés par le poids avec gradient quantifié dans un environnement distribué, à la fois théoriquement et empiriquement."
"Deep neural networks are usually huge, which significantly limits the deployment on low-end devices.In recent years, manyweight-quantized models have  been proposed.They have small storage and fast inference, but training can still be time-consuming.This can be improved with distributed learning.To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.We show  that(i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;(ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and(iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization.Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ryM_IoAqYX,Analysis of Quantized Models,"Cet article étudie les propriétés de convergence de la quantification de poids tenant compte des pertes avec différentes précisions de gradient dans l'environnement distribué, et fournit une analyse de convergence pour la quantification de poids avec des gradients à précision complète, quantifiés et quantifiés coupés."
"Deep neural networks are usually huge, which significantly limits the deployment on low-end devices.In recent years, manyweight-quantized models have  been proposed.They have small storage and fast inference, but training can still be time-consuming.This can be improved with distributed learning.To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.We show  that(i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;(ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and(iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization.Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ryM_IoAqYX,Analysis of Quantized Models,Les auteurs proposent une analyse de l'effet de la quantification simultanée des poids et des gradients dans l'apprentissage d'un modèle paramétré dans un environnement distribué entièrement synchronisé.
"Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task.In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them.To achieve Selfless Sequential Learning we study different regularization strategies and activation functions.We find thatimposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity.In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition.It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks.As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain.We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network.We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Bkxbrn0cYX,Selfless Sequential Learning,Une stratégie de régularisation pour améliorer les performances de l'apprentissage séquentiel
"Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task.In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them.To achieve Selfless Sequential Learning we study different regularization strategies and activation functions.We find thatimposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity.In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition.It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks.As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain.We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network.We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Bkxbrn0cYX,Selfless Sequential Learning,"Une nouvelle approche, basée sur la régularisation, du problème de l'apprentissage séquentiel en utilisant un modèle de taille fixe qui ajoute des termes supplémentaires à la perte, encourageant la sparsité de la représentation et combattant l'oubli catastrophique."
"Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task.In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them.To achieve Selfless Sequential Learning we study different regularization strategies and activation functions.We find thatimposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity.In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition.It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks.As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain.We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network.We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Bkxbrn0cYX,Selfless Sequential Learning,Cet article traite du problème de l'oubli catastrophique dans l'apprentissage tout au long de la vie en proposant des stratégies d'apprentissage régularisées.
"A Synaptic Neural Network (SynaNN) consists of synapses and neurons.Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space.Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution.In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network.Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms.In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.","[0, 0, 0, 1, 0, 0, 0]",[],ryGpEiAcFQ,A Synaptic Neural Network and Synapse Learning,Un réseau neuronal synaptique avec graphe de synapses et apprentissage qui présente la caractéristique de conjugaison topologique et de distribution de Bose-Einstein dans l'espace de surprise.  
"A Synaptic Neural Network (SynaNN) consists of synapses and neurons.Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space.Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution.In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network.Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms.In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.","[0, 0, 0, 1, 0, 0, 0]",[],ryGpEiAcFQ,A Synaptic Neural Network and Synapse Learning,Les auteurs proposent un réseau neuronal hybride composé d'un graphe de synapses qui peut être intégré dans un réseau neuronal standard.
"A Synaptic Neural Network (SynaNN) consists of synapses and neurons.Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space.Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution.In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network.Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms.In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers.","[0, 0, 0, 1, 0, 0, 0]",[],ryGpEiAcFQ,A Synaptic Neural Network and Synapse Learning,Présente un modèle de réseau neuronal d'inspiration biologique basé sur les canaux ioniques excitateurs et inhibiteurs des membranes de cellules réelles.
"Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.","[1, 0, 0, 0]",[],SJd0EAy0b,Generalized Graph Embedding Models,Modèles d'intégration de graphes généralisés
"Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.","[1, 0, 0, 0]",[],SJd0EAy0b,Generalized Graph Embedding Models,"Une approche généralisée d'intégration de graphes de connaissances qui apprend les intégrations sur la base de trois objectifs simultanés différents, et dont les performances sont équivalentes ou même supérieures à celles des approches existantes de l'état de l'art."
"Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.","[1, 0, 0, 0]",[],SJd0EAy0b,Generalized Graph Embedding Models,S'attaque à la tâche d'apprentissage d'encastrements de graphes multi-relationnels à l'aide d'un réseau de neurones.
"Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.","[1, 0, 0, 0]",[],SJd0EAy0b,Generalized Graph Embedding Models,"Propose une nouvelle méthode, GEN, pour calculer les enchâssements de graphes multirelations, en particulier que les soi-disant E-Cells et R-Cells peuvent répondre à des requêtes de la forme (h,r, ?),(?r,t), et (h, ?,t)"
"We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning.The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages.At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset.MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement.We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance.Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set.Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.","[0, 0, 0, 1, 0, 0, 0, 0]",[],BywyFQlAW,Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,L'apprentissage curriculaire Minimax est une méthode d'apprentissage automatique impliquant l'augmentation de la dureté souhaitable et la réduction programmée de la diversité.
"We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning.The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages.At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset.MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement.We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance.Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set.Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.","[0, 0, 0, 1, 0, 0, 0, 0]",[],BywyFQlAW,Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity, Une approche d'apprentissage de programmes d'études utilisant une fonction d'ensemble submodulaire qui capture la diversité des exemples choisis pendant la formation. 
"We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning.The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages.At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset.MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement.We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance.Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set.Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee.","[0, 0, 0, 1, 0, 0, 0, 0]",[],BywyFQlAW,Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity,L'article présente l'apprentissage curriculaire MiniMax comme une approche permettant de former de manière adaptative des modèles en leur fournissant différents sous-ensembles de données. 
"Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.In experiments, we scale Bayesian inference on up to a billion genetic measurements.We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,Modèles implicites appliqués à la causalité et à la génétique
"Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.In experiments, we scale Bayesian inference on up to a billion genetic measurements.We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,Les auteurs proposent d'utiliser le modèle implicite pour s'attaquer au problème des associations à l'échelle du génome.
"Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.In experiments, we scale Bayesian inference on up to a billion genetic measurements.We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,"Cet article propose des solutions aux problèmes que posent, dans les études d'association à l'échelle du génome, la confusion due à la structure de la population et la présence potentielle d'interactions non linéaires entre différentes parties du génome, et établit un pont entre la génétique statistique et la ML."
"Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.In experiments, we scale Bayesian inference on up to a billion genetic measurements.We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SyELrEeAb,Implicit Causal Models for Genome-wide Association Studies,"Présente un modèle génératif non linéaire pour GWAS qui modélise la structure de la population, où les non-linéarités sont modélisées à l'aide de réseaux neuronaux comme approximateurs de fonctions non linéaires et où l'inférence est effectuée à l'aide d'une inférence variationnelle sans vraisemblance."
"Few-shot learning trains image classifiers over datasets with few examples per category. It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories.In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.For a new category, even though its images are not seen by the model, some objects may appear in the training images.Hence, object-level relation is useful for inferring the relation of images from unseen categories.Consequently, our model generalizes well for new categories without fine-tuning.Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,Apprentissage en quelques clics en exploitant la relation au niveau de l'objet pour apprendre la relation au niveau de l'image (similarité).
"Few-shot learning trains image classifiers over datasets with few examples per category. It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories.In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.For a new category, even though its images are not seen by the model, some objects may appear in the training images.Hence, object-level relation is useful for inferring the relation of images from unseen categories.Consequently, our model generalizes well for new categories without fine-tuning.Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,Cet article traite du problème de l'apprentissage en quelques clics en proposant une approche basée sur l'intégration qui apprend à comparer les caractéristiques de niveau objet entre les exemples de l'ensemble de support et de l'ensemble de requête.
"Few-shot learning trains image classifiers over datasets with few examples per category. It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories.In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.For a new category, even though its images are not seen by the model, some objects may appear in the training images.Hence, object-level relation is useful for inferring the relation of images from unseen categories.Consequently, our model generalizes well for new categories without fine-tuning.Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkzcvoA9YX,Few-Shot Learning by Exploiting Object Relation,propose une méthode d'apprentissage en quelques clics qui exploite la relation au niveau de l'objet entre différentes images sur la base de la recherche de voisins proches et concatène les cartes de caractéristiques de deux images d'entrée en une seule carte de caractéristiques.
"Word embeddings are widely used in machine learning based natural language processing systems.It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance.There has been a recent interest in applying natural language processing techniques to programming languages.However, none of this recent work uses pre-trained embeddings on code tokens.Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\% improvement in test loss, 4\% improvement in F1 scores, and resistance to over-fitting.We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   ","[0, 0, 0, 0, 0, 1]",[],H1glKiCqtm,The Effectiveness of Pre-Trained Code Embeddings,"Les chercheurs qui explorent les techniques de traitement du langage naturel appliquées au code source n'utilisent aucune forme d'enchâssement pré-formé, nous montrons qu'ils devraient le faire."
"Word embeddings are widely used in machine learning based natural language processing systems.It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance.There has been a recent interest in applying natural language processing techniques to programming languages.However, none of this recent work uses pre-trained embeddings on code tokens.Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\% improvement in test loss, 4\% improvement in F1 scores, and resistance to over-fitting.We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   ","[0, 0, 0, 0, 0, 1]",[],H1glKiCqtm,The Effectiveness of Pre-Trained Code Embeddings,Cet article cherche à comprendre si le pré-entraînement des mots incorporés pour le code du langage de programmation en utilisant des modèles de langage de type NLP a un impact sur la tâche de résumé de code extrême.
"Word embeddings are widely used in machine learning based natural language processing systems.It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance.There has been a recent interest in applying natural language processing techniques to programming languages.However, none of this recent work uses pre-trained embeddings on code tokens.Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\% improvement in test loss, 4\% improvement in F1 scores, and resistance to over-fitting.We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   ","[0, 0, 0, 0, 0, 1]",[],H1glKiCqtm,The Effectiveness of Pre-Trained Code Embeddings,Ce travail montre comment le pré-entraînement des vecteurs de mots à l'aide de corpus de code conduit à des représentations qui sont plus appropriées que les représentations initialisées et entraînées de manière aléatoire pour la prédiction des noms de fonctions/méthodes.
"Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).In these two-player games, a reward is always received at the end of the game.However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate.This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.Autodidactic Iteration is able to learn how to solve the Rubik’s Cube and the 15-puzzle without relying on human data.Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.","[0, 0, 0, 0, 0, 0, 1, 0]",[],Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,Nous résolvons le Rubik's Cube avec de l'apprentissage par renforcement pur.
"Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).In these two-player games, a reward is always received at the end of the game.However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate.This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.Autodidactic Iteration is able to learn how to solve the Rubik’s Cube and the 15-puzzle without relying on human data.Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.","[0, 0, 0, 0, 0, 0, 1, 0]",[],Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,Solution pour résoudre le Rubik cube en utilisant l'apprentissage par renforcement (RL) avec la recherche d'arbre de Monte-Carlo (MCTS) par itération autodidactique. 
"Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).In these two-player games, a reward is always received at the end of the game.However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate.This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.Autodidactic Iteration is able to learn how to solve the Rubik’s Cube and the 15-puzzle without relying on human data.Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.","[0, 0, 0, 0, 0, 0, 1, 0]",[],Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,"Ce travail résout le Rubik's Cube en utilisant une méthode d'itération de politique approximative appelée itération autodidactique, en surmontant le problème des récompenses éparses en créant son propre système de récompenses."
"Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).In these two-player games, a reward is always received at the end of the game.However, the Rubik’s Cube has only a single solved state, and episodes are not guaranteed to terminate.This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.Autodidactic Iteration is able to learn how to solve the Rubik’s Cube and the 15-puzzle without relying on human data.Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves — less than or equal to solvers that employ human domain knowledge.","[0, 0, 0, 0, 0, 0, 1, 0]",[],Hyfn2jCcKm,Solving the Rubik's Cube with Approximate Policy Iteration,Présente un algorithme RL profond pour résoudre le Rubik's cube qui gère l'énorme espace d'état et la récompense très éparse du Rubik's cube.
"Answering compositional questions requiring multi-step reasoning is challenging for current models.We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics.Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning.Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question.For example, to interpret ‘not green’, the model will represent ‘green’ as a set of entities, ‘not’ as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation.For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent.We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task.The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines.We will release our code.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkaqxm-0b,Neural Compositional Denotational Semantics for Question Answering,"Nous décrivons un modèle différentiable de bout en bout pour l'AQ qui apprend à représenter des portions de texte dans la question comme des dénotations dans le graphe de connaissances, en apprenant à la fois des modules neuronaux pour la composition et la structure syntaxique de la phrase."
"Answering compositional questions requiring multi-step reasoning is challenging for current models.We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics.Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning.Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question.For example, to interpret ‘not green’, the model will represent ‘green’ as a set of entities, ‘not’ as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation.For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent.We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task.The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines.We will release our code.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkaqxm-0b,Neural Compositional Denotational Semantics for Question Answering,"Cet article présente un modèle de réponse aux questions visuelles qui peut apprendre à la fois les paramètres et les prédicteurs de structure pour un réseau neuronal modulaire, sans structures supervisées ou assistance d'un analyseur syntaxique."
"Answering compositional questions requiring multi-step reasoning is challenging for current models.We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics.Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning.Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question.For example, to interpret ‘not green’, the model will represent ‘green’ as a set of entities, ‘not’ as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation.For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent.We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task.The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines.We will release our code.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkaqxm-0b,Neural Compositional Denotational Semantics for Question Answering,propose d'entraîner un modèle de réponse aux questions à partir des réponses uniquement et d'une base de données en apprenant des arbres latents qui capturent la syntaxe et apprennent la sémantique des mots.
"Deep learning software demands reliability and performance.However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","[0, 1, 0, 0, 0]",[],ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,Nous présentons une nouvelle infrastructure de compilateur qui répond aux lacunes des cadres d'apprentissage profond existants.
"Deep learning software demands reliability and performance.However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","[0, 1, 0, 0, 0]",[],ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,Proposition de passer de la génération de code ad hoc dans les moteurs d'apprentissage profond aux meilleures pratiques en matière de compilateurs et de langages.
"Deep learning software demands reliability and performance.However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","[0, 1, 0, 0, 0]",[],ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,"Cet article présente un cadre de compilateur qui permet de définir des langages spécifiques à un domaine pour les systèmes d'apprentissage profond, et définit des étapes de compilation qui peuvent tirer parti des optimisations standard et des optimisations spécialisées pour les réseaux neuronaux."
"Deep learning software demands reliability and performance.However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","[0, 1, 0, 0, 0]",[],ryG6xZ-RZ,DLVM: A modern compiler infrastructure for deep learning systems,Cet article introduit un DLVM pour tirer parti des aspects de compilation d'un compilateur tensoriel
"In this work, we focus on the problem of grounding language by training an agentto follow a set of natural language instructions and navigate to a target objectin a 2D grid environment.The agent receives visual information through rawpixels and a natural language instruction telling what task needs to be achieved.Other than these two sources of information, our model does not have any priorinformation of both the visual and textual modalities and is end-to-end trainable.We develop an attention mechanism for multi-modal fusion of visual and textualmodalities that allows the agent to learn to complete the navigation tasks and alsoachieve language grounding.Our experimental results show that our attentionmechanism outperforms the existing multi-modal fusion mechanisms proposed inorder to solve the above mentioned navigation task.We demonstrate through thevisualization of attention weights that our model learns to correlate attributes ofthe object referred in the instruction with visual representations and also showthat the learnt textual representations are semantically meaningful as they followvector arithmetic and are also consistent enough to induce translation between instructionsin different natural languages.We also show that our model generalizeseffectively to unseen scenarios and exhibit zero-shot generalization capabilities.In order to simulate the above described challenges, we introduce a new 2D environmentfor an agent to jointly learn visual and textual modalities","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJPSN3gRW,Learning to navigate by distilling visual information and natural language instructions,Architecture basée sur l'attention pour l'ancrage du langage via l'apprentissage par renforcement dans un nouvel environnement de grille 2D personnalisable.  
"In this work, we focus on the problem of grounding language by training an agentto follow a set of natural language instructions and navigate to a target objectin a 2D grid environment.The agent receives visual information through rawpixels and a natural language instruction telling what task needs to be achieved.Other than these two sources of information, our model does not have any priorinformation of both the visual and textual modalities and is end-to-end trainable.We develop an attention mechanism for multi-modal fusion of visual and textualmodalities that allows the agent to learn to complete the navigation tasks and alsoachieve language grounding.Our experimental results show that our attentionmechanism outperforms the existing multi-modal fusion mechanisms proposed inorder to solve the above mentioned navigation task.We demonstrate through thevisualization of attention weights that our model learns to correlate attributes ofthe object referred in the instruction with visual representations and also showthat the learnt textual representations are semantically meaningful as they followvector arithmetic and are also consistent enough to induce translation between instructionsin different natural languages.We also show that our model generalizeseffectively to unseen scenarios and exhibit zero-shot generalization capabilities.In order to simulate the above described challenges, we introduce a new 2D environmentfor an agent to jointly learn visual and textual modalities","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJPSN3gRW,Learning to navigate by distilling visual information and natural language instructions,L'article aborde le problème de la navigation à partir d'une instruction et propose une approche permettant de combiner les informations textuelles et visuelles via un mécanisme d'attention.
"In this work, we focus on the problem of grounding language by training an agentto follow a set of natural language instructions and navigate to a target objectin a 2D grid environment.The agent receives visual information through rawpixels and a natural language instruction telling what task needs to be achieved.Other than these two sources of information, our model does not have any priorinformation of both the visual and textual modalities and is end-to-end trainable.We develop an attention mechanism for multi-modal fusion of visual and textualmodalities that allows the agent to learn to complete the navigation tasks and alsoachieve language grounding.Our experimental results show that our attentionmechanism outperforms the existing multi-modal fusion mechanisms proposed inorder to solve the above mentioned navigation task.We demonstrate through thevisualization of attention weights that our model learns to correlate attributes ofthe object referred in the instruction with visual representations and also showthat the learnt textual representations are semantically meaningful as they followvector arithmetic and are also consistent enough to induce translation between instructionsin different natural languages.We also show that our model generalizeseffectively to unseen scenarios and exhibit zero-shot generalization capabilities.In order to simulate the above described challenges, we introduce a new 2D environmentfor an agent to jointly learn visual and textual modalities","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJPSN3gRW,Learning to navigate by distilling visual information and natural language instructions,"Cet article considère le problème de suivre des instructions en langage naturel en ayant une vue à la première personne d'un environnement a priori inconnu, et propose une méthode d'architecture neuronale."
"In this work, we focus on the problem of grounding language by training an agentto follow a set of natural language instructions and navigate to a target objectin a 2D grid environment.The agent receives visual information through rawpixels and a natural language instruction telling what task needs to be achieved.Other than these two sources of information, our model does not have any priorinformation of both the visual and textual modalities and is end-to-end trainable.We develop an attention mechanism for multi-modal fusion of visual and textualmodalities that allows the agent to learn to complete the navigation tasks and alsoachieve language grounding.Our experimental results show that our attentionmechanism outperforms the existing multi-modal fusion mechanisms proposed inorder to solve the above mentioned navigation task.We demonstrate through thevisualization of attention weights that our model learns to correlate attributes ofthe object referred in the instruction with visual representations and also showthat the learnt textual representations are semantically meaningful as they followvector arithmetic and are also consistent enough to induce translation between instructionsin different natural languages.We also show that our model generalizeseffectively to unseen scenarios and exhibit zero-shot generalization capabilities.In order to simulate the above described challenges, we introduce a new 2D environmentfor an agent to jointly learn visual and textual modalities","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HJPSN3gRW,Learning to navigate by distilling visual information and natural language instructions,Étudie le problème de la navigation vers un objet cible dans un environnement quadrillé en 2D en suivant une description donnée en langage naturel et en recevant des informations visuelles sous forme de pixels bruts.
" Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention.Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.The speed-up gain allows us to train the model with much more data.We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.","[1, 0, 0, 0, 0, 0, 0]",[],B14TlG-RW,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Une architecture simple composée de convolutions et d'attention permet d'obtenir des résultats comparables à ceux des modèles récurrents les mieux documentés.
" Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention.Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.The speed-up gain allows us to train the model with much more data.We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.","[1, 0, 0, 0, 0, 0, 0]",[],B14TlG-RW,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Une méthode rapide et performante d'augmentation des données basée sur la paraphrase et un modèle non récurrent de compréhension de la lecture utilisant uniquement des convolutions et l'attention.
" Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention.Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.The speed-up gain allows us to train the model with much more data.We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.","[1, 0, 0, 0, 0, 0, 0]",[],B14TlG-RW,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Cet article propose d'appliquer des modules CNN + auto-attention au lieu de LSTM et d'améliorer la formation du modèle RC avec des paraphrases de passages générées par un modèle neuronal de paraphrase afin d'améliorer la performance RC.
" Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention.Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.The speed-up gain allows us to train the model with much more data.We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.","[1, 0, 0, 0, 0, 0, 0]",[],B14TlG-RW,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,Cet article présente un modèle de compréhension de la lecture utilisant des convolutions et l'attention et propose d'augmenter les données d'entraînement supplémentaires par une paraphrase basée sur une traduction automatique neuronale standard.
"Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.However, a number of problems of recent interest have created a demand for models that can analyze spherical images.Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.In this paper we introduce the building blocks for constructing spherical CNNs.We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.","[0, 0, 0, 0, 0, 0, 0, 1]",[],Hkbd5xZRb,Spherical CNNs,"Nous présentons les CNN sphériques, un réseau convolutif pour les signaux sphériques, et l'appliquons à la reconnaissance de modèles 3D et à la régression de l'énergie moléculaire."
"Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.However, a number of problems of recent interest have created a demand for models that can analyze spherical images.Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.In this paper we introduce the building blocks for constructing spherical CNNs.We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.","[0, 0, 0, 0, 0, 0, 0, 1]",[],Hkbd5xZRb,Spherical CNNs,L'article propose un cadre pour la construction de réseaux convolutifs sphériques basé sur une nouvelle synthèse de plusieurs concepts existants.
"Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.However, a number of problems of recent interest have created a demand for models that can analyze spherical images.Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.In this paper we introduce the building blocks for constructing spherical CNNs.We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.","[0, 0, 0, 0, 0, 0, 0, 1]",[],Hkbd5xZRb,Spherical CNNs,"Cet article se concentre sur la manière d'étendre les réseaux neuronaux convolutifs pour qu'ils aient une invariance sphérique intégrée, et adapte les outils de l'analyse harmonique non abélienne pour atteindre cet objectif."
"Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.However, a number of problems of recent interest have created a demand for models that can analyze spherical images.Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.In this paper we introduce the building blocks for constructing spherical CNNs.We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.","[0, 0, 0, 0, 0, 0, 0, 1]",[],Hkbd5xZRb,Spherical CNNs,Les auteurs développent un nouveau schéma pour représenter les données sphériques à partir de la base.
"We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness.We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions.We highlight that our method has two distinct benefits over other automated design approaches.First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest.Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods.These two strengths work together to overcome some of the current shortcomings of automated design.","[1, 0, 0, 0, 0, 0, 0]",[],ByaQIGg0-,AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT,Une méthode pour effectuer une conception automatisée sur des objets du monde réel tels que des dissipateurs thermiques et des profils d'aile qui utilise des réseaux neuronaux et la descente de gradient.
"We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness.We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions.We highlight that our method has two distinct benefits over other automated design approaches.First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest.Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods.These two strengths work together to overcome some of the current shortcomings of automated design.","[1, 0, 0, 0, 0, 0, 0]",[],ByaQIGg0-,AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT,Réseau neuronal (paramétrage et prédiction) et descente de gradient (rétropropogation) pour concevoir automatiquement des tâches d'ingénierie. 
"We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness.We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions.We highlight that our method has two distinct benefits over other automated design approaches.First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest.Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods.These two strengths work together to overcome some of the current shortcomings of automated design.","[1, 0, 0, 0, 0, 0, 0]",[],ByaQIGg0-,AUTOMATED DESIGN USING NEURAL NETWORKS AND GRADIENT DESCENT,"Cet article présente l'utilisation d'un réseau profond pour approximer le comportement d'un système physique complexe, puis la conception de dispositifs optimaux en optimisant ce réseau par rapport à ses entrées."
"Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.","[0, 0, 0, 1, 0, 0]",[],BkA7gfZAb,Stable Distribution Alignment Using the Dual of the Adversarial Distance, Nous proposons une version duale de la distance adversariale logistique pour l'alignement des caractéristiques et nous montrons qu'elle produit des itérations de gradient plus stables que l'objectif min-max.
"Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.","[0, 0, 0, 1, 0, 0]",[],BkA7gfZAb,Stable Distribution Alignment Using the Dual of the Adversarial Distance,L'article traite de la correction des GAN au niveau du calcul.
"Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.","[0, 0, 0, 1, 0, 0]",[],BkA7gfZAb,Stable Distribution Alignment Using the Dual of the Adversarial Distance,"Cet article étudie une formulation double d'une perte contradictoire basée sur une limite supérieure de la perte logistique, et transforme le problème standard min max de la formation contradictoire en un seul problème de minimisation."
"Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.","[0, 0, 0, 1, 0, 0]",[],BkA7gfZAb,Stable Distribution Alignment Using the Dual of the Adversarial Distance,Propose de reformuler l'objectif du point selle du GAN (pour un discriminateur de régression logistique) comme un problème de minimisation en dualisant l'objectif du maximum de vraisemblance pour la régression logistique régularisée.
"  There are many applications scenarios for which the computational  performance and memory footprint of the prediction phase of Deep  Neural Networks (DNNs) need to be optimized.Binary Deep Neural  Networks (BDNNs) have been shown to be an effective way of achieving  this objective.In this paper, we show how Convolutional Neural  Networks (CNNs) can be implemented using binary  representations.Espresso is a compact, yet powerful  library written in C/CUDA that features all the functionalities  required for the forward propagation of CNNs, in a binary file less  than 400KB, without any external dependencies.Although it is mainly  designed to take advantage of massive GPU parallelism, Espresso also  provides an equivalent CPU implementation for CNNs.Espresso  provides special convolutional and dense layers for BCNNs,  leveraging bit-packing and bit-wise computations  for efficient execution.These techniques provide a speed-up of  matrix-multiplication routines, and at the same time, reduce memory  usage when storing parameters and activations.We experimentally  show that Espresso is significantly faster than existing  implementations of optimized binary neural networks (~ 2  orders of magnitude).Espresso is released under the Apache 2.0  license and is available at http://github.com/organization/project.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Sk6fD5yCb,Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,mise en œuvre de réseaux neuronaux binaires à la pointe de la performance de calcul
"  There are many applications scenarios for which the computational  performance and memory footprint of the prediction phase of Deep  Neural Networks (DNNs) need to be optimized.Binary Deep Neural  Networks (BDNNs) have been shown to be an effective way of achieving  this objective.In this paper, we show how Convolutional Neural  Networks (CNNs) can be implemented using binary  representations.Espresso is a compact, yet powerful  library written in C/CUDA that features all the functionalities  required for the forward propagation of CNNs, in a binary file less  than 400KB, without any external dependencies.Although it is mainly  designed to take advantage of massive GPU parallelism, Espresso also  provides an equivalent CPU implementation for CNNs.Espresso  provides special convolutional and dense layers for BCNNs,  leveraging bit-packing and bit-wise computations  for efficient execution.These techniques provide a speed-up of  matrix-multiplication routines, and at the same time, reduce memory  usage when storing parameters and activations.We experimentally  show that Espresso is significantly faster than existing  implementations of optimized binary neural networks (~ 2  orders of magnitude).Espresso is released under the Apache 2.0  license and is available at http://github.com/organization/project.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Sk6fD5yCb,Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,L'article présente une bibliothèque écrite en C/CUDA qui comporte toutes les fonctionnalités nécessaires à la propagation vers l'avant des BCNN.
"  There are many applications scenarios for which the computational  performance and memory footprint of the prediction phase of Deep  Neural Networks (DNNs) need to be optimized.Binary Deep Neural  Networks (BDNNs) have been shown to be an effective way of achieving  this objective.In this paper, we show how Convolutional Neural  Networks (CNNs) can be implemented using binary  representations.Espresso is a compact, yet powerful  library written in C/CUDA that features all the functionalities  required for the forward propagation of CNNs, in a binary file less  than 400KB, without any external dependencies.Although it is mainly  designed to take advantage of massive GPU parallelism, Espresso also  provides an equivalent CPU implementation for CNNs.Espresso  provides special convolutional and dense layers for BCNNs,  leveraging bit-packing and bit-wise computations  for efficient execution.These techniques provide a speed-up of  matrix-multiplication routines, and at the same time, reduce memory  usage when storing parameters and activations.We experimentally  show that Espresso is significantly faster than existing  implementations of optimized binary neural networks (~ 2  orders of magnitude).Espresso is released under the Apache 2.0  license and is available at http://github.com/organization/project.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Sk6fD5yCb,Espresso: Efficient Forward Propagation for Binary Deep Neural Networks,"Cet article s'appuie sur Binary-NET et l'étend aux architectures CNN, fournit des optimisations qui améliorent la vitesse de la passe avant, et fournit un code optimisé pour Binary CNN."
"Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization.In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task.Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set.We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training.The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.","[0, 1, 0, 0, 0, 0]",[],r1GaAjRcF7,Differentiable Greedy Networks,Nous proposons un algorithme de sélection de sous-ensembles qui peut être entraîné avec des méthodes basées sur le gradient tout en obtenant des performances quasi optimales par optimisation submodulaire.
"Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization.In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task.Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set.We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training.The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.","[0, 1, 0, 0, 0, 0]",[],r1GaAjRcF7,Differentiable Greedy Networks,"Propose un modèle basé sur un réseau de neurones qui intègre une fonction submodulaire en combinant une technique d'optimisation basée sur le gradient avec un cadre submodulaire appelé ""Differentiable Greedy Network"" (DGN)."
"Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization.In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task.Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set.We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training.The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.","[0, 1, 0, 0, 0, 0]",[],r1GaAjRcF7,Differentiable Greedy Networks,"propose un réseau neuronal qui vise à sélectionner un sous-ensemble d'éléments (par exemple, la sélection de k phrases qui sont principalement liées à une demande à partir d'un ensemble de documents récupérés)."
"The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years.In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations.To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model.Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components.This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy.In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features.We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1ERcs09KQ,Hierarchically Clustered Representation Learning,"Nous introduisons l'apprentissage de représentation hiérarchiquement groupé (HCRL), qui optimise simultanément l'apprentissage de représentation et le regroupement hiérarchique dans l'espace d'intégration."
"The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years.In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations.To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model.Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components.This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy.In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features.We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1ERcs09KQ,Hierarchically Clustered Representation Learning,L'article propose d'utiliser le CRP imbriqué comme un modèle de regroupement plutôt que comme un modèle thématique.
"The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years.In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations.To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model.Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components.This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy.In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features.We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.","[0, 0, 1, 0, 0, 0, 0, 0]",[],H1ERcs09KQ,Hierarchically Clustered Representation Learning,Présente une nouvelle méthode de regroupement hiérarchique sur un espace d'intégration où l'espace d'intégration et le regroupement hiérarchique sont appris simultanément.
"We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers.Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry.Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers.This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition).We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.","[1, 0, 0, 0, 0]",[],BJeapjA5FX,GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS,Nous développons un cadre d'augmentation de l'apprentissage non supervisé statistique-géométrique pour les réseaux neuronaux profonds afin de les rendre robustes aux attaques adverses.
"We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers.Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry.Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers.This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition).We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.","[1, 0, 0, 0, 0]",[],BJeapjA5FX,GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS,Transformation des réseaux neuronaux profonds traditionnels en calssifieurs robustes adversaires à l'aide de GRN.
"We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers.Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry.Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers.This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition).We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.","[1, 0, 0, 0, 0]",[],BJeapjA5FX,GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS,propose une défense basée sur des distributions de caractéristiques conditionnelles à la classe pour transformer les réseaux de neurones profonds en classificateurs robustes.
"Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient.Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover.In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces.The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space.This approach enables lower sample complexity, while preserving policy expressivity.In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning.The benefits of our learning approach are that1) it is principled,2) simple to implement,3) easily scalable to settings with many actions and4) easily composable with existing deep learning approaches.We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space.In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE).We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines.Moreover, we show that our hierarchical structure leads to meaningful agent coordination.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyunpgbR-,Structured Exploration via Hierarchical Variational Policy Networks,Rendre plus efficace l'apprentissage par renforcement profond dans de grands espaces état-action en utilisant l'exploration structurée avec des politiques hiérarchiques profondes.
"Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient.Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover.In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces.The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space.This approach enables lower sample complexity, while preserving policy expressivity.In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning.The benefits of our learning approach are that1) it is principled,2) simple to implement,3) easily scalable to settings with many actions and4) easily composable with existing deep learning approaches.We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space.In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE).We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines.Moreover, we show that our hierarchical structure leads to meaningful agent coordination.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyunpgbR-,Structured Exploration via Hierarchical Variational Policy Networks,"Une méthode pour coordonner le comportement d'un agent en utilisant des politiques qui ont une structure latente partagée, une méthode d'optimisation variationnelle des politiques pour optimiser les politiques coordonnées, et une dérivation de la mise à jour variationnelle et hiérarchique des auteurs."
"Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient.Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover.In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces.The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space.This approach enables lower sample complexity, while preserving policy expressivity.In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning.The benefits of our learning approach are that1) it is principled,2) simple to implement,3) easily scalable to settings with many actions and4) easily composable with existing deep learning approaches.We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space.In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE).We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines.Moreover, we show that our hierarchical structure leads to meaningful agent coordination.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyunpgbR-,Structured Exploration via Hierarchical Variational Policy Networks,Cet article propose une innovation algorithmique consistant en des variables latentes hiérarchiques pour l'exploration coordonnée dans des environnements multi-agents.
"Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance.Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks.However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks.We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks.In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR.Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size.This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data.Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent.Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ryfMLoCqtQ,An analytic theory of generalization dynamics and transfer learning in deep linear networks,Nous fournissons de nombreuses informations sur la généralisation des réseaux neuronaux à partir du cas linéaire théoriquement traitable.
"Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance.Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks.However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks.We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks.In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR.Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size.This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data.Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent.Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ryfMLoCqtQ,An analytic theory of generalization dynamics and transfer learning in deep linear networks,Les auteurs étudient un modèle simple de réseaux linéaires pour comprendre l'apprentissage par généralisation et transfert.
"We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem.Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation.As a result, the gradient vanishing/explosion problem is avoided.Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets.","[0, 0, 1, 0]",[],r1Kr3TyAb,ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,"La normalisation par lots maintient la variance du gradient tout au long de la formation, ce qui stabilise l'optimisation."
"We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem.Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation.As a result, the gradient vanishing/explosion problem is avoided.Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets.","[0, 0, 1, 0]",[],r1Kr3TyAb,ANALYSIS ON GRADIENT PROPAGATION IN BATCH NORMALIZED RESIDUAL NETWORKS,Ce document analyse l'effet de la normalisation des lots sur la rétropropagation par gradient dans les réseaux résiduels.
"To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.","[1, 0, 0, 0]",[],ryxSrhC9KX,Revealing interpretable object representations from human behavior,Les jugements comportementaux humains sont utilisés pour obtenir des représentations éparses et interprétables d'objets qui se généralisent à d'autres tâches.
"To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.","[1, 0, 0, 0]",[],ryxSrhC9KX,Revealing interpretable object representations from human behavior,Cet article décrit une expérience à grande échelle sur les représentations objet/sémantique humaines et un modèle de ces représentations.
"To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.","[1, 0, 0, 0]",[],ryxSrhC9KX,Revealing interpretable object representations from human behavior,Cet article développe un nouveau système de représentation d'objets à partir d'un entraînement sur des données collectées à partir de jugements humains impairs d'images.
"To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.","[1, 0, 0, 0]",[],ryxSrhC9KX,Revealing interpretable object representations from human behavior,"Une nouvelle approche pour apprendre un espace sémantique clairsemé, positif et interprétable qui maximise les jugements de similarité humains en s'entraînant pour maximiser spécifiquement la prédiction des jugements de similarité humains."
"We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient.We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!.The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.","[0, 1, 0, 0, 0, 0]",[],S1CChZ-CZ,Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,Nous proposons un agent qui se place entre l'utilisateur et un système de réponse aux questions de type boîte noire et qui apprend à reformuler les questions pour obtenir les meilleures réponses possibles.
"We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient.We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!.The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.","[0, 1, 0, 0, 0, 0]",[],S1CChZ-CZ,Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,Cet article propose une réponse active aux questions via une approche d'apprentissage par renforcement qui apprend à reformuler les questions de manière à fournir les meilleures réponses possibles.
"We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient.We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!.The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.","[0, 1, 0, 0, 0, 0]",[],S1CChZ-CZ,Ask the Right Questions: Active Question Reformulation with Reinforcement Learning,décrit clairement comment les chercheurs ont conçu et formé activement deux modèles pour la reformulation des questions et la sélection des réponses pendant les épisodes de réponse aux questions
"Most deep latent factor models choose simple priors for simplicity, tractabilityor not knowing what prior to use.Recent studies show that the choice ofthe prior may have a profound effect on the expressiveness of the model,especially when its generative network has limited capacity.In this paper, we propose to learn a proper prior from data for adversarial autoencoders(AAEs).We introduce the notion of code generators to transform manually selectedsimple priors into ones that can better characterize the data distribution.Experimental results show that the proposed model can generate better image quality and learn better disentangled representations thanAAEs in both supervised and unsupervised settings.Lastly, we present itsability to do cross-domain translation in a  text-to-image synthesis task.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJSr0GZR-,Learning Priors for Adversarial Autoencoders,Apprentissage de prieurs pour les autoencodeurs adversariaux
"Most deep latent factor models choose simple priors for simplicity, tractabilityor not knowing what prior to use.Recent studies show that the choice ofthe prior may have a profound effect on the expressiveness of the model,especially when its generative network has limited capacity.In this paper, we propose to learn a proper prior from data for adversarial autoencoders(AAEs).We introduce the notion of code generators to transform manually selectedsimple priors into ones that can better characterize the data distribution.Experimental results show that the proposed model can generate better image quality and learn better disentangled representations thanAAEs in both supervised and unsupervised settings.Lastly, we present itsability to do cross-domain translation in a  text-to-image synthesis task.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJSr0GZR-,Learning Priors for Adversarial Autoencoders,Propose une extension simple des auto-encodeurs adversariaux pour la génération d'images conditionnelles.
"Most deep latent factor models choose simple priors for simplicity, tractabilityor not knowing what prior to use.Recent studies show that the choice ofthe prior may have a profound effect on the expressiveness of the model,especially when its generative network has limited capacity.In this paper, we propose to learn a proper prior from data for adversarial autoencoders(AAEs).We introduce the notion of code generators to transform manually selectedsimple priors into ones that can better characterize the data distribution.Experimental results show that the proposed model can generate better image quality and learn better disentangled representations thanAAEs in both supervised and unsupervised settings.Lastly, we present itsability to do cross-domain translation in a  text-to-image synthesis task.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJSr0GZR-,Learning Priors for Adversarial Autoencoders,"Se concentre sur les auto-codeurs adversaires et introduit un réseau de générateurs de codes pour transformer une antériorité simple en une antériorité qui, avec le générateur, peut mieux s'adapter à la distribution des données."
"In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs).GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer.In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data.Attempts have been made for utilizing GANs with word embeddings for text generation.This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures.The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.","[0, 0, 0, 0, 0, 1]",[],SkGMOi05FQ, Generating Text through Adversarial Training using Skip-Thought Vectors,Génération de texte à l'aide d'enchâssements de phrases à partir de vecteurs Skip-Thought avec l'aide de réseaux adversariaux génératifs.
"In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs).GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer.In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data.Attempts have been made for utilizing GANs with word embeddings for text generation.This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures.The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.","[0, 0, 0, 0, 0, 1]",[],SkGMOi05FQ, Generating Text through Adversarial Training using Skip-Thought Vectors,Décrit l'application des réseaux adversariens génératifs pour la modélisation des données textuelles à l'aide de vecteurs de pensées et d'expériences avec différentes saveurs de GAN pour deux ensembles de données différents.
"The novel \emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models.It works in a streaming fashion and avoids backtracking through past activations and inputs.UORO is computationally as costly as \emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \cite{jaeger2002tutorial}.  UORO is a modification of \emph{NoBackTrack} \cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed.On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.","[1, 0, 0, 0, 0, 0, 0]",[],rJQDjk-0b,Unbiased Online Recurrent Optimization,"Présente une estimation du gradient en ligne, sans biais et facile à mettre en œuvre pour les modèles récurrents."
"The novel \emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models.It works in a streaming fashion and avoids backtracking through past activations and inputs.UORO is computationally as costly as \emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \cite{jaeger2002tutorial}.  UORO is a modification of \emph{NoBackTrack} \cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed.On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.","[1, 0, 0, 0, 0, 0, 0]",[],rJQDjk-0b,Unbiased Online Recurrent Optimization,Les auteurs présentent une nouvelle approche de l'apprentissage en ligne des paramètres des réseaux neuronaux récurrents à partir de longues séquences qui surmonte l'imitation de la rétropropagation tronquée dans le temps.
"The novel \emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models.It works in a streaming fashion and avoids backtracking through past activations and inputs.UORO is computationally as costly as \emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \cite{jaeger2002tutorial}.  UORO is a modification of \emph{NoBackTrack} \cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed.On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.","[1, 0, 0, 0, 0, 0, 0]",[],rJQDjk-0b,Unbiased Online Recurrent Optimization,"Cet article aborde l'apprentissage en ligne des RNN de manière raisonnée, et propose une modification de RTRL et l'utilisation d'une approche prospective pour le calcul du gradient."
"We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels.This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs.This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available.Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution.We compare our algorithm with models that are trained on high-resolution data and show that1) we can achieve similar performance using only low-resolution data; and2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training.We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkxwShA9Ym,Label super-resolution networks,"Super résolution des étiquettes grossières en étiquettes au niveau du pixel, appliquée à l'imagerie aérienne et aux scans médicaux."
"We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels.This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs.This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available.Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution.We compare our algorithm with models that are trained on high-resolution data and show that1) we can achieve similar performance using only low-resolution data; and2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training.We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkxwShA9Ym,Label super-resolution networks,Procédé de superrésolution d'étiquettes de segmentation grossières à basse résolution si la distribution conjointe des étiquettes à basse et haute résolution est connue.
"We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions.Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems.Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process.Importantly, we do not assume any individual correspondence (partial or complete) between data points.Instead, we rely on our assumption that a subset of data features have correspondence across datasets.We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets.We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features.We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets.Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together.We show that this approach successfully corrects misalignment artifacts, and allows for integrated data.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkGNrnC9FQ,Manifold Alignment via Feature Correspondence,Nous proposons une méthode pour aligner les caractéristiques latentes apprises à partir de différents ensembles de données en utilisant les corrélations harmoniques.
"We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions.Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems.Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process.Importantly, we do not assume any individual correspondence (partial or complete) between data points.Instead, we rely on our assumption that a subset of data features have correspondence across datasets.We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets.We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features.We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets.Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together.We show that this approach successfully corrects misalignment artifacts, and allows for integrated data.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkGNrnC9FQ,Manifold Alignment via Feature Correspondence,Propose d'utiliser les correspondances des caractéristiques pour préformer l'alignement des collecteurs entre les lots de données provenant des mêmes échantillons afin d'éviter la collecte de mesures bruitées.
"Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments.When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment.However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations.Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure.Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance.We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena.Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. A video is available at: www.youtube.com/watch?v=eei6Rgom3YY","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJgWl3A5YX,The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution,L'évolution de la forme du corps des agents contrôlés par RL améliore leurs performances (et favorise l'apprentissage).
"Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments.When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment.However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations.Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure.Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance.We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena.Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. A video is available at: www.youtube.com/watch?v=eei6Rgom3YY","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJgWl3A5YX,The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution,Algorithme PEOM qui incorpore la valeur de Shapley pour accélérer l'évolution en identifiant la contribution de chaque partie du corps.
"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never.Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy.In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe.This score acts as a penalty on the Q-learning objective.Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\epsilon$-close average return under weaker assumptions.Our analysis also shows robustness to classification errors.Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,Associer la récompense à la motivation intrinsèque pour éviter les états catastrophiques et atténuer l'oubli catastrophique.
"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never.Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy.In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe.This score acts as a penalty on the Q-learning objective.Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\epsilon$-close average return under weaker assumptions.Our analysis also shows robustness to classification errors.Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,Un algorithme RL qui combine l'algorithme DQN avec un modèle de peur entraîné en parallèle pour prédire les états catastrophiques.
"Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never.Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy.In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe.This score acts as a penalty on the Q-learning objective.Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\epsilon$-close average return under weaker assumptions.Our analysis also shows robustness to classification errors.Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B16yEqkCZ,Avoiding Catastrophic States with Intrinsic Fear,"L'article étudie l'oubli catastrophique en RL, en mettant l'accent sur les tâches où un DQN est capable d'apprendre à éviter les événements catastrophiques tant qu'il évite l'oubli."
"Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks.Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges.In this work, we propose a novel `""volumetric convolution"" operation that can effectively convolve arbitrary functions in B^3.We develop a theoretical framework for ""volumetric convolution"" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks.Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks.We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.","[0, 1, 0, 0, 0, 0]",[],SkfhIo0qtQ,Volumetric Convolution: Automatic Representation Learning in Unit Ball,Un nouvel opérateur de convolution pour l'apprentissage automatique de la représentation à l'intérieur de la boule unitaire
"Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks.Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges.In this work, we propose a novel `""volumetric convolution"" operation that can effectively convolve arbitrary functions in B^3.We develop a theoretical framework for ""volumetric convolution"" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks.Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks.We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.","[0, 1, 0, 0, 0, 0]",[],SkfhIo0qtQ,Volumetric Convolution: Automatic Representation Learning in Unit Ball,Ce travail est lié aux récents articles sur les réseaux sphériques CNN et SE(n) équivariants et étend les idées précédentes aux données volumétriques dans la boule unitaire.
"Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks.Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges.In this work, we propose a novel `""volumetric convolution"" operation that can effectively convolve arbitrary functions in B^3.We develop a theoretical framework for ""volumetric convolution"" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks.Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks.We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.","[0, 1, 0, 0, 0, 0]",[],SkfhIo0qtQ,Volumetric Convolution: Automatic Representation Learning in Unit Ball,Propose d'utiliser des convolutions volumétriques sur des réseaux de convolutions afin d'apprendre la balle unitaire et discute de la méthodologie et des résultats du processus.
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent’s learning through trial-and-error.For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large.Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions.We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from.Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions.In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively.We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions.We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions.The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],BJemQ209FQ,Learning to Navigate the Web,"Nous formons des politiques d'apprentissage par renforcement en utilisant l'augmentation de la récompense, l'apprentissage par programme et le méta-apprentissage pour naviguer avec succès dans les pages Web."
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent’s learning through trial-and-error.For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large.Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions.We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from.Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions.In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively.We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions.We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions.The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],BJemQ209FQ,Learning to Navigate the Web,"Développe une méthode d'apprentissage par curriculum pour former un agent RL à la navigation sur le web, basée sur l'idée de décomposer une instruction en de multiples sous-instructions."
"Labeled text classification datasets are typically only available in a few select languages.In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options.The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder.The encoder will give almost identical representations to multilingual versions of the same text.This means that labeled data in one language can be used to train a classifier that works for the rest of the languages.The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.","[0, 0, 1, 0, 0, 0, 0]",[],S1XXq6lRW,Zero-shot Cross Language Text Classification,Classification interlinguistique des textes par codage universel
"Labeled text classification datasets are typically only available in a few select languages.In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options.The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder.The encoder will give almost identical representations to multilingual versions of the same text.This means that labeled data in one language can be used to train a classifier that works for the rest of the languages.The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.","[0, 0, 1, 0, 0, 0, 0]",[],S1XXq6lRW,Zero-shot Cross Language Text Classification,Cet article propose une approche de la classification de textes multilingues par l'utilisation de corpus comparables.
"Labeled text classification datasets are typically only available in a few select languages.In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options.The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder.The encoder will give almost identical representations to multilingual versions of the same text.This means that labeled data in one language can be used to train a classifier that works for the rest of the languages.The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available.","[0, 0, 1, 0, 0, 0, 0]",[],S1XXq6lRW,Zero-shot Cross Language Text Classification,Apprentissage d'enchâssements interlinguistiques et apprentissage d'un classificateur utilisant des données étiquetées dans la langue source pour aborder l'apprentissage d'un catégoriseur de texte interlinguistique sans informations étiquetées dans la langue cible.
"Syntax is a powerful abstraction for language understanding.Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs).Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text.To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains.Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks.Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method.DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJeq43AqF7,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ,"Dans ce travail, nous proposons des auto-encodeurs récursifs intérieurs-extérieurs profonds (DIORA), une méthode entièrement non supervisée pour découvrir la syntaxe tout en apprenant simultanément des représentations pour les constituants découverts. "
"Syntax is a powerful abstraction for language understanding.Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs).Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text.To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains.Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks.Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method.DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJeq43AqF7,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ,Un modèle neuronal d'arbre latent entraîné avec un objectif d'auto-encodage qui atteint l'état de l'art sur l'analyse syntaxique non supervisée des constituants et capture la structure syntaxique mieux que d'autres modèles d'arbre latent.
"Syntax is a powerful abstraction for language understanding.Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs).Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text.To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains.Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks.Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method.DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HJeq43AqF7,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders ,L'article propose un modèle pour l'analyse syntaxique non supervisée des dépendances (induction d'arbres latents) qui est basé sur une combinaison de l'algorithme inside-outside avec la modélisation neuronale (auto-encodeurs récursifs). 
"Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training.There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled.But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training.We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias.We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons.We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area.We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.","[0, 0, 0, 0, 0, 0, 1]",[],H1MczcgR-,Understanding Short-Horizon Bias in Stochastic Meta-Optimization,Nous étudions le biais de l'objectif de méta-optimisation à court terme.
"Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training.There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled.But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training.We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias.We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons.We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area.We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.","[0, 0, 0, 0, 0, 0, 1]",[],H1MczcgR-,Understanding Short-Horizon Bias in Stochastic Meta-Optimization,Cet article propose un modèle et un problème simplifiés pour démontrer le biais à court terme de la méta-optimisation du taux d'apprentissage.
"Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training.There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled.But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training.We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias.We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons.We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area.We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.","[0, 0, 0, 0, 0, 0, 1]",[],H1MczcgR-,Understanding Short-Horizon Bias in Stochastic Meta-Optimization,Cet article étudie la question de la rétropropagation tronquée pour la méta-optimisation par le biais d'un certain nombre d'expériences sur un problème fictif.
"Mainstream captioning models often follow a sequential structure to generate cap-tions, leading to issues such as introduction of irrelevant semantics, lack of diversityin the generated captions, and inadequate generalization performance.In this paper,we present an alternative paradigm for image captioning, which factorizes thecaptioning procedure into two stages: (1) extracting an explicit semantic represen-tation from the given image; and (2) constructing the caption based on a recursivecompositional procedure in a bottom-up manner.Compared to conventional ones,our paradigm better preserves the semantic content through an explicit factorizationof semantics and syntax.By using the compositional generation procedure, captionconstruction follows a recursive structure, which naturally fits the properties ofhuman language.Moreover, the proposed compositional procedure requires lessdata to train, generalizes better, and yields more diverse captions.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],SJxyZ81IYQ,A Neural Compositional Paradigm for Image Captioning,une manière hiérarchique et compositionnelle de générer des légendes
"Mainstream captioning models often follow a sequential structure to generate cap-tions, leading to issues such as introduction of irrelevant semantics, lack of diversityin the generated captions, and inadequate generalization performance.In this paper,we present an alternative paradigm for image captioning, which factorizes thecaptioning procedure into two stages: (1) extracting an explicit semantic represen-tation from the given image; and (2) constructing the caption based on a recursivecompositional procedure in a bottom-up manner.Compared to conventional ones,our paradigm better preserves the semantic content through an explicit factorizationof semantics and syntax.By using the compositional generation procedure, captionconstruction follows a recursive structure, which naturally fits the properties ofhuman language.Moreover, the proposed compositional procedure requires lessdata to train, generalizes better, and yields more diverse captions.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],SJxyZ81IYQ,A Neural Compositional Paradigm for Image Captioning,Cet article présente une méthode plus interprétable pour le sous-titrage des images.
"While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data.Measures for characterizing and monitoring structural properties, however, have not been developed.In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs.To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization.Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.","[0, 0, 1, 0, 0]",[],ByxkijC5FQ,Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology,Nous développons une nouvelle mesure de complexité topologique pour les réseaux neuronaux profonds et démontrons qu'elle capture leurs propriétés saillantes.
"While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data.Measures for characterizing and monitoring structural properties, however, have not been developed.In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs.To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization.Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.","[0, 0, 1, 0, 0]",[],ByxkijC5FQ,Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology,"Cet article propose la notion de persistance neuronale, une mesure topologique permettant d'attribuer des scores aux couches entièrement connectées d'un réseau neuronal."
"While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data.Measures for characterizing and monitoring structural properties, however, have not been developed.In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs.To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization.Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.","[0, 0, 1, 0, 0]",[],ByxkijC5FQ,Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology,L'article propose d'analyser la complexité d'un réseau neuronal en utilisant son homologie persistante zéro-ième.
"Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs.Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models.However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls.In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models.First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.These examples successfully evade a defense that only considers a small ball around an input instance.Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes.We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples.Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples.Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BkpiPMbA-,Decision Boundary Analysis of Adversarial Examples,L'examen des frontières de décision autour d'une entrée vous donne plus d'informations qu'un petit voisinage fixe.
"Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs.Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models.However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls.In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models.First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.These examples successfully evade a defense that only considers a small ball around an input instance.Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes.We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples.Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples.Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BkpiPMbA-,Decision Boundary Analysis of Adversarial Examples,Les auteurs présentent une nouvelle attaque pour générer des exemples contradictoires. Ils attaquent les classificateurs créés par la classification aléatoire de petites perturbations L2.
"Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs.Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models.However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls.In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models.First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.These examples successfully evade a defense that only considers a small ball around an input instance.Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes.We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples.Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples.Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BkpiPMbA-,Decision Boundary Analysis of Adversarial Examples,"Une nouvelle approche pour générer des attaques adverses contre un réseau neuronal, et une méthode pour défendre un réseau neuronal contre ces attaques."
Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.,"[0, 0, 1, 0, 0]",[],SJIA6ZWC-,Stochastic Hyperparameter Optimization through Hypernetworks,Nous entraînons un réseau neuronal à produire des poids approximativement optimaux en fonction des hyperparamètres.
Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.,"[0, 0, 1, 0, 0]",[],SJIA6ZWC-,Stochastic Hyperparameter Optimization through Hypernetworks,Hyper-réseaux pour l'optimisation des hyper-paramètres dans les réseaux neuronaux.
"Estimating covariances between financial assets plays an important role in risk management.In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.Finally, we discuss some financial applications of the GP-LVM model.","[0, 0, 1, 0, 0, 0]",[],ryEquiR9KX,Applications of Gaussian Processes in Finance,Estimation de la matrice de covariance des actifs financiers avec des modèles à variables latentes à processus gaussien
"Estimating covariances between financial assets plays an important role in risk management.In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.Finally, we discuss some financial applications of the GP-LVM model.","[0, 0, 1, 0, 0, 0]",[],ryEquiR9KX,Applications of Gaussian Processes in Finance,Illustre comment le modèle de processus gaussien à variables latentes (GP-LVM) peut remplacer les modèles factoriels linéaires classiques pour l'estimation des matrices de covariance dans les problèmes d'optimisation de portefeuille.
"Estimating covariances between financial assets plays an important role in risk management.In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.Finally, we discuss some financial applications of the GP-LVM model.","[0, 0, 1, 0, 0, 0]",[],ryEquiR9KX,Applications of Gaussian Processes in Finance,"Cet article utilise des GPLVM standard pour modéliser la structure de covariance et une représentation d'espace latent des séries chronologiques financières du S&P500, afin d'optimiser les portefeuilles et de prédire les valeurs manquantes."
"Estimating covariances between financial assets plays an important role in risk management.In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.Finally, we discuss some financial applications of the GP-LVM model.","[0, 0, 1, 0, 0, 0]",[],ryEquiR9KX,Applications of Gaussian Processes in Finance,Ce document propose d'utiliser un GPLVM pour modéliser les rendements financiers.
"We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution.In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator.As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables.In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training.We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.","[0, 0, 1, 0, 0]",[],rkeZRGbRW,Variance Regularizing Adversarial Learning,"Nous introduisons l'apprentissage méta-adversarial, une nouvelle technique pour régulariser les GAN, et proposons une méthode d'apprentissage en contrôlant explicitement la distribution de la sortie du discriminateur."
"We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution.In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator.As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables.In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training.We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients.","[0, 0, 1, 0, 0]",[],rkeZRGbRW,Variance Regularizing Adversarial Learning,L'article propose un apprentissage contradictoire de régularisation de la variance pour l'entraînement des GAN afin de garantir que le gradient du générateur ne disparaisse pas.
"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration.The parameters of the noise are learned with gradient descent along with the remaining network weights.  NoisyNet is straightforward to implement and adds little computational overhead.We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.","[1, 0, 0, 0]",[],rywHCPkAW,Noisy Networks For Exploration,Un agent d'apprentissage par renforcement profond avec un bruit paramétrique ajouté à ses poids peut être utilisé pour faciliter une exploration efficace.
"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration.The parameters of the noise are learned with gradient descent along with the remaining network weights.  NoisyNet is straightforward to implement and adds little computational overhead.We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.","[1, 0, 0, 0]",[],rywHCPkAW,Noisy Networks For Exploration,"Cet article présente les NoisyNets, des réseaux neuronaux dont les paramètres sont perturbés par une fonction de bruit paramétrique, qui obtiennent une amélioration substantielle des performances par rapport aux algorithmes d'apprentissage par renforcement profond de base."
"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration.The parameters of the noise are learned with gradient descent along with the remaining network weights.  NoisyNet is straightforward to implement and adds little computational overhead.We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.","[1, 0, 0, 0]",[],rywHCPkAW,Noisy Networks For Exploration,"Nouvelle méthode d'exploration pour les RL profonds en injectant du bruit dans les poids des réseaux profonds, le bruit pouvant prendre différentes formes."
"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment.Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent.We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently.The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization.Active Neural Localizer is trained end-to-end with reinforcement learning.We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine.The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations.We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ry6-G_66b,Active Neural Localization,"""Active Neural Localizer"", un réseau neuronal entièrement différentiable qui apprend à localiser efficacement en utilisant l'apprentissage par renforcement profond."
"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment.Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent.We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently.The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization.Active Neural Localizer is trained end-to-end with reinforcement learning.We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine.The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations.We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ry6-G_66b,Active Neural Localization,Cet article formule le problème de la localisation sur une carte connue à l'aide d'un réseau de croyances comme un problème de RL où le but de l'agent est de minimiser le nombre d'étapes pour se localiser.
"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment.Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent.We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize efficiently.The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization.Active Neural Localizer is trained end-to-end with reinforcement learning.We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine.The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations.We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ry6-G_66b,Active Neural Localization,Il s'agit d'un article clair et intéressant qui construit un réseau paramétré pour sélectionner les actions d'un robot dans un environnement simulé.
"Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy.Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency.Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time.However, they could only achieve inferior accuracy compared with ART models.To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models.Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks.In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines.It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],r1gGpjActQ,Hint-based Training for Non-Autoregressive Translation,"Nous développons un algorithme de formation pour les modèles de traduction automatique non autorégressifs, qui permet d'obtenir une précision comparable à celle des modèles de base fortement autorégressifs, mais qui est plus rapide d'un ordre de grandeur dans l'inférence.  "
"Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy.Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency.Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time.However, they could only achieve inferior accuracy compared with ART models.To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models.Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks.In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines.It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],r1gGpjActQ,Hint-based Training for Non-Autoregressive Translation,Distille des connaissances à partir des états cachés intermédiaires et des poids d'attention pour améliorer la traduction automatique neuronale non autorégressive.
"Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy.Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency.Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time.However, they could only achieve inferior accuracy compared with ART models.To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models.Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks.In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines.It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],r1gGpjActQ,Hint-based Training for Non-Autoregressive Translation,Propose de tirer parti d'un modèle autorégressif bien entraîné pour informer les états cachés et l'alignement des mots des modèles non autorégressifs de traduction automatique neuronale.
"Artificial neural networks are built on the basic operation of linear combination and non-linear activation function.Theoretically this structure can approximate any continuous function with three layer architecture.But in practice learning  the parameters of such network can be hard.Also the choice of activation function can greatly impact the performance of the network.In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function.To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons.We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks.The results show that our network perform favorably when compared with similar structured network.We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SyxknjC9KQ,Dense Morphological Network: An Universal Function Approximator,"En utilisant des opérations morphologiques (dilatation et érosion), nous avons défini une classe de réseau qui peut approximer toute fonction continue. "
"Artificial neural networks are built on the basic operation of linear combination and non-linear activation function.Theoretically this structure can approximate any continuous function with three layer architecture.But in practice learning  the parameters of such network can be hard.Also the choice of activation function can greatly impact the performance of the network.In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function.To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons.We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks.The results show that our network perform favorably when compared with similar structured network.We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SyxknjC9KQ,Dense Morphological Network: An Universal Function Approximator,"Cet article propose de remplacer les unités standard RELU/tanh par une combinaison d'opérations de dilatation et d'érosion, en observant que le nouvel opérateur crée plus d'hyperplans et a plus de pouvoir expressif."
"Artificial neural networks are built on the basic operation of linear combination and non-linear activation function.Theoretically this structure can approximate any continuous function with three layer architecture.But in practice learning  the parameters of such network can be hard.Also the choice of activation function can greatly impact the performance of the network.In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function.To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons.We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks.The results show that our network perform favorably when compared with similar structured network.We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],SyxknjC9KQ,Dense Morphological Network: An Universal Function Approximator,"Les auteurs présentent Morph-Net, un réseau neuronal à une seule couche où la mise en correspondance est effectuée à l'aide de la dilatation et de l'érosion morphologiques."
"With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment.This work aims to advance the compression beyond the weights to the activations of DNNs.We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency.The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets.With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.","[0, 1, 0, 0, 0, 0]",[],HyevnsCqtQ,Integral Pruning on Activations and Weights for Efficient Neural Networks,Ce travail fait progresser la compression des DNN au-delà des poids vers les activations en intégrant l'élagage des activations avec l'élagage des poids. 
"With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment.This work aims to advance the compression beyond the weights to the activations of DNNs.We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency.The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets.With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.","[0, 1, 0, 0, 0, 0]",[],HyevnsCqtQ,Integral Pruning on Activations and Weights for Efficient Neural Networks,"Une méthode de compression intégrale du modèle qui gère à la fois l'élagage des poids et des activations, ce qui permet un calcul plus efficace du réseau et une réduction effective du nombre de multiplications et d'accumulations."
"With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment.This work aims to advance the compression beyond the weights to the activations of DNNs.We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency.The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets.With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively.","[0, 1, 0, 0, 0, 0]",[],HyevnsCqtQ,Integral Pruning on Activations and Weights for Efficient Neural Networks,Cet article présente une nouvelle approche pour réduire le coût de calcul des réseaux neuronaux profonds en intégrant l'élagage des activations avec l'élagage des poids et montre que les techniques courantes d'élagage exclusif des poids augmentent le nombre d'activations non nulles après ReLU.
"The Variational Auto Encoder (VAE) is a popular generative latent variable model that is often applied for representation learning.Standard VAEs assume continuous valued latent variables and are trained by maximizationof the evidence lower bound (ELBO).Conventional methods obtain a differentiable estimate of the ELBO with reparametrized sampling andoptimize it with Stochastic Gradient Descend (SGD).However, this is not possible if we want to train VAEs with discrete valued latent variables, since reparametrized sampling is not possible.Till now, thereexist no simple solutions to circumvent this problem.In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations.Therefore, we use a differentiableestimator for the ELBO which is based on importance sampling.In experiments, we verify the approach andtrain two different VAEs architectures with Bernoulli and Categorically distributed latent representations on two different benchmarkdatasets.	","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkNSOjR9Y7,Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling,"Nous proposons une méthode simple pour entraîner les encodeurs automatiques variationnels (VAE) avec des représentations latentes discrètes, en utilisant l'échantillonnage par importance."
"The Variational Auto Encoder (VAE) is a popular generative latent variable model that is often applied for representation learning.Standard VAEs assume continuous valued latent variables and are trained by maximizationof the evidence lower bound (ELBO).Conventional methods obtain a differentiable estimate of the ELBO with reparametrized sampling andoptimize it with Stochastic Gradient Descend (SGD).However, this is not possible if we want to train VAEs with discrete valued latent variables, since reparametrized sampling is not possible.Till now, thereexist no simple solutions to circumvent this problem.In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations.Therefore, we use a differentiableestimator for the ELBO which is based on importance sampling.In experiments, we verify the approach andtrain two different VAEs architectures with Bernoulli and Categorically distributed latent representations on two different benchmarkdatasets.	","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkNSOjR9Y7,Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling,Introduction d'une distribution d'échantillonnage par importance et utilisation d'échantillons de la distribution pour calculer l'estimation du gradient pondéré par importance
"The Variational Auto Encoder (VAE) is a popular generative latent variable model that is often applied for representation learning.Standard VAEs assume continuous valued latent variables and are trained by maximizationof the evidence lower bound (ELBO).Conventional methods obtain a differentiable estimate of the ELBO with reparametrized sampling andoptimize it with Stochastic Gradient Descend (SGD).However, this is not possible if we want to train VAEs with discrete valued latent variables, since reparametrized sampling is not possible.Till now, thereexist no simple solutions to circumvent this problem.In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations.Therefore, we use a differentiableestimator for the ELBO which is based on importance sampling.In experiments, we verify the approach andtrain two different VAEs architectures with Bernoulli and Categorically distributed latent representations on two different benchmarkdatasets.	","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkNSOjR9Y7,Training Variational Auto Encoders with Discrete Latent Representations using Importance Sampling,Cet article propose d'utiliser l'échantillonnage important pour optimiser le VAE avec des variables latentes discrètes.
"Distributed computing can significantly reduce the training time of neural networks.Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers.In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.    We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead.DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule.For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.","[0, 0, 0, 0, 0, 1, 0]",[],SkGQujR5FX,DANA: Scalable Out-of-the-box Distributed ASGD Without Retuning,Un nouvel algorithme SGD asynchrone distribué qui atteint une précision de pointe sur les architectures existantes sans réglage ou surcharge supplémentaire.
"Distributed computing can significantly reduce the training time of neural networks.Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers.In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.    We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead.DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule.For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.","[0, 0, 0, 0, 0, 1, 0]",[],SkGQujR5FX,DANA: Scalable Out-of-the-box Distributed ASGD Without Retuning,Propose une amélioration des approches ASGD existantes à l'échelle moyenne en utilisant le momentum avec SGD pour la formation asynchrone à travers un pool de travailleurs distribués.
"Distributed computing can significantly reduce the training time of neural networks.Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers.In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.    We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead.DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule.For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline.","[0, 0, 0, 0, 0, 1, 0]",[],SkGQujR5FX,DANA: Scalable Out-of-the-box Distributed ASGD Without Retuning,"Cet article aborde le problème de la stagnation du gradient par rapport aux performances parallèles dans l'apprentissage profond distribué, et propose une approche pour estimer les futurs paramètres du modèle aux esclaves afin de réduire les effets de latence de communication."
"This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes.We propose a cascaded learning strategy for these local networks.In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement.Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.","[1, 0, 0, 0, 0]",[],B1x-LjAcKX,Local Critic Training of Deep Neural Networks,"Nous proposons un nouvel algorithme d'apprentissage des réseaux neuronaux profonds, qui débloque la dépendance des couches de la rétropropagation."
"This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes.We propose a cascaded learning strategy for these local networks.In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement.Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.","[1, 0, 0, 0, 0]",[],B1x-LjAcKX,Local Critic Training of Deep Neural Networks,"Un paradigme de formation alternatif pour les DNI dans lequel le module auxiliaire est formé pour se rapprocher directement de la sortie finale du modèle original, offrant des avantages secondaires."
"This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes.We propose a cascaded learning strategy for these local networks.In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement.Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters.","[1, 0, 0, 0, 0]",[],B1x-LjAcKX,Local Critic Training of Deep Neural Networks,Décrit une méthode d'entraînement de réseaux neuronaux sans verrouillage des mises à jour.
"\emph{Truncated Backpropagation Through Time} (truncated BPTT, \cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs.Truncated BPTT keeps the computational benefits of \emph{Backpropagation Through Time} (BPTT \cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory.We introduce \emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation.We check the viability of ARTBP on two tasks.First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably.Second, on Penn Treebank character-level language modelling \cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkrWCJWAW,Unbiasing Truncated Backpropagation Through Time,Fournit une version non biaisée de la rétropropagation tronquée en échantillonnant les longueurs de troncature et en les repondérant en conséquence.
"\emph{Truncated Backpropagation Through Time} (truncated BPTT, \cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs.Truncated BPTT keeps the computational benefits of \emph{Backpropagation Through Time} (BPTT \cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory.We introduce \emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation.We check the viability of ARTBP on two tasks.First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably.Second, on Penn Treebank character-level language modelling \cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkrWCJWAW,Unbiasing Truncated Backpropagation Through Time,Propose des méthodes de détermination stochastique des points de troncature dans la rétro-propagation dans le temps.
"\emph{Truncated Backpropagation Through Time} (truncated BPTT, \cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs.Truncated BPTT keeps the computational benefits of \emph{Backpropagation Through Time} (BPTT \cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory.We introduce \emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation.We check the viability of ARTBP on two tasks.First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably.Second, on Penn Treebank character-level language modelling \cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rkrWCJWAW,Unbiasing Truncated Backpropagation Through Time,Une nouvelle approximation de la rétropropagation dans le temps pour surmonter les charges de calcul et de mémoire qui surviennent lorsqu'on doit apprendre à partir de longues séquences.
"Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes.However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks.In this paper, we investigate methods attacking GCNs by adding fake nodes.A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones.In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.  Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node.","[0, 0, 0, 1, 0, 0, 0]",[],rke8ZhCcFQ,ATTACK GRAPH CONVOLUTIONAL NETWORKS BY ADDING FAKE NODES,attaque non ciblée et ciblée sur GCN en ajoutant de faux nœuds
"Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes.However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks.In this paper, we investigate methods attacking GCNs by adding fake nodes.A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones.In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.  Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node.","[0, 0, 0, 1, 0, 0, 0]",[],rke8ZhCcFQ,ATTACK GRAPH CONVOLUTIONAL NETWORKS BY ADDING FAKE NODES,"Les auteurs proposent une nouvelle technique contradictoire permettant d'ajouter de ""faux"" nœuds pour tromper un classificateur basé sur le GCN."
"Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain.Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer.RNN uses a chain of repeating cells to model the sequence data.However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling.Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer.ART is in a recurrent manner that different cells share the same parameters.Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain.This strategy enables ART to capture the word collocation across domains in a more flexible way.We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis).ART outperforms the state-of-the-arts over all experiments.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],ByldlhAqYQ,Transfer Learning for Sequences via Learning to Collocate,Apprentissage par transfert pour les séquences via l'apprentissage de l'alignement des informations au niveau cellulaire entre les domaines.
"Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain.Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer.RNN uses a chain of repeating cells to model the sequence data.However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling.Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer.ART is in a recurrent manner that different cells share the same parameters.Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain.This strategy enables ART to capture the word collocation across domains in a more flexible way.We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis).ART outperforms the state-of-the-arts over all experiments.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],ByldlhAqYQ,Transfer Learning for Sequences via Learning to Collocate,L'article propose d'utiliser le RNN/LSTM avec alignement de collocation comme méthode d'apprentissage de la représentation pour l'apprentissage par transfert/adaptation de domaine en PNL.
"Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world.We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution.Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function.To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state.Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.","[0, 1, 0, 0, 0]",[],SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,Nous formulons l'incertitude du modèle dans l'apprentissage par renforcement sous la forme d'un processus de décision de Markov adaptatif de Bayes continu et présentons une méthode d'optimisation de politique bayésienne pratique et évolutive.
"Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world.We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution.Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function.To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state.Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.","[0, 1, 0, 0, 0]",[],SJGvns0qK7,Bayesian Policy Optimization for Model Uncertainty,L'utilisation d'une approche bayésienne permet d'obtenir un meilleur compromis entre l'exploration et l'exploitation en RL.
"For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model.In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions.The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples.We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.","[0, 1, 0, 0, 0]",[],HkxKH2AcFm,Towards GAN Benchmarks Which Require Generalization,Nous soutenons que les benchmarks GAN doivent exiger un grand échantillon du modèle pour pénaliser la mémorisation et nous cherchons à savoir si les divergences des réseaux neuronaux ont cette propriété.
"For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model.In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions.The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples.We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.","[0, 1, 0, 0, 0]",[],HkxKH2AcFm,Towards GAN Benchmarks Which Require Generalization,Les auteurs proposent un critère d'évaluation de la qualité des échantillons produits par un Réseau Adversarial Génératif.
"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data.In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.The dialogue acts are generally designed and reveal how people engage in social chat.Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation.With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","[0, 1, 0, 0, 0]",[],Bym0cU1CZ,Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,génération de dialogue dans un domaine ouvert avec des actes de dialogue
"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data.In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.The dialogue acts are generally designed and reveal how people engage in social chat.Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation.With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","[0, 1, 0, 0, 0]",[],Bym0cU1CZ,Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,Les auteurs utilisent une technique de supervision à distance pour ajouter des balises d'actes de dialogue comme facteur de conditionnement pour générer des réponses dans des dialogues à domaine ouvert.
"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data.In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.The dialogue acts are generally designed and reveal how people engage in social chat.Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation.With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","[0, 1, 0, 0, 0]",[],Bym0cU1CZ,Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts,L'article décrit une technique permettant d'incorporer des actes de dialogue dans des agents conversationnels neuronaux.
"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics.Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed.Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping.This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique.The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping.These are verified extensively.In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1VjBebR-,The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,"Notre hypothèse est qu'étant donné deux domaines, la cartographie la moins complexe qui présente une faible divergence se rapproche de la cartographie cible."
"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics.Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed.Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping.This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique.The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping.These are verified extensively.In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1VjBebR-,The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,"L'article aborde le problème de l'apprentissage de mappings entre différents domaines sans aucune supervision, en énonçant trois conjectures."
"We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics.Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed.Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping.This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique.The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping.These are verified extensively.In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1VjBebR-,The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings,"Démontre qu'en apprentissage non supervisé sur des données non alignées, il est possible d'apprendre la correspondance entre domaines en utilisant uniquement le GAN sans perte de reconstruction."
We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming.This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.,"[0, 1]",[],HJgeEh09KQ,Boosting Robustness Certification of Neural Networks,Nous affinons les résultats de sur-approximation des vérificateurs incomplets en utilisant des solveurs MILP pour prouver des propriétés de robustesse supérieures à l'état de l'art. 
We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming.This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.,"[0, 1]",[],HJgeEh09KQ,Boosting Robustness Certification of Neural Networks,"Présente un vérificateur qui permet d'améliorer la précision des vérificateurs incomplets et l'évolutivité des vérificateurs complets en utilisant la surparamétrisation, la programmation linéaire en nombres entiers mixtes et la relaxation de la programmation linéaire."
We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming.This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.,"[0, 1]",[],HJgeEh09KQ,Boosting Robustness Certification of Neural Networks,"Une stratégie mixte pour obtenir une meilleure précision sur les vérifications de la robustesse des réseaux neuronaux feed-forward avec des fonctions d'activation linéaires par morceaux, en obtenant une meilleure précision que les vérificateurs incomplets et plus d'évolutivité que les vérificateurs complets."
"A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data.In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)).Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs?In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization.In particular, we investigate three key design factors—independence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearities—in order to pin down their empirical effects.We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction.","[0, 0, 0, 1, 0, 0]",[],rJxEso0osm,Bridging HMMs and RNNs through Architectural Transformations,"Les HMMs sont-ils un cas particulier des RNNs ? Nous étudions une série de transformations architecturales entre les HMM et les RNN, à la fois par des dérivations théoriques et par une hybridation empirique, et nous fournissons de nouvelles perspectives."
"A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data.In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)).Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs?In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization.In particular, we investigate three key design factors—independence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearities—in order to pin down their empirical effects.We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction.","[0, 0, 0, 1, 0, 0]",[],rJxEso0osm,Bridging HMMs and RNNs through Architectural Transformations,Cet article examine si les HMM sont un cas particulier de RNN en utilisant la modélisation du langage et le balisage POS.
"Deep neural networks have been tremendously successful in a number of tasks.One of the main reasons for this is their capability to automaticallylearn representations of data in levels of abstraction,increasingly disentangling the data as the internal transformations are applied.In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ByzvHagA-,Disentangled activations in deep networks,Nous proposons une nouvelle méthode de régularisation qui pénalise la covariance entre les dimensions des couches cachées d'un réseau.
"Deep neural networks have been tremendously successful in a number of tasks.One of the main reasons for this is their capability to automaticallylearn representations of data in levels of abstraction,increasingly disentangling the data as the internal transformations are applied.In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ByzvHagA-,Disentangled activations in deep networks,Cet article présente un mécanisme de régularisation qui pénalise la covariance entre toutes les dimensions de la représentation latente d'un réseau neuronal afin de démêler la représentation latente.
"This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only.Classification of an input can be inferred from the maximum response of the filters.A multiple check with multiple versions of filters can diminish fluctuation and yields better performance.This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure.In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.","[0, 0, 0, 0, 0, 1]",[],r1gKNs0qYX,Filter Training and Maximum Response: Classification via Discerning,Le système proposé imite le processus de classification médiatisé par une série de prélèvements à une composante.
"This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only.Classification of an input can be inferred from the maximum response of the filters.A multiple check with multiple versions of filters can diminish fluctuation and yields better performance.This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure.In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.","[0, 0, 0, 0, 0, 1]",[],r1gKNs0qYX,Filter Training and Maximum Response: Classification via Discerning,"Une méthode pour augmenter la précision des réseaux profonds sur des tâches de classification multi-classes, apparemment par une réduction de la classification multi-classes en classification binaire."
"This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only.Classification of an input can be inferred from the maximum response of the filters.A multiple check with multiple versions of filters can diminish fluctuation and yields better performance.This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure.In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking.","[0, 0, 0, 0, 0, 1]",[],r1gKNs0qYX,Filter Training and Maximum Response: Classification via Discerning,"Une nouvelle procédure de classification basée sur le discernement, la réponse maximale et la vérification multiple pour améliorer la précision des réseaux médiocres et améliorer les réseaux à action directe."
"A long-held conventional wisdom states that larger models train more slowly when using gradient descent.This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step.In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal.Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.","[0, 0, 1, 0, 0]",[],S1lPShAqFm,Empirically Characterizing Overparameterization Impact on Convergence,"L'expérience montre que les modèles plus grands s'entraînent en moins d'étapes d'entraînement, car tous les facteurs de la traversée de l'espace de poids s'améliorent."
"A long-held conventional wisdom states that larger models train more slowly when using gradient descent.This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step.In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal.Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.","[0, 0, 1, 0, 0]",[],S1lPShAqFm,Empirically Characterizing Overparameterization Impact on Convergence,"Cet article montre que les RNN plus larges améliorent la vitesse de convergence lorsqu'ils sont appliqués à des problèmes de PNL et, par extension, l'effet de l'augmentation des largeurs dans les réseaux neuronaux profonds sur la convergence de l'optimisation."
"A long-held conventional wisdom states that larger models train more slowly when using gradient descent.This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step.In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal.Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.","[0, 0, 1, 0, 0]",[],S1lPShAqFm,Empirically Characterizing Overparameterization Impact on Convergence,"Cet article caractérise l'impact de la sur-paramétrisation sur le nombre d'itérations nécessaires à un algorithme pour converger, et présente d'autres observations empiriques sur les effets de la sur-paramétrisation dans la formation des réseaux neuronaux."
"Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research.Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes.In this work, we consider a recently identified class of bugs called variable-misuse bugs.The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction.We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.We present multi-headed pointer networks for this purpose, with one head each for localization and repair.The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.","[0, 0, 0, 0, 1, 0, 0]",[],ByloJ20qtm,Neural Program Repair by Jointly Learning to Localize and Repair,Réseaux de pointeurs à têtes multiples pour l'apprentissage conjoint afin de localiser et de réparer les bogues d'utilisation de la variable
"Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research.Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes.In this work, we consider a recently identified class of bugs called variable-misuse bugs.The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction.We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.We present multi-headed pointer networks for this purpose, with one head each for localization and repair.The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.","[0, 0, 0, 0, 1, 0, 0]",[],ByloJ20qtm,Neural Program Repair by Jointly Learning to Localize and Repair,Propose un modèle basé sur LSTM avec des pointeurs pour décomposer le problème de VarMisuse en plusieurs étapes.
"Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research.Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes.In this work, we consider a recently identified class of bugs called variable-misuse bugs.The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction.We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.We present multi-headed pointer networks for this purpose, with one head each for localization and repair.The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.","[0, 0, 0, 0, 1, 0, 0]",[],ByloJ20qtm,Neural Program Repair by Jointly Learning to Localize and Repair,"Cet article présente un modèle basé sur LSTM pour la détection et la réparation du bug VarMisuse, et démontre des améliorations significatives par rapport aux approches précédentes sur plusieurs ensembles de données."
"Classification and clustering have been studied separately in machine learning and computer vision.Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution.We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints.Instead, we emphasize on the compositionality of the real world structures and objects.In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms.The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance.This, by no means, suggests that other methods do not hold merits.For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).","[0, 0, 1, 0, 0, 0, 0, 0]",[],Skvin0GWM,Human-like Clustering with Deep Convolutional Neural Networks,Classification de type humain avec des CNN
"Classification and clustering have been studied separately in machine learning and computer vision.Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution.We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints.Instead, we emphasize on the compositionality of the real world structures and objects.In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms.The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance.This, by no means, suggests that other methods do not hold merits.For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).","[0, 0, 1, 0, 0, 0, 0, 0]",[],Skvin0GWM,Human-like Clustering with Deep Convolutional Neural Networks,L'article valide l'idée que les réseaux neuronaux convolutifs profonds pourraient apprendre à regrouper les données d'entrée mieux que d'autres méthodes de regroupement en notant leur capacité à interpréter le contexte de chaque point d'entrée grâce à un large champ de vision.
"Classification and clustering have been studied separately in machine learning and computer vision.Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution.We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints.Instead, we emphasize on the compositionality of the real world structures and objects.In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms.The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance.This, by no means, suggests that other methods do not hold merits.For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).","[0, 0, 1, 0, 0, 0, 0, 0]",[],Skvin0GWM,Human-like Clustering with Deep Convolutional Neural Networks,Ce travail combine l'apprentissage profond pour la représentation des caractéristiques avec la tâche de regroupement non supervisé de type humain.
"Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features.Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models.We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory.We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.","[0, 0, 0, 1, 0, 0, 0]",[],S1E3Ko09F7,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,"Nous développons deux algorithmes à complexité linéaire pour l'interprétation de modèles diagnostiques basés sur la valeur de Shapley, dans le cas où la contribution des caractéristiques à la cible est bien approchée par une factorisation structurée en graphes."
"Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features.Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models.We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory.We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.","[0, 0, 0, 1, 0, 0, 0]",[],S1E3Ko09F7,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,L'article propose deux approximations de la valeur de Shapley utilisée pour générer des scores de caractéristiques pour l'interprétabilité.
"Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features.Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models.We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory.We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.","[0, 0, 0, 1, 0, 0, 0]",[],S1E3Ko09F7,L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data,"Cet article propose deux méthodes de notation de l'importance des caractéristiques par instance en utilisant les valeurs de Shapely, et fournit deux méthodes efficaces de calcul des valeurs de Shapely approximatives lorsqu'il existe une structure connue des caractéristiques."
"According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations.This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct.However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks.In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data.Using a 1-hot output code drastically decreases the number of local codes on the hidden layer.The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks.This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex.The findings highlight how local codes should not be dismissed out of hand.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJXOfZ-AZ,When and where do feed-forward neural networks learn localist representations?,Des codes locaux ont été trouvés dans les réseaux neuronaux à action directe.
"According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations.This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct.However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks.In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data.Using a 1-hot output code drastically decreases the number of local codes on the hidden layer.The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks.This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex.The findings highlight how local codes should not be dismissed out of hand.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJXOfZ-AZ,When and where do feed-forward neural networks learn localist representations?,"Une méthode pour déterminer dans quelle mesure les neurones individuels dans une couche cachée d'un MLP codent un code localiste, qui est étudié pour différentes représentations d'entrée."
"According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations.This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct.However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks.In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data.Using a 1-hot output code drastically decreases the number of local codes on the hidden layer.The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks.This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex.The findings highlight how local codes should not be dismissed out of hand.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJXOfZ-AZ,When and where do feed-forward neural networks learn localist representations?,Étudie le développement de représentations localistes dans les couches cachées des réseaux neuronaux à action directe.
"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values.In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities.We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities.We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy.Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.","[0, 0, 1, 0, 0, 0]",[],By03VlJGG,Embedding Multimodal Relational Data,Extension de la modélisation relationnelle pour prendre en charge les données multimodales à l'aide d'encodeurs neuronaux.
"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values.In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities.We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities.We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy.Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.","[0, 0, 1, 0, 0, 0]",[],By03VlJGG,Embedding Multimodal Relational Data,"Cet article propose d'effectuer la prédiction de liens dans les bases de connaissances en complétant les entités originales par des informations multimodales, et présente un modèle capable d'encoder toutes sortes d'informations lors de la notation des triples."
"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values.In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities.We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities.We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy.Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes.","[0, 0, 1, 0, 0, 0]",[],By03VlJGG,Embedding Multimodal Relational Data,L'article porte sur l'intégration d'informations provenant de différentes modalités dans les approches de prédiction de liens.
"An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.For example, in LSTM based language modeling (LM), we obtain 21\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\% of model parameters and 70\% of computations in total, as compared to the baseline large LSTM LM.To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1uOhfb0W,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,"Proposition d'une nouvelle méthode intégrant l'échantillonnage SG-MCMC, l'antériorité de groupe clairsemé et l'élagage de réseau pour apprendre l'ensemble structuré clairsemé (SSE) avec des performances améliorées et un coût considérablement réduit par rapport aux méthodes traditionnelles. "
"An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.For example, in LSTM based language modeling (LM), we obtain 21\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\% of model parameters and 70\% of computations in total, as compared to the baseline large LSTM LM.To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1uOhfb0W,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,Les auteurs proposent une procédure permettant de générer un ensemble de modèles structurés épars.
"An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.For example, in LSTM based language modeling (LM), we obtain 21\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\% of model parameters and 70\% of computations in total, as compared to the baseline large LSTM LM.To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1uOhfb0W,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,"Un nouveau cadre pour la formation de réseaux neuronaux d'ensemble qui utilise les méthodes SG-MCMC dans le cadre de l'apprentissage profond, puis augmente l'efficacité du calcul par la sparsité+élagage de groupe."
"An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.For example, in LSTM based language modeling (LM), we obtain 21\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\% of model parameters and 70\% of computations in total, as compared to the baseline large LSTM LM.To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1uOhfb0W,Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning,Cet article explore l'utilisation de FNN et de LSTM pour rendre la moyenne des modèles bayésiens plus facilement calculable et améliorer la performance moyenne des modèles.
"This paper introduces a new framework for data efficient and versatile learning.Specifically:1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction.ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce \Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass.\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.3) We evaluate \Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time.The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.","[0, 0, 1, 0, 0, 0, 0, 0]",[],HkxStoC5F7,Meta-Learning Probabilistic Inference for Prediction,Nouveau cadre pour le méta-apprentissage qui unifie et étend une large classe de méthodes d'apprentissage few-shot existantes. Il permet d'obtenir d'excellentes performances dans les tests d'apprentissage en quelques points sans nécessiter d'inférence itérative au moment du test.   
"This paper introduces a new framework for data efficient and versatile learning.Specifically:1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction.ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce \Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass.\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.3) We evaluate \Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time.The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.","[0, 0, 1, 0, 0, 0, 0, 0]",[],HkxStoC5F7,Meta-Learning Probabilistic Inference for Prediction,"Ce travail s'attaque à l'apprentissage en quelques coups du point de vue de l'inférence probabiliste, en atteignant l'état de l'art malgré une configuration plus simple que celle de nombreux concurrents."
"In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions.However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results.This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state.To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives.Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS).We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.","[0, 0, 0, 0, 1, 0]",[],rkx0g3R5tX,Partially Mutual Exclusive Softmax for Positive and Unlabeled data,Définition d'une perte softmax partiellement mutuelle exclusive pour les données positives et mise en œuvre d'un schéma d'échantillonnage basé sur la coopération.
"In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions.However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results.This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state.To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives.Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS).We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.","[0, 0, 0, 0, 1, 0]",[],rkx0g3R5tX,Partially Mutual Exclusive Softmax for Positive and Unlabeled data,Cet article présente l'échantillonnage par importance coopératif afin de résoudre le problème de l'hypothèse mutuellement exclusive du softmax traditionnel qui est biaisé lorsque les échantillons négatifs ne sont pas explicitement définis.
"In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions.However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results.This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state.To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives.Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS).We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance.","[0, 0, 0, 0, 1, 0]",[],rkx0g3R5tX,Partially Mutual Exclusive Softmax for Positive and Unlabeled data,"Cet article propose des méthodes PMES pour assouplir l'hypothèse de résultat exclusif dans la perte softmax, démontrant ainsi le mérite empirique de l'amélioration des modèles d'intégration de type word2vec."
"Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention.Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos.In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video.The idea is to then train the student to minimize (i)  the difference between the final representation computed by the student and the teacher and/or(ii) the difference between the distributions predicted by the teacher and the student.This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification.We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],H1GWAoRcKX,A Teacher Student Network For Faster Video Classification,"Cadre ""enseignant-élève"" pour une classification vidéo efficace utilisant moins de trames "
"Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention.Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos.In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video.The idea is to then train the student to minimize (i)  the difference between the final representation computed by the student and the teacher and/or(ii) the difference between the distributions predicted by the teacher and the student.This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification.We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],H1GWAoRcKX,A Teacher Student Network For Faster Video Classification,L'article propose une idée pour distiller à partir d'un modèle de classification vidéo complet un petit modèle qui ne reçoit qu'un nombre réduit de trames.
"Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention.Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos.In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video.The idea is to then train the student to minimize (i)  the difference between the final representation computed by the student and the teacher and/or(ii) the difference between the distributions predicted by the teacher and the student.This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification.We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],H1GWAoRcKX,A Teacher Student Network For Faster Video Classification,"Les auteurs présentent un réseau professeur-étudiant pour résoudre le problème de la classification des vidéos, en proposant des algorithmes d'apprentissage en série et en parallèle visant à réduire les coûts de calcul."
"Deep generative models have achieved impressive success in recent years.Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively.This paper aims to establish formal connections between GANs and VAEs through a new formulation of them.We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively.The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way.For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples.Experiments show generality and effectiveness of the transfered techniques.","[0, 0, 0, 0, 0, 0, 1]",[],rylSzl-R-,On Unifying Deep Generative Models,Une vision statistique unifiée de la vaste classe des modèles génératifs profonds 
"Deep generative models have achieved impressive success in recent years.Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively.This paper aims to establish formal connections between GANs and VAEs through a new formulation of them.We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively.The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way.For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples.Experiments show generality and effectiveness of the transfered techniques.","[0, 0, 0, 0, 0, 0, 1]",[],rylSzl-R-,On Unifying Deep Generative Models,L'article développe un cadre interprétant les algorithmes GAN comme effectuant une forme d'inférence variationnelle sur un modèle génératif reconstruisant une variable indicatrice de l'appartenance d'un échantillon à la vraie distribution générative des données.
"Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications.However, the interpretability of those models are often lacking.On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists).In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes.The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists.Thanks to the prediction power of neural networks, the rule lists from				 prototypes are more concise and hence provide better interpretability.On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%.Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1gnQ20qYX,Pearl: Prototype lEArning via Rule Lists,une méthode combinant l'apprentissage de listes de règles et l'apprentissage de prototypes 
"Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications.However, the interpretability of those models are often lacking.On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists).In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes.The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists.Thanks to the prediction power of neural networks, the rule lists from				 prototypes are more concise and hence provide better interpretability.On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%.Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1gnQ20qYX,Pearl: Prototype lEArning via Rule Lists,"Présente un nouveau cadre de prédiction interprétable, qui combine l'apprentissage basé sur des règles, l'apprentissage par prototype et les NN, particulièrement applicable aux données longitudinales."
"Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications.However, the interpretability of those models are often lacking.On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists).In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes.The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists.Thanks to the prediction power of neural networks, the rule lists from				 prototypes are more concise and hence provide better interpretability.On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%.Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1gnQ20qYX,Pearl: Prototype lEArning via Rule Lists,"Cet article vise à remédier au manque d'interprétabilité des modèles d'apprentissage profond et propose l'apprentissage par prototype via des listes de règles (PEARL), qui combine l'apprentissage par règle et l'apprentissage par prototype pour obtenir une classification plus précise et simplifier la tâche d'interprétabilité."
"Generative Adversarial Networks (GANs) are powerful tools for realistic image generation.However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time.In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \cite{deligan}.The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process.Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence.We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.","[0, 1, 0, 0, 0, 0]",[],HyMuaiAqY7,Deli-Fisher GAN: Stable and Efficient Image Generation With Structured Latent Generative Space,"Cet article propose un nouveau réseau adversarial génératif qui est plus stable, plus efficace et qui produit de meilleures images que celles du statu quo. "
"Generative Adversarial Networks (GANs) are powerful tools for realistic image generation.However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time.In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \cite{deligan}.The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process.Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence.We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.","[0, 1, 0, 0, 0, 0]",[],HyMuaiAqY7,Deli-Fisher GAN: Stable and Efficient Image Generation With Structured Latent Generative Space,Ce document combine Fisher-GAN et Deli-GAN.
"Generative Adversarial Networks (GANs) are powerful tools for realistic image generation.However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time.In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \cite{deligan}.The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process.Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence.We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score.","[0, 1, 0, 0, 0, 0]",[],HyMuaiAqY7,Deli-Fisher GAN: Stable and Efficient Image Generation With Structured Latent Generative Space,"Cet article combine Deli-GAN, qui a une distribution préalable de mélange dans l'espace latent, et Fisher GAN, qui utilise Fisher IPM au lieu de JSD comme objectif."
"Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially.We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups.This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise.We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset.The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset.Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.","[0, 1, 0, 0, 0, 0]",[],Bk346Ok0W,Sensor Transformation Attention Networks,Nous présentons une architecture de réseau multi-capteurs modulaire avec un mécanisme attentionnel qui permet une sélection dynamique des capteurs sur des données bruyantes du monde réel provenant de CHiME-3.
"Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially.We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups.This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise.We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset.The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset.Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.","[0, 1, 0, 0, 0, 0]",[],Bk346Ok0W,Sensor Transformation Attention Networks,Une architecture neuronale générique capable d'apprendre l'attention qui doit être portée aux différents canaux d'entrée en fonction de la qualité relative de chaque capteur par rapport aux autres.
"Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially.We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups.This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise.We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset.The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset.Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments.","[0, 1, 0, 0, 0, 0]",[],Bk346Ok0W,Sensor Transformation Attention Networks," Examine l'utilisation de l'attention pour la sélection des capteurs ou des canaux, avec des résultats sur TIDIGITS et GRID montrant un avantage de l'attention sur la concaténation des caractéristiques."
"Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints.Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection.To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud.The local neural network (NN) is used to generate the feature representations.To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs.The cloud NN is then trained based on the extracted intermediate representations for the target learning task.We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task.Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage.The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJcjQTJ0W,PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,"Pour permettre l'entraînement des DNN dans le nuage tout en protégeant simultanément la confidentialité des données, nous proposons d'exploiter les représentations intermédiaires des données, ce qui est réalisé en divisant les DNN et en les déployant séparément sur des plateformes locales et dans le nuage."
"Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints.Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection.To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud.The local neural network (NN) is used to generate the feature representations.To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs.The cloud NN is then trained based on the extracted intermediate representations for the target learning task.We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task.Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage.The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset.","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJcjQTJ0W,PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,"Cet article propose une technique pour privatiser les données en apprenant une représentation des caractéristiques qui est difficile à utiliser pour la reconstruction d'images, mais utile pour la classification des images."
"Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,"GANs récurrents conditionnels pour la génération de séquences médicales à valeur réelle, présentant de nouvelles approches d'évaluation et une analyse empirique de la confidentialité."
"Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,propose d'utiliser des données synthétiques générées par des GAN pour remplacer les données personnelles identifiables dans l'entraînement des modèles ML pour les applications sensibles à la vie privée.
"Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,"Les auteurs proposent une nouvelle architecture GAN récurrente qui génère des séquences de domaine continues, et l'évaluent sur plusieurs tâches synthétiques et une tâche de données chronologiques ICU."
"Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],B1ZZTfZAW,Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs,Propose d'utiliser les RGAN et les RCGAN pour générer des séquences synthétiques de données réelles.
"Emphasis effects – visual changes that make certain elements moreprominent – are commonly used in information visualization to drawthe user’s attention or to indicate importance.Although theoreticalframeworks of emphasis exist (that link visually diverse emphasiseffects through the idea of visual prominence compared to backgroundelements), most metrics for predicting how emphasis effectswill be perceived by users come from abstract models of humanvision which may not apply to visualization design.In particular,it is difficult for designers to know, when designing a visualization,how different emphasis effects will compare and what level of oneeffect is equivalent to what level of another.To address this gap,we carried out two studies that provide empirical evidence abouthow users perceive different emphasis effects, using three visualvariables (colour, size, and blur/focus) and eight strength levels.Results from gaze tracking, mouse clicks, and subjective responsesshow that there are significant differences between visual variablesand between levels, and allow us to develop an initial understandingof perceptual equivalence.We developed a model from the data inour first study, and used it to predict the results in the second; themodel was accurate, with high correlations between predictions andreal values.Our studies and empirical models provide valuable newinformation for designers who want to understand and control howemphasis effects will be perceived by users.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],NxvF-PleYy,A Baseline Study of Emphasis Effects in Information Visualization,Nos études et modèles empiriques fournissent de nouvelles informations précieuses pour les concepteurs qui souhaitent comprendre et contrôler la manière dont les effets d'accentuation seront perçus par les utilisateurs.
"Emphasis effects – visual changes that make certain elements moreprominent – are commonly used in information visualization to drawthe user’s attention or to indicate importance.Although theoreticalframeworks of emphasis exist (that link visually diverse emphasiseffects through the idea of visual prominence compared to backgroundelements), most metrics for predicting how emphasis effectswill be perceived by users come from abstract models of humanvision which may not apply to visualization design.In particular,it is difficult for designers to know, when designing a visualization,how different emphasis effects will compare and what level of oneeffect is equivalent to what level of another.To address this gap,we carried out two studies that provide empirical evidence abouthow users perceive different emphasis effects, using three visualvariables (colour, size, and blur/focus) and eight strength levels.Results from gaze tracking, mouse clicks, and subjective responsesshow that there are significant differences between visual variablesand between levels, and allow us to develop an initial understandingof perceptual equivalence.We developed a model from the data inour first study, and used it to predict the results in the second; themodel was accurate, with high correlations between predictions andreal values.Our studies and empirical models provide valuable newinformation for designers who want to understand and control howemphasis effects will be perceived by users.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],NxvF-PleYy,A Baseline Study of Emphasis Effects in Information Visualization,Cet article examine quelle mise en évidence visuelle est perçue plus rapidement dans la visualisation de données et comment les différentes méthodes de mise en évidence se comparent les unes aux autres.
"Emphasis effects – visual changes that make certain elements moreprominent – are commonly used in information visualization to drawthe user’s attention or to indicate importance.Although theoreticalframeworks of emphasis exist (that link visually diverse emphasiseffects through the idea of visual prominence compared to backgroundelements), most metrics for predicting how emphasis effectswill be perceived by users come from abstract models of humanvision which may not apply to visualization design.In particular,it is difficult for designers to know, when designing a visualization,how different emphasis effects will compare and what level of oneeffect is equivalent to what level of another.To address this gap,we carried out two studies that provide empirical evidence abouthow users perceive different emphasis effects, using three visualvariables (colour, size, and blur/focus) and eight strength levels.Results from gaze tracking, mouse clicks, and subjective responsesshow that there are significant differences between visual variablesand between levels, and allow us to develop an initial understandingof perceptual equivalence.We developed a model from the data inour first study, and used it to predict the results in the second; themodel was accurate, with high correlations between predictions andreal values.Our studies and empirical models provide valuable newinformation for designers who want to understand and control howemphasis effects will be perceived by users.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],NxvF-PleYy,A Baseline Study of Emphasis Effects in Information Visualization,"Deux études sur l'efficacité des effets d'accentuation, l'une évaluant les niveaux de différences utiles, et l'autre plus appliquée utilisant des visualisations réelles différentes pour une investigation plus écologiquement valide."
"Memory Network based models have shown a remarkable progress on the task of relational reasoning.Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. As a result, our model is as simple as RN but the computational complexity is reduced to linear time.It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset.","[0, 0, 1, 0, 0, 0, 0]",[],ByquB-WC-,Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,"Une architecture de raisonnement simple basée sur le réseau de mémoire (MemNN) et le réseau de relations (RN), réduisant la complexité temporelle par rapport au RN et permettant d'obtenir des résultats à la pointe de la technologie pour l'assurance qualité basée sur l'histoire bAbI et le dialogue bAbI."
"Memory Network based models have shown a remarkable progress on the task of relational reasoning.Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. As a result, our model is as simple as RN but the computational complexity is reduced to linear time.It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset.","[0, 0, 1, 0, 0, 0, 0]",[],ByquB-WC-,Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning,"Introduit le réseau de mémoire connexe (RMN), une amélioration des réseaux de relations (RN)."
"We investigate in this paper the architecture of deep convolutional networks.Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.The use of branches brings an additional form of regularization.In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.We refer to this branched architecture as ""coupled ensembles"".The approach is very generic and can be applied with almost any neural network architecture.With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Hk2MHt-3-,Coupled Ensembles of Neural Networks,Nous montrons que la division d'un réseau neuronal en branches parallèles améliore les performances et qu'un couplage approprié des branches améliore encore plus les performances.
"We investigate in this paper the architecture of deep convolutional networks.Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.The use of branches brings an additional form of regularization.In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.We refer to this branched architecture as ""coupled ensembles"".The approach is very generic and can be applied with almost any neural network architecture.With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Hk2MHt-3-,Coupled Ensembles of Neural Networks,"Ce travail propose une reconfiguration du modèle CNN existant à la pointe de la technologie en utilisant une nouvelle architecture de branchement, avec de meilleures performances."
"We investigate in this paper the architecture of deep convolutional networks.Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.The use of branches brings an additional form of regularization.In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.We refer to this branched architecture as ""coupled ensembles"".The approach is very generic and can be applied with almost any neural network architecture.With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Hk2MHt-3-,Coupled Ensembles of Neural Networks,Cet article montre les avantages de l'ensemblisme couplé en termes d'économie de paramètres.
"We investigate in this paper the architecture of deep convolutional networks.Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.The use of branches brings an additional form of regularization.In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.We refer to this branched architecture as ""coupled ensembles"".The approach is very generic and can be applied with almost any neural network architecture.With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]",[],Hk2MHt-3-,Coupled Ensembles of Neural Networks,Présente une architecture de réseau profond qui traite les données à l'aide de plusieurs branches parallèles et combine les données postérieures de ces branches pour calculer les scores finaux.
"Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few.Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices.To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly.Yet, this acceleration comes at the cost of a larger error.The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy.This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations.We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.","[0, 0, 0, 0, 0, 1, 0]",[],HyfyN30qt7,NICE: noise injection and clamping estimation for neural network quantization,"Combinaison de l'injection de bruit, de la quantification graduelle et de l'apprentissage par blocage d'activation pour obtenir une quantification de pointe à 3, 4 et 5 bits."
"Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few.Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices.To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly.Yet, this acceleration comes at the cost of a larger error.The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy.This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations.We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.","[0, 0, 0, 0, 0, 1, 0]",[],HyfyN30qt7,NICE: noise injection and clamping estimation for neural network quantization,Propose d'injecter du bruit pendant la formation et de fixer les valeurs des paramètres d'une couche ainsi que la sortie d'activation dans la quantification du réseau neuronal.
"Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few.Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices.To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly.Yet, this acceleration comes at the cost of a larger error.The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy.This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations.We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications.","[0, 0, 0, 0, 0, 1, 0]",[],HyfyN30qt7,NICE: noise injection and clamping estimation for neural network quantization,"Procédé de quantification de réseaux neuronaux profonds pour la classification et la régression, utilisant l'injection de bruit, le bridage avec des activations maximales apprises, et la quantification progressive par blocs pour obtenir des performances égales ou supérieures aux méthodes de pointe."
"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks.Approaches that transfer information contained only in the final parameters of a source model will therefore struggle.Instead, transfer learning at at higher level of abstraction is needed.We propose Leap, a framework that achieves this by transferring knowledge across learning processes.We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path.Our framework leverages only information obtained during training and can be computed on the fly at negligible cost.We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks.Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HygBZnRctX,Transferring Knowledge across Learning Processes,"Nous proposons Leap, un cadre qui transfère les connaissances entre les processus d'apprentissage en minimisant la distance attendue que le processus de formation parcourt sur la surface de perte d'une tâche."
"In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks.Approaches that transfer information contained only in the final parameters of a source model will therefore struggle.Instead, transfer learning at at higher level of abstraction is needed.We propose Leap, a framework that achieves this by transferring knowledge across learning processes.We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path.Our framework leverages only information obtained during training and can be computed on the fly at negligible cost.We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks.Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HygBZnRctX,Transferring Knowledge across Learning Processes,L'article propose un nouvel objectif de méta-apprentissage visant à surpasser les approches de pointe lorsqu'il s'agit de collections de tâches qui présentent une diversité inter-tâches substantielle.
"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned.Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones.DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance.When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy.The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains.We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","[0, 0, 0, 1, 0, 0, 0]",[],ryj0790hb,Incremental Learning through Deep Adaptation,"Une alternative à l'apprentissage par transfert qui apprend plus rapidement, nécessite beaucoup moins de paramètres (3-13 %), obtient généralement de meilleurs résultats et préserve précisément les performances sur les anciennes tâches."
"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned.Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones.DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance.When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy.The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains.We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","[0, 0, 0, 1, 0, 0, 0]",[],ryj0790hb,Incremental Learning through Deep Adaptation,Modules de contrôle pour l'apprentissage par incrémentation sur des ensembles de données de classification d'images
"High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications.This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including1) channel-wise scale factors of weights, especially for depthwise convolution,2) Winograd convolution, and3) topology-wise 8-bit support.We experiment the techniques on top of a widely-used deep learning framework.The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining.We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution.We show that the inference throughputand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline.We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks.All the code and models will be publicly available soon.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SklzIjActX,HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS,Nous présentons une technique générale permettant l'inférence à faible précision de 8 bits des réseaux neuronaux convolutifs. 
"High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications.This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including1) channel-wise scale factors of weights, especially for depthwise convolution,2) Winograd convolution, and3) topology-wise 8-bit support.We experiment the techniques on top of a widely-used deep learning framework.The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining.We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution.We show that the inference throughputand latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline.We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks.All the code and models will be publicly available soon.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SklzIjActX,HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS,Cet article conçoit un système pour quantifier automatiquement les modèles CNN prétraités.
"Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space.We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model.Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference.Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.","[0, 1, 0, 0, 0]",[],rJxHsjRqFQ,Hyperbolic Attention Networks,Nous proposons d'incorporer des biais inductifs et des opérations provenant de la géométrie hyperbolique pour améliorer le mécanisme d'attention des réseaux neuronaux.
"Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space.We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model.Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference.Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.","[0, 1, 0, 0, 0]",[],rJxHsjRqFQ,Hyperbolic Attention Networks,"Cet article remplace la similarité du produit scalaire utilisée dans les mécanismes d'attention par la distance hyperbolique négative et l'applique au modèle Transformer existant, aux réseaux d'attention graphique et aux réseaux de relations."
"Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space.We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model.Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference.Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact.","[0, 1, 0, 0, 0]",[],rJxHsjRqFQ,Hyperbolic Attention Networks,Les auteurs proposent une nouvelle approche pour améliorer l'attention relationnelle en changeant les fonctions de correspondance et d'agrégation pour utiliser une géométrie hyperbolique. 
"We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies.We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies.Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies.This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region.Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm.Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst"" possible adversarial disturbances.","[0, 1, 0, 0, 0, 0]",[],rkc_hGb0Z,A dynamic game approach to training robust deep policies,Cet article démontre comment la théorie de contrôle de l'infinité H peut aider à mieux concevoir des politiques profondes et robustes pour les moteurs de robots.
"We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies.We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies.Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies.This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region.Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm.Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst"" possible adversarial disturbances.","[0, 1, 0, 0, 0, 0]",[],rkc_hGb0Z,A dynamic game approach to training robust deep policies,Propose d'incorporer des éléments de contrôle robuste dans la recherche sur les politiques guidées afin de concevoir une méthode qui résiste aux perturbations et à l'inadéquation des modèles.
"We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies.We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies.Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies.This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region.Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm.Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst"" possible adversarial disturbances.","[0, 1, 0, 0, 0, 0]",[],rkc_hGb0Z,A dynamic game approach to training robust deep policies,"L'article présente une méthode d'évaluation de la sensibilité et de la robustesse des politiques de RL profond, et propose une approche de jeu dynamique pour l'apprentissage de politiques robustes."
"Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.Under such conditions, we prove the existence of small universal perturbations.Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.","[0, 1, 0, 0, 0, 0]",[],ByrZyglCb,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,Analyse de la vulnérabilité des classificateurs aux perturbations universelles et relation avec la courbure de la frontière de décision.
"Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.Under such conditions, we prove the existence of small universal perturbations.Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.","[0, 1, 0, 0, 0, 0]",[],ByrZyglCb,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,L'article fournit une analyse intéressante liant la géométrie des limites de décision du classificateur à de petites perturbations adverses universelles.
"Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.Under such conditions, we prove the existence of small universal perturbations.Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.","[0, 1, 0, 0, 0, 0]",[],ByrZyglCb,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,Cet article traite des perturbations universelles - des perturbations qui peuvent induire en erreur un classificateur entraîné si elles sont ajoutées à la plupart des points de données d'entrée.
"Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.Under such conditions, we prove the existence of small universal perturbations.Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.","[0, 1, 0, 0, 0, 0]",[],ByrZyglCb,Robustness of Classifiers to Universal Perturbations: A Geometric Perspective,L'article développe des modèles qui tentent d'expliquer l'existence de perturbations universelles qui trompent les réseaux neuronaux.
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning.However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration.Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines.However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task.In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill.Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly.In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.","[0, 0, 0, 0, 0, 0, 1]",[],HkgSEnA5KQ,Guiding Policies with Language via Meta-Learning,Nous proposons une méthode de méta-apprentissage pour la correction interactive des politiques en langage naturel.
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning.However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration.Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines.However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task.In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill.Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly.In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.","[0, 0, 0, 0, 0, 0, 1]",[],HkgSEnA5KQ,Guiding Policies with Language via Meta-Learning,"Cet article présente un cadre de méta-apprentissage qui montre comment apprendre de nouvelles tâches dans une configuration interactive. Chaque tâche est apprise par une configuration d'apprentissage par renforcement, puis la tâche est mise à jour en observant de nouvelles instructions."
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning.However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration.Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines.However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task.In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill.Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly.In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.","[0, 0, 0, 0, 0, 0, 1]",[],HkgSEnA5KQ,Guiding Policies with Language via Meta-Learning,Cet article apprend aux agents à accomplir des tâches via des instructions en langage naturel dans un processus itératif.
"Deep generative models such as Generative Adversarial Networks (GANs) andVariational Auto-Encoders (VAEs) are important tools to capture and investigatethe properties of complex empirical data.However, the complexity of their innerelements makes their functionment challenging to assess and modify.In thisrespect, these architectures behave as black box models.In order to betterunderstand the function of such networks, we analyze their modularity based onthe counterfactual manipulation of their internal variables.Our experiments on thegeneration of human faces with VAEs and GANs support that modularity betweenactivation maps distributed over channels of generator architectures is achievedto some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.erate and edit the content of generated images.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],Byldr3RqKX,Tinkering with black boxes: counterfactuals uncover modularity in generative models,Nous étudions la modularité des modèles génératifs profonds.
"Deep generative models such as Generative Adversarial Networks (GANs) andVariational Auto-Encoders (VAEs) are important tools to capture and investigatethe properties of complex empirical data.However, the complexity of their innerelements makes their functionment challenging to assess and modify.In thisrespect, these architectures behave as black box models.In order to betterunderstand the function of such networks, we analyze their modularity based onthe counterfactual manipulation of their internal variables.Our experiments on thegeneration of human faces with VAEs and GANs support that modularity betweenactivation maps distributed over channels of generator architectures is achievedto some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.erate and edit the content of generated images.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],Byldr3RqKX,Tinkering with black boxes: counterfactuals uncover modularity in generative models,"L'article fournit un moyen d'étudier la structure modulaire du modèle génératif profond, avec le concept clé de distribution sur les canaux des architectures de générateur."
"Relational databases store a significant amount of the worlds data.However, accessing this data currently requires users to understand a query language such as SQL.We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss.Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem.In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets.By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.","[0, 0, 0, 0, 0, 1, 0]",[],Syx6bz-Ab,Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,"Nous présentons Seq2SQL, qui traduit les questions en requêtes SQL en utilisant les récompenses de l'exécution des requêtes en ligne, et WikiSQL, un ensemble de données de tables/questions/requêtes SQL plusieurs fois plus grand que les ensembles de données existants."
"Relational databases store a significant amount of the worlds data.However, accessing this data currently requires users to understand a query language such as SQL.We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss.Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem.In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets.By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.","[0, 0, 0, 0, 0, 1, 0]",[],Syx6bz-Ab,Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning ,Un nouvel ensemble de données d'analyse sémantique qui se concentre sur la génération de SQL à partir du langage naturel en utilisant un modèle basé sur l'apprentissage par renforcement.
"We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness.We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis.Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components.We show that models learnt with our approach perform remarkably well against a wide-range of attacks.Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.","[0, 1, 0, 0, 0, 0]",[],rkMk9j0qYm,Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness,La modélisation du bruit à l'entrée pendant la formation discriminante améliore la robustesse des adversaires. Proposer une métrique d'évaluation basée sur l'ACP pour la robustesse des adversaires.
"We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness.We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis.Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components.We show that models learnt with our approach perform remarkably well against a wide-range of attacks.Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.","[0, 1, 0, 0, 0, 0]",[],rkMk9j0qYm,Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness,"Cet article propose, ExL, une méthode d'apprentissage contradictoire utilisant un bruit multiplié qui s'avère utile pour se défendre contre les attaques de type boîte noire sur trois ensembles de données."
"We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness.We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis.Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components.We show that models learnt with our approach perform remarkably well against a wide-range of attacks.Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.","[0, 1, 0, 0, 0, 0]",[],rkMk9j0qYm,Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness,"Cet article inclut un bruit multiplicatif N dans les données d'entraînement afin d'obtenir une robustesse à l'adversité, lors de l'entraînement à la fois sur les paramètres du modèle theta et sur le bruit lui-même."
"We propose a method which can visually explain the classification decision of deep neural networks (DNNs).There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  All of these methods try to gain insight into why the network ""chose class A"" as an answer.Humans, when searching for explanations, ask two types of questions.The first question is, ""Why did you choose this answer?""The second question asks, ""Why did you not choose answer B over A?""The previously proposed methods are either not able to provide the latter directly or efficiently.We introduce a method capable of answering the second question both directly and efficiently.In this work, we limit the inputs to be images.In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation.We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyNmRiCqtm,CDeepEx: Contrastive Deep Explanations,"Une méthode pour répondre à la question ""pourquoi pas la classe B ?"" pour expliquer les réseaux profonds"
"We propose a method which can visually explain the classification decision of deep neural networks (DNNs).There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  All of these methods try to gain insight into why the network ""chose class A"" as an answer.Humans, when searching for explanations, ask two types of questions.The first question is, ""Why did you choose this answer?""The second question asks, ""Why did you not choose answer B over A?""The previously proposed methods are either not able to provide the latter directly or efficiently.We introduce a method capable of answering the second question both directly and efficiently.In this work, we limit the inputs to be images.In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation.We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyNmRiCqtm,CDeepEx: Contrastive Deep Explanations,L'article propose une approche visant à fournir des explications visuelles contrastives pour les réseaux neuronaux profonds.
"We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.We provide theoretical and numerical results on the inverse of ReLU-layers.First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation.Next, we move to robustness via analyzing local effects on the inverse.To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.","[1, 0, 0, 0, 0]",[],SyxYEoA5FX,Invariance and Inverse Stability under ReLU,Nous analysons l'inversibilité des réseaux neuronaux profonds en étudiant les préimages des couches ReLU et la stabilité de l'inverse.
"We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.We provide theoretical and numerical results on the inverse of ReLU-layers.First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation.Next, we move to robustness via analyzing local effects on the inverse.To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.","[1, 0, 0, 0, 0]",[],SyxYEoA5FX,Invariance and Inverse Stability under ReLU,"Cet article étudie le volume de préimage de l'activation d'un réseau ReLU à une certaine couche, et il s'appuie sur la linéarité par morceaux de la fonction forward d'un réseau ReLU. "
"We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.We provide theoretical and numerical results on the inverse of ReLU-layers.First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation.Next, we move to robustness via analyzing local effects on the inverse.To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.","[1, 0, 0, 0, 0]",[],SyxYEoA5FX,Invariance and Inverse Stability under ReLU,Cet article présente une analyse de l'invariance inverse des réseaux ReLU et fournit des limites supérieures sur les valeurs singulières d'un réseau de train.
"While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs.This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important.Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks.In this paper we investigate how this approach scales as we increase the computational budget given to the defender.We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model.Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.","[0, 0, 0, 0, 0, 1]",[],HJguLo0cKQ,Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles,L'entraînement d'ensembles par des adversaires offre une robustesse aux exemples d'adversité supérieure à celle observée dans les modèles entraînés par des adversaires et les ensembles entraînés indépendamment de ceux-ci.
"While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs.This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important.Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks.In this paper we investigate how this approach scales as we increase the computational budget given to the defender.We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model.Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.","[0, 0, 0, 0, 0, 1]",[],HJguLo0cKQ,Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles," Propose d'entraîner un ensemble de modèles conjointement, où à chaque étape temporelle, un ensemble d'exemples qui sont contradictoires pour l'ensemble lui-même est incorporé dans l'apprentissage."
"Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer.To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm.A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks.A function block may be any neural network – for example a fully-connected or a convolutional layer.Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached.In this way the routing network dynamically composes different function blocks for each input.We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks.We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets.Our experiments demonstrate a significant improvement in accuracy, with sharper convergence.In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks.On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],ry8dvM-R-,Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,les réseaux de routage : un nouveau type de réseau neuronal qui apprend à router son entrée de manière adaptative pour l'apprentissage multitâche.
"Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer.To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm.A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks.A function block may be any neural network – for example a fully-connected or a convolutional layer.Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached.In this way the routing network dynamically composes different function blocks for each input.We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks.We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets.Our experiments demonstrate a significant improvement in accuracy, with sharper convergence.In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks.On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],ry8dvM-R-,Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,"L'article suggère d'utiliser un réseau modulaire avec un contrôleur qui prend des décisions, à chaque pas de temps, concernant le prochain nodule à appliquer."
"Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer.To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm.A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks.A function block may be any neural network – for example a fully-connected or a convolutional layer.Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached.In this way the routing network dynamically composes different function blocks for each input.We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks.We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets.Our experiments demonstrate a significant improvement in accuracy, with sharper convergence.In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks.On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],ry8dvM-R-,Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning,"L'article présente une nouvelle formulation pour l'apprentissage de l'architecture optimale d'un réseau neuronal dans un cadre d'apprentissage multi-tâche en utilisant l'apprentissage par renforcement multi-agent pour trouver une politique, et montre une amélioration par rapport aux architectures codées en dur avec des couches partagées."
"We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero.Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization.AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization.However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function.We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero.We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters.We further propose the \emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid.The parameters of the distribution over the gates can then be jointly optimized with the original network parameters.As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way.We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1Y8hhg0b,Learning Sparse Neural Networks through L_0 Regularization,Nous montrons comment optimiser la norme L_0 attendue des modèles paramétriques avec la descente de gradient et introduisons une nouvelle distribution qui facilite le hard gating.
"We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero.Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization.AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization.However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function.We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero.We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters.We further propose the \emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid.The parameters of the distribution over the gates can then be jointly optimized with the original network parameters.As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way.We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],H1Y8hhg0b,Learning Sparse Neural Networks through L_0 Regularization,Les auteurs présentent une approche basée sur le gradient pour minimiser une fonction objective avec une pénalité clairsemée L0 afin de faciliter l'apprentissage de réseaux neuronaux clairsemés.
"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches.These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer.Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models.This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small.This in turn allows a room for designing more innovative propagation layers.Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph.The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions.In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods.By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJg4YGWRb,Attention-based Graph Neural Network for Semi-supervised Learning,Nous proposons une nouvelle architecture de réseau neuronal graphique interprétable basée sur l'attention qui surpasse les réseaux neuronaux graphiques actuels dans des ensembles de données de référence standard.
"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches.These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer.Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models.This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small.This in turn allows a room for designing more innovative propagation layers.Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph.The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions.In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods.By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJg4YGWRb,Attention-based Graph Neural Network for Semi-supervised Learning,"Les auteurs proposent deux extensions des GCN, en supprimant les non-linéarités intermédiaires du calcul des GCN et en ajoutant un mécanisme d'attention dans la couche d'agrégation."
"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches.These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer.Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models.This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small.This in turn allows a room for designing more innovative propagation layers.Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph.The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions.In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods.By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJg4YGWRb,Attention-based Graph Neural Network for Semi-supervised Learning,"L'article propose un algorithme d'apprentissage semi-supervisé pour la classification des nœuds de graphe, inspiré des réseaux neuronaux graphiques."
"Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality.We argue that this discrepancy may contribute to the difficulties in training generative models.We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space.The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model.With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities.When combined with VAE, PGA can generate sharper samples than vanilla VAE.","[0, 0, 0, 0, 1, 0]",[],rkxkr8UKuN,Perceptual Generative Autoencoders,"Un cadre pour la formation de modèles génératifs basés sur des autoencodeurs, avec des pertes non contradictoires et des architectures de réseaux neuronaux non restreintes."
"Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality.We argue that this discrepancy may contribute to the difficulties in training generative models.We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space.The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model.With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities.When combined with VAE, PGA can generate sharper samples than vanilla VAE.","[0, 0, 0, 0, 1, 0]",[],rkxkr8UKuN,Perceptual Generative Autoencoders,Cet article utilise des autoencodeurs pour effectuer une correspondance de distribution dans un espace à haute dimension.
"The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset.We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6points in Spearman rank correlation on similarity tasksand 3.4 points on analogy accuracy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,Les espaces d'intégration de collecteurs de produits à courbure hétérogène donnent des représentations améliorées par rapport aux espaces d'intégration traditionnels pour une variété de structures.
"The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset.We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6points in Spearman rank correlation on similarity tasksand 3.4 points on analogy accuracy.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJxeWnCcF7,Learning Mixed-Curvature Representations in Product Spaces,"Propose une méthode de réduction de la dimensionnalité qui incorpore les données dans un collecteur produit de collecteurs sphériques, euclidiens et hyperboliques. L'algorithme est basé sur la correspondance entre les distances géodésiques sur le collecteur produit et les distances des graphes."
"Synthesizing user-intended programs from a small number of input-output exam-ples is a challenging problem with several important applications like spreadsheetmanipulation, data wrangling and code refactoring.Existing synthesis systemseither completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and ingeneral fail to provide real-time synthesis on challenging benchmarks.In this work,we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis techniquethat combines the best of both symbolic logic techniques and statistical models.Thus, it produces programs that satisfy the provided specifications by constructionand generalize well on unseen examples, similar to data-driven systems.Ourtechnique effectively utilizes the deductive search framework to reduce the learningproblem of the neural component to a simple supervised learning setup.Further,this allows us to both train on sparingly available real-world data and still leveragepowerful recurrent neural network encoders.We demonstrate the effectivenessof our method by evaluating on real-world customer scenarios by synthesizingaccurate programs with up to 12× speed-up compared to state-of-the-art systems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rywDjg-RW,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,Nous intégrons des méthodes symboliques (déductives) et statistiques (basées sur les neurones) pour permettre la synthèse de programmes en temps réel avec une généralisation presque parfaite à partir d'un exemple d'entrée-sortie.
"Synthesizing user-intended programs from a small number of input-output exam-ples is a challenging problem with several important applications like spreadsheetmanipulation, data wrangling and code refactoring.Existing synthesis systemseither completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and ingeneral fail to provide real-time synthesis on challenging benchmarks.In this work,we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis techniquethat combines the best of both symbolic logic techniques and statistical models.Thus, it produces programs that satisfy the provided specifications by constructionand generalize well on unseen examples, similar to data-driven systems.Ourtechnique effectively utilizes the deductive search framework to reduce the learningproblem of the neural component to a simple supervised learning setup.Further,this allows us to both train on sparingly available real-world data and still leveragepowerful recurrent neural network encoders.We demonstrate the effectivenessof our method by evaluating on real-world customer scenarios by synthesizingaccurate programs with up to 12× speed-up compared to state-of-the-art systems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rywDjg-RW,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,"L'article présente une approche de type ""branch-and-bound"" pour apprendre de bons programmes, dans laquelle un LSTM est utilisé pour prédire quelles branches de l'arbre de recherche devraient conduire à de bons programmes."
"Synthesizing user-intended programs from a small number of input-output exam-ples is a challenging problem with several important applications like spreadsheetmanipulation, data wrangling and code refactoring.Existing synthesis systemseither completely rely on deductive logic techniques that are extensively hand-engineered or on purely statistical models that need massive amounts of data, and ingeneral fail to provide real-time synthesis on challenging benchmarks.In this work,we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis techniquethat combines the best of both symbolic logic techniques and statistical models.Thus, it produces programs that satisfy the provided specifications by constructionand generalize well on unseen examples, similar to data-driven systems.Ourtechnique effectively utilizes the deductive search framework to reduce the learningproblem of the neural component to a simple supervised learning setup.Further,this allows us to both train on sparingly available real-world data and still leveragepowerful recurrent neural network encoders.We demonstrate the effectivenessof our method by evaluating on real-world customer scenarios by synthesizingaccurate programs with up to 12× speed-up compared to state-of-the-art systems.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rywDjg-RW,Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples,Proposition d'un système qui synthétise des programmes à partir d'un seul exemple et dont la généralisation est meilleure que l'état de l'art antérieur.
"Variational auto-encoders (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models.However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classiﬁcation) and human interpretation.We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution.We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efﬁcient as in the standard VAE case.With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models.We show that these sparse representations are advantageous over standard VAE representations on two benchmark classiﬁcation tasks (MNIST and Fashion-MNIST) by demonstrating improved classiﬁcation accuracy and signiﬁcantly increased robustness to the number of latent dimensions.Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.","[0, 0, 0, 0, 0, 0, 1]",[],SkeJ6iR9Km,Variational Sparse Coding,Nous explorons l'intersection des VAE et du codage clairsemé.
"Variational auto-encoders (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models.However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classiﬁcation) and human interpretation.We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution.We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational efﬁcient as in the standard VAE case.With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models.We show that these sparse representations are advantageous over standard VAE representations on two benchmark classiﬁcation tasks (MNIST and Fashion-MNIST) by demonstrating improved classiﬁcation accuracy and signiﬁcantly increased robustness to the number of latent dimensions.Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation.","[0, 0, 0, 0, 0, 0, 1]",[],SkeJ6iR9Km,Variational Sparse Coding,Cet article propose une extension des VAE avec des prieurs et des a posteriori épars pour apprendre des représentations interprétables éparses.
"A widely observed phenomenon in deep learning is the degradation problem: increasingthe depth of a network leads to a decrease in performance on both test and training data.Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms.However, the degradation problem persists in the context of plain feed-forward networks.In this work we propose a simple method to address this issue.The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers.This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner.We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],BJQPG5lR-,Avoiding degradation in deep feed-forward networks by phasing out skip-connections,L'élimination progressive et raisonnée des connexions sautées évite la dégradation des réseaux profonds à action directe.
"A widely observed phenomenon in deep learning is the degradation problem: increasingthe depth of a network leads to a decrease in performance on both test and training data.Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms.However, the degradation problem persists in the context of plain feed-forward networks.In this work we propose a simple method to address this issue.The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers.This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner.We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],BJQPG5lR-,Avoiding degradation in deep feed-forward networks by phasing out skip-connections,"Les auteurs présentent une nouvelle stratégie de formation, VAN, pour la formation de réseaux feed-forward très profonds sans saut de connexion."
"A widely observed phenomenon in deep learning is the degradation problem: increasingthe depth of a network leads to a decrease in performance on both test and training data.Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms.However, the degradation problem persists in the context of plain feed-forward networks.In this work we propose a simple method to address this issue.The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers.This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner.We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets.","[0, 0, 0, 0, 0, 0, 1, 0]",[],BJQPG5lR-,Avoiding degradation in deep feed-forward networks by phasing out skip-connections,L'article présente une architecture qui interpole linéairement entre les ResNets et les réseaux profonds vanille sans sauter de connexions.
"Deep learning is becoming more widespread in its application due to its power in solving complex classification problems.However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications.This work addresses the problem by proposing methods {\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight.Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem.Our method that mini-batch SVRG with $\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates.Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\times$ without affecting their test accuracy.","[0, 1, 0, 0, 0, 0]",[],Sk0pHeZAW,Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,Compression de réseaux neuronaux profonds déployés sur un dispositif embarqué. 
"Deep learning is becoming more widespread in its application due to its power in solving complex classification problems.However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications.This work addresses the problem by proposing methods {\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight.Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem.Our method that mini-batch SVRG with $\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates.Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\times$ without affecting their test accuracy.","[0, 1, 0, 0, 0, 0]",[],Sk0pHeZAW,Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,Les auteurs présentent un algorithme de formation basé sur le SVRG régularisé l-1 qui est capable de forcer de nombreux poids du réseau à être 0.
"Deep learning is becoming more widespread in its application due to its power in solving complex classification problems.However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications.This work addresses the problem by proposing methods {\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight.Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem.Our method that mini-batch SVRG with $\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates.Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\times$ without affecting their test accuracy.","[0, 1, 0, 0, 0, 0]",[],Sk0pHeZAW,Sparse Regularized Deep Neural Networks For Efficient Embedded Learning,Ce travail permet de réduire les besoins en mémoire.
"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment.For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus.Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment.However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain.In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment.Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm.To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training.Despite the novel features of these objects the model is able to infer the latent representations for them.Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],Hy8hkYeRb,A Deep Predictive Coding Network for Learning Latent Representations,Un algorithme d'apprentissage basé sur le codage prédictif pour construire des modèles de réseaux neuronaux profonds du cerveau
"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment.For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus.Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment.However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain.In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment.Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm.To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training.Despite the novel features of these objects the model is able to infer the latent representations for them.Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],Hy8hkYeRb,A Deep Predictive Coding Network for Learning Latent Representations,L'article considère l'apprentissage d'un réseau neuronal génératif en utilisant une configuration de codage prédictif.
"In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category.A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.","[1, 0, 0, 0, 0, 0]",[],rynniUpQM,Learning with Mental Imagery,"La reconnaissance d'instances d'objets avec des autoencodeurs adversariaux a été réalisée avec une nouvelle cible ""image mentale"" qui est une représentation canonique de l'image d'entrée."
"In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category.A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.","[1, 0, 0, 0, 0, 0]",[],rynniUpQM,Learning with Mental Imagery,"L'article propose une méthode d'apprentissage de caractéristiques pour la reconnaissance d'objets qui est invariante à diverses transformations de l'objet, notamment la pose de l'objet."
"In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category.A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  We demonstrate our approach on object instance recognition and handwritten digit recognition tasks.","[1, 0, 0, 0, 0, 0]",[],rynniUpQM,Learning with Mental Imagery,Cet article s'est penché sur la reconnaissance de quelques clichés par le biais d'une image mentale générée comme représentation intermédiaire de l'image d'entrée.
"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill.The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task.We present a framework that combines techniques in \textit{formal methods} with \textit{hierarchical reinforcement learning} (HRL).The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning.We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.","[1, 0, 0, 0, 0]",[],BJgVaG-Ab,AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,Combiner la logique temporelle avec l'apprentissage par renforcement hiérarchique pour la composition des compétences
"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill.The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task.We present a framework that combines techniques in \textit{formal methods} with \textit{hierarchical reinforcement learning} (HRL).The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning.We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.","[1, 0, 0, 0, 0]",[],BJgVaG-Ab,AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,L'article propose une stratégie pour construire un PDM produit à partir d'un PDM original et de l'automate associé à une formule LTL.
"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill.The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task.We present a framework that combines techniques in \textit{formal methods} with \textit{hierarchical reinforcement learning} (HRL).The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning.We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.","[1, 0, 0, 0, 0]",[],BJgVaG-Ab,AUTOMATA GUIDED HIERARCHICAL REINFORCEMENT LEARNING FOR ZERO-SHOT SKILL COMPOSITION,Propose de joindre la logique temporelle à l'apprentissage par renforcement hiérarchique pour simplifier la composition des compétences.
"The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems.As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance.However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks.In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs.Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties.The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression.For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account.We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HylDpoActX,N-Ary Quantization for CNN Model Compression and Inference Acceleration,Nous proposons un schéma de quantification pour les poids et les activations des réseaux neuronaux profonds. Ce schéma réduit considérablement l'empreinte mémoire et accélère l'inférence.
"The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems.As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance.However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks.In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs.Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties.The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression.For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account.We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet.","[0, 0, 0, 1, 0, 0, 0, 0]",[],HylDpoActX,N-Ary Quantization for CNN Model Compression and Inference Acceleration,Compression de modèles CNN et accélération de l'inférence par quantification.
"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently.This means that we must not only specify what to do, but also the much larger space of what not to do.It is easy to forget these preferences, since these preferences are already satisfied in our environment.This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.We can therefore use this implicit preference information from the state to fill in the blanks.We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties.We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.Our code can be found at https://github.com/HumanCompatibleAI/rlsp.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkevMnRqYQ,Preferences Implicit in the State of the World,"Lorsqu'un robot est déployé dans un environnement dans lequel des humains ont agi, l'état de l'environnement est déjà optimisé en fonction de ce que les humains veulent, et nous pouvons nous en servir pour déduire les préférences humaines."
"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently.This means that we must not only specify what to do, but also the much larger space of what not to do.It is easy to forget these preferences, since these preferences are already satisfied in our environment.This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.We can therefore use this implicit preference information from the state to fill in the blanks.We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties.We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.Our code can be found at https://github.com/HumanCompatibleAI/rlsp.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkevMnRqYQ,Preferences Implicit in the State of the World,Les auteurs proposent d'augmenter la fonction de récompense explicite d'un agent RL avec des récompenses/coûts auxiliaires déduits de l'état initial et d'un modèle de la dynamique de l'état.
"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently.This means that we must not only specify what to do, but also the much larger space of what not to do.It is easy to forget these preferences, since these preferences are already satisfied in our environment.This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.We can therefore use this implicit preference information from the state to fill in the blanks.We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties.We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.Our code can be found at https://github.com/HumanCompatibleAI/rlsp.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkevMnRqYQ,Preferences Implicit in the State of the World,Ce travail propose une façon de déduire l'information implicite dans l'état initial en utilisant l'IRL et de combiner la récompense déduite avec une récompense spécifiée.
"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other.In our work we present a novel, systematic, unifying taxonomy to categorize existing methods.We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures.We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on.We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories.This helps revealing links and fundamental similarities between them.Finally, we include practical recommendations both for users and for developers of new regularization methods.","[0, 0, 0, 0, 0, 0, 1]",[],SkHkeixAW,Regularization for Deep Learning: A Taxonomy,"Catégorisation systématique des méthodes de régularisation pour l'apprentissage profond, révélant leurs similitudes."
"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other.In our work we present a novel, systematic, unifying taxonomy to categorize existing methods.We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures.We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on.We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories.This helps revealing links and fundamental similarities between them.Finally, we include practical recommendations both for users and for developers of new regularization methods.","[0, 0, 0, 0, 0, 0, 1]",[],SkHkeixAW,Regularization for Deep Learning: A Taxonomy,Tente de construire une taxonomie pour les techniques de régularisation employées dans l'apprentissage profond.
"Deep neural networks are surprisingly efficient at solving practical tasks,but the theory behind this phenomenon is only starting to catch up withthe practice.Numerous works show that depth is the key to this efficiency.A certain class of deep convolutional networks – namely those that correspondto the Hierarchical Tucker (HT) tensor decomposition – has beenproven to have exponentially higher expressive power than shallow networks.I.e. a shallow network of exponential width is required to realizethe same score function as computed by the deep architecture.In this paper,we prove the expressive power theorem (an exponential lower bound onthe width of the equivalent shallow network) for a class of recurrent neuralnetworks – ones that correspond to the Tensor Train (TT) decomposition.This means that even processing an image patch by patch with an RNNcan be exponentially more efficient than a (shallow) convolutional networkwith one hidden layer.Using theoretical results on the relation betweenthe tensor decompositions we compare expressive powers of the HT- andTT-Networks.We also implement the recurrent TT-Networks and providenumerical evidence of their expressivity.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1WRibb0Z,Expressive power of recurrent neural networks,Nous prouvons l'efficacité exponentielle des réseaux neuronaux de type récurrent par rapport aux réseaux peu profonds.
"Deep neural networks are surprisingly efficient at solving practical tasks,but the theory behind this phenomenon is only starting to catch up withthe practice.Numerous works show that depth is the key to this efficiency.A certain class of deep convolutional networks – namely those that correspondto the Hierarchical Tucker (HT) tensor decomposition – has beenproven to have exponentially higher expressive power than shallow networks.I.e. a shallow network of exponential width is required to realizethe same score function as computed by the deep architecture.In this paper,we prove the expressive power theorem (an exponential lower bound onthe width of the equivalent shallow network) for a class of recurrent neuralnetworks – ones that correspond to the Tensor Train (TT) decomposition.This means that even processing an image patch by patch with an RNNcan be exponentially more efficient than a (shallow) convolutional networkwith one hidden layer.Using theoretical results on the relation betweenthe tensor decompositions we compare expressive powers of the HT- andTT-Networks.We also implement the recurrent TT-Networks and providenumerical evidence of their expressivity.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S1WRibb0Z,Expressive power of recurrent neural networks,Les auteurs comparent la complexité des réseaux à train tensoriel avec les réseaux structurés par décomposition CP
"Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN).In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior.Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference.Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution.Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.","[0, 1, 0, 0, 0]",[],H1l7bnR5Ym,ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees,Un nouveau traitement probabiliste pour le GAN avec une garantie théorique.
"Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN).In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior.Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference.Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution.Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.","[0, 1, 0, 0, 0]",[],H1l7bnR5Ym,ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees,Cet article propose un GAN bayésien qui a des garanties théoriques de convergence vers la distribution réelle et met les vraisemblances sur le générateur et le discriminateur avec des logarithmes proportionnels aux fonctions objectives traditionnelles du GAN.
"In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\bfx$ that $F$ classifies correctly, and applies a \emph{small perturbation} to  $\bfx$ to produce another point $\bfx'$ that $F$ classifies \emph{incorrectly}. In this paper, we propose taking into account \emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its prediction?). Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \emph{confident regions of a good model should be well separated}.We give formalizations of this property and examine existing robust training objectives in view of them.Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence.However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions.We thus propose embedding objectives and algorithms, and perform an empirical study using this method.Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Hk-FlMbAZ,The Manifold Assumption and Defenses Against Adversarial Perturbations,Défense contre les perturbations adverses des réseaux neuronaux à partir de l'hypothèse du collecteur 
"In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\bfx$ that $F$ classifies correctly, and applies a \emph{small perturbation} to  $\bfx$ to produce another point $\bfx'$ that $F$ classifies \emph{incorrectly}. In this paper, we propose taking into account \emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its prediction?). Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \emph{confident regions of a good model should be well separated}.We give formalizations of this property and examine existing robust training objectives in view of them.Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence.However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions.We thus propose embedding objectives and algorithms, and perform an empirical study using this method.Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Hk-FlMbAZ,The Manifold Assumption and Defenses Against Adversarial Perturbations,Le manuscrit propose deux fonctions objectives basées sur l'hypothèse du collecteur comme mécanismes de défense contre les exemples contradictoires.
"In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\bfx$ that $F$ classifies correctly, and applies a \emph{small perturbation} to  $\bfx$ to produce another point $\bfx'$ that $F$ classifies \emph{incorrectly}. In this paper, we propose taking into account \emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its prediction?). Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \emph{confident regions of a good model should be well separated}.We give formalizations of this property and examine existing robust training objectives in view of them.Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence.However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions.We thus propose embedding objectives and algorithms, and perform an empirical study using this method.Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Hk-FlMbAZ,The Manifold Assumption and Defenses Against Adversarial Perturbations,Défense contre les attaques adverses basées sur l'hypothèse du manifold des données naturelles
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.In DSO-NAS, we provide a novel model pruning view to NAS problem.In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.Next, we impose sparse regularizations to prune useless connections in the architecture.Lastly, we derive an efficient and theoretically sound optimization method to solve it.Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,recherche d'une architecture neuronale à un seul coup via l'optimisation parcimonieuse directe
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.In DSO-NAS, we provide a novel model pruning view to NAS problem.In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.Next, we impose sparse regularizations to prune useless connections in the architecture.Lastly, we derive an efficient and theoretically sound optimization method to solve it.Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,Présente une méthode de recherche d'architecture où les connexions sont supprimées avec une régularisation éparse.
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.In DSO-NAS, we provide a novel model pruning view to NAS problem.In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.Next, we impose sparse regularizations to prune useless connections in the architecture.Lastly, we derive an efficient and theoretically sound optimization method to solve it.Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,"Cet article propose l'optimisation spartiate directe, qui est une méthode permettant d'obtenir des architectures neuronales sur des problèmes spécifiques, à un coût de calcul raisonnable."
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.In DSO-NAS, we provide a novel model pruning view to NAS problem.In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.Next, we impose sparse regularizations to prune useless connections in the architecture.Lastly, we derive an efficient and theoretically sound optimization method to solve it.Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours.","[0, 0, 0, 0, 1, 0, 0, 0]",[],ryxjH3R5KQ,Single Shot Neural Architecture Search Via Direct Sparse Optimization,Cet article propose une méthode de recherche d'une architecture neuronale basée sur une optimisation directe et éparse.
"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning.One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices.This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks.The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels.The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures.We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction.In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.","[0, 0, 0, 0, 0, 0, 0, 1]",[],S1XolQbRW,Model compression via distillation and quantization,Obtient une précision de pointe pour les réseaux quantifiés et peu profonds en tirant parti de la distillation. 
"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning.One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices.This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks.The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels.The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures.We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction.In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.","[0, 0, 0, 0, 0, 0, 0, 1]",[],S1XolQbRW,Model compression via distillation and quantization,propose des modèles de petite taille et peu coûteux en combinant distillation et quantification pour les expériences de vision et de traduction automatique neuronale
"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning.One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices.This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks.The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels.The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures.We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction.In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.","[0, 0, 0, 0, 0, 0, 0, 1]",[],S1XolQbRW,Model compression via distillation and quantization,Cet article présente un cadre d'utilisation du modèle de l'enseignant pour aider à la compression du modèle d'apprentissage profond dans le contexte de la compression de modèle.
"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation.However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful.In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation.We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side.Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","[0, 0, 0, 0, 1]",[],Bkl1uWb0Z,Inducing Grammars with and for Neural Machine Translation,améliorer la NMT avec des arbres latents
"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation.However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful.In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation.We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side.Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","[0, 0, 0, 0, 1]",[],Bkl1uWb0Z,Inducing Grammars with and for Neural Machine Translation,Cet article décrit une méthode permettant d'induire des structures de dépendance côté source au service de la traduction automatique neuronale.
"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards.We explore a method to improve the sample efficiency when we have access to demonstrations.Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task.Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state.Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency.This includes reward shaping, behavioral cloning, and reverse curriculum generation.","[0, 0, 0, 1, 0, 0]",[],H1xk8jAqKQ,Backplay: 'Man muss immer umkehren',"Apprenez en travaillant à rebours à partir d'une seule démonstration, même inefficace, et demandez progressivement à l'agent de faire une plus grande partie de la résolution lui-même."
"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards.We explore a method to improve the sample efficiency when we have access to demonstrations.Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task.Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state.Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency.This includes reward shaping, behavioral cloning, and reverse curriculum generation.","[0, 0, 0, 1, 0, 0]",[],H1xk8jAqKQ,Backplay: 'Man muss immer umkehren',Cet article présente une méthode permettant d'accroître l'efficacité des méthodes RL à récompense clairsemée par un cursus à rebours sur des démonstrations d'experts. 
"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards.We explore a method to improve the sample efficiency when we have access to demonstrations.Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task.Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state.Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency.This includes reward shaping, behavioral cloning, and reverse curriculum generation.","[0, 0, 0, 1, 0, 0]",[],H1xk8jAqKQ,Backplay: 'Man muss immer umkehren',L'article présente une stratégie pour résoudre des tâches de récompense éparses avec RL en échantillonnant les états initiaux à partir de démonstrations.
"Episodic memory is a psychology term which refers to the ability to recall specific events from the past.We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful.Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states.The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on.Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory.Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.","[0, 0, 0, 0, 0, 1]",[],ByJDAIe0b,Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,Mémoire externe pour l'apprentissage par renforcement en ligne basée sur l'estimation des gradients sur une nouvelle technique d'échantillonnage des réservoirs.
"Episodic memory is a psychology term which refers to the ability to recall specific events from the past.We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful.Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states.The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on.Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory.Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.","[0, 0, 0, 0, 0, 1]",[],ByJDAIe0b,Integrating Episodic Memory into a Reinforcement Learning Agent Using Reservoir Sampling,"L'article propose une approche modifiée de la RL, où une ""mémoire épisodique"" supplémentaire est conservée par l'agent et utilise un ""réseau d'interrogation"" qui se base sur l'état actuel."
"We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases.Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.","[1, 0, 0]",[],rkMt1bWAZ,Bias-Variance Decomposition for Boltzmann Machines,Nous réalisons une décomposition biais-variance pour les machines de Boltzmann en utilisant une formulation géométrique de l'information.
"We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases.Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.","[1, 0, 0]",[],rkMt1bWAZ,Bias-Variance Decomposition for Boltzmann Machines,"L'objectif de cet article est d'analyser l'efficacité et la généralisation de l'apprentissage profond en présentant une analyse théorique de la décomposition biais-variance pour les modèles hiérarchiques, en particulier les machines de Boltzmann.  "
"We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases.Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation.","[1, 0, 0]",[],rkMt1bWAZ,Bias-Variance Decomposition for Boltzmann Machines,L'article arrive à la conclusion principale qu'il est possible de réduire à la fois le biais et la variance dans un modèle hiérarchique.
"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.  Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.  We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","[0, 1, 0, 0, 0, 0]",[],HkxF5RgC-,Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,"Combinaison de l'élagage du réseau et des noyaux persistants dans une mise en œuvre pratique, rapide et précise du réseau."
"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.  Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.  We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","[0, 1, 0, 0, 0, 0]",[],HkxF5RgC-,Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip,"Cet article présente les RNN persistants épars, un mécanisme permettant d'ajouter l'élagage aux travaux existants consistant à stocker les poids des RNN sur une puce."
"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy.Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources.Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists.In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed.In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm.The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process.Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate.Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","[0, 0, 0, 1, 0, 0, 0, 0]",[],S1D8MPxA-,Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,Nous présentons une nouvelle méthode d'élagage et un format de matrice clairsemé pour permettre un taux de compression d'index élevé et un processus de décodage d'index parallèle.
"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy.Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources.Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists.In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed.In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm.The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process.Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate.Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","[0, 0, 0, 1, 0, 0, 0, 0]",[],S1D8MPxA-,Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio,"Les auteurs utilisent le codage de Viterbi pour compresser de façon spectaculaire l'index de la matrice éparse d'un réseau élagué, réduisant ainsi l'une des principales surcharges de mémoire et accélérant l'inférence dans le cadre parallèle."
"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL).It is also a requirement for its deployment in real-world scenarios.This paper proposes a novel framework for efficient multi-task reinforcement learning.Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.This enables agents to continually acquire new skills during different stages of training.Each learned task corresponds to a human language description.Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices.In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills.We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJJQVZW0b,Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,"Un nouveau réseau de politiques hiérarchiques qui peut réutiliser des compétences précédemment acquises en même temps que de nouvelles compétences et en tant que sous-composantes de celles-ci, en découvrant les relations sous-jacentes entre les compétences."
"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL).It is also a requirement for its deployment in real-world scenarios.This paper proposes a novel framework for efficient multi-task reinforcement learning.Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.This enables agents to continually acquire new skills during different stages of training.Each learned task corresponds to a human language description.Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices.In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills.We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJJQVZW0b,Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,Cet article vise à apprendre des politiques hiérarchiques en utilisant une structure de politique récursive régulée par une grammaire temporelle stochastique.
"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL).It is also a requirement for its deployment in real-world scenarios.This paper proposes a novel framework for efficient multi-task reinforcement learning.Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.This enables agents to continually acquire new skills during different stages of training.Each learned task corresponds to a human language description.Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices.In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills.We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],SJJQVZW0b,Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning,"Cet article propose une approche de l'apprentissage des politiques hiérarchiques dans un contexte d'apprentissage tout au long de la vie en empilant les politiques et en utilisant ensuite une politique explicite de ""basculement""."
"Embeddings are a fundamental component of many modern machine learning and natural language processing models.Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study.The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.","[0, 0, 0, 1, 0, 0, 0]",[],Skz3Q2CcFX,Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae,Nous proposons d'utiliser la projection de formules algébriques vectorielles explicites comme moyen alternatif de visualiser les espaces d'encastrement spécifiquement adaptés aux tâches d'analyse orientées vers un but précis et elle surpasse t-SNE dans notre étude auprès des utilisateurs.
"Embeddings are a fundamental component of many modern machine learning and natural language processing models.Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study.The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.","[0, 0, 0, 1, 0, 0, 0]",[],Skz3Q2CcFX,Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae,Analyse des psaces d'encastrement de manière non paramétrique (basée sur des exemples)
"Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample.In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.","[0, 0, 0, 1, 0, 0, 0]",[],SyG1QnRqF7,Towards Resisting Large Data Variations via Introspective Learning,Nous proposons une approche fondée sur des principes qui confère aux classificateurs la capacité de résister à des variations plus importantes entre les données d'entraînement et les données d'essai de manière intelligente et efficace.
"Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample.In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.","[0, 0, 0, 1, 0, 0, 0]",[],SyG1QnRqF7,Towards Resisting Large Data Variations via Introspective Learning,Utilisation de l'apprentissage introspectif pour gérer les variations de données au moment du test
"Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample.In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.","[0, 0, 0, 1, 0, 0, 0]",[],SyG1QnRqF7,Towards Resisting Large Data Variations via Introspective Learning,"Cet article suggère l'utilisation de réseaux de transformation appris, intégrés dans des réseaux introspectifs pour améliorer les performances de classification avec des exemples synthétisés."
"It is well known that it is possible to construct ""adversarial examples""for neural networks: inputs which are misclassified by the networkyet indistinguishable from true data.We propose a simplemodification to standard neural network architectures, thermometerencoding, which significantly increases the robustness of the network toadversarial examples.We demonstrate this robustness with experimentson the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show thatmodels with thermometer-encoded inputs consistently have higher accuracyon adversarial examples, without decreasing generalization.State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.We explore the properties of these networks, providing evidencethat thermometer encodings help neural networks tofind more-non-linear decision boundaries.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S18Su--CW,Thermometer Encoding: One Hot Way To Resist Adversarial Examples,La discrétisation de l'entrée conduit à la robustesse contre les exemples adverses.
"It is well known that it is possible to construct ""adversarial examples""for neural networks: inputs which are misclassified by the networkyet indistinguishable from true data.We propose a simplemodification to standard neural network architectures, thermometerencoding, which significantly increases the robustness of the network toadversarial examples.We demonstrate this robustness with experimentson the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show thatmodels with thermometer-encoded inputs consistently have higher accuracyon adversarial examples, without decreasing generalization.State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.We explore the properties of these networks, providing evidencethat thermometer encodings help neural networks tofind more-non-linear decision boundaries.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],S18Su--CW,Thermometer Encoding: One Hot Way To Resist Adversarial Examples,Les auteurs présentent une étude approfondie de la discrétisation/quantification de l'entrée comme défense contre les exemples adverses.
"Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.","[0, 0, 0, 0, 1, 0]",[],ryeX-nC9YQ,Dimension-Free Bounds for Low-Precision Training,nous avons prouvé l'existence de limites indépendantes de la dimension pour les algorithmes d'apprentissage de faible précision.
"Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.","[0, 0, 0, 0, 1, 0]",[],ryeX-nC9YQ,Dimension-Free Bounds for Low-Precision Training,Cet article discute des conditions dans lesquelles la convergence des modèles d'entraînement avec des poids de faible précision ne dépend pas de la dimension du modèle.
We consider the problem of exploration in meta reinforcement learning.Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2.Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments.We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.,"[0, 0, 0, 1]",[],Skk3Jm96W,Some Considerations on Learning to Explore via Meta-Reinforcement Learning,Modifications du MAML et du RL2 qui devraient permettre une meilleure exploration. 
We consider the problem of exploration in meta reinforcement learning.Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2.Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments.We show E-MAML and ERL2 deliver better performance on tasks where exploration is important.,"[0, 0, 0, 1]",[],Skk3Jm96W,Some Considerations on Learning to Explore via Meta-Reinforcement Learning,"L'article propose une astuce pour étendre les fonctions objectives afin de piloter l'exploration dans le méta-RL, en plus de deux algorithmes récents de méta-RL"
"We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs.The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions.We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively.Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work.","[1, 0, 0, 0]",[],ryxhB3CcK7,Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering,"Un modèle symbolique neuronal probabiliste avec un espace de programme latent, pour des réponses aux questions plus interprétables"
"We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs.The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions.We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively.Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work.","[1, 0, 0, 0]",[],ryxhB3CcK7,Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering,Cet article propose un modèle de variable latente discrète et structurée pour la réponse à des questions visuelles qui implique une généralisation et un raisonnement compositionnels avec un gain significatif de performance et de capacité.
"The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks.We propose to address this difficulty through formal verification techniques.We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.We use this technique to assess recently suggested attack and defense techniques.","[0, 0, 0, 1, 0, 0, 0]",[],Hki-ZlbA-,Ground-Truth Adversarial Examples,Nous utilisons la vérification formelle pour évaluer l'efficacité des techniques de recherche d'exemples contradictoires ou de défense contre les exemples contradictoires.
"The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks.We propose to address this difficulty through formal verification techniques.We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.We use this technique to assess recently suggested attack and defense techniques.","[0, 0, 0, 1, 0, 0, 0]",[],Hki-ZlbA-,Ground-Truth Adversarial Examples,Cet article propose une méthode pour calculer des exemples contradictoires avec une distance minimale par rapport aux entrées originales.
"The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks.We propose to address this difficulty through formal verification techniques.We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.We use this technique to assess recently suggested attack and defense techniques.","[0, 0, 0, 1, 0, 0, 0]",[],Hki-ZlbA-,Ground-Truth Adversarial Examples,Les auteurs proposent d'utiliser des exemples à distance minimale prouvée comme outil pour évaluer la robustesse d'un réseau entraîné.
"The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks.We propose to address this difficulty through formal verification techniques.We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.We use this technique to assess recently suggested attack and defense techniques.","[0, 0, 0, 1, 0, 0, 0]",[],Hki-ZlbA-,Ground-Truth Adversarial Examples,L'article décrit une méthode pour générer des exemples contradictoires qui ont une distance minimale par rapport à l'exemple d'entraînement utilisé pour les générer.
"This paper introduces a new framework for open-domain question answering in which the retriever and the reader \emph{iteratively interact} with each other.The framework is agnostic to the architecture of the machine reading model provided it has \emph{access} to the token-level hidden representations of the reader.The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs.A gated recurrent unit updates the query at each step conditioned on the \emph{state} of the reader and the \emph{reformulated} query is used to re-rank the paragraphs by the retriever.We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\drqa and \bidaf) on various large open-domain datasets ---\tqau, \quasart, \searchqa, and \squado\footnote{Code and pretrained models are available at \url{https://github.com/rajarshd/Multi-Step-Reasoning}}.","[1, 0, 0, 0, 0, 0]",[],HkfPSh05K7,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,"Le récupérateur de paragraphes et le lecteur automatique interagissent grâce à l'apprentissage par renforcement, ce qui permet d'obtenir de grandes améliorations sur des ensembles de données en domaine ouvert."
"This paper introduces a new framework for open-domain question answering in which the retriever and the reader \emph{iteratively interact} with each other.The framework is agnostic to the architecture of the machine reading model provided it has \emph{access} to the token-level hidden representations of the reader.The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs.A gated recurrent unit updates the query at each step conditioned on the \emph{state} of the reader and the \emph{reformulated} query is used to re-rank the paragraphs by the retriever.We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\drqa and \bidaf) on various large open-domain datasets ---\tqau, \quasart, \searchqa, and \squado\footnote{Code and pretrained models are available at \url{https://github.com/rajarshd/Multi-Step-Reasoning}}.","[1, 0, 0, 0, 0, 0]",[],HkfPSh05K7,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,"L'article présente un nouveau cadre d'interaction bidirectionnelle entre le récupérateur de documents et le lecteur pour répondre à des questions dans un domaine ouvert, avec l'idée d'un ""état du lecteur"" du lecteur au récupérateur."
"This paper introduces a new framework for open-domain question answering in which the retriever and the reader \emph{iteratively interact} with each other.The framework is agnostic to the architecture of the machine reading model provided it has \emph{access} to the token-level hidden representations of the reader.The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs.A gated recurrent unit updates the query at each step conditioned on the \emph{state} of the reader and the \emph{reformulated} query is used to re-rank the paragraphs by the retriever.We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\drqa and \bidaf) on various large open-domain datasets ---\tqau, \quasart, \searchqa, and \squado\footnote{Code and pretrained models are available at \url{https://github.com/rajarshd/Multi-Step-Reasoning}}.","[1, 0, 0, 0, 0, 0]",[],HkfPSh05K7,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,L'article propose un modèle de lecture automatique extractive multi-documents composé de 3 parties distinctes et d'un algorithme.
"Many imaging tasks require global information about all pixels in an image.Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output.But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs.To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost.This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution.SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images.SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.","[0, 0, 0, 0, 0, 1, 0]",[],BJgFcj0qKX,Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,Présente une nouvelle architecture qui exploite le pouvoir de globalisation de l'information des u-nets dans des réseaux plus profonds et qui donne de bons résultats dans toutes les tâches sans aucun artifice.
"Many imaging tasks require global information about all pixels in an image.Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output.But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs.To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost.This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution.SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images.SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.","[0, 0, 0, 0, 0, 1, 0]",[],BJgFcj0qKX,Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,"Une architecture de réseau pour la segmentation sémantique d'images, basée sur la composition d'une pile d'architectures U-Net de base, qui réduit le nombre de paramètres et améliore les résultats."
"Many imaging tasks require global information about all pixels in an image.Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output.But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs.To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost.This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution.SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images.SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.","[0, 0, 0, 0, 0, 1, 0]",[],BJgFcj0qKX,Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation,Cette étude propose une architecture U-Net empilée pour la segmentation d'images.
"Asking questions is an important ability for a chatbot.This paper focuses on question generation.Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem.In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics.One almost never asks a random question in a conversation.Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use.To solve the problem, we propose a novel neural network that is able to generate topic-specific questions.One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers.Experimental results show that our model outperforms the state-of-the-art baseline.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rk3pnae0b,Topic-Based Question Generation,Nous proposons un réseau neuronal capable de générer des questions spécifiques à un sujet.
"Asking questions is an important ability for a chatbot.This paper focuses on question generation.Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem.In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics.One almost never asks a random question in a conversation.Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use.To solve the problem, we propose a novel neural network that is able to generate topic-specific questions.One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers.Experimental results show that our model outperforms the state-of-the-art baseline.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rk3pnae0b,Topic-Based Question Generation,"Présente une approche basée sur un réseau neuronal pour générer des questions spécifiques à un sujet, en partant du principe que les questions thématiques sont plus significatives dans les applications pratiques."
"Asking questions is an important ability for a chatbot.This paper focuses on question generation.Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem.In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics.One almost never asks a random question in a conversation.Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use.To solve the problem, we propose a novel neural network that is able to generate topic-specific questions.One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers.Experimental results show that our model outperforms the state-of-the-art baseline.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rk3pnae0b,Topic-Based Question Generation,propose une méthode de génération basée sur les thèmes en utilisant un LSTM pour extraire les thèmes en utilisant une technique de codage en deux étapes.
"Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable optionto restore voluntary movements after paralysis.These devices are based on theability to extract information about movement intent from neural signals recordedusing multi-electrode arrays chronically implanted in the motor cortices of thebrain.However, the inherent loss and turnover of recorded neurons requires repeatedrecalibrations of the interface, which can potentially alter the day-to-dayuser experience.The resulting need for continued user adaptation interferes withthe natural, subconscious use of the BMI.Here, we introduce a new computationalapproach that decodes movement intent from a low-dimensional latent representationof the neural data.We implement various domain adaptation methodsto stabilize the interface over significantly long times.This includes CanonicalCorrelation Analysis used to align the latent variables across days; this methodrequires prior point-to-point correspondence of the time series across domains.Alternatively, we match the empirical probability distributions of the latent variablesacross days through the minimization of their Kullback-Leibler divergence.These two methods provide a significant and comparable improvement in the performanceof the interface.However, implementation of an Adversarial DomainAdaptation Network trained to match the empirical probability distribution of theresiduals of the reconstructed neural signals outperforms the two methods basedon latent variables, while requiring remarkably few data points to solve the domainadaptation problem.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hyx6Bi0qYm,Adversarial Domain Adaptation for Stable Brain-Machine Interfaces,Nous mettons en œuvre un réseau d'adaptation de domaine contradictoire pour stabiliser une interface cerveau-machine fixe contre les changements progressifs des signaux neuronaux enregistrés.
"Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable optionto restore voluntary movements after paralysis.These devices are based on theability to extract information about movement intent from neural signals recordedusing multi-electrode arrays chronically implanted in the motor cortices of thebrain.However, the inherent loss and turnover of recorded neurons requires repeatedrecalibrations of the interface, which can potentially alter the day-to-dayuser experience.The resulting need for continued user adaptation interferes withthe natural, subconscious use of the BMI.Here, we introduce a new computationalapproach that decodes movement intent from a low-dimensional latent representationof the neural data.We implement various domain adaptation methodsto stabilize the interface over significantly long times.This includes CanonicalCorrelation Analysis used to align the latent variables across days; this methodrequires prior point-to-point correspondence of the time series across domains.Alternatively, we match the empirical probability distributions of the latent variablesacross days through the minimization of their Kullback-Leibler divergence.These two methods provide a significant and comparable improvement in the performanceof the interface.However, implementation of an Adversarial DomainAdaptation Network trained to match the empirical probability distribution of theresiduals of the reconstructed neural signals outperforms the two methods basedon latent variables, while requiring remarkably few data points to solve the domainadaptation problem.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hyx6Bi0qYm,Adversarial Domain Adaptation for Stable Brain-Machine Interfaces,Décrit une nouvelle approche pour l'interface cerveau-machine implantée afin de résoudre les problèmes de calibration et de décalage des covariables. 
"Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable optionto restore voluntary movements after paralysis.These devices are based on theability to extract information about movement intent from neural signals recordedusing multi-electrode arrays chronically implanted in the motor cortices of thebrain.However, the inherent loss and turnover of recorded neurons requires repeatedrecalibrations of the interface, which can potentially alter the day-to-dayuser experience.The resulting need for continued user adaptation interferes withthe natural, subconscious use of the BMI.Here, we introduce a new computationalapproach that decodes movement intent from a low-dimensional latent representationof the neural data.We implement various domain adaptation methodsto stabilize the interface over significantly long times.This includes CanonicalCorrelation Analysis used to align the latent variables across days; this methodrequires prior point-to-point correspondence of the time series across domains.Alternatively, we match the empirical probability distributions of the latent variablesacross days through the minimization of their Kullback-Leibler divergence.These two methods provide a significant and comparable improvement in the performanceof the interface.However, implementation of an Adversarial DomainAdaptation Network trained to match the empirical probability distribution of theresiduals of the reconstructed neural signals outperforms the two methods basedon latent variables, while requiring remarkably few data points to solve the domainadaptation problem.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],Hyx6Bi0qYm,Adversarial Domain Adaptation for Stable Brain-Machine Interfaces,Les auteurs définissent un IMC qui utilise un auto-codeur et abordent ensuite le problème de la dérive des données dans l'IMC.
"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds.To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words.While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so.Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions.For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world.We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge.We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","[0, 0, 0, 0, 0, 0, 1]",[],ByZmGjkA-,Understanding Grounded Language Learning Agents,Analyser et comprendre comment les agents des réseaux neuronaux apprennent à comprendre un langage simple et ancré dans le sol.
"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds.To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words.While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so.Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions.For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world.We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge.We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","[0, 0, 0, 0, 0, 0, 1]",[],ByZmGjkA-,Understanding Grounded Language Learning Agents,Les auteurs relient les méthodes expérimentales psychologiques à la compréhension de la façon dont la boîte noire des méthodes d'apprentissage profond résout les problèmes.
"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds.To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words.While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so.Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions.For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world.We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge.We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","[0, 0, 0, 0, 0, 0, 1]",[],ByZmGjkA-,Understanding Grounded Language Learning Agents,Cet article présente une analyse des agents qui apprennent le langage de base par apprentissage par renforcement dans un environnement simple qui combine des instructions verbales et des informations visuelles.
"Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future.In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos.Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future.Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.","[1, 0, 0, 0]",[],rJe10iC5K7,"Unsupervised Discovery of Parts, Structure, and Dynamics","Apprendre les parties d'un objet, sa structure hiérarchique et sa dynamique en observant son mouvement."
"Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future.In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos.Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future.Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.","[1, 0, 0, 0]",[],rJe10iC5K7,"Unsupervised Discovery of Parts, Structure, and Dynamics","Propose un modèle d'apprentissage non supervisé qui apprend à démêler les objets en parties, à prédire la structure hiérarchique des parties et, sur la base des parties démêlées et de la hiérarchie, à prédire le mouvement."
"A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR).Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality.However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them.In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks.We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs.As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks.Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.","[0, 0, 0, 0, 1, 0, 0]",[],BkgGmh09FQ,Understanding Opportunities for Efficiency in Single-image Super Resolution Networks,Nous développons une compréhension des techniques efficaces en termes de ressources sur la super-résolution.
"A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR).Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality.However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them.In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks.We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs.As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks.Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.","[0, 0, 0, 0, 1, 0, 0]",[],BkgGmh09FQ,Understanding Opportunities for Efficiency in Single-image Super Resolution Networks,L'article propose une évaluation empirique détaillée des compromis obtenus par différents réseaux neuronaux convolutifs sur le problème de la super résolution.
"A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR).Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality.However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them.In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks.We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs.As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks.Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR.","[0, 0, 0, 0, 1, 0, 0]",[],BkgGmh09FQ,Understanding Opportunities for Efficiency in Single-image Super Resolution Networks,Cet article propose d'améliorer l'efficacité des ressources du système pour les réseaux à super résolution.
"Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy.In this work we present properties of neural networks that complement this aspect of expressivity.By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior.Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples.We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior.Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.","[0, 0, 0, 1, 0, 0]",[],r1gR2sC9FX,On the Spectral Bias of Neural Networks,Nous étudions les réseaux ReLU dans le domaine de Fourier et démontrons un comportement particulier.
"Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy.In this work we present properties of neural networks that complement this aspect of expressivity.By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior.Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples.We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior.Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.","[0, 0, 0, 1, 0, 0]",[],r1gR2sC9FX,On the Spectral Bias of Neural Networks,"Analyse de Fourier du réseau ReLU, qui montre qu'il est biaisé vers l'apprentissage des basses fréquences. "
"Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy.In this work we present properties of neural networks that complement this aspect of expressivity.By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior.Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples.We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior.Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.","[0, 0, 0, 1, 0, 0]",[],r1gR2sC9FX,On the Spectral Bias of Neural Networks,Cet article présente des contributions théoriques et empiriques sur le thème des coefficients de Fourier des réseaux neuronaux.
"Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering.Many metric learning methods represent the input as a single point in the embedding space.Often the distance between points is used as a proxy for match confidence.However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness.This work addresses this issue and explicitly models the uncertainty by “hedging” the location of each input in the embedding space.We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018).Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of “hedging its bets” across the embedding space upon encountering ambiguous inputs.This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.","[0, 0, 1, 0, 0, 0, 0, 0]",[],r1xQQhAqKX,Modeling Uncertainty with Hedged Instance Embeddings,L'article propose d'utiliser des distributions de probabilité au lieu de points pour les tâches d'intégration d'instances telles que la reconnaissance et la vérification.
"Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering.Many metric learning methods represent the input as a single point in the embedding space.Often the distance between points is used as a proxy for match confidence.However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness.This work addresses this issue and explicitly models the uncertainty by “hedging” the location of each input in the embedding space.We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018).Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of “hedging its bets” across the embedding space upon encountering ambiguous inputs.This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.","[0, 0, 1, 0, 0, 0, 0, 0]",[],r1xQQhAqKX,Modeling Uncertainty with Hedged Instance Embeddings,L'article propose une alternative à l'incorporation de points actuelle et une technique pour les former.
"Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering.Many metric learning methods represent the input as a single point in the embedding space.Often the distance between points is used as a proxy for match confidence.However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness.This work addresses this issue and explicitly models the uncertainty by “hedging” the location of each input in the embedding space.We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018).Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of “hedging its bets” across the embedding space upon encountering ambiguous inputs.This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance.","[0, 0, 1, 0, 0, 0, 0, 0]",[],r1xQQhAqKX,Modeling Uncertainty with Hedged Instance Embeddings,L'article propose un modèle utilisant des inscriptions incertaines pour étendre l'apprentissage profond aux applications bayésiennes.
"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks.Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. We present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation.By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S16FPMgRZ,Tensor Contraction & Regression Networks,"Nous proposons des couches de contraction tensorielle et de régression tensorielle à faible rang afin de préserver et d'exploiter la structure multi-linéaire dans l'ensemble du réseau, ce qui permet de réaliser d'énormes économies d'espace avec peu ou pas d'impact sur les performances."
"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks.Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. We present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation.By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S16FPMgRZ,Tensor Contraction & Regression Networks,Cet article propose de nouvelles architectures de couches de réseaux neuronaux utilisant une représentation à faible rang des tenseurs.
"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks.Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. We present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation.By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],S16FPMgRZ,Tensor Contraction & Regression Networks,Cet article intègre la décomposition tensorielle et la régression tensorielle dans le CNN en utilisant une nouvelle couche de régression tensorielle.
"We explore ways of incorporating bilingual dictionaries to enable semi-supervisedneural machine translation.Conventional back-translation methods have shownsuccess in leveraging target side monolingual data.However, since the quality ofback-translation models is tied to the size of the available parallel corpora, thiscould adversely impact the synthetically generated sentences in a low resourcesetting.We propose a simple data augmentation technique to address both thisshortcoming.We incorporate widely available bilingual dictionaries that yieldword-by-word translations to generate synthetic sentences.This automaticallyexpands the vocabulary of the model while maintaining high quality content.Ourmethod shows an appreciable improvement in performance over strong baselines.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1ecYsqSuN,INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION,Nous utilisons des dictionnaires bilingues pour augmenter les données de la traduction automatique neuronale.
"We explore ways of incorporating bilingual dictionaries to enable semi-supervisedneural machine translation.Conventional back-translation methods have shownsuccess in leveraging target side monolingual data.However, since the quality ofback-translation models is tied to the size of the available parallel corpora, thiscould adversely impact the synthetically generated sentences in a low resourcesetting.We propose a simple data augmentation technique to address both thisshortcoming.We incorporate widely available bilingual dictionaries that yieldword-by-word translations to generate synthetic sentences.This automaticallyexpands the vocabulary of the model while maintaining high quality content.Ourmethod shows an appreciable improvement in performance over strong baselines.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],B1ecYsqSuN,INCORPORATING BILINGUAL DICTIONARIES FOR LOW RESOURCE SEMI-SUPERVISED NEURAL MACHINE TRANSLATION,Cet article étudie l'utilisation de dictionnaires bilingues pour créer des sources synthétiques de données monolingues côté cible afin d'améliorer les modèles NMT entraînés avec de petites quantités de données parallèles.
"Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity.One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning.In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus.Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward.We propose a new curiosity method which uses episodic memory to form the novelty bonus.To determine the bonus, the current observation is compared with the observations in memory.Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics.This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences.We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo.In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM.In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.The code is available at https://github.com/google-research/episodic-curiosity/.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SkeK3s0qKQ,Episodic Curiosity through Reachability,"Nous proposons un nouveau modèle de curiosité basé sur la mémoire épisodique et les idées d'atteignabilité qui nous permet de surmonter les problèmes connus de ""couch-potato"" des travaux antérieurs."
"Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity.One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning.In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus.Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward.We propose a new curiosity method which uses episodic memory to form the novelty bonus.To determine the bonus, the current observation is compared with the observations in memory.Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics.This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences.We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo.In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM.In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.The code is available at https://github.com/google-research/episodic-curiosity/.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SkeK3s0qKQ,Episodic Curiosity through Reachability,Propose de donner des bonus d'exploration dans les algorithmes RL en donnant des bonus plus importants aux observations qui sont plus éloignées dans les étapes de l'environnement.
"Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity.One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning.In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus.Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward.We propose a new curiosity method which uses episodic memory to form the novelty bonus.To determine the bonus, the current observation is compared with the observations in memory.Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics.This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences.We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo.In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM.In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.The code is available at https://github.com/google-research/episodic-curiosity/.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],SkeK3s0qKQ,Episodic Curiosity through Reachability,Les auteurs proposent un bonus d'exploration destiné à faciliter les problèmes de récompense éparse en RL et envisagent de nombreuses expériences sur des environnements 3D complexes.
"We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task.We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''.Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.","[1, 0, 0]",[],SkZxCk-0Z,Can Neural Networks Understand Logical Entailment?,Nous introduisons un nouveau jeu de données d'implications logiques dans le but de mesurer la capacité des modèles à capturer et à exploiter la structure des expressions logiques dans le cadre d'une tâche de prédiction d'implication.
"We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task.We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''.Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.","[1, 0, 0]",[],SkZxCk-0Z,Can Neural Networks Understand Logical Entailment?,L'article propose un nouveau modèle pour utiliser des modèles profonds afin de détecter l'implication logique comme un produit de fonctions continues sur des mondes possibles.
"We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task.We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''.Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.","[1, 0, 0]",[],SkZxCk-0Z,Can Neural Networks Understand Logical Entailment?,Propose un nouveau modèle conçu pour l'apprentissage automatique avec prédiction de l'implication logique.
"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.Also, previously seen training samples may not be available at the time of retraining.We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.We evaluated the proposed scheme on several recognition applications.The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","[1, 0, 0, 0, 0, 0, 0, 0]",[],Hy-lXyDWG,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,L'article porte sur une nouvelle méthode d'apprentissage incrémental efficace sur le plan énergétique.
"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.Also, previously seen training samples may not be available at the time of retraining.We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.We evaluated the proposed scheme on several recognition applications.The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","[1, 0, 0, 0, 0, 0, 0, 0]",[],Hy-lXyDWG,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,Propose une procédure d'apprentissage incrémental comme apprentissage par transfert.
"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.Also, previously seen training samples may not be available at the time of retraining.We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.We evaluated the proposed scheme on several recognition applications.The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","[1, 0, 0, 0, 0, 0, 0, 0]",[],Hy-lXyDWG,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,"L'article présente une méthode pour former des réseaux neuronaux convolutifs profonds de manière incrémentielle, dans laquelle les données sont disponibles par petits lots sur une période donnée."
"Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.Also, previously seen training samples may not be available at the time of retraining.We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.We evaluated the proposed scheme on several recognition applications.The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing).","[1, 0, 0, 0, 0, 0, 0, 0]",[],Hy-lXyDWG,Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing,Présente une approche de l'apprentissage incrémental par classe à l'aide de réseaux profonds en proposant trois stratégies d'apprentissage différentes dans l'approche finale/meilleure.
"Recurrent neural networks (RNNs) are widely used to model sequential data buttheir non-linear dependencies between sequence elements prevent parallelizingtraining over sequence length.We show the training of RNNs with only linearsequential dependencies can be parallelized over the sequence length using theparallel scan algorithm, leading to rapid training on long sequences even withsmall minibatch size.We develop a parallel linear recurrence CUDA kernel andshow that it can be applied to immediately speed up training and inference ofseveral state of the art RNN architectures by up to 9x. We abstract recent workon linear RNNs into a new framework of linear surrogate RNNs and develop alinear surrogate model for the long short-term memory unit, the GILR-LSTM, thatutilizes parallel linear recurrence. We extend sequence learning to newextremely long sequence regimes that were previously out of reach bysuccessfully training a GILR-LSTM on a synthetic sequence classification taskwith a one million timestep dependency.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyUNwulC-,Parallelizing Linear Recurrent Neural Nets Over Sequence Length,utiliser le balayage parallèle pour paralléliser les réseaux neuronaux récurrents linéaires. entraîner le modèle sur une longueur de 1 million de dépendances
"Recurrent neural networks (RNNs) are widely used to model sequential data buttheir non-linear dependencies between sequence elements prevent parallelizingtraining over sequence length.We show the training of RNNs with only linearsequential dependencies can be parallelized over the sequence length using theparallel scan algorithm, leading to rapid training on long sequences even withsmall minibatch size.We develop a parallel linear recurrence CUDA kernel andshow that it can be applied to immediately speed up training and inference ofseveral state of the art RNN architectures by up to 9x. We abstract recent workon linear RNNs into a new framework of linear surrogate RNNs and develop alinear surrogate model for the long short-term memory unit, the GILR-LSTM, thatutilizes parallel linear recurrence. We extend sequence learning to newextremely long sequence regimes that were previously out of reach bysuccessfully training a GILR-LSTM on a synthetic sequence classification taskwith a one million timestep dependency.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyUNwulC-,Parallelizing Linear Recurrent Neural Nets Over Sequence Length,Propose d'accélérer le RNN en appliquant la méthode de Blelloch.
"Recurrent neural networks (RNNs) are widely used to model sequential data buttheir non-linear dependencies between sequence elements prevent parallelizingtraining over sequence length.We show the training of RNNs with only linearsequential dependencies can be parallelized over the sequence length using theparallel scan algorithm, leading to rapid training on long sequences even withsmall minibatch size.We develop a parallel linear recurrence CUDA kernel andshow that it can be applied to immediately speed up training and inference ofseveral state of the art RNN architectures by up to 9x. We abstract recent workon linear RNNs into a new framework of linear surrogate RNNs and develop alinear surrogate model for the long short-term memory unit, the GILR-LSTM, thatutilizes parallel linear recurrence. We extend sequence learning to newextremely long sequence regimes that were previously out of reach bysuccessfully training a GILR-LSTM on a synthetic sequence classification taskwith a one million timestep dependency.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],HyUNwulC-,Parallelizing Linear Recurrent Neural Nets Over Sequence Length,"Les auteurs proposent un algorithme parallèle pour les RNN de substitution linéaire, qui produit des accélérations par rapport aux implémentations existantes de Quasi-RNN, SRU et LSTM."
"Neural text generation models are often autoregressive language models or seq2seq models.Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks.These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality.Language models are typically trained via maximum likelihood and most often with teacher forcing.Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time.We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation.GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them.We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ByOExmWAb,MaskGAN: Better Text Generation via Filling in the _______,GAN en langage naturel pour remplir les blancs
"Neural text generation models are often autoregressive language models or seq2seq models.Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks.These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality.Language models are typically trained via maximum likelihood and most often with teacher forcing.Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time.We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation.GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them.We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ByOExmWAb,MaskGAN: Better Text Generation via Filling in the _______,Cet article propose de générer du texte à l'aide de GANs.
"Neural text generation models are often autoregressive language models or seq2seq models.Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks.These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality.Language models are typically trained via maximum likelihood and most often with teacher forcing.Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time.We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation.GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them.We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ByOExmWAb,MaskGAN: Better Text Generation via Filling in the _______,Génération d'échantillons de texte à l'aide de GAN et d'un mécanisme permettant de compléter les mots manquants en fonction du texte environnant.
"Parametric texture models have been applied successfully to synthesize artificial images.Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.Here, we implemented a human-vision-inspired novelty detection approach.Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJEOOsCqKm,Psychophysical vs. learnt texture representations in novelty detection,Comparaison des représentations psychophysiques et des représentations de texture codées par CNN dans une application de détection de nouveauté par réseau neuronal à une classe.
"Parametric texture models have been applied successfully to synthesize artificial images.Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.Here, we implemented a human-vision-inspired novelty detection approach.Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJEOOsCqKm,Psychophysical vs. learnt texture representations in novelty detection,Cet article se concentre sur la détection de la nouveauté et montre que les représentations psychophysiques peuvent surpasser les caractéristiques du codeur VGG dans certaines parties de cette tâche.
"Parametric texture models have been applied successfully to synthesize artificial images.Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.Here, we implemented a human-vision-inspired novelty detection approach.Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJEOOsCqKm,Psychophysical vs. learnt texture representations in novelty detection,Cet article considère la détection des anomalies dans les textures et propose une fonction de perte originale.
"Parametric texture models have been applied successfully to synthesize artificial images.Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.Here, we implemented a human-vision-inspired novelty detection approach.Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJEOOsCqKm,Psychophysical vs. learnt texture representations in novelty detection,Propose l'entraînement de deux détecteurs d'anomalies à partir de trois modèles différents pour détecter les anomalies perceptives dans les textures visuelles.
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.","[0, 1, 0, 0]",[],r1kjEuHpZ,Learning Less-Overlapping Representations,"Nous proposons un nouveau type d'approche de régularisation qui encourage le non-recouvrement dans l'apprentissage de la représentation, dans le but d'améliorer l'interprétabilité et de réduire le surajustement."
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.","[0, 1, 0, 0]",[],r1kjEuHpZ,Learning Less-Overlapping Representations,L'article introduit un régularisateur matriciel pour induire simultanément la sparsité et l'orthogonalité approximative.
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.","[0, 1, 0, 0]",[],r1kjEuHpZ,Learning Less-Overlapping Representations,L'article étudie une méthode de régularisation pour promouvoir l'éparpillement et réduire le chevauchement entre les supports des vecteurs de poids dans les représentations apprises afin d'améliorer l'interprétabilité et d'éviter le surajustement.
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.","[0, 1, 0, 0]",[],r1kjEuHpZ,Learning Less-Overlapping Representations,L'article propose une nouvelle approche de régularisation qui encourage simultanément les vecteurs de poids (W) à être clairsemés et orthogonaux les uns par rapport aux autres.
"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.	We prove that learnable gates in a recurrent model formally provide \emph{quasi-invariance to general time transformations} in the input data.We recover part of the LSTM architecture from a simple axiomatic approach.	This result leads to a new way of initializing gate biases in LSTMs and GRUs.Experimentally, this new \emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.","[0, 0, 0, 0, 1, 0]",[],SJcKhk-Ab,Can recurrent neural networks warp time?,Il prouve que les mécanismes de déclenchement sont invariants par rapport aux transformations temporelles. Introduit et teste une nouvelle initialisation pour les LSTMs à partir de cette idée.
"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.	We prove that learnable gates in a recurrent model formally provide \emph{quasi-invariance to general time transformations} in the input data.We recover part of the LSTM architecture from a simple axiomatic approach.	This result leads to a new way of initializing gate biases in LSTMs and GRUs.Experimentally, this new \emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.","[0, 0, 0, 0, 1, 0]",[],SJcKhk-Ab,Can recurrent neural networks warp time?,"L'article établit un lien entre le concept de réseau récurrent et son effet sur la façon dont le réseau réagit aux transformations temporelles, et l'utilise pour développer un schéma simple d'initialisation du biais."
"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}.In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.We call this approach ``learning to teach''.In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJewuJWCZ,Learning to Teach,"Nous proposons et vérifions l'efficacité de l'apprentissage pour enseigner, un nouveau cadre pour guider automatiquement le processus d'apprentissage automatique."
"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}.In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.We call this approach ``learning to teach''.In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJewuJWCZ,Learning to Teach,"Cet article se concentre sur l'""enseignement automatique"" et propose de tirer parti de l'apprentissage par renforcement en définissant la récompense comme la vitesse d'apprentissage de l'apprenant et en utilisant le gradient de politique pour mettre à jour les paramètres de l'enseignant."
"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}.In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.We call this approach ``learning to teach''.In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJewuJWCZ,Learning to Teach,"Les auteurs définissent un modèle d'apprentissage profond composé de quatre éléments : un modèle d'élève, un modèle d'enseignant, une fonction de perte et un ensemble de données. "
"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}.In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.We call this approach ``learning to teach''.In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJewuJWCZ,Learning to Teach,"Suggère un cadre ""apprendre à enseigner"", correspondant à des choix sur les données présentées à l'apprenant."
"We present DL2, a system for training and querying neural networks with logical constraints.The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints.We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.","[1, 0, 0]",[],H1faSn0qY7,DL2: Training and Querying Neural Networks with Logic,Une perte différentiable pour les contraintes logiques pour l'entraînement et l'interrogation des réseaux de neurones.
"We present DL2, a system for training and querying neural networks with logical constraints.The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints.We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.","[1, 0, 0]",[],H1faSn0qY7,DL2: Training and Querying Neural Networks with Logic,Un cadre pour transformer les requêtes sur les paramètres et les paires d'entrée et de sortie des réseaux neuronaux en fonctions de perte différentiables et un langage déclaratif associé pour spécifier ces requêtes.
"We present DL2, a system for training and querying neural networks with logical constraints.The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints.We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets.","[1, 0, 0]",[],H1faSn0qY7,DL2: Training and Querying Neural Networks with Logic,Cet article aborde le problème de la combinaison des approches logiques avec les réseaux neuronaux en traduisant une formule logique en une fonction de perte non négative pour un réseau neuronal.
"Genetic algorithms have been widely used in many practical optimization problems.Inspired by natural selection, operators, including mutation, crossoverand selection, provide effective heuristics for search and black-box optimization.However, they have not been shown useful for deep reinforcement learning, possiblydue to the catastrophic consequence of parameter crossovers of neural networks.Here, we present Genetic Policy Optimization (GPO), a new genetic algorithmfor sample-efficient deep policy optimization.GPO uses imitation learningfor policy crossover in the state space and applies policy gradient methods for mutation.Our experiments on MuJoCo tasks show that GPO as a genetic algorithmis able to provide superior performance over the state-of-the-art policy gradientmethods and achieves comparable or higher sample efficiency.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ByOnmlWC-,Policy Optimization by Genetic Distillation ,Approche basée sur les algorithmes génétiques pour l'optimisation des politiques de réseaux neuronaux profonds
"Genetic algorithms have been widely used in many practical optimization problems.Inspired by natural selection, operators, including mutation, crossoverand selection, provide effective heuristics for search and black-box optimization.However, they have not been shown useful for deep reinforcement learning, possiblydue to the catastrophic consequence of parameter crossovers of neural networks.Here, we present Genetic Policy Optimization (GPO), a new genetic algorithmfor sample-efficient deep policy optimization.GPO uses imitation learningfor policy crossover in the state space and applies policy gradient methods for mutation.Our experiments on MuJoCo tasks show that GPO as a genetic algorithmis able to provide superior performance over the state-of-the-art policy gradientmethods and achieves comparable or higher sample efficiency.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ByOnmlWC-,Policy Optimization by Genetic Distillation ,Les auteurs présentent un algorithme d'entraînement d'ensembles de réseaux de politiques qui mélange régulièrement les différentes politiques de l'ensemble.
"Genetic algorithms have been widely used in many practical optimization problems.Inspired by natural selection, operators, including mutation, crossoverand selection, provide effective heuristics for search and black-box optimization.However, they have not been shown useful for deep reinforcement learning, possiblydue to the catastrophic consequence of parameter crossovers of neural networks.Here, we present Genetic Policy Optimization (GPO), a new genetic algorithmfor sample-efficient deep policy optimization.GPO uses imitation learningfor policy crossover in the state space and applies policy gradient methods for mutation.Our experiments on MuJoCo tasks show that GPO as a genetic algorithmis able to provide superior performance over the state-of-the-art policy gradientmethods and achieves comparable or higher sample efficiency.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],ByOnmlWC-,Policy Optimization by Genetic Distillation ,"Cet article propose une méthode d'optimisation des politiques inspirée des algorithmes génétiques, qui imite les opérateurs de mutation et de croisement sur les réseaux de politiques."
"To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights.One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping.Despite its empirical success, little is understood about why the straight-through gradient method works.Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov’s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method.ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness.For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization.We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.","[0, 1, 0, 0, 0, 0, 0]",[],HyzMyhCcK7,ProxQuant: Quantized Neural Networks via Proximal Operators,"Un cadre de principe pour la quantification de modèles utilisant la méthode du gradient proximal, avec une évaluation empirique et des analyses de convergence théoriques."
"To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights.One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping.Despite its empirical success, little is understood about why the straight-through gradient method works.Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov’s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method.ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness.For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization.We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.","[0, 1, 0, 0, 0, 0, 0]",[],HyzMyhCcK7,ProxQuant: Quantized Neural Networks via Proximal Operators,Propose la méthode ProxQuant pour entraîner les réseaux neuronaux avec des poids quantifiés.
"To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights.One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping.Despite its empirical success, little is understood about why the straight-through gradient method works.Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov’s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method.ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness.For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization.We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting.","[0, 1, 0, 0, 0, 0, 0]",[],HyzMyhCcK7,ProxQuant: Quantized Neural Networks via Proximal Operators,Propose de résoudre les réseaux binaires et ses variantes en utilisant la descente proximale du gradient.
"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions.However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \Omega\(N) units. Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss.We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\tilde{\Omega}(\sqrt{N}), and a more realistic number of d_1=\tilde{\Omega}(N/d_0) hidden units.We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","[1, 0, 0, 0, 0]",[],Hkfmn5n6W,Exponentially vanishing sub-optimal local minima in multilayer neural networks,"Les ""mauvais"" minima locaux disparaissent dans un réseau neuronal multicouche : une preuve avec des hypothèses plus raisonnables qu'auparavant"
"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions.However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \Omega\(N) units. Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss.We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\tilde{\Omega}(\sqrt{N}), and a more realistic number of d_1=\tilde{\Omega}(N/d_0) hidden units.We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","[1, 0, 0, 0, 0]",[],Hkfmn5n6W,Exponentially vanishing sub-optimal local minima in multilayer neural networks,"Dans les réseaux à une seule couche cachée, le volume des minima locaux sous-optimaux diminue de manière exponentielle par rapport aux minima globaux."
"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions.However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \Omega\(N) units. Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss.We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\tilde{\Omega}(\sqrt{N}), and a more realistic number of d_1=\tilde{\Omega}(N/d_0) hidden units.We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","[1, 0, 0, 0, 0]",[],Hkfmn5n6W,Exponentially vanishing sub-optimal local minima in multilayer neural networks,"Cet article vise à répondre à la question de savoir pourquoi les algorithmes standard basés sur le SGD sur les réseaux neuronaux convergent vers de ""bonnes"" solutions."
"Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations.An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications.Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples.In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models.Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples.Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method.Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.","[0, 0, 0, 0, 0, 1, 0, 0]",[],BJzVUj0qtQ,Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift,"Nous proposons une méthode d'attaque invariante de l'attention pour générer des exemples adverses plus transférables pour les attaques de type boîte noire, qui peuvent tromper les défenses de pointe avec un taux de réussite élevé."
"Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations.An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications.Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples.In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models.Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples.Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method.Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.","[0, 0, 0, 0, 0, 1, 0, 0]",[],BJzVUj0qtQ,Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift,L'article propose une nouvelle façon de surmonter les défenses de l'état de l'art contre les attaques adverses sur CNN.
"Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations.An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications.Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples.In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models.Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples.Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method.Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques.","[0, 0, 0, 0, 0, 1, 0, 0]",[],BJzVUj0qtQ,Evading Defenses to Transferable Adversarial Examples by Mitigating Attention Shift,"Cet article suggère que le ""déplacement de l'attention"" est une propriété clé qui explique l'échec du transfert des attaques adverses et propose une méthode d'attaque invariante en fonction de l'attention."
"We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\log{K})$ memory while only requiring $O(K\log{K} + d\log{K})$ operation for inference.MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\log{K})$ memory without any assumption on the relationship between classes.MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\%, which is the best-reported accuracy on this dataset.Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\% accuracy.  In contrast, MACH can achieve 9\% accuracy with 480x reduction in the model size (of mere 0.6GB).With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1RQdCg0W,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$",Comment former 100 000 classes sur un seul GPU ?
"We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\log{K})$ memory while only requiring $O(K\log{K} + d\log{K})$ operation for inference.MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\log{K})$ memory without any assumption on the relationship between classes.MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\%, which is the best-reported accuracy on this dataset.Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\% accuracy.  In contrast, MACH can achieve 9\% accuracy with 480x reduction in the model size (of mere 0.6GB).With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1RQdCg0W,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$","Propose une méthode de hachage efficace MACH pour l'approximation softmax dans le contexte d'un grand espace de sortie, qui économise à la fois la mémoire et le calcul."
"We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\log{K})$ memory while only requiring $O(K\log{K} + d\log{K})$ operation for inference.MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\log{K})$ memory without any assumption on the relationship between classes.MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\%, which is the best-reported accuracy on this dataset.Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\% accuracy.  In contrast, MACH can achieve 9\% accuracy with 480x reduction in the model size (of mere 0.6GB).With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1RQdCg0W,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$",Une méthode de classification pour les problèmes impliquant un grand nombre de classes dans un cadre multi-classes démontrée sur les jeux de données ODP et Imagenet-21K
"We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\log{K})$ memory while only requiring $O(K\log{K} + d\log{K})$ operation for inference.MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\log{K})$ memory without any assumption on the relationship between classes.MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\%, which is the best-reported accuracy on this dataset.Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\% accuracy.  In contrast, MACH can achieve 9\% accuracy with 480x reduction in the model size (of mere 0.6GB).With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU.","[0, 0, 0, 0, 1, 0, 0, 0]",[],r1RQdCg0W,"MACH: Embarrassingly parallel $K$-class classification in $O(d\log{K})$ memory and $O(K\log{K} + d\log{K})$ time, instead of $O(Kd)$",L'article présente un schéma basé sur le hachage pour réduire la mémoire et le temps de calcul pour la classification à K voies lorsque K est grand.
"Gradient-based optimization is the foundation of deep learning and reinforcement learning.Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy.We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings.We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic.We also demonstrate this framework for training discrete latent-variable models.","[0, 0, 1, 0, 0, 0]",[],SyzKd1bCW,Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,Nous présentons une méthode générale d'estimation sans biais des gradients de fonctions boîte noire de variables aléatoires. Nous appliquons cette méthode à l'inférence variationnelle discrète et à l'apprentissage par renforcement. 
"Gradient-based optimization is the foundation of deep learning and reinforcement learning.Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy.We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings.We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic.We also demonstrate this framework for training discrete latent-variable models.","[0, 0, 1, 0, 0, 0]",[],SyzKd1bCW,Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,Suggère une nouvelle approche de la descente de gradient pour l'optimisation de la boîte noire ou l'entraînement de modèles à variables latentes discrètes.
"Do GANS (Generative Adversarial Nets) actually learn the target distribution?The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.It showed that the training objective can approach its optimum value even if the generated distribution has very low support.In other words, the training objective is unable to prevent mode collapse.The current paper makes two contributions.(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],BJehNfW0-,Do GANs learn the distribution? Some Theory and Empirics,"Nous proposons un estimateur de la taille du support de la distribution apprise des GANs pour montrer qu'ils souffrent effectivement de l'effondrement des modes, et nous prouvons que les GANs codeurs-décodeurs n'évitent pas non plus ce problème."
"Do GANS (Generative Adversarial Nets) actually learn the target distribution?The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.It showed that the training objective can approach its optimum value even if the generated distribution has very low support.In other words, the training objective is unable to prevent mode collapse.The current paper makes two contributions.(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],BJehNfW0-,Do GANs learn the distribution? Some Theory and Empirics,L'article tente d'estimer expérimentalement la taille du support des solutions produites par des GANs typiques. 
"Do GANS (Generative Adversarial Nets) actually learn the target distribution?The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.It showed that the training objective can approach its optimum value even if the generated distribution has very low support.In other words, the training objective is unable to prevent mode collapse.The current paper makes two contributions.(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],BJehNfW0-,Do GANs learn the distribution? Some Theory and Empirics,Cet article propose un nouveau test astucieux basé sur le paradoxe d'anniversaire pour mesurer la diversité dans un échantillon généré. Les résultats de l'expérience sont interprétés comme signifiant que l'effondrement des modes est fort dans un certain nombre de modèles génératifs de pointe.
"Do GANS (Generative Adversarial Nets) actually learn the target distribution?The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.It showed that the training objective can approach its optimum value even if the generated distribution has very low support.In other words, the training objective is unable to prevent mode collapse.The current paper makes two contributions.(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],BJehNfW0-,Do GANs learn the distribution? Some Theory and Empirics,L'article utilise le paradoxe de l'anniversaire pour montrer que certaines architectures GAN génèrent des distributions avec un support assez faible.
"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed.Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail.However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language.In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks.We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation.We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","[0, 0, 0, 0, 1, 0]",[],H1BLjgZCb,Generating Natural Adversarial Examples,"Nous proposons un cadre pour générer des adversaires naturels contre les classificateurs boîte noire pour les domaines visuels et textuels, en effectuant la recherche d'adversaires dans l'espace sémantique latent."
"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed.Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail.However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language.In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks.We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation.We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","[0, 0, 0, 0, 1, 0]",[],H1BLjgZCb,Generating Natural Adversarial Examples,Propose une méthode pour la création d'exemples d'adversaires sémantiques.
"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed.Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail.However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language.In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks.We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation.We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","[0, 0, 0, 0, 1, 0]",[],H1BLjgZCb,Generating Natural Adversarial Examples,propose un cadre pour générer des exemples contradictoires naturels en recherchant des adversaires dans un espace latent de représentation de données denses et continues. 
"Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.","[0, 0, 0, 1, 0, 0]",[],HyMTkQZAb,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,Nous étendons la méthode K-FAC aux RNN en développant une nouvelle famille d'approximations de Fisher.
"Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.","[0, 0, 0, 1, 0, 0]",[],HyMTkQZAb,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,"Les auteurs étendent la méthode K-FAC aux RNN et présentent 3 façons d'approximer F, en montrant des résultats d'optimisation sur 3 ensembles de données, qui surclassent ADAM à la fois en nombre de mises à jour et en temps de calcul."
"Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.","[0, 0, 0, 1, 0, 0]",[],HyMTkQZAb,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,Propose d'étendre la méthode d'optimisation de la courbure appropriée du facteur Kronecker au cadre des réseaux neuronaux récurrents.
"Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.","[0, 0, 0, 1, 0, 0]",[],HyMTkQZAb,Kronecker-factored Curvature Approximations for Recurrent Neural Networks,Les auteurs présentent une méthode de second ordre spécialement conçue pour les RNN.
"Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.","[0, 1, 0, 0]",[],ByGuynAct7,The Deep Weight Prior,"Le modèle génératif pour les noyaux des réseaux neuronaux convolutifs, qui agit comme une distribution préalable lors de la formation sur de nouveaux ensembles de données."
"Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.","[0, 1, 0, 0]",[],ByGuynAct7,The Deep Weight Prior,Une méthode de modélisation des réseaux neuronaux convolutifs utilisant une méthode de Bayes.
"Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.","[0, 1, 0, 0]",[],ByGuynAct7,The Deep Weight Prior,"Propose le ""deep weight prior"" : l'idée est d'obtenir un prior sur un ensemble de données auxiliaire et ensuite d'utiliser ce prior sur les filtres CNN pour démarrer l'inférence pour un ensemble de données d'intérêt."
"Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.","[0, 1, 0, 0]",[],ByGuynAct7,The Deep Weight Prior,Cet article explore l'apprentissage de priorités informatives pour les modèles de réseaux neuronaux convolutifs avec des domaines de problèmes similaires en utilisant des autoencodeurs pour obtenir une priorité expressive sur les poids filtrés des réseaux formés.
"The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.","[0, 0, 0, 1, 0]",[],HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,Nous avons appliqué des techniques d'apprentissage profond à la segmentation d'images hyperspectrales et à l'échantillonnage itératif de caractéristiques.
"The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.","[0, 0, 0, 1, 0]",[],HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,Proposition d'un schéma gourmand pour sélectionner un sous-ensemble de caractéristiques spectrales hautement corrélées dans une tâche de classification.
"The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.","[0, 0, 0, 1, 0]",[],HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,Cet article explore l'utilisation des réseaux neuronaux pour la classification et la segmentation de l'imagerie hyperspectrale (HSI) des cellules.
"The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation.","[0, 0, 0, 1, 0]",[],HkanP0lRW,Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation,Classification des cellules et mise en œuvre de la segmentation cellulaire basée sur des techniques d'apprentissage profond avec réduction des caractéristiques d'entrée.
"Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \textit{etc}.Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \{plot, question, answer\}.Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited(i) $x-y$ axes variables(ii) question templates and(iii) answer vocabulary and hence do not adequately capture the challenges posed by this task.To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators.First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \textit{etc.} extracted from  real word data sources such as World Bank, government sites, \textit{etc}.Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots.Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary.As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task.We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to(i) extract relevant data from the plot and convert it to a semi-structured table(ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived.We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SygeznA9YX,Data Interpretation and Reasoning Over Scientific Plots,Nous avons créé un nouvel ensemble de données pour l'interprétation des données sur les parcelles et proposons également une ligne de base pour la même chose.
"Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \textit{etc}.Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \{plot, question, answer\}.Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited(i) $x-y$ axes variables(ii) question templates and(iii) answer vocabulary and hence do not adequately capture the challenges posed by this task.To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators.First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \textit{etc.} extracted from  real word data sources such as World Bank, government sites, \textit{etc}.Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots.Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary.As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task.We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to(i) extract relevant data from the plot and convert it to a semi-structured table(ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived.We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SygeznA9YX,Data Interpretation and Reasoning Over Scientific Plots,"Les auteurs proposent un pipeline pour résoudre le problème DIP impliquant l'apprentissage à partir de jeux de données contenant des triplets de la forme {plot, question, réponse}."
"Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \textit{etc}.Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \{plot, question, answer\}.Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited(i) $x-y$ axes variables(ii) question templates and(iii) answer vocabulary and hence do not adequately capture the challenges posed by this task.To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators.First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \textit{etc.} extracted from  real word data sources such as World Bank, government sites, \textit{etc}.Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots.Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary.As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task.We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to(i) extract relevant data from the plot and convert it to a semi-structured table(ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived.We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SygeznA9YX,Data Interpretation and Reasoning Over Scientific Plots,Propose un algorithme capable d'interpréter les données présentées dans les graphiques scientifiques.
"Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing.Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model.PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule.Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well.Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train.Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation.Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting.In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs.In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJJ23bW0b,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,Amélioration des réseaux neuronaux récurrents à état prédictif par des caractéristiques aléatoires orthogonales
"Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing.Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model.PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule.Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well.Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train.Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation.Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting.In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs.In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJJ23bW0b,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,Propose d'améliorer les performances des réseaux neuronaux récurrents à états prédictifs en considérant des caractéristiques aléatoires orthogonales.
"Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing.Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model.PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule.Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well.Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train.Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation.Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting.In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs.In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HJJ23bW0b,Initialization matters: Orthogonal Predictive State Recurrent Neural Networks,Cet article aborde le problème de la formation des réseaux de neurones récurrents à état prédictif et apporte deux contributions.
"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.This creates a fundamental quality- versus-quantity trade-off in the learning process.Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.To this end, we propose “fidelity-weighted learning” (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).Both student and teacher are learned from the data.We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],B1X0mzZCW,Fidelity-Weighted Learning,"Nous proposons l'apprentissage pondéré par la fidélité, une approche maître-élève semi-supervisée pour la formation de réseaux neuronaux utilisant des données faiblement étiquetées."
"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.This creates a fundamental quality- versus-quantity trade-off in the learning process.Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.To this end, we propose “fidelity-weighted learning” (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).Both student and teacher are learned from the data.We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],B1X0mzZCW,Fidelity-Weighted Learning,Cet article propose une approche pour l'apprentissage avec une faible supervision en utilisant un ensemble de données propres et un ensemble de données bruitées et en supposant un enseignant et des réseaux d'étudiants.
"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.This creates a fundamental quality- versus-quantity trade-off in the learning process.Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.To this end, we propose “fidelity-weighted learning” (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).Both student and teacher are learned from the data.We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],B1X0mzZCW,Fidelity-Weighted Learning,L'article vise à former des modèles de réseaux neuronaux profonds avec peu d'échantillons d'entraînement étiquetés.
"Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.This creates a fundamental quality- versus-quantity trade-off in the learning process.Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.To this end, we propose “fidelity-weighted learning” (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).Both student and teacher are learned from the data.We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations.","[0, 0, 0, 0, 1, 0, 0, 0]",[],B1X0mzZCW,Fidelity-Weighted Learning,Les auteurs proposent une approche pour la formation de modèles d'apprentissage profond pour les situations où il n'y a pas assez de données annotées fiables.
" Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving.When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role.The purpose of this paper is to propose new online learning approaches for supervised dimension reduction.Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner.The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in.We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm.We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.","[0, 0, 0, 0, 0, 1, 0]",[],B1MUroRct7,Online Learning for Supervised Dimension Reduction,"Nous avons proposé deux nouvelles approches, la régression inverse incrémentale en tranches et la régression inverse incrémentale en tranches avec chevauchement, pour mettre en œuvre la réduction de dimension supervisée de manière à l'apprendre en ligne."
" Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving.When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role.The purpose of this paper is to propose new online learning approaches for supervised dimension reduction.Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner.The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in.We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm.We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.","[0, 0, 0, 0, 0, 1, 0]",[],B1MUroRct7,Online Learning for Supervised Dimension Reduction,Etudie le problème de la réduction de la dimension suffisante et propose un algorithme de régression inverse incrémentielle en tranches.
" Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving.When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role.The purpose of this paper is to propose new online learning approaches for supervised dimension reduction.Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner.The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in.We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm.We verify the effectiveness and efficiency of both algorithms by simulations and real data applications.","[0, 0, 0, 0, 0, 1, 0]",[],B1MUroRct7,Online Learning for Supervised Dimension Reduction,"Cet article propose un algorithme d'apprentissage en ligne pour la réduction supervisée de la dimension, appelé régression inverse tranchée incrémentielle."
"This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes.The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training.This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy.Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training.We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks.For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight.With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model.To achieve the similar classification error, the conventional binary neural network model requires 4× more data storage for weights than our proposed model.For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6× storage savings to achieve the similar accuracy.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rkONG0xAW,Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,"Nous proposons un modèle d'apprentissage permettant aux DNN d'apprendre avec seulement 2 bits/poids, ce qui est particulièrement utile pour l'apprentissage sur appareil."
"This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes.The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training.This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy.Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training.We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks.For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight.With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model.To achieve the similar classification error, the conventional binary neural network model requires 4× more data storage for weights than our proposed model.For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6× storage savings to achieve the similar accuracy.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],rkONG0xAW,Recursive Binary Neural Network Learning Model  with 2-bit/weight Storage Requirement,Propose une méthode pour discrétiser un NN de manière incrémentale afin d'améliorer la mémoire et les performances.
"Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.).We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks.In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error.Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space.These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification.The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification.We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJL6pz-CZ,Transfer Learning on Manifolds via Learned Transport Operators,L'apprentissage des opérateurs de transport sur les collecteurs constitue une représentation précieuse pour effectuer des tâches telles que l'apprentissage par transfert.
"Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.).We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks.In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error.Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space.These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification.The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification.We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJL6pz-CZ,Transfer Learning on Manifolds via Learned Transport Operators,Utilise un cadre d'apprentissage par dictionnaire pour apprendre des opérateurs de transport multiples sur des chiffres USPS augmentés.
"Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.).We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks.In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error.Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space.These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification.The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification.We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation.","[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJL6pz-CZ,Transfer Learning on Manifolds via Learned Transport Operators,"L'article considère le cadre de l'apprentissage de l'opérateur de transport multiple de Culpepper et Olshausen (2009), et l'interprète comme l'obtention d'une estimation MAP sous un modèle génératif probabiliste."
"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkN2Il-RZ,SCAN: Learning Hierarchical Compositional Visual Concepts,Nous présentons un modèle neuronal variationnel pour l'apprentissage de concepts visuels compositionnels guidés par le langage.
"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkN2Il-RZ,SCAN: Learning Hierarchical Compositional Visual Concepts,Propose une nouvelle architecture de réseau neuronal qui apprend les concepts d'objet en combinant un bêta-VAE et un SCAN.
"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkN2Il-RZ,SCAN: Learning Hierarchical Compositional Visual Concepts,"Cet article présente un modèle basé sur les VAE pour la traduction entre les images et le texte. Leur représentation latente est bien adaptée à l'application d'opérations symboliques, ce qui leur donne un langage plus expressif pour l'échantillonnage des images à partir du texte. "
"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","[0, 0, 0, 1, 0, 0, 0, 0, 0]",[],rkN2Il-RZ,SCAN: Learning Hierarchical Compositional Visual Concepts,Cet article propose un nouveau modèle appelé SCAN (Symbol-Concept Association Network) pour l'apprentissage hiérarchique des concepts et permet la généralisation à de nouveaux concepts composés à partir de concepts existants en utilisant des opérateurs logiques.
"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.","[0, 1, 0, 0, 0]",[],S1GUgxgCW,Latent Topic Conversational Models,"Latent Topic Conversational Model, un hybride de seq2seq et de modèle thématique neuronal pour générer des réponses plus diverses et plus intéressantes."
"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.","[0, 1, 0, 0, 0]",[],S1GUgxgCW,Latent Topic Conversational Models,Cet article propose la combinaison du modèle thématique et du modèle conversationnel seq2seq.
"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.","[0, 1, 0, 0, 0]",[],S1GUgxgCW,Latent Topic Conversational Models,Nous proposons un modèle conversationnel avec des informations topiques en combinant le modèle seq2seq avec des modèles thématiques neuronaux et nous montrons que le modèle proposé surpasse le modèle de base seq2seq et d'autres variantes de modèles à variables latentes de seq2seq.
"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.","[0, 1, 0, 0, 0]",[],S1GUgxgCW,Latent Topic Conversational Models,Cet article aborde la question de la topicalité durable dans les modèles de conversation et propose un modèle qui est une combinaison d'un modèle thématique neuronal et d'un système de dialogue basé sur seq2seq. 
"Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs.Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function.In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph.Accordingly, message passing may not propagate helpful information throughout the graph.We show that this conventional approach can fail to learn to perform even simple graph classification tasks.We alleviate this serious shortcoming of the GNNs by making them a two step method.In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph.The embedding algorithm represents the graph as a point-cloud in the embedding space.In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method.The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors.In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed.Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm.We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors.We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets.For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hkes0iR9KX,DEEP GEOMETRICAL GRAPH CLASSIFICATION,Le problème de l'analyse des graphes est transformé en un problème d'analyse des nuages de points. 
"Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs.Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function.In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph.Accordingly, message passing may not propagate helpful information throughout the graph.We show that this conventional approach can fail to learn to perform even simple graph classification tasks.We alleviate this serious shortcoming of the GNNs by making them a two step method.In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph.The embedding algorithm represents the graph as a point-cloud in the embedding space.In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method.The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors.In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed.Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm.We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors.We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets.For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hkes0iR9KX,DEEP GEOMETRICAL GRAPH CLASSIFICATION,Propose un réseau GNN profond pour les problèmes de classification de graphes en utilisant leur couche adaptative de mise en commun des graphes.
"Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs.Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function.In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph.Accordingly, message passing may not propagate helpful information throughout the graph.We show that this conventional approach can fail to learn to perform even simple graph classification tasks.We alleviate this serious shortcoming of the GNNs by making them a two step method.In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph.The embedding algorithm represents the graph as a point-cloud in the embedding space.In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method.The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors.In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed.Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm.We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors.We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets.For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Hkes0iR9KX,DEEP GEOMETRICAL GRAPH CLASSIFICATION,Les auteurs proposent une méthode d'apprentissage de représentations pour les graphes
"Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs.Such adversarial examples can mislead DNNs to produce adversary-selected results.Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  We apply AdvGAN in both semi-whitebox and black-box attack settings.In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks.In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks.Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,Nous proposons de générer des exemples contradictoires basés sur des réseaux génératifs contradictoires dans des contextes de boîte noire et de boîte semi-blanche.
"Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs.Such adversarial examples can mislead DNNs to produce adversary-selected results.Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  We apply AdvGAN in both semi-whitebox and black-box attack settings.In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks.In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks.Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,"Décrit AdvGAN, un GAN conditionnel avec perte adversariale, et évalue AdvGAN en boîte semi-blanche et boîte noire, en rapportant les résultats de l'état de l'art."
"Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs.Such adversarial examples can mislead DNNs to produce adversary-selected results.Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  We apply AdvGAN in both semi-whitebox and black-box attack settings.In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks.In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks.Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HknbyQbC-,Generating Adversarial Examples with Adversarial Networks,Cet article propose un moyen de générer des exemples contradictoires qui trompent les systèmes de classification et remporte le défi mnist de MadryLab.
"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.We empirically demonstrate that:a) deep autoencoder models generalize much better than the shallow ones,b) non-linear activation functions with negative parts are crucial for training deep models, andc) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.The new algorithm significantly speeds up training and improves model performance.Our code is publicly available.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkNQeiRpb,Training Deep AutoEncoders for Recommender Systems,Cet article démontre comment former des autoencodeurs profonds de bout en bout pour obtenir des résultats SoA sur un ensemble de données Netflix fractionnées dans le temps.
"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.We empirically demonstrate that:a) deep autoencoder models generalize much better than the shallow ones,b) non-linear activation functions with negative parts are crucial for training deep models, andc) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.The new algorithm significantly speeds up training and improves model performance.Our code is publicly available.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkNQeiRpb,Training Deep AutoEncoders for Recommender Systems,Cet article présente un modèle d'autoencodeur profond pour la prédiction de classement qui surpasse les autres approches de pointe sur le jeu de données de prix Netflix. 
"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.We empirically demonstrate that:a) deep autoencoder models generalize much better than the shallow ones,b) non-linear activation functions with negative parts are crucial for training deep models, andc) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.The new algorithm significantly speeds up training and improves model performance.Our code is publicly available.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkNQeiRpb,Training Deep AutoEncoders for Recommender Systems,Propose d'utiliser un AE profond pour effectuer des tâches de prédiction de classement dans les systèmes de recommandation.
"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.We empirically demonstrate that:a) deep autoencoder models generalize much better than the shallow ones,b) non-linear activation functions with negative parts are crucial for training deep models, andc) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.The new algorithm significantly speeds up training and improves model performance.Our code is publicly available.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkNQeiRpb,Training Deep AutoEncoders for Recommender Systems,"Les auteurs présentent un modèle pour des recommandations Netflix plus précises, démontrant qu'un autoencodeur profond peut surpasser les modèles plus complexes basés sur les RNN qui ont des informations temporelles. "
"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.However, their inherently sequential computation makes them slow to train.Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete.Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HyzdRiR9Y7,Universal Transformers,"Nous présentons le transformateur universel, un modèle de séquence récurrent parallèle en temps auto-attentif qui surpasse les transformateurs et les LSTM dans un large éventail de tâches de séquence à séquence, y compris la traduction automatique."
"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.However, their inherently sequential computation makes them slow to train.Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete.Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HyzdRiR9Y7,Universal Transformers,"Propose un nouveau modèle UT, basé sur le modèle Transformer, avec récurrence ajoutée et arrêt dynamique de la récurrence."
"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.However, their inherently sequential computation makes them slow to train.Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete.Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HyzdRiR9Y7,Universal Transformers,"Cet article étend Transformer en appliquant récursivement un bloc d'auto-attention à têtes multiples, plutôt que d'empiler plusieurs blocs dans le Transformer vanille."
"We present a framework for interpretable continual learning (ICL).We show that explanations of previously performed tasks can be used to improve performance on future tasks.ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task.The ICL idea is general and may be applied to many continual learning approaches.Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting.We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality.Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.","[0, 0, 0, 0, 1, 0, 0]",[],S1g9N2A5FX,Interpretable Continual Learning,"L'article développe un cadre d'apprentissage continu interprétable où les explications des tâches terminées sont utilisées pour renforcer l'attention de l'apprenant pendant les tâches futures, et où une métrique d'explication est également proposée. "
"We present a framework for interpretable continual learning (ICL).We show that explanations of previously performed tasks can be used to improve performance on future tasks.ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task.The ICL idea is general and may be applied to many continual learning approaches.Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting.We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality.Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.","[0, 0, 0, 0, 1, 0, 0]",[],S1g9N2A5FX,Interpretable Continual Learning,Les auteurs proposent un cadre pour l'apprentissage continu basé sur des explications pour les classifications effectuées des tâches apprises précédemment.
"We present a framework for interpretable continual learning (ICL).We show that explanations of previously performed tasks can be used to improve performance on future tasks.ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task.The ICL idea is general and may be applied to many continual learning approaches.Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting.We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality.Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric.","[0, 0, 0, 0, 1, 0, 0]",[],S1g9N2A5FX,Interpretable Continual Learning,Cet article propose une extension du cadre d'apprentissage continu en utilisant l'apprentissage continu variationnel existant comme méthode de base avec le poids de la preuve.
"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017).On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10).In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations.The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator.We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput.To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision","[0, 0, 0, 0, 0, 0, 1]",[],H135uzZ0-,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,Pipeline d'entraînement à précision mixte utilisant des entiers de 16 bits sur un matériel universel ; précision SOTA pour les CNN de classe ImageNet ; meilleure précision rapportée pour la tâche de classification ImageNet-1K avec tout entraînement à précision réduite ;
"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017).On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10).In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations.The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator.We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput.To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision","[0, 0, 0, 0, 0, 0, 1]",[],H135uzZ0-,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,Cet article montre qu'une mise en œuvre soignée du calcul dynamique à virgule fixe en précision mixte permet d'atteindre une précision de pointe en utilisant un modèle d'apprentissage profond à précision réduite avec une représentation entière de 16 bits.
"The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017).On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10).In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations.The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator.We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput.To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision","[0, 0, 0, 0, 0, 0, 1]",[],H135uzZ0-,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,"Propose un schéma de ""point fixe dynamique"" qui partage la partie exposant pour un tenseur et développe des procédures pour faire du calcul NN avec ce format et en fait la démonstration pour une formation à précision limitée."
"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model.The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed.Key ingredients for the acoustic model are Gated Linear Units and high dropout.We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.","[1, 0, 0, 0]",[],Hyig0zb0Z,Gated ConvNets for Letter-Based ASR,Un modèle acoustique ConvNet basé sur les lettres permet d'obtenir un pipeline de reconnaissance vocale simple et compétitif.
"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model.The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed.Key ingredients for the acoustic model are Gated Linear Units and high dropout.We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.","[1, 0, 0, 0]",[],Hyig0zb0Z,Gated ConvNets for Letter-Based ASR,"Cet article applique les réseaux de neurones convolutifs à grille à la reconnaissance de la parole, en utilisant le critère de formation ASG."
"Generative adversarial networks (GANs) are a powerful framework for generative tasks.However, they are difficult to train and tend to miss modes of the true data generation process.Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space.We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations.Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes.We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.","[0, 0, 0, 1, 0, 0]",[],ry0WOxbRZ,IVE-GAN: Invariant Encoding Generative Adversarial Networks,Un cadre GAN noval qui utilise des caractéristiques invariantes de transformation pour apprendre des représentations riches et des générateurs forts.
"Generative adversarial networks (GANs) are a powerful framework for generative tasks.However, they are difficult to train and tend to miss modes of the true data generation process.Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space.We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations.Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes.We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.","[0, 0, 0, 1, 0, 0]",[],ry0WOxbRZ,IVE-GAN: Invariant Encoding Generative Adversarial Networks,Propose un objectif GAN modifié composé d'un terme GAN classique et d'un terme d'encodage invariant.
"Generative adversarial networks (GANs) are a powerful framework for generative tasks.However, they are difficult to train and tend to miss modes of the true data generation process.Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space.We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations.Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes.We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.","[0, 0, 0, 1, 0, 0]",[],ry0WOxbRZ,IVE-GAN: Invariant Encoding Generative Adversarial Networks,"Cet article présente le IVE-GAN, un modèle qui introduit l'encodeur dans le cadre du Generative Adversarial Network."
"We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.","[1, 0, 0, 0, 0, 0, 0]",[],SJgsCjCqt7,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,Nous proposons une méthode d'apprentissage de la structure de dépendance latente dans les autoencodeurs variationnels.
"We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.","[1, 0, 0, 0, 0, 0, 0]",[],SJgsCjCqt7,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,Utilise une matrice de variables aléatoires binaires pour capturer les dépendances entre les variables latentes dans un modèle génératif profond hiérarchique.
"We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.","[1, 0, 0, 0, 0, 0, 0]",[],SJgsCjCqt7,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,Cet article présente une approche VAE dans laquelle une structure de dépendance sur la variable latente est apprise pendant la formation.
"We propose a method for learning the dependency structure between latent variables in deep latent variable models.  Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model.","[1, 0, 0, 0, 0, 0, 0]",[],SJgsCjCqt7,Variational Autoencoders with Jointly Optimized Latent Dependency Structure,"Les auteurs proposent d'augmenter l'espace latent d'un VAE avec une structure auto-régressive, afin d'améliorer l'expressivité du réseau d'inférence et du préalable latent."
"Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change.Detecting changes is important as they may indicate critical events.However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales.To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales.Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing.Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.","[0, 0, 0, 1, 0, 0]",[],HkGTwjCctm,Pyramid Recurrent Neural Networks for Multi-Scale Change-Point Detection,Nous présentons une architecture de réseau neuronal à échelle invariable pour la détection des points de changement dans les séries temporelles multivariées.
"Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change.Detecting changes is important as they may indicate critical events.However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales.To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales.Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing.Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.","[0, 0, 0, 1, 0, 0]",[],HkGTwjCctm,Pyramid Recurrent Neural Networks for Multi-Scale Change-Point Detection,L'article exploite le concept de transformée en ondelettes dans une architecture profonde pour résoudre la détection des points de changement.
"Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change.Detecting changes is important as they may indicate critical events.However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales.To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales.Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing.Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training.","[0, 0, 0, 1, 0, 0]",[],HkGTwjCctm,Pyramid Recurrent Neural Networks for Multi-Scale Change-Point Detection,Cet article propose un réseau de neurones basé sur une pyramide et l'applique à des signaux 1D avec des processus sous-jacents se produisant à différentes échelles de temps où la tâche est la détection de points de changement.
"We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables.The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way.For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.","[1, 0, 0, 0]",[],HkeyZhC9F7,Learning Heuristics for Automated Reasoning through Reinforcement Learning,RL trouve de meilleures heuristiques pour les algorithmes de raisonnement automatique.
"We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables.The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way.For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.","[1, 0, 0, 0]",[],HkeyZhC9F7,Learning Heuristics for Automated Reasoning through Reinforcement Learning,"L'objectif est d'apprendre une heuristique pour un algorithme de recherche par retour en arrière en utilisant l'apprentissage par renforcement et propose un modèle qui utilise des réseaux de neurones graphiques pour produire un encastrement des littérateurs et des clauses, et les utiliser pour prédire la qualité de chaque littérateur afin de décider de la probabilité de chaque action."
"We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables.The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way.For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics.","[1, 0, 0, 0]",[],HkeyZhC9F7,Learning Heuristics for Automated Reasoning through Reinforcement Learning,L'article propose une approche pour l'apprentissage automatique d'heuristiques de sélection de variables pour QBF en utilisant l'apprentissage profond.
"We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data.We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution.We use this procedure to assess the performance of several modern generative adversarial network architectures.We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.","[1, 0, 0, 0, 0, 0]",[],ByuI-mW0W,Towards a Testable Notion of Generalization for Generative Adversarial Networks,Évaluez si votre GAN fait réellement autre chose que de mémoriser les données d'entraînement.
"We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data.We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution.We use this procedure to assess the performance of several modern generative adversarial network architectures.We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.","[1, 0, 0, 0, 0, 0]",[],ByuI-mW0W,Towards a Testable Notion of Generalization for Generative Adversarial Networks,L'objectif est de fournir une mesure/test de qualité pour les GAN et propose d'évaluer l'approximation actuelle d'une distribution apprise par un GAN en utilisant la distance de Wasserstein entre deux distributions constituées d'une somme de Diracs comme performance de base. 
"We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data.We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution.We use this procedure to assess the performance of several modern generative adversarial network architectures.We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.","[1, 0, 0, 0, 0, 0]",[],ByuI-mW0W,Towards a Testable Notion of Generalization for Generative Adversarial Networks,"Cet article propose une procédure d'évaluation des performances des GANs en reconsidérant la clé d'observation, en utilisant la procédure pour tester et améliorer les GANs actuels."
"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.In this work, we propose to leverage automatic search techniques to discover new activation functions.Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SkBYYyZRZ,Searching for Activation Functions,"Nous utilisons des techniques de recherche pour découvrir de nouvelles fonctions d'activation, et notre meilleure fonction d'activation découverte, f(x) = x * sigmoïde(beta * x), surpasse ReLU sur un certain nombre de tâches difficiles comme ImageNet."
"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.In this work, we propose to leverage automatic search techniques to discover new activation functions.Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SkBYYyZRZ,Searching for Activation Functions,Propose une approche basée sur l'apprentissage par renforcement pour trouver la non-linéarité en recherchant des combinaisons à partir d'un ensemble d'opérateurs unaires et binaires.
"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.In this work, we propose to leverage automatic search techniques to discover new activation functions.Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SkBYYyZRZ,Searching for Activation Functions,Cet article utilise l'apprentissage par renforcement pour rechercher la combinaison d'un ensemble de fonctions unaires et binaires résultant en une nouvelle fonction d'activation.
"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.In this work, we propose to leverage automatic search techniques to discover new activation functions.Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],SkBYYyZRZ,Searching for Activation Functions,L'auteur utilise l'apprentissage par renforcement pour trouver de nouvelles fonctions d'activation potentielles à partir d'un riche ensemble de candidats possibles. 
"Successful training of convolutional neural networks is often associated with suffi-ciently deep architectures composed of high amounts of features.These networkstypically rely on a variety of regularization and pruning techniques to convergeto less redundant states.We introduce a novel bottom-up approach to expandrepresentations in fixed-depth architectures.These architectures start from just asingle feature per layer and greedily increase width of individual layers to attaineffective representational capacities needed for a specific task.While networkgrowth can rely on a family of metrics, we propose a computationally efficientversion based on feature time evolution and demonstrate its potency in determin-ing feature importance and a networks’ effective capacity.We demonstrate howautomatically expanded architectures converge to similar topologies that benefitfrom lesser amount of parameters or improved accuracy and exhibit systematiccorrespondence in representational complexity with the specified task.In contrastto conventional design patterns with a typical monotonic increase in the amount offeatures with increased depth, we observe that CNNs perform better when there ismore learnable parameters in intermediate, with falloffs to earlier and later layers.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkffVjUaW,Building effective deep neural networks one feature at a time,Un algorithme ascendant qui étend les CNNs commençant avec une caractéristique par couche à des architectures avec une capacité de représentation suffisante.
"Successful training of convolutional neural networks is often associated with suffi-ciently deep architectures composed of high amounts of features.These networkstypically rely on a variety of regularization and pruning techniques to convergeto less redundant states.We introduce a novel bottom-up approach to expandrepresentations in fixed-depth architectures.These architectures start from just asingle feature per layer and greedily increase width of individual layers to attaineffective representational capacities needed for a specific task.While networkgrowth can rely on a family of metrics, we propose a computationally efficientversion based on feature time evolution and demonstrate its potency in determin-ing feature importance and a networks’ effective capacity.We demonstrate howautomatically expanded architectures converge to similar topologies that benefitfrom lesser amount of parameters or improved accuracy and exhibit systematiccorrespondence in representational complexity with the specified task.In contrastto conventional design patterns with a typical monotonic increase in the amount offeatures with increased depth, we observe that CNNs perform better when there ismore learnable parameters in intermediate, with falloffs to earlier and later layers.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkffVjUaW,Building effective deep neural networks one feature at a time,"Propose d'ajuster dynamiquement la profondeur de la carte de caractéristiques d'un réseau neuronal entièrement convolutif, en formulant une mesure d'auto-resemblance et en améliorant les performances."
"Successful training of convolutional neural networks is often associated with suffi-ciently deep architectures composed of high amounts of features.These networkstypically rely on a variety of regularization and pruning techniques to convergeto less redundant states.We introduce a novel bottom-up approach to expandrepresentations in fixed-depth architectures.These architectures start from just asingle feature per layer and greedily increase width of individual layers to attaineffective representational capacities needed for a specific task.While networkgrowth can rely on a family of metrics, we propose a computationally efficientversion based on feature time evolution and demonstrate its potency in determin-ing feature importance and a networks’ effective capacity.We demonstrate howautomatically expanded architectures converge to similar topologies that benefitfrom lesser amount of parameters or improved accuracy and exhibit systematiccorrespondence in representational complexity with the specified task.In contrastto conventional design patterns with a typical monotonic increase in the amount offeatures with increased depth, we observe that CNNs perform better when there ismore learnable parameters in intermediate, with falloffs to earlier and later layers.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkffVjUaW,Building effective deep neural networks one feature at a time,"Introduit une métrique simple basée sur la corrélation pour mesurer si les filtres des réseaux neuronaux sont utilisés efficacement, en tant qu'indicateur de la capacité effective."
"Successful training of convolutional neural networks is often associated with suffi-ciently deep architectures composed of high amounts of features.These networkstypically rely on a variety of regularization and pruning techniques to convergeto less redundant states.We introduce a novel bottom-up approach to expandrepresentations in fixed-depth architectures.These architectures start from just asingle feature per layer and greedily increase width of individual layers to attaineffective representational capacities needed for a specific task.While networkgrowth can rely on a family of metrics, we propose a computationally efficientversion based on feature time evolution and demonstrate its potency in determin-ing feature importance and a networks’ effective capacity.We demonstrate howautomatically expanded architectures converge to similar topologies that benefitfrom lesser amount of parameters or improved accuracy and exhibit systematiccorrespondence in representational complexity with the specified task.In contrastto conventional design patterns with a typical monotonic increase in the amount offeatures with increased depth, we observe that CNNs perform better when there ismore learnable parameters in intermediate, with falloffs to earlier and later layers.","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SkffVjUaW,Building effective deep neural networks one feature at a time,Vise à résoudre le problème de recherche d'architecture d'apprentissage profond par l'ajout et la suppression incrémentiels de canaux dans les couches intermédiaires du réseau.
"Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation).Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way.In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient.Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network.In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation.This feed-forward network learns to approximate the state of the fixed-point using a local learning rule.After training, we can simply use this initializing network for inference, resulting in a learned feedforward network.Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation.This shows how we might go about training deep networks without using backpropagation.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1GMDsR5tm,Initialized Equilibrium Propagation for Backprop-Free Training,Nous formons un réseau feedforward sans backprop en utilisant un modèle basé sur l'énergie pour fournir des cibles locales.
"Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation).Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way.In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient.Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network.In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation.This feed-forward network learns to approximate the state of the fixed-point using a local learning rule.After training, we can simply use this initializing network for inference, resulting in a learned feedforward network.Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation.This shows how we might go about training deep networks without using backpropagation.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1GMDsR5tm,Initialized Equilibrium Propagation for Backprop-Free Training,"Cet article vise à accélérer la procédure d'inférence itérative dans les modèles basés sur l'énergie entraînés avec la propagation de l'équilibre (EP), en proposant d'entraîner un réseau feedforward pour prédire un point fixe du ""réseau équilibrant"". "
"Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation).Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way.In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient.Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network.In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation.This feed-forward network learns to approximate the state of the fixed-point using a local learning rule.After training, we can simply use this initializing network for inference, resulting in a learned feedforward network.Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation.This shows how we might go about training deep networks without using backpropagation.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],B1GMDsR5tm,Initialized Equilibrium Propagation for Backprop-Free Training,Formation d'un réseau distinct pour initialiser les réseaux récurrents formés à l'aide de la propagation de l'équilibre 
"We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.A single object may have many attributes which when altered do not change the identity of the object itself.Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses.The attribute of wearing glasses can be changed without changing the identity of the person.However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task.Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses').We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute.We refer to this specific synthesis process as image attribute manipulation.We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJfRpoA9YX,Adversarial Information Factorization,Apprenez les représentations pour les images qui tiennent compte d'un seul attribut.
"We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.A single object may have many attributes which when altered do not change the identity of the object itself.Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses.The attribute of wearing glasses can be changed without changing the identity of the person.However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task.Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses').We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute.We refer to this specific synthesis process as image attribute manipulation.We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJfRpoA9YX,Adversarial Information Factorization,Cet article s'appuie sur les GAN VAE conditionnels pour permettre la manipulation des attributs dans le processus de synthèse.
"We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.A single object may have many attributes which when altered do not change the identity of the object itself.Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses.The attribute of wearing glasses can be changed without changing the identity of the person.However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task.Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses').We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute.We refer to this specific synthesis process as image attribute manipulation.We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],BJfRpoA9YX,Adversarial Information Factorization,"Cet article propose un modèle génératif pour apprendre la représentation qui peut séparer l'identité d'un objet d'un attribut, et étend l'autoencodeur adverse en ajoutant un réseau auxiliaire."
"Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames.These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive.We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points.For example, our model can ""jump"" and directly sample frames at the end of the video, without sampling intermediate frames.Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity.We also apply our framework to a 3D scene reconstruction dataset.Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain.Reconstructions and videos are available at https://bit.ly/2O4Pc4R.","[1, 0, 0, 0, 0, 0, 0, 0]",[],S1gQ5sRcFm,Consistent Jumpy Predictions for Videos and Scenes,"Nous présentons un modèle pour la reconstruction 3D cohérente et la prédiction vidéo sautée, c'est-à-dire la production de trames d'image plusieurs pas de temps dans le futur sans générer de trames intermédiaires."
"Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames.These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive.We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points.For example, our model can ""jump"" and directly sample frames at the end of the video, without sampling intermediate frames.Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity.We also apply our framework to a 3D scene reconstruction dataset.Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain.Reconstructions and videos are available at https://bit.ly/2O4Pc4R.","[1, 0, 0, 0, 0, 0, 0, 0]",[],S1gQ5sRcFm,Consistent Jumpy Predictions for Videos and Scenes,"Cet article propose une méthode générale de modélisation des données indexées en codant les informations de l'index avec l'observation dans un réseau neuronal, puis en décodant la condition d'observation sur l'index cible."
"Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames.These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive.We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points.For example, our model can ""jump"" and directly sample frames at the end of the video, without sampling intermediate frames.Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity.We also apply our framework to a 3D scene reconstruction dataset.Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain.Reconstructions and videos are available at https://bit.ly/2O4Pc4R.","[1, 0, 0, 0, 0, 0, 0, 0]",[],S1gQ5sRcFm,Consistent Jumpy Predictions for Videos and Scenes,Propose d'utiliser un VAE qui code la vidéo d'entrée d'une manière invariable par permutation pour prédire les images futures d'une vidéo.
"The ADAM optimizer is exceedingly popular in the deep learning community.Often it works very well, sometimes it doesn’t.Why?We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM ’s inner workings.Transferring the ""variance adaptation” to momentum- SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.","[1, 0, 0, 0, 0, 0]",[],S1EwLkW0W,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients",Analyse de l'optimiseur populaire Adam
"The ADAM optimizer is exceedingly popular in the deep learning community.Often it works very well, sometimes it doesn’t.Why?We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM ’s inner workings.Transferring the ""variance adaptation” to momentum- SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.","[1, 0, 0, 0, 0, 0]",[],S1EwLkW0W,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients",L'article tente d'améliorer Adam en se basant sur l'adaptation de la variance avec momentum en proposant deux algorithmes
"The ADAM optimizer is exceedingly popular in the deep learning community.Often it works very well, sometimes it doesn’t.Why?We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM ’s inner workings.Transferring the ""variance adaptation” to momentum- SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.","[1, 0, 0, 0, 0, 0]",[],S1EwLkW0W,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","Cet article analyse l'invariance d'échelle et la forme particulière du taux d'apprentissage utilisé dans Adam, en soutenant que la mise à jour d'Adam est une combinaison d'un sign-update et d'un taux d'apprentissage basé sur la variance."
"The ADAM optimizer is exceedingly popular in the deep learning community.Often it works very well, sometimes it doesn’t.Why?We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM ’s inner workings.Transferring the ""variance adaptation” to momentum- SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.","[1, 0, 0, 0, 0, 0]",[],S1EwLkW0W,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","L'article divise l'algorithme ADAM en deux composantes : la direction stochastique en signe de gradient et la direction adaptative par étapes avec variance relative, et deux algorithmes sont proposés pour tester chacune d'entre elles."
"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation.However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space.In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices.Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power.Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm.We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.","[1, 0, 0, 0, 0, 0, 0]",[],S1uxsye0Z,Adaptive Dropout with Rademacher Complexity Regularization,Nous proposons un nouveau cadre pour ajuster de manière adaptative les taux d'abandon pour le réseau neuronal profond basé sur une limite de complexité de Rademacher.
"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation.However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space.In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices.Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power.Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm.We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.","[1, 0, 0, 0, 0, 0, 0]",[],S1uxsye0Z,Adaptive Dropout with Rademacher Complexity Regularization,Les auteurs relient les paramètres d'abandon à une limite de la complexité de Rademacher du réseau.
"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation.However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space.In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices.Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power.Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm.We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms.","[1, 0, 0, 0, 0, 0, 0]",[],S1uxsye0Z,Adaptive Dropout with Rademacher Complexity Regularization,Relie la complexité de l'apprenabilité des réseaux aux taux d'abandon dans la rétropropagation.
"Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control.Deep neural networks have been adopted for sensor fusion in a body of recent studies.Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN).In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights.Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.","[0, 1, 0, 0, 0]",[],Syeil309tX,Optimized Gated Deep Learning Architectures for Sensor Fusion,Une architecture d'apprentissage profond optimisée pour la fusion de capteurs est proposée.
"Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control.Deep neural networks have been adopted for sensor fusion in a body of recent studies.Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN).In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights.Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.","[0, 1, 0, 0, 0]",[],Syeil309tX,Optimized Gated Deep Learning Architectures for Sensor Fusion,Les auteurs améliorent plusieurs limitations de l'architecture négative de base en proposant une architecture de fusion à granularité plus grossière et une architecture de fusion à granularité en deux étapes.
"Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control.Deep neural networks have been adopted for sensor fusion in a body of recent studies.Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN).In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights.Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.","[0, 1, 0, 0, 0]",[],Syeil309tX,Optimized Gated Deep Learning Architectures for Sensor Fusion,"Il propose deux architectures d'apprentissage profond à déclenchement pour la fusion de capteurs et, grâce aux caractéristiques groupées, il démontre une meilleure performance, en particulier en présence de bruit et de défaillances aléatoires des capteurs."
"We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization.Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function.Indeed, batch normalization itself is the cause of gradient explosion.As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations.While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections.Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range.Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SyMDXnCcF7,A Mean Field Theory of Batch Normalization,La normalisation des lots provoque l'explosion des gradients dans les réseaux feedforward de type vanille.
"We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization.Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function.Indeed, batch normalization itself is the cause of gradient explosion.As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations.While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections.Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range.Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SyMDXnCcF7,A Mean Field Theory of Batch Normalization,Développe une théorie du champ moyen pour la normalisation par lots (BN) dans les réseaux entièrement connectés avec des poids initialisés de façon aléatoire.
"We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization.Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function.Indeed, batch normalization itself is the cause of gradient explosion.As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations.While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections.Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range.Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.","[1, 0, 0, 0, 0, 0, 0, 0]",[],SyMDXnCcF7,A Mean Field Theory of Batch Normalization,Fournit une perspective dynamique sur les réseaux neuronaux profonds en utilisant l'évolution de la matrice de covariance avec les couches.
"We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.","[1, 0, 0]",[],HJMC_iA5tm,Learning a SAT Solver from Single-Bit Supervision,"Nous formons un réseau de graphes pour prédire la satisfiabilité booléenne et montrons qu'il apprend à chercher des solutions, et que les solutions qu'il trouve peuvent être décodées à partir de ses activations."
"We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.","[1, 0, 0]",[],HJMC_iA5tm,Learning a SAT Solver from Single-Bit Supervision,L'article décrit une architecture générale de réseau neuronal pour la prédiction de la satisfiabilité.
"We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.","[1, 0, 0]",[],HJMC_iA5tm,Learning a SAT Solver from Single-Bit Supervision,Cet article présente l'architecture NeuroSAT qui utilise un réseau neuronal profond à passage de messages pour prédire la satisfiabilité des instances CNF.
"Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain.Traffic forecasting is one canonical example of such learning task.The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting.To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling.We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines","[0, 0, 0, 1, 0, 0]",[],SJiHXGWAZ,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,Un modèle de séquence neuronale qui apprend à prévoir sur un graphe dirigé.
"Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain.Traffic forecasting is one canonical example of such learning task.The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting.To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling.We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines","[0, 0, 0, 1, 0, 0]",[],SJiHXGWAZ,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,L'article propose une architecture de réseau neuronal convolutif récurrent de diffusion pour le problème de la prévision spatio-temporelle du trafic.
"Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain.Traffic forecasting is one canonical example of such learning task.The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting.To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling.We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines","[0, 0, 0, 1, 0, 0]",[],SJiHXGWAZ,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,Propose de construire un modèle de prévision du trafic en utilisant un processus de diffusion pour les réseaux de neurones récurrents convolutifs afin de traiter l'autocorrélation saptio-temporelle.
"Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge.Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior.In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution.We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.The key idea is to train the model to output high uncertainty for data points outside of the training distribution.NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs.NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training.Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning.We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HkgxasA5Ym,Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,Nous entraînons les réseaux neuronaux à être incertains sur des entrées bruyantes afin d'éviter des prédictions trop sûres en dehors de la distribution d'entraînement.
"Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge.Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior.In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution.We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.The key idea is to train the model to output high uncertainty for data points outside of the training distribution.NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs.NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training.Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning.We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HkgxasA5Ym,Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,Présente une approche permettant d'obtenir des estimations de l'incertitude pour les prédictions de réseaux neuronaux qui a de bonnes performances lors de la quantification de l'incertitude prédictive à des points qui sont en dehors de la distribution d'apprentissage.
"Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge.Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior.In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution.We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.The key idea is to train the model to output high uncertainty for data points outside of the training distribution.NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs.NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training.Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning.We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],HkgxasA5Ym,Reliable Uncertainty Estimates in Deep Neural Networks using Noise Contrastive Priors,L'article considère le problème de l'estimation de l'incertitude des réseaux neuronaux et propose d'utiliser l'approche bayésienne avec un antécédent contrastif nocif.
"Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition.This has lead to the speculation that both systems use similar mechanisms to perform recognition.In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask.When trained on these modified set of images, the model did not show any bias towards selecting shapes as features.Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels.We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.","[0, 0, 1, 0, 0, 0, 0]",[],ByePUo05K7,What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation,"Cette étude met en évidence une différence essentielle entre la vision humaine et les CNN : alors que la reconnaissance des objets chez l'homme repose sur l'analyse des formes, les CNN n'ont pas ce biais de forme."
"Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition.This has lead to the speculation that both systems use similar mechanisms to perform recognition.In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask.When trained on these modified set of images, the model did not show any bias towards selecting shapes as features.Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels.We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.","[0, 0, 1, 0, 0, 0, 0]",[],ByePUo05K7,What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation,"Cherche à établir, via une série d'expériences bien conçues, que les CNN formés pour la classification d'images n'encodent pas de biais de forme comme la vision humaine."
"Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition.This has lead to the speculation that both systems use similar mechanisms to perform recognition.In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask.When trained on these modified set of images, the model did not show any bias towards selecting shapes as features.Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels.We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets.","[0, 0, 1, 0, 0, 0, 0]",[],ByePUo05K7,What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation,Cet article met en évidence le fait que les CNN n'apprendront pas nécessairement à reconnaître les objets sur la base de leur forme et montre qu'ils surdétermineront les caractéristiques basées sur le bruit.
"The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content.Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.Our model can be used on problems with a huge, potentially infinite, number of categories.We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ryRh0bb0Z,Multi-View Data Generation Without View Supervision,"Nous décrivons un nouveau modèle génératif multi-vues qui peut générer plusieurs vues du même objet, ou plusieurs objets dans la même vue sans avoir besoin d'étiqueter les vues."
"The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content.Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.Our model can be used on problems with a huge, potentially infinite, number of categories.We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ryRh0bb0Z,Multi-View Data Generation Without View Supervision,Cet article présente une méthode de génération d'images basée sur un GAN qui tente de séparer les variables latentes décrivant le contenu de l'image de celles décrivant les propriétés de la vue.
"The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content.Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.Our model can be used on problems with a huge, potentially infinite, number of categories.We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ryRh0bb0Z,Multi-View Data Generation Without View Supervision,"Cet article propose une architecture GAN qui vise à décomposer la distribution sous-jacente d'une classe particulière en ""contenu"" et ""vue""."
"The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content.Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.Our model can be used on problems with a huge, potentially infinite, number of categories.We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]",[],ryRh0bb0Z,Multi-View Data Generation Without View Supervision,Propose un nouveau modèle génératif basé sur le Generative Adversarial Network (GAN) qui démêle le contenu et la vue des objets sans supervision de la vue et étend le GMV en un modèle génératif conditionnel qui prend une image d'entrée et génère différentes vues de l'objet dans l'image d'entrée. 
"The huge size of deep networks hinders their use in small computing devices.In this paper, we consider compressing the network by weight quantization.We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","[0, 0, 0, 1]",[],BkrSv0lA-,Loss-aware Weight Quantization of Deep Networks,"Un algorithme de quantification de poids tenant compte de la perte est proposé, qui considère directement son effet sur la perte."
"The huge size of deep networks hinders their use in small computing devices.In this paper, we consider compressing the network by weight quantization.We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","[0, 0, 0, 1]",[],BkrSv0lA-,Loss-aware Weight Quantization of Deep Networks,Propose une méthode de compression de réseau au moyen de la ternarisation des poids. 
"The huge size of deep networks hinders their use in small computing devices.In this paper, we consider compressing the network by weight quantization.We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","[0, 0, 0, 1]",[],BkrSv0lA-,Loss-aware Weight Quantization of Deep Networks,"L'article propose une nouvelle méthode pour former des DNN avec des poids quantifiés, en incluant la quantification comme une contrainte dans un algorithme proximal quasi-Newton, qui apprend simultanément une mise à l'échelle pour les valeurs quantifiées."
"The huge size of deep networks hinders their use in small computing devices.In this paper, we consider compressing the network by weight quantization.We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","[0, 0, 0, 1]",[],BkrSv0lA-,Loss-aware Weight Quantization of Deep Networks,L'article étend le schéma de binarisation pondérée en fonction des pertes à la terarisation et à la quantification m-bit arbitraire et démontre ses performances prometteuses.
"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments.The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable.Indeed, it is preferable to learn options directly from interaction with the environment.Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels.Experimental results show that the options learned can be interpreted.Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rJIgf7bAZ,An inference-based policy gradient method for learning options,Nous développons une nouvelle méthode de gradient de politique pour l'apprentissage automatique de politiques avec options en utilisant une étape d'inférence différentiable.
"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments.The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable.Indeed, it is preferable to learn options directly from interaction with the environment.Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels.Experimental results show that the options learned can be interpreted.Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rJIgf7bAZ,An inference-based policy gradient method for learning options,"L'article présente une nouvelle technique de gradient de politique pour l'apprentissage des options, où un seul échantillon peut être utilisé pour mettre à jour toutes les options."
"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments.The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable.Indeed, it is preferable to learn options directly from interaction with the environment.Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels.Experimental results show that the options learned can be interpreted.Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rJIgf7bAZ,An inference-based policy gradient method for learning options,Propose une méthode hors politique pour l'apprentissage des options dans les problèmes continus complexes.
"The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000).The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.","[1, 0, 0]",[],ByxF-nAqYX,Locally Linear Unsupervised Feature Selection,Sélection non supervisée de caractéristiques par la capture de la structure linéaire locale des données
"The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000).The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.","[1, 0, 0]",[],ByxF-nAqYX,Locally Linear Unsupervised Feature Selection,Propose une sélection de caractéristiques non supervisée et localement linéaire.
"The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000).The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.","[1, 0, 0]",[],ByxF-nAqYX,Locally Linear Unsupervised Feature Selection,L'article propose la méthode LLUFS pour la sélection des caractéristiques.
"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax.""In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task.However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly.We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","[0, 0, 1, 0, 0, 0, 0]",[],H18WqugAb,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,"À l'aide d'une tâche de navigation simple axée sur le langage, nous étudions les capacités de composition des réseaux récurrents seq2seq modernes."
"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax.""In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task.However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly.We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","[0, 0, 1, 0, 0, 0, 0]",[],H18WqugAb,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,Cet article se concentre sur les capacités de composition de l'apprentissage zéro coup des RNN modernes de séquence à séquence et expose les lacunes des architectures RNN seq2seq actuelles.
"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax.""In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task.However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly.We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","[0, 0, 1, 0, 0, 0, 0]",[],H18WqugAb,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,"Le document analyse les capacités de composition des RNN, en particulier, la capacité de généralisation des RNN sur un sous-ensemble aléatoire de commandes SCAN, sur des commandes SCAN plus longues, et de composition sur des commandes primitives. "
"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax.""In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task.However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly.We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","[0, 0, 1, 0, 0, 0, 0]",[],H18WqugAb,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,Les auteurs présentent un nouveau jeu de données qui facilite l'analyse d'un cas d'apprentissage Seq2Seq.
"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.","[1, 0, 0, 0, 0]",[],S1xiOjC9F7,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,"Nous abordons le problème de l'apprentissage de la similarité pour les objets structurés avec des applications notamment en sécurité informatique, et proposons un nouveau modèle de réseaux de correspondance de graphes qui excelle dans cette tâche."
"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.","[1, 0, 0, 0, 0]",[],S1xiOjC9F7,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Les auteurs présentent un réseau d'appariement de graphes pour la récupération et l'appariement d'objets structurés en graphes.
"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.","[1, 0, 0, 0, 0]",[],S1xiOjC9F7,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Les auteurs s'attaquent au problème de l'appariement des graphes en proposant une extension des réseaux d'insertion de graphes.
"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.","[1, 0, 0, 0, 0]",[],S1xiOjC9F7,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Les auteurs présentent deux méthodes d'apprentissage d'un score de similarité entre des paires de graphes et montrent l'intérêt d'introduire des idées issues de l'appariement de graphes dans les réseaux neuronaux de graphes.
"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","[0, 1, 0, 0, 0, 0]",[],Bk7wvW-C-,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,Nous avons proposé un modèle d'encodage-décodage RNN-CNN pour un apprentissage rapide et non supervisé de la représentation des phrases.
"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","[0, 1, 0, 0, 0, 0]",[],Bk7wvW-C-,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,Modifications du cadre d'apprentissage de l'intégration des phrases par le saut de pensée.
"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","[0, 1, 0, 0, 0, 0]",[],Bk7wvW-C-,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,"Cet article présente une nouvelle conception hybride d'encodeur RNN et de décodeur CNN à utiliser en pré-entraînement, qui ne nécessite pas de décodeur autorégressif lors du pré-entraînement des encodeurs."
"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","[0, 1, 0, 0, 0, 0]",[],Bk7wvW-C-,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,Les auteurs étendent Skip-thought en décodant une seule phrase cible à l'aide d'un décodeur CNN.
"Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs).VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function.GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data.The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems.In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on.In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework.Our numerical results on several datasets demonstrate consistent trends with the proposed theory.","[0, 0, 0, 1, 0, 0, 0]",[],BygMAiRqK7,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Une approche statistique pour calculer les vraisemblances d'échantillon dans les réseaux adversariaux génératifs.
"Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs).VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function.GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data.The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems.In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on.In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework.Our numerical results on several datasets demonstrate consistent trends with the proposed theory.","[0, 0, 0, 1, 0, 0, 0]",[],BygMAiRqK7,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Montrer que le WGAN avec régularisation entropique maximise une borne inférieure sur la vraisemblance de la distribution des données observées.
"Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs).VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function.GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data.The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems.In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on.In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework.Our numerical results on several datasets demonstrate consistent trends with the proposed theory.","[0, 0, 0, 1, 0, 0, 0]",[],BygMAiRqK7,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,"Les auteurs affirment qu'il est possible de tirer parti de la limite supérieure d'un transport optimal régularisé par l'entropie pour obtenir une mesure de la ""probabilité de l'échantillon""."
"We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.Our contribution is threefold.First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode–-thus considerably facilitating the application of Riemannian geometry in machine learning.In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package","[1, 0, 0, 0, 0, 0]",[],rklXaoAcFX,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,"Nous présentons geomstats, un paquetage Python efficace pour la modélisation et l'optimisation riemannienne sur les manifolds, compatible avec numpy et tensorflow ."
"We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.Our contribution is threefold.First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode–-thus considerably facilitating the application of Riemannian geometry in machine learning.In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package","[1, 0, 0, 0, 0, 0]",[],rklXaoAcFX,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,"L'article présente le progiciel geomstats, qui permet d'utiliser simplement les manifolds et les métriques riemanniennes dans les modèles d'apprentissage automatique."
"We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.Our contribution is threefold.First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode–-thus considerably facilitating the application of Riemannian geometry in machine learning.In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package","[1, 0, 0, 0, 0, 0]",[],rklXaoAcFX,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,Propose un paquet Python pour l'optimisation et les applications sur les manifolds riemanniens et souligne les différences entre le paquet Geomstats et les autres paquets.
"We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.Our contribution is threefold.First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode–-thus considerably facilitating the application of Riemannian geometry in machine learning.In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package","[1, 0, 0, 0, 0, 0]",[],rklXaoAcFX,Geomstats: a Python Package for Riemannian Geometry in Machine Learning,"Présente une boîte à outils géométrique, Geomstats, pour l'apprentissage automatique sur les collecteurs riemanniens."
"We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference.The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices.However, most of the previous studies optimize for inference while neglect training or even complicate it.Training is far more intractable, since(i) the neurons dominate the memory cost rather than the weights in inference;(ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;(iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity.To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection.Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1goBoR9F7,Dynamic Sparse Graph for Efficient Deep Learning,Nous construisons un graphe dynamique clairsemé par le biais d'une recherche de réduction de la dimension afin de réduire les coûts de calcul et de mémoire à la fois dans l'apprentissage et l'inférence du DNN.
"We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference.The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices.However, most of the previous studies optimize for inference while neglect training or even complicate it.Training is far more intractable, since(i) the neurons dominate the memory cost rather than the weights in inference;(ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;(iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity.To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection.Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1goBoR9F7,Dynamic Sparse Graph for Efficient Deep Learning,Les auteurs proposent d'utiliser un graphe de calcul dynamique clairsemé pour réduire le coût en mémoire et en temps de calcul dans un réseau neuronal profond (DNN).
"We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference.The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices.However, most of the previous studies optimize for inference while neglect training or even complicate it.Training is far more intractable, since(i) the neurons dominate the memory cost rather than the weights in inference;(ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;(iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity.To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection.Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1goBoR9F7,Dynamic Sparse Graph for Efficient Deep Learning,Cet article propose une méthode pour accélérer la formation et l'inférence des réseaux neuronaux profonds en utilisant l'élagage dynamique du graphe de calcul.
"Efficient exploration remains a major challenge for reinforcement learning.One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic.Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting.Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning.As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning.The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise.We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.","[0, 0, 0, 0, 0, 1, 0]",[],Byx83s09Km,Information-Directed Exploration for Deep Reinforcement Learning,"Nous développons une extension pratique de l'échantillonnage dirigé par l'information pour l'apprentissage par renforcement, qui tient compte de l'incertitude paramétrique et de l'hétéroscédasticité dans la distribution des retours pour l'exploration."
"Efficient exploration remains a major challenge for reinforcement learning.One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic.Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting.Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning.As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning.The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise.We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.","[0, 0, 0, 0, 0, 1, 0]",[],Byx83s09Km,Information-Directed Exploration for Deep Reinforcement Learning,Les auteurs proposent une façon d'étendre l'échantillonnage dirigé par l'information à l'apprentissage par renforcement en combinant deux types d'incertitude pour obtenir une stratégie d'exploration simple basée sur l'IDS. 
"Efficient exploration remains a major challenge for reinforcement learning.One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic.Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting.Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning.As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning.The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise.We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.","[0, 0, 0, 0, 0, 1, 0]",[],Byx83s09Km,Information-Directed Exploration for Deep Reinforcement Learning,Cet article étudie des approches d'exploration sophistiques pour l'apprentissage par renforcement basées sur l'échantillonnage direct de l'information et sur l'apprentissage par renforcement distributif.
"We address the problem of learning structured policies for continuous control.In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent.In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.","[0, 0, 1, 0, 0, 0, 0]",[],S1sqHMZCb,NerveNet: Learning Structured Policy with Graph Neural Networks,l'utilisation d'un réseau de graphes neuronaux pour modéliser les informations structurelles des agents afin d'améliorer la politique et la transférabilité. 
"We address the problem of learning structured policies for continuous control.In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent.In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.","[0, 0, 1, 0, 0, 0, 0]",[],S1sqHMZCb,NerveNet: Learning Structured Policy with Graph Neural Networks,Méthode de représentation et d'apprentissage d'une politique structurée pour des tâches de contrôle continu à l'aide de réseaux neuronaux graphiques.
"We address the problem of learning structured policies for continuous control.In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent.In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.","[0, 0, 1, 0, 0, 0, 0]",[],S1sqHMZCb,NerveNet: Learning Structured Policy with Graph Neural Networks,"La soumission propose d'incorporer une structure supplémentaire dans les problèmes d'apprentissage par renforcement, en particulier la structure de la morphologie de l'agent."
"We address the problem of learning structured policies for continuous control.In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent.In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.","[0, 0, 1, 0, 0, 0, 0]",[],S1sqHMZCb,NerveNet: Learning Structured Policy with Graph Neural Networks,"Proposer une application des réseaux de neurones graphiques à l'apprentissage de politiques de contrôle de robots ""mille-pattes"" de différentes longueurs."
"Real-world tasks are often highly structured.Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Hyl_vjC5KQ,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Cet article présente un cadre d'apprentissage par renforcement hiérarchique basé sur des politiques d'option déterministes et la maximisation de l'information mutuelle. 
"Real-world tasks are often highly structured.Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Hyl_vjC5KQ,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Propose un algorithme HRL qui tente d'apprendre les options qui maximisent leur information mutuelle avec la densité état-action sous la politique optimale.
"Real-world tasks are often highly structured.Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Hyl_vjC5KQ,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Cet article propose un système HRL dans lequel l'information mutuelle de la variable latente et des paires état-action est approximativement maximisée.
"Real-world tasks are often highly structured.Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","[0, 0, 0, 0, 0, 0, 0, 1, 0]",[],Hyl_vjC5KQ,Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Propose un critère qui vise à maximiser l'information mutuelle entre les options et les paires état-action et montre empiriquement que les options apprises décomposent l'espace état-action mais pas l'espace état. 
"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs.We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkEqro0ctQ,Hierarchical interpretations for neural network predictions,"Nous introduisons et validons les interprétations locales hiérarchiques, la première technique pour rechercher et afficher automatiquement les interactions importantes pour les prédictions individuelles faites par les LSTM et les CNN."
"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs.We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkEqro0ctQ,Hierarchical interpretations for neural network predictions,Une nouvelle approche pour expliquer les prédictions des réseaux neuronaux par l'apprentissage de représentations hiérarchiques de groupes de caractéristiques d'entrée et leur contribution à la prédiction finale.
"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs.We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkEqro0ctQ,Hierarchical interpretations for neural network predictions,Étend une méthode existante d'interprétation des caractéristiques pour les LSTM à des DNN plus génériques et introduit un regroupement hiérarchique des caractéristiques d'entrée et les contributions de chaque regroupement à la prédiction finale.
"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs.We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkEqro0ctQ,Hierarchical interpretations for neural network predictions,Cet article propose une extension hiérarchique de la décomposition contextuelle.
"Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.","[0, 1, 0, 0, 0, 0]",[],rkl42iA5t7,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,"Nous proposons une méthode facile à mettre en œuvre, mais efficace, pour la compression des réseaux neuronaux. La PFA exploite la corrélation intrinsèque entre les réponses des filtres au sein des couches du réseau pour recommander une empreinte de réseau plus petite."
"Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.","[0, 1, 0, 0, 0, 0]",[],rkl42iA5t7,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,"Propose d'élaguer les réseaux convolutifs en analysant la corrélation observée entre les filtres d'une même couche, exprimée par le spectre des valeurs propres de leur matrice de covariance."
"Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.","[0, 1, 0, 0, 0, 0]",[],rkl42iA5t7,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,Cet article présente une approche de la compression des réseaux neuronaux en examinant la corrélation des réponses des filtres dans chaque couche via deux stratégies.
"Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.","[0, 1, 0, 0, 0, 0]",[],rkl42iA5t7,NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES,Ce document propose une méthode de compression basée sur l'analyse spectrale.
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.We show that the improved performance is due to the increased diversity of reformulation strategies.","[1, 0, 0, 0, 0]",[],rkMhusC5Y7,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,Des agents de reformulation de requêtes multiples et diversifiées formés par apprentissage par renforcement pour améliorer les moteurs de recherche.
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.We show that the improved performance is due to the increased diversity of reformulation strategies.","[1, 0, 0, 0, 0]",[],rkMhusC5Y7,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,"Parelléisation de la méthode d'ensemble dans l'apprentissage par réinvestissement pour la reformulation de requêtes, accélérant l'apprentissage et améliorant la diversité des formulations apprises."
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.We show that the improved performance is due to the increased diversity of reformulation strategies.","[1, 0, 0, 0, 0]",[],rkMhusC5Y7,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,"Les auteurs proposent de former plusieurs agents distincts, chacun sur un sous-ensemble différent de l'ensemble de formation."
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.We show that the improved performance is due to the increased diversity of reformulation strategies.","[1, 0, 0, 0, 0]",[],rkMhusC5Y7,Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation,Les auteurs proposent une approche d'ensemble pour la reformulation des requêtes.
"Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\mathbb{R}^d$.Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label').In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc.To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.).We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity.Finally, we illustrate the potential of CNE for network visualization.","[0, 0, 0, 0, 0, 0, 0, 1]",[],ryepUj0qtX,Conditional Network Embeddings,"Nous présentons une méthode d'intégration de réseau qui tient compte des informations préalables sur le réseau, ce qui permet d'obtenir des performances empiriques supérieures."
"Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\mathbb{R}^d$.Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label').In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc.To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.).We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity.Finally, we illustrate the potential of CNE for network visualization.","[0, 0, 0, 0, 0, 0, 0, 1]",[],ryepUj0qtX,Conditional Network Embeddings,"L'article propose d'utiliser une distribution préalable pour contraindre l'intégration du réseau. Pour la formulation, cet article a utilisé des distributions gaussiennes très restreintes."
"Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\mathbb{R}^d$.Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label').In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc.To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.).We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity.Finally, we illustrate the potential of CNE for network visualization.","[0, 0, 0, 0, 0, 0, 0, 1]",[],ryepUj0qtX,Conditional Network Embeddings,Propose d'apprendre des encastrements de nœuds non supervisés en considérant les propriétés structurelles des réseaux.
"This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients.This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad.Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question.In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\log{T}/\sqrt{T})$ for non-convex stochastic optimization.Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum).We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge.Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.","[0, 0, 0, 0, 0, 1, 0]",[],H1x-x309tm,On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization,"Nous analysons la convergence des algorithmes de type Adam et fournissons des conditions suffisantes légères pour garantir leur convergence, nous montrons également que la violation des conditions peut faire diverger un algorithme."
"This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients.This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad.Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question.In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\log{T}/\sqrt{T})$ for non-convex stochastic optimization.Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum).We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge.Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.","[0, 0, 0, 0, 0, 1, 0]",[],H1x-x309tm,On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization,Présente une analyse de convergence dans le cadre non convexe pour une famille d'algorithmes d'optimisation.
"This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients.This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad.Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question.In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\log{T}/\sqrt{T})$ for non-convex stochastic optimization.Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum).We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge.Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior.","[0, 0, 0, 0, 0, 1, 0]",[],H1x-x309tm,On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization,Cet article étudie la condition de convergence des optimiseurs de type Adam dans les problèmes d'optimisation non convexes sans contrainte.
"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data.The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required.This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture.Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","[0, 0, 1, 0, 0]",[],rkhxwltab,AANN: Absolute Artificial Neural Network,"Auto-encodeur à poids liés avec la fonction abs comme fonction d'activation, apprend à faire de la classification dans le sens direct et de la régression dans le sens inverse grâce à une fonction de coût spécialement définie."
"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data.The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required.This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture.Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","[0, 0, 1, 0, 0]",[],rkhxwltab,AANN: Absolute Artificial Neural Network,L'article propose d'utiliser la fonction d'activation de la valeur absolue dans une architecture d'auto-codage avec un terme supplémentaire d'apprentissage supervisé dans la fonction objectif.
"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data.The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required.This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture.Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","[0, 0, 1, 0, 0]",[],rkhxwltab,AANN: Absolute Artificial Neural Network,Cet article présente un réseau réversible dont la fonction d'activation est la valeur absolue.
"Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.Similarly, pre-processing introduces an additional source of error.To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.Furthermore, we observe a significant increase in sample efficiency.With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.We open-source our trained models, experiments, and source code.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BJgrxbqp67,Improving Relation Extraction by Pre-trained Language Representations,Nous proposons un modèle d'extraction de relations basé sur un transformateur qui utilise des représentations linguistiques pré-entraînées au lieu de caractéristiques linguistiques explicites.
"Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.Similarly, pre-processing introduces an additional source of error.To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.Furthermore, we observe a significant increase in sample efficiency.With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.We open-source our trained models, experiments, and source code.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BJgrxbqp67,Improving Relation Extraction by Pre-trained Language Representations,Présente un modèle d'extraction de relations basé sur un transformateur qui tire parti du pré-entraînement sur du texte non étiqueté avec un objectif de modélisation du langage.
"Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.Similarly, pre-processing introduces an additional source of error.To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.Furthermore, we observe a significant increase in sample efficiency.With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.We open-source our trained models, experiments, and source code.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BJgrxbqp67,Improving Relation Extraction by Pre-trained Language Representations,Cet article décrit une nouvelle application des réseaux Transformer pour l'extraction de relations.
"Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.Similarly, pre-processing introduces an additional source of error.To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.Furthermore, we observe a significant increase in sample efficiency.With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.We open-source our trained models, experiments, and source code.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],BJgrxbqp67,Improving Relation Extraction by Pre-trained Language Representations,"L'article présente une architecture basée sur un transformateur pour l'extraction de la relaxation, évaluée sur deux ensembles de données."
"Neural networks have recently had a lot of success for many tasks.However, neuralnetwork architectures that perform well are still typically designed manuallyby experts in a cumbersome trial-and-error process.We propose a new methodto automatically search for well-performing CNN architectures based on a simplehill climbing procedure whose operators apply network morphisms, followedby short optimization runs by cosine annealing.Surprisingly, this simple methodyields competitive results, despite only requiring resources in the same order ofmagnitude as training a single network.E.g., on CIFAR-10, our method designsand trains networks with an error rate below 6% in only 12 hours on a single GPU;training for one day reduces this error further, to almost 5%.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SySaJ0xCZ,Simple and efficient architecture search for Convolutional Neural Networks,Nous proposons une méthode simple et efficace de recherche d'architecture pour les réseaux de neurones convolutifs.
"Neural networks have recently had a lot of success for many tasks.However, neuralnetwork architectures that perform well are still typically designed manuallyby experts in a cumbersome trial-and-error process.We propose a new methodto automatically search for well-performing CNN architectures based on a simplehill climbing procedure whose operators apply network morphisms, followedby short optimization runs by cosine annealing.Surprisingly, this simple methodyields competitive results, despite only requiring resources in the same order ofmagnitude as training a single network.E.g., on CIFAR-10, our method designsand trains networks with an error rate below 6% in only 12 hours on a single GPU;training for one day reduces this error further, to almost 5%.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SySaJ0xCZ,Simple and efficient architecture search for Convolutional Neural Networks,Propose une méthode de recherche par architecture neuronale qui atteint une précision proche de l'état de l'art sur CIFAR10 et nécessite beaucoup moins de ressources informatiques.
"Neural networks have recently had a lot of success for many tasks.However, neuralnetwork architectures that perform well are still typically designed manuallyby experts in a cumbersome trial-and-error process.We propose a new methodto automatically search for well-performing CNN architectures based on a simplehill climbing procedure whose operators apply network morphisms, followedby short optimization runs by cosine annealing.Surprisingly, this simple methodyields competitive results, despite only requiring resources in the same order ofmagnitude as training a single network.E.g., on CIFAR-10, our method designsand trains networks with an error rate below 6% in only 12 hours on a single GPU;training for one day reduces this error further, to almost 5%.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SySaJ0xCZ,Simple and efficient architecture search for Convolutional Neural Networks,"Présente une méthode de recherche d'architectures de réseaux neuronaux en même temps que la formation, ce qui permet de réduire considérablement le temps de formation et le temps de recherche des architectures."
"Neural networks have recently had a lot of success for many tasks.However, neuralnetwork architectures that perform well are still typically designed manuallyby experts in a cumbersome trial-and-error process.We propose a new methodto automatically search for well-performing CNN architectures based on a simplehill climbing procedure whose operators apply network morphisms, followedby short optimization runs by cosine annealing.Surprisingly, this simple methodyields competitive results, despite only requiring resources in the same order ofmagnitude as training a single network.E.g., on CIFAR-10, our method designsand trains networks with an error rate below 6% in only 12 hours on a single GPU;training for one day reduces this error further, to almost 5%.","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SySaJ0xCZ,Simple and efficient architecture search for Convolutional Neural Networks,propose une variante de la recherche d'architecture neuronale utilisant des morphismes de réseau pour définir un espace de recherche utilisant des architectures CNN remplissant la tâche de classification d'images CIFAR
"We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective.GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph.Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties.We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them.Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.","[1, 0, 0, 0, 0, 0, 0]",[],H15RufWAW,GraphGAN: Generating Graphs via Random Walks,Utilisation des GAN pour générer des graphes par marche aléatoire.
"We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective.GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph.Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties.We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them.Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.","[1, 0, 0, 0, 0, 0, 0]",[],H15RufWAW,GraphGAN: Generating Graphs via Random Walks,"Les auteurs ont proposé un modèle génératif de marches aléatoires sur des graphes qui permet un apprentissage agnostique, un ajustement contrôlable, la génération de graphes d'ensemble."
"We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective.GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph.Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties.We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them.Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks.","[1, 0, 0, 0, 0, 0, 0]",[],H15RufWAW,GraphGAN: Generating Graphs via Random Walks,Propose une formulation WGAN pour générer des graphes basés sur des marches aléatoires en utilisant l'intégration des nœuds et une architecture LSTM pour la modélisation.
"The ability of a classifier to recognize unknown inputs is important for many classification-based systems.We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.We propose a method based on the Generative Adversarial Networks (GAN) framework.We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.","[0, 0, 0, 0, 0, 1]",[],Hy7EPh10W,Novelty Detection with GAN,Nous proposons de résoudre un problème de classification et de détection de nouveauté simultanées dans le cadre du GAN.
"The ability of a classifier to recognize unknown inputs is important for many classification-based systems.We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.We propose a method based on the Generative Adversarial Networks (GAN) framework.We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.","[0, 0, 0, 0, 0, 1]",[],Hy7EPh10W,Novelty Detection with GAN,Propose un GAN pour unifier la classification et la détection de la nouveauté.
"The ability of a classifier to recognize unknown inputs is important for many classification-based systems.We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.We propose a method based on the Generative Adversarial Networks (GAN) framework.We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.","[0, 0, 0, 0, 0, 1]",[],Hy7EPh10W,Novelty Detection with GAN,L'article présente une méthode de détection de la nouveauté basée sur un GAN multi-classes qui est entraîné à produire des images générées à partir d'un mélange des distributions nominale et nouvelle.
"The ability of a classifier to recognize unknown inputs is important for many classification-based systems.We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.We propose a method based on the Generative Adversarial Networks (GAN) framework.We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.","[0, 0, 0, 0, 0, 1]",[],Hy7EPh10W,Novelty Detection with GAN,L'article propose un GAN pour la détection de la nouveauté en utilisant un générateur de mélange avec une perte de correspondance des caractéristiques.
"  Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security.A crucial requirement of such speaker verification systems is to be domain robust.Performance should not degrade even if speakers are talking in languages not seen during training.To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks.We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes.We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy.Our results also show that models with adversarial adaptation perform significantly better than unadapted models.In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances.Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Byx4xH3is7,SpeakerGAN: Recognizing Speakers in New Languages with Generative Adversarial Networks,"La performance de la vérification du locuteur peut être améliorée de manière significative en adaptant le modèle aux données du domaine à l'aide de réseaux adversariaux génératifs. En outre, l'adaptation peut être effectuée de manière non supervisée."
"  Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security.A crucial requirement of such speaker verification systems is to be domain robust.Performance should not degrade even if speakers are talking in languages not seen during training.To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks.We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes.We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy.Our results also show that models with adversarial adaptation perform significantly better than unadapted models.In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances.Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],Byx4xH3is7,SpeakerGAN: Recognizing Speakers in New Languages with Generative Adversarial Networks,Proposer un certain nombre de variantes de GAN sur la tâche de reconnaissance du locuteur dans la condition de non correspondance de domaine.
"Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.","[0, 0, 0, 0, 1, 0, 0]",[],Skl3M20qYQ,Non-Synergistic Variational Autoencoders,Minimiser l'information mutuelle synergique entre les latents et les données pour la tâche de démêlage en utilisant le cadre VAE.
"Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.","[0, 0, 0, 0, 1, 0, 0]",[],Skl3M20qYQ,Non-Synergistic Variational Autoencoders,Propose une nouvelle fonction objectif pour l'apprentissage de représentations dientangulaires dans un cadre variationnel en minimisant la synergie des informations fournies.
"Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.","[0, 0, 0, 0, 1, 0, 0]",[],Skl3M20qYQ,Non-Synergistic Variational Autoencoders,"Les auteurs visent à former un VAE qui a démêlé les représentations latentes de manière ""synergique"" maximale. "
"Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents.","[0, 0, 0, 0, 1, 0, 0]",[],Skl3M20qYQ,Non-Synergistic Variational Autoencoders,Cet article propose une nouvelle approche pour imposer le désenchevêtrement dans les VAE en utilisant un terme qui pénalise l'information mutuelle synergique entre les variables latentes.
"   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations.Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random.In this work, we propose the use of {\em structured arrangements} through randomized {\em microbatches} of examples that are more likely to include similar ones.We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\%.Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.","[0, 0, 0, 0, 0, 1, 0]",[],r1erRoCqtX,LSH Microbatches for Stochastic Gradients:  Value in Rearrangement,Accélérer le SGD en disposant les exemples différemment
"   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations.Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random.In this work, we propose the use of {\em structured arrangements} through randomized {\em microbatches} of examples that are more likely to include similar ones.We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\%.Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.","[0, 0, 0, 0, 0, 1, 0]",[],r1erRoCqtX,LSH Microbatches for Stochastic Gradients:  Value in Rearrangement,L'article présente une méthode permettant d'améliorer le taux de convergence de la descente de gradient stochastique pour l'apprentissage des encastrements en regroupant les échantillons d'entraînement similaires.
"   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations.Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random.In this work, we propose the use of {\em structured arrangements} through randomized {\em microbatches} of examples that are more likely to include similar ones.We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\%.Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment.","[0, 0, 0, 0, 0, 1, 0]",[],r1erRoCqtX,LSH Microbatches for Stochastic Gradients:  Value in Rearrangement,Propose une stratégie d'échantillonnage non-uniforme pour construire des minibatchs dans SGD pour la tâche d'apprentissage des embeddings pour les associations d'objets.
"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-enddeep neural network models directly from the conversation data.One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words.In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method.For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.","[0, 0, 1, 0, 0, 0]",[],rJ7yZ2P6-,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,Combinaison d'informations entre l'intégration de mots préconstruite et la représentation de mots spécifiques à la tâche pour résoudre le problème du hors-vocabulaire.
"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-enddeep neural network models directly from the conversation data.One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words.In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method.For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.","[0, 0, 1, 0, 0, 0]",[],rJ7yZ2P6-,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,Cet article propose une approche visant à améliorer la prédiction de l'incorporation hors vocabulaire pour la tâche de modélisation des conversations de dialogue avec des gains considérables par rapport aux lignes de base.
"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-enddeep neural network models directly from the conversation data.One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words.In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method.For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.","[0, 0, 1, 0, 0, 0]",[],rJ7yZ2P6-,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,Propose de combiner les incorporations de mots pré-entraînées externes et les incorporations de mots pré-entraînées sur les données de formation en les conservant comme deux vues.
"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-enddeep neural network models directly from the conversation data.One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words.In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method.For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.","[0, 0, 1, 0, 0, 0]",[],rJ7yZ2P6-,Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus,Proposition d'une méthode pour étendre la couverture des encastrements de mots pré-entraînés afin de traiter le problème de l'OOV qui se pose lorsqu'on les applique à des ensembles de données conversationnelles et application de nouvelles variantes du modèle LSTM à la tâche de sélection des réponses dans la modélisation du dialogue.
"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.This allows us to train neural networks to perform optimization faster than well tuned first-order methods.Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJxwAo09KQ,Learned optimizers that outperform on wall-clock and validation loss,"Nous analysons les problèmes qui se posent lors de l'entraînement des optimiseurs appris, nous traitons ces problèmes par le biais de l'optimisation variationnelle en utilisant deux estimateurs de gradient complémentaires, et nous entraînons des optimiseurs qui sont 5 fois plus rapides en temps d'horloge murale que les optimiseurs de base (par exemple Adam)."
"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.This allows us to train neural networks to perform optimization faster than well tuned first-order methods.Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJxwAo09KQ,Learned optimizers that outperform on wall-clock and validation loss,Cet article utilise l'optimisation non roulée pour apprendre des réseaux neuronaux pour l'optimisation.
"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.This allows us to train neural networks to perform optimization faster than well tuned first-order methods.Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJxwAo09KQ,Learned optimizers that outperform on wall-clock and validation loss,"Cet article aborde le problème de l'apprentissage d'un optimiseur, en particulier les auteurs se concentrent sur l'obtention de gradients plus propres à partir de la procédure d'apprentissage non enroulée."
"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.This allows us to train neural networks to perform optimization faster than well tuned first-order methods.Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement","[0, 0, 1, 0, 0, 0, 0, 0, 0]",[],HJxwAo09KQ,Learned optimizers that outperform on wall-clock and validation loss,"Présente une méthode pour ""apprendre un optimiseur"" en utilisant une optimisation variationnelle pour la perte de l'optimiseur ""externe"" et propose l'idée de combiner à la fois le gradient reparamétré et l'estimateur de la fonction de score pour l'objectif variationnel et les pondère en utilisant une formule de produit de Gauss pour la moyenne."
"Asynchronous distributed gradient descent algorithms for training of deep neuralnetworks are usually considered as inefficient, mainly because of the Gradient delayproblem.In this paper, we propose a novel asynchronous distributed algorithmthat tackles this limitation by well-thought-out averaging of model updates, computedby workers.The algorithm allows computing gradients along the processof gradient merge, thus, reducing or even completely eliminating worker idle timedue to communication overhead, which is a pitfall of existing asynchronous methods.We provide theoretical analysis of the proposed asynchronous algorithm,and show its regret bounds.According to our analysis, the crucial parameter forkeeping high convergence rate is the maximal discrepancy between local parametervectors of any pair of workers.As long as it is kept relatively small, theconvergence rate of the algorithm is shown to be the same as the one of a sequentialonline learning.Furthermore, in our algorithm, this discrepancy is boundedby an expression that involves the staleness parameter of the algorithm, and isindependent on the number of workers.This is the main differentiator betweenour approach and other solutions, such as Elastic Asynchronous SGD or DownpourSGD, in which that maximal discrepancy is bounded by an expression thatdepends on the number of workers, due to gradient delay problem.To demonstrateeffectiveness of our approach, we conduct a series of experiments on imageclassification task on a cluster with 4 machines, equipped with a commodity communicationswitch and with a single GPU card per machine.Our experimentsshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,while eliminating almost completely worker idle time.Since our method allowsusing commodity communication switch, it paves a way for large scale distributedtraining performed on commodity clusters.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,Une méthode pour une formation distribuée asynchrone efficace de modèles d'apprentissage profond ainsi que des limites de regret théoriques.
"Asynchronous distributed gradient descent algorithms for training of deep neuralnetworks are usually considered as inefficient, mainly because of the Gradient delayproblem.In this paper, we propose a novel asynchronous distributed algorithmthat tackles this limitation by well-thought-out averaging of model updates, computedby workers.The algorithm allows computing gradients along the processof gradient merge, thus, reducing or even completely eliminating worker idle timedue to communication overhead, which is a pitfall of existing asynchronous methods.We provide theoretical analysis of the proposed asynchronous algorithm,and show its regret bounds.According to our analysis, the crucial parameter forkeeping high convergence rate is the maximal discrepancy between local parametervectors of any pair of workers.As long as it is kept relatively small, theconvergence rate of the algorithm is shown to be the same as the one of a sequentialonline learning.Furthermore, in our algorithm, this discrepancy is boundedby an expression that involves the staleness parameter of the algorithm, and isindependent on the number of workers.This is the main differentiator betweenour approach and other solutions, such as Elastic Asynchronous SGD or DownpourSGD, in which that maximal discrepancy is bounded by an expression thatdepends on the number of workers, due to gradient delay problem.To demonstrateeffectiveness of our approach, we conduct a series of experiments on imageclassification task on a cluster with 4 machines, equipped with a commodity communicationswitch and with a single GPU card per machine.Our experimentsshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,while eliminating almost completely worker idle time.Since our method allowsusing commodity communication switch, it paves a way for large scale distributedtraining performed on commodity clusters.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,L'article propose un algorithme pour limiter la stagnation dans les SGD asynchrones et fournit une analyse théorique.
"Asynchronous distributed gradient descent algorithms for training of deep neuralnetworks are usually considered as inefficient, mainly because of the Gradient delayproblem.In this paper, we propose a novel asynchronous distributed algorithmthat tackles this limitation by well-thought-out averaging of model updates, computedby workers.The algorithm allows computing gradients along the processof gradient merge, thus, reducing or even completely eliminating worker idle timedue to communication overhead, which is a pitfall of existing asynchronous methods.We provide theoretical analysis of the proposed asynchronous algorithm,and show its regret bounds.According to our analysis, the crucial parameter forkeeping high convergence rate is the maximal discrepancy between local parametervectors of any pair of workers.As long as it is kept relatively small, theconvergence rate of the algorithm is shown to be the same as the one of a sequentialonline learning.Furthermore, in our algorithm, this discrepancy is boundedby an expression that involves the staleness parameter of the algorithm, and isindependent on the number of workers.This is the main differentiator betweenour approach and other solutions, such as Elastic Asynchronous SGD or DownpourSGD, in which that maximal discrepancy is bounded by an expression thatdepends on the number of workers, due to gradient delay problem.To demonstrateeffectiveness of our approach, we conduct a series of experiments on imageclassification task on a cluster with 4 machines, equipped with a commodity communicationswitch and with a single GPU card per machine.Our experimentsshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,while eliminating almost completely worker idle time.Since our method allowsusing commodity communication switch, it paves a way for large scale distributedtraining performed on commodity clusters.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1lo3sC9KX,Asynchronous SGD without gradient delay for efficient distributed training,Propose un algorithme hybride pour éliminer le retard du gradient des méthodes asynchrones.
"Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs.However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks.This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks.We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise.Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference.Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches.As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,Nous avons conçu une nouvelle méthodologie de quantification pour optimiser conjointement l'efficacité et la robustesse des modèles d'apprentissage profond.
"Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs.However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks.This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks.We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise.Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference.Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches.As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.","[0, 0, 1, 0, 0, 0, 0, 0]",[],ryetZ20ctX,Defensive Quantization: When Efficiency Meets Robustness,Propose un schéma de régularisation pour protéger les réseaux neuronaux quantifiés contre les attaques adverses en utilisant un filtrage constant de Lipschitz des entrées-sorties des couches internes.
"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks.However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies.In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.This model can also be encouraged to perform fewer state updates through a budget constraint.We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.","[0, 0, 0, 1, 0, 0, 0]",[],HkwVAXyCW,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,Une modification pour les architectures RNN existantes qui leur permet de sauter les mises à jour d'état tout en préservant les performances des architectures originales.
"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks.However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies.In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.This model can also be encouraged to perform fewer state updates through a budget constraint.We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.","[0, 0, 0, 1, 0, 0, 0]",[],HkwVAXyCW,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,"Propose le modèle Skip RNN qui permet à un réseau récurrent d'ignorer sélectivement la mise à jour de son état caché pour certaines entrées, ce qui permet de réduire le calcul au moment du test."
"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks.However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies.In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.This model can also be encouraged to perform fewer state updates through a budget constraint.We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/.","[0, 0, 0, 1, 0, 0, 0]",[],HkwVAXyCW,Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks,Propose un nouveau modèle de RNN dans lequel l'entrée et la mise à jour de l'état des cellules récurrentes sont sautées de manière adaptative pour certains pas de temps.
"We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement.Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step.Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network.This estimate has the same size and is similar to the momentum variable that is commonly used in SGD.No estimate of the Hessian is maintained.We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle.We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning.We also show our optimiser's generality by testing on a large set of randomly-generated architectures.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Sygx4305KQ,Small steps and giant leaps: Minimal Newton solvers for Deep Learning,Un solveur rapide du second ordre pour l'apprentissage profond qui fonctionne sur des problèmes à l'échelle d'ImageNet sans réglage des hyperparamètres.
"We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement.Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step.Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network.This estimate has the same size and is similar to the momentum variable that is commonly used in SGD.No estimate of the Hessian is maintained.We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle.We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning.We also show our optimiser's generality by testing on a large set of randomly-generated architectures.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Sygx4305KQ,Small steps and giant leaps: Minimal Newton solvers for Deep Learning,"Choix de la direction en utilisant une seule étape de descente du gradient ""vers l'étape de Newton"" à partir d'une estimation originale, puis en prenant cette direction au lieu du gradient original."
"We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement.Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step.Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network.This estimate has the same size and is similar to the momentum variable that is commonly used in SGD.No estimate of the Hessian is maintained.We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle.We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning.We also show our optimiser's generality by testing on a large set of randomly-generated architectures.","[1, 0, 0, 0, 0, 0, 0, 0, 0]",[],Sygx4305KQ,Small steps and giant leaps: Minimal Newton solvers for Deep Learning,Une nouvelle méthode d'optimisation approximative du second ordre à faible coût de calcul qui remplace le calcul de la matrice hessienne par une seule étape de gradient et une stratégie de démarrage à chaud.
"The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development.However, to push this idea towards practical implementation, we need better models and better ways of training.We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function.We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes.With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.","[0, 0, 1, 0, 0]",[],ByxBFsRqYm,"Attention, Learn to Solve Routing Problems!",Modèle basé sur l'attention entraîné avec REINFORCE avec une ligne de base pour l'apprentissage d'heuristiques avec des résultats compétitifs sur TSP et d'autres problèmes de routage.
"The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development.However, to push this idea towards practical implementation, we need better models and better ways of training.We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function.We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes.With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.","[0, 0, 1, 0, 0]",[],ByxBFsRqYm,"Attention, Learn to Solve Routing Problems!",Présente une approche basée sur l'attention pour apprendre une politique de résolution de TSP et d'autres problèmes d'optimisation combinatoire de type routage.
"The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development.However, to push this idea towards practical implementation, we need better models and better ways of training.We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function.We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes.With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.","[0, 0, 1, 0, 0]",[],ByxBFsRqYm,"Attention, Learn to Solve Routing Problems!",Ce document tente d'apprendre des heuristiques pour résoudre des problèmes d'optimisation combinatoire.
"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.Finally, we show its effectiveness on two distinct tasks.","[1, 0, 0]",[],H1OQukZ0-,Online Hyper-Parameter Optimization,Un algorithme pour optimiser les hyper-paramètres de régularisation pendant l'apprentissage.
"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.Finally, we show its effectiveness on two distinct tasks.","[1, 0, 0]",[],H1OQukZ0-,Online Hyper-Parameter Optimization,L'article propose un moyen de réinitialiser y à chaque mise à jour de lambda et une procédure d'écrêtage de y pour maintenir la stabilité du système dynamique.
"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.Finally, we show its effectiveness on two distinct tasks.","[1, 0, 0]",[],H1OQukZ0-,Online Hyper-Parameter Optimization,Propose un algorithme pour l'optimisation des hyperparamètres qui peut être vu comme une extension de Franceschi 2017 où certaines estimations sont recommencées à chaud pour augmenter la stabilité de la méthode.
"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.Finally, we show its effectiveness on two distinct tasks.","[1, 0, 0]",[],H1OQukZ0-,Online Hyper-Parameter Optimization,Propose une extension à une méthode existante pour optimiser les hyperparamètres de régularisation.
"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks.However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation.We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models.We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.","[0, 0, 1, 0]",[],ByJHuTgA-,On the State of the Art of Evaluation in Neural Language Models,"Montrer que les LSTM sont aussi bons, voire meilleurs, que les innovations récentes pour LM et que l'évaluation des modèles est souvent peu fiable."
"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks.However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation.We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models.We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.","[0, 0, 1, 0]",[],ByJHuTgA-,On the State of the Art of Evaluation in Neural Language Models,"Cet article décrit une validation complète des modèles de langage de mots et de caractères basés sur les LSTM, ce qui a conduit à un résultat significatif dans la modélisation du langage et à une étape importante dans l'apprentissage profond."
"  Residual and skip connections play an important role in many current  generative models.Although their theoretical and numerical advantages  are understood, their role in speech enhancement systems has not been  investigated so far.When performing spectral speech enhancement,  residual connections are very similar in nature to spectral subtraction,  which is the one of the most commonly employed speech enhancement approaches.  Highway networks, on the other hand, can be seen as a combination of spectral  masking and spectral subtraction.However, when using deep neural networks, such operations would  normally happen in a transformed spectral domain, as opposed to traditional speech  enhancement where all operations are often done directly on the spectrum.  In this paper, we aim to investigate the role of residual and highway  connections in deep neural networks for speech enhancement, and verify whether  or not they operate similarly to their traditional, digital signal processing  counterparts.We visualize the outputs of such connections, projected back to  the spectral domain, in models trained for speech denoising, and show that while  skip connections do not necessarily improve performance with regards to the  number of parameters, they make speech enhancement models more interpretable.","[0, 0, 0, 0, 0, 0, 1]",[],rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,"Nous montrons comment l'utilisation de connexions sautées peut rendre les modèles d'amélioration de la parole plus interprétables, car ils utilisent des mécanismes similaires à ceux qui ont été explorés dans la littérature DSP."
"  Residual and skip connections play an important role in many current  generative models.Although their theoretical and numerical advantages  are understood, their role in speech enhancement systems has not been  investigated so far.When performing spectral speech enhancement,  residual connections are very similar in nature to spectral subtraction,  which is the one of the most commonly employed speech enhancement approaches.  Highway networks, on the other hand, can be seen as a combination of spectral  masking and spectral subtraction.However, when using deep neural networks, such operations would  normally happen in a transformed spectral domain, as opposed to traditional speech  enhancement where all operations are often done directly on the spectrum.  In this paper, we aim to investigate the role of residual and highway  connections in deep neural networks for speech enhancement, and verify whether  or not they operate similarly to their traditional, digital signal processing  counterparts.We visualize the outputs of such connections, projected back to  the spectral domain, in models trained for speech denoising, and show that while  skip connections do not necessarily improve performance with regards to the  number of parameters, they make speech enhancement models more interpretable.","[0, 0, 0, 0, 0, 0, 1]",[],rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,"Les auteurs proposent d'incorporer les blocs Residual, Highway et Masking dans un pipeline entièrement convolutif afin de comprendre comment l'inférence itérative de la sortie et du masquage est effectuée dans une tâche d'amélioration de la parole."
"  Residual and skip connections play an important role in many current  generative models.Although their theoretical and numerical advantages  are understood, their role in speech enhancement systems has not been  investigated so far.When performing spectral speech enhancement,  residual connections are very similar in nature to spectral subtraction,  which is the one of the most commonly employed speech enhancement approaches.  Highway networks, on the other hand, can be seen as a combination of spectral  masking and spectral subtraction.However, when using deep neural networks, such operations would  normally happen in a transformed spectral domain, as opposed to traditional speech  enhancement where all operations are often done directly on the spectrum.  In this paper, we aim to investigate the role of residual and highway  connections in deep neural networks for speech enhancement, and verify whether  or not they operate similarly to their traditional, digital signal processing  counterparts.We visualize the outputs of such connections, projected back to  the spectral domain, in models trained for speech denoising, and show that while  skip connections do not necessarily improve performance with regards to the  number of parameters, they make speech enhancement models more interpretable.","[0, 0, 0, 0, 0, 0, 1]",[],rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,"Les auteurs interprètent les connexions d'autoroute, résiduelles et de masquage. "
"  Residual and skip connections play an important role in many current  generative models.Although their theoretical and numerical advantages  are understood, their role in speech enhancement systems has not been  investigated so far.When performing spectral speech enhancement,  residual connections are very similar in nature to spectral subtraction,  which is the one of the most commonly employed speech enhancement approaches.  Highway networks, on the other hand, can be seen as a combination of spectral  masking and spectral subtraction.However, when using deep neural networks, such operations would  normally happen in a transformed spectral domain, as opposed to traditional speech  enhancement where all operations are often done directly on the spectrum.  In this paper, we aim to investigate the role of residual and highway  connections in deep neural networks for speech enhancement, and verify whether  or not they operate similarly to their traditional, digital signal processing  counterparts.We visualize the outputs of such connections, projected back to  the spectral domain, in models trained for speech denoising, and show that while  skip connections do not necessarily improve performance with regards to the  number of parameters, they make speech enhancement models more interpretable.","[0, 0, 0, 0, 0, 0, 1]",[],rkzeXBDos7,Investigating the effect of residual and highway connections in speech enhancement models,Les auteurs génèrent leur propre discours bruyant en ajoutant artificiellement du bruit provenant d'un ensemble de données de bruit bien établi à un ensemble de données de discours propre moins connu.
"Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data.Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient.With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates.We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances.Combining these two innovations, the resulting method is highly efficient and robust.On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.","[0, 0, 0, 1, 0, 0, 0]",[],B1l08oAct7,Deterministic Variational Inference for Robust Bayesian Neural Networks,Méthode d'élimination de la variance du gradient et de réglage automatique des prieurs pour un entraînement efficace des réseaux neuronaux bayésiens
"Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data.Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient.With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates.We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances.Combining these two innovations, the resulting method is highly efficient and robust.On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.","[0, 0, 0, 1, 0, 0, 0]",[],B1l08oAct7,Deterministic Variational Inference for Robust Bayesian Neural Networks,Propose une nouvelle approche pour effectuer l'inférence variationnelle déterministe pour les BNN à action directe avec des fonctions d'activation non linéaires spécifiques par l'approximation des moments de la couche.
"Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data.Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient.With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates.We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances.Combining these two innovations, the resulting method is highly efficient and robust.On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.","[0, 0, 0, 1, 0, 0, 0]",[],B1l08oAct7,Deterministic Variational Inference for Robust Bayesian Neural Networks,L'article considère une approche purement déterministe de l'apprentissage des approximations variationnelles postérieures pour les réseaux neuronaux bayésiens.
Skills learned through (deep) reinforcement learning often generalizes poorlyacross tasks and re-training is necessary when presented with a new task.Wepresent a framework that combines techniques in formal methods with reinforcementlearning (RL) that allows for the convenient specification of complex temporaldependent tasks with logical expressions and construction of new skills from existingones with no additional exploration.We provide theoretical results for ourcomposition technique and evaluate on a simple grid world simulation as well asa robotic manipulation task.,"[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkfwpiA9KX,Automata Guided Skill Composition,Une approche par méthode formelle de la composition des compétences dans les tâches d'apprentissage par renforcement.
Skills learned through (deep) reinforcement learning often generalizes poorlyacross tasks and re-training is necessary when presented with a new task.Wepresent a framework that combines techniques in formal methods with reinforcementlearning (RL) that allows for the convenient specification of complex temporaldependent tasks with logical expressions and construction of new skills from existingones with no additional exploration.We provide theoretical results for ourcomposition technique and evaluate on a simple grid world simulation as well asa robotic manipulation task.,"[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkfwpiA9KX,Automata Guided Skill Composition,L'article combine RL et les contraintes exprimées par des formules logiques en mettant en place une automatisation à partir de formules scTLTL.
Skills learned through (deep) reinforcement learning often generalizes poorlyacross tasks and re-training is necessary when presented with a new task.Wepresent a framework that combines techniques in formal methods with reinforcementlearning (RL) that allows for the convenient specification of complex temporaldependent tasks with logical expressions and construction of new skills from existingones with no additional exploration.We provide theoretical results for ourcomposition technique and evaluate on a simple grid world simulation as well asa robotic manipulation task.,"[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkfwpiA9KX,Automata Guided Skill Composition,Propose une méthode qui aide à construire une politique à partir de sous-tâches apprises sur le thème de la combinaison de tâches RL avec des formules de logique temporelle linéaire.
"The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.","[0, 0, 1, 0]",[],rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,Dérivation d'une formulation générale d'une VAE multimodale à partir de la log-vraisemblance marginale conjointe.
"The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.","[0, 0, 1, 0]",[],rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,Propose un VAE multi-modal avec une limite variationnelle dérivée de la règle de la chaîne.
"The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.","[0, 0, 1, 0]",[],rJl8FoRcY7,Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data,"Cet article propose un objectif, M^2VAE, pour les VAE multimodaux, qui est censé apprendre une représentation de l'espace latent plus significative."
"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models.Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions.We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning.We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","[0, 0, 1, 0]",[],BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,"Nous nous basons sur le Monte Carlo séquentiel à encodage automatique, nous obtenons de nouvelles perspectives théoriques et nous développons une procédure de formation améliorée basée sur ces perspectives."
"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models.Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions.We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning.We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","[0, 0, 1, 0]",[],BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,L'article propose une version de la formation de type IWAE qui utilise le SMC au lieu de l'échantillonnage par importance classique.
"We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models.Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions.We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning.We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.","[0, 0, 1, 0]",[],BJ8c3f-0b,Auto-Encoding Sequential Monte Carlo,"Ce travail propose un Monte Carlo séquentiel (SMC) à codage automatique, étendant le cadre VAE à un nouvel objectif de Monte Carto basé sur le SMC. "
"A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ","[0, 0, 0, 0, 0, 0, 1]",[],rJleN20qK7,Two-Timescale Networks for Nonlinear Value Function Approximation,Nous proposons une architecture pour l'apprentissage des fonctions de valeur qui permet l'utilisation de n'importe quel algorithme linéaire d'évaluation de politique en tandem avec l'apprentissage non linéaire de caractéristiques.
"A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ","[0, 0, 0, 0, 0, 0, 1]",[],rJleN20qK7,Two-Timescale Networks for Nonlinear Value Function Approximation,L'article propose un cadre à deux échelles temporelles pour l'apprentissage de la fonction de valeur et d'une représentation d'état en même temps que des approximateurs non linéaires.
"A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ","[0, 0, 0, 0, 0, 0, 1]",[],rJleN20qK7,Two-Timescale Networks for Nonlinear Value Function Approximation,Cet article propose des réseaux à deux échelles de temps (TTN) et prouve la convergence de cette méthode en utilisant des méthodes d'approximation stochastique à deux échelles de temps. 
"A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    ","[0, 0, 0, 0, 0, 0, 1]",[],rJleN20qK7,Two-Timescale Networks for Nonlinear Value Function Approximation,Cet article présente un réseau à deux échelles de temps (TTN) qui permet d'utiliser des méthodes linéaires pour apprendre des valeurs. 
"Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP).However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable.In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain).To show the effectiveness of our method across different tasks, we examine two settings:1) compressing core LSTM layers in Language Models,2) compressing biLSTM layers of ELMo~\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering).The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations.Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BylahsR9tX,Low-Rank Matrix Factorization of LSTM as Effective Model Compression,"Nous proposons des algorithmes simples, mais efficaces, de factorisation matricielle (MF) à faible rang pour accélérer le temps d'exécution, économiser la mémoire et améliorer les performances des LSTM."
"Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP).However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable.In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain).To show the effectiveness of our method across different tasks, we examine two settings:1) compressing core LSTM layers in Language Models,2) compressing biLSTM layers of ELMo~\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering).The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations.Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.","[0, 0, 1, 0, 0, 0, 0, 0]",[],BylahsR9tX,Low-Rank Matrix Factorization of LSTM as Effective Model Compression,Propose d'accélérer les LSTM en utilisant la MF comme stratégie de compression post-traitement et mène des expériences approfondies pour montrer les performances.
"Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.We propose a solution based on a 3-branch Siamese Convolutional Neural Network.The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.","[1, 0, 0, 0, 0, 0, 0]",[],rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",Une métrique médico-légale pour déterminer si une image donnée est une copie (avec manipulation possible) d'une autre image d'un ensemble de données donné.
"Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.We propose a solution based on a 3-branch Siamese Convolutional Neural Network.The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.","[1, 0, 0, 0, 0, 0, 0]",[],rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation","Présente le réseau siamois pour identifier les images dupliquées et copiées/modifiées, qui peut être utilisé pour améliorer la surveillance de la littérature publiée et en révision par les pairs."
"Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.We propose a solution based on a 3-branch Siamese Convolutional Neural Network.The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.","[1, 0, 0, 0, 0, 0, 0]",[],rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",L'article présente une application des réseaux convolutifs profonds pour la détection d'images en double.
"Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.We propose a solution based on a 3-branch Siamese Convolutional Neural Network.The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models.","[1, 0, 0, 0, 0, 0, 0]",[],rJxY_oCqKQ,"A Forensic Representation to Detect Non-Trivial Image Duplicates, and How it Applies to Semantic Segmentation",Ce travail aborde le problème de la recherche d'images dupliquées ou presque dupliquées dans des publications biomédicales. Il propose un CNN standard et des fonctions de perte et l'applique à ce domaine.
"Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.","[0, 0, 1, 0, 0, 0]",[],SJahqJZAW,Stabilizing GAN Training with Multiple Random Projections,"Entraînement stable du GAN en haute dimension par l'utilisation d'un réseau de discriminateurs, chacun ayant une vue en basse dimension des échantillons générés."
"Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.","[0, 0, 1, 0, 0, 0]",[],SJahqJZAW,Stabilizing GAN Training with Multiple Random Projections,"L'article propose de stabiliser la formation du GAN en utilisant un ensemble de discriminateurs, chacun travaillant sur une projection aléatoire des données d'entrée, pour fournir le signal de formation du modèle générateur."
"Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.","[0, 0, 1, 0, 0, 0]",[],SJahqJZAW,Stabilizing GAN Training with Multiple Random Projections,L'article propose une méthode de formation GAN pour améliorer la stabilité de la formation. 
"Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.","[0, 0, 1, 0, 0, 0]",[],SJahqJZAW,Stabilizing GAN Training with Multiple Random Projections,"L'article propose une nouvelle approche de la formation du GAN, qui fournit des gradients stables pour former le générateur."
"We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).Inclusion relations among N-balls implicitly encode subordinate relations among categories.The similarity measurement in terms of the cosine function is enriched by category information.Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.A new benchmark data set is created for validating the category of unknown words.Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.Source codes and data-sets are free for public access \url{https://github.com/gnodisnait/nball4tree.git} and \url{https://github.com/gnodisnait/bp94nball.git}.","[1, 0, 0, 0, 0, 0, 0]",[],rJlWOj0qF7,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,nous montrons une méthode géométrique qui permet d'encoder parfaitement les informations de l'arbre des catégories dans des mots-embeddings pré-entraînés.
"We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).Inclusion relations among N-balls implicitly encode subordinate relations among categories.The similarity measurement in terms of the cosine function is enriched by category information.Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.A new benchmark data set is created for validating the category of unknown words.Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.Source codes and data-sets are free for public access \url{https://github.com/gnodisnait/nball4tree.git} and \url{https://github.com/gnodisnait/bp94nball.git}.","[1, 0, 0, 0, 0, 0, 0]",[],rJlWOj0qF7,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,L'article propose l'intégration de N-balles pour les données taxonomiques où une N-balle est une paire d'un vecteur centroïde et du rayon du centre.
"We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).Inclusion relations among N-balls implicitly encode subordinate relations among categories.The similarity measurement in terms of the cosine function is enriched by category information.Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.A new benchmark data set is created for validating the category of unknown words.Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.Source codes and data-sets are free for public access \url{https://github.com/gnodisnait/nball4tree.git} and \url{https://github.com/gnodisnait/bp94nball.git}.","[1, 0, 0, 0, 0, 0, 0]",[],rJlWOj0qF7,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,"L'article présente une méthode permettant de modifier les incorporations vectorielles existantes d'objets catégoriels (tels que les mots), afin de les convertir en incorporations de balles qui suivent des hiérarchies."
"We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).Inclusion relations among N-balls implicitly encode subordinate relations among categories.The similarity measurement in terms of the cosine function is enriched by category information.Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.A new benchmark data set is created for validating the category of unknown words.Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.Source codes and data-sets are free for public access \url{https://github.com/gnodisnait/nball4tree.git} and \url{https://github.com/gnodisnait/bp94nball.git}.","[1, 0, 0, 0, 0, 0, 0]",[],rJlWOj0qF7,Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction,Se concentre sur l'ajustement des encastrements de mots pré-entraînés afin qu'ils respectent la relation hypernymie/hyponymie par une encapsulation appropriée de la boule n.
"For the challenging semantic image segmentation task the best performing modelshave traditionally combined the structured modelling capabilities of ConditionalRandom Fields (CRFs) with the feature extraction power of CNNs.In more recentworks however, CRF post-processing has fallen out of favour.We argue that thisis mainly due to the slow training and inference speeds of CRFs, as well as thedifficulty of learning the internal CRF parameters.To overcome both issues wepropose to add the assumption of conditional independence to the framework offully-connected CRFs.This allows us to reformulate the inference in terms ofconvolutions, which can be implemented highly efficiently on GPUs.Doing sospeeds up inference and training by two orders of magnitude.All parameters ofthe convolutional CRFs can easily be optimized using backpropagation.Towardsthe goal of facilitating further CRF research we have made our implementationspublicly available.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1xEwsR9FX,Convolutional CRFs for Semantic Segmentation,"Nous proposons les CRFs convolutifs comme une alternative rapide, puissante et entraînable aux CRFs entièrement connectés."
"For the challenging semantic image segmentation task the best performing modelshave traditionally combined the structured modelling capabilities of ConditionalRandom Fields (CRFs) with the feature extraction power of CNNs.In more recentworks however, CRF post-processing has fallen out of favour.We argue that thisis mainly due to the slow training and inference speeds of CRFs, as well as thedifficulty of learning the internal CRF parameters.To overcome both issues wepropose to add the assumption of conditional independence to the framework offully-connected CRFs.This allows us to reformulate the inference in terms ofconvolutions, which can be implemented highly efficiently on GPUs.Doing sospeeds up inference and training by two orders of magnitude.All parameters ofthe convolutional CRFs can easily be optimized using backpropagation.Towardsthe goal of facilitating further CRF research we have made our implementationspublicly available.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1xEwsR9FX,Convolutional CRFs for Semantic Segmentation,Les auteurs remplacent la grande étape de filtrage dans le réseau permutoédrique par un noyau convolutif variant dans l'espace et montrent que l'inférence est plus efficace et l'apprentissage plus facile. 
"For the challenging semantic image segmentation task the best performing modelshave traditionally combined the structured modelling capabilities of ConditionalRandom Fields (CRFs) with the feature extraction power of CNNs.In more recentworks however, CRF post-processing has fallen out of favour.We argue that thisis mainly due to the slow training and inference speeds of CRFs, as well as thedifficulty of learning the internal CRF parameters.To overcome both issues wepropose to add the assumption of conditional independence to the framework offully-connected CRFs.This allows us to reformulate the inference in terms ofconvolutions, which can be implemented highly efficiently on GPUs.Doing sospeeds up inference and training by two orders of magnitude.All parameters ofthe convolutional CRFs can easily be optimized using backpropagation.Towardsthe goal of facilitating further CRF research we have made our implementationspublicly available.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],H1xEwsR9FX,Convolutional CRFs for Semantic Segmentation,Propose d'effectuer le passage de messages sur un CRF à noyau gaussien tronqué en utilisant un noyau défini et un passage de messages parallélisé sur GPU.
"Deep Learning NLP domain lacks procedures for the analysis of model robustness.In this paper we propose a framework which validates robustness of any Question Answering model through model explainers.We propose that output of a robust model should be invariant to alterations that do not change its semantics.We test this property by manipulating question in two ways: swapping important question word for1) its semantically correct synonym and2) for word vector that is close in embedding space.We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME).With these two steps we compare state-of-the-art Q&A models.We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input.We can choose architecture that is more immune to attacks and thus more robust and stable in production environment.Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure.Our findings help to understand which models are more stable and how they can be improved.In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],SJlgRqjssQ,Are you tough enough? Framework for Robustness Validation of Machine Comprehension Systems,Nous proposons une approche agnostique du modèle pour la validation de la robustesse des systèmes de questions-réponses et démontrons les résultats sur les modèles de questions-réponses les plus récents.
"Deep Learning NLP domain lacks procedures for the analysis of model robustness.In this paper we propose a framework which validates robustness of any Question Answering model through model explainers.We propose that output of a robust model should be invariant to alterations that do not change its semantics.We test this property by manipulating question in two ways: swapping important question word for1) its semantically correct synonym and2) for word vector that is close in embedding space.We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME).With these two steps we compare state-of-the-art Q&A models.We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input.We can choose architecture that is more immune to attacks and thus more robust and stable in production environment.Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure.Our findings help to understand which models are more stable and how they can be improved.In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],SJlgRqjssQ,Are you tough enough? Framework for Robustness Validation of Machine Comprehension Systems,Aborde le problème de la robustesse aux informations contradictoires dans la réponse aux questions.
"Deep Learning NLP domain lacks procedures for the analysis of model robustness.In this paper we propose a framework which validates robustness of any Question Answering model through model explainers.We propose that output of a robust model should be invariant to alterations that do not change its semantics.We test this property by manipulating question in two ways: swapping important question word for1) its semantically correct synonym and2) for word vector that is close in embedding space.We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME).With these two steps we compare state-of-the-art Q&A models.We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input.We can choose architecture that is more immune to attacks and thus more robust and stable in production environment.Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure.Our findings help to understand which models are more stable and how they can be improved.In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]",[],SJlgRqjssQ,Are you tough enough? Framework for Robustness Validation of Machine Comprehension Systems,Amélioration de la robustesse de la compréhension automatique/réponse aux questions.
"In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution.In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions.To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure.To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel.This means that our work can be scaled up to large parallel computation frameworks.We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator.We also show how to introduce the decay factor to stabilize the training procedure.We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10.These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rJHcpW-CW,NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,"multi-générateur pour capturer les données Pdata, résoudre le problème de la concurrence et du ""one-beat-all""."
"In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution.In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions.To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure.To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel.This means that our work can be scaled up to large parallel computation frameworks.We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator.We also show how to introduce the decay factor to stabilize the training procedure.We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10.These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rJHcpW-CW,NOVEL AND EFFECTIVE PARALLEL MIX-GENERATOR GENERATIVE ADVERSARIAL NETWORKS,Propose des GANs parallèles pour éviter l'effondrement des modes dans les GANs par une combinaison de plusieurs générateurs faibles. 
"We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.","[1, 0, 0, 0, 0]",[],SyYYPdg0-,Counterfactual Image Networks,Segmentation d'images faiblement supervisée utilisant la structure compositionnelle des images et des modèles génératifs.
"We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.","[1, 0, 0, 0, 0]",[],SyYYPdg0-,Counterfactual Image Networks,Cet article crée une représentation en couches afin de mieux apprendre la segmentation à partir d'images non étiquetées.
"We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.","[1, 0, 0, 0, 0]",[],SyYYPdg0-,Counterfactual Image Networks,"Cet article propose un modèle génératif basé sur un GAN qui décompose les images en plusieurs couches, l'objectif du GAN étant de distinguer les images réelles des images formées par la combinaison des couches."
"We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.","[1, 0, 0, 0, 0]",[],SyYYPdg0-,Counterfactual Image Networks,Cet article propose une architecture de réseau neuronal autour de l'idée de composition de scènes en couches.
"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models.We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples.In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples.Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly.Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.","[0, 1, 0, 0, 0]",[],H1lug3R5FX,On the Geometry of Adversarial Examples,Nous présentons un cadre géométrique pour prouver les garanties de robustesse et nous soulignons l'importance de la codimension dans les exemples contradictoires. 
"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models.We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples.In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples.Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly.Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.","[0, 1, 0, 0, 0]",[],H1lug3R5FX,On the Geometry of Adversarial Examples,"Cet article présente une analyse théorique des exemples contradictoires, montrant qu'il existe un compromis entre la robustesse dans différentes normes, que la formation contradictoire est inefficace en termes d'échantillonnage et que le classificateur du plus proche voisin peut être robuste dans certaines conditions."
"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data.In this paper, we confront NMT models with synthetic and natural sources of noise.We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.","[0, 1, 0, 0, 0, 0]",[],BJ8vJebC-,Synthetic and Natural Noise Both Break Neural Machine Translation,CharNMT est fragile
"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data.In this paper, we confront NMT models with synthetic and natural sources of noise.We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.","[0, 1, 0, 0, 0, 0]",[],BJ8vJebC-,Synthetic and Natural Noise Both Break Neural Machine Translation,Cet article étudie l'impact du bruit au niveau des caractères sur quatre systèmes de traduction automatique neuronale différents.
"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data.In this paper, we confront NMT models with synthetic and natural sources of noise.We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.","[0, 1, 0, 0, 0, 0]",[],BJ8vJebC-,Synthetic and Natural Noise Both Break Neural Machine Translation,"Cet article étudie de manière empirique les performances des systèmes de T.N.M. au niveau des caractères face au bruit au niveau des caractères, qu'il soit synthétisé ou naturel."
"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data.In this paper, we confront NMT models with synthetic and natural sources of noise.We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.","[0, 1, 0, 0, 0, 0]",[],BJ8vJebC-,Synthetic and Natural Noise Both Break Neural Machine Translation,Cet article étudie l'impact des données bruitées sur la traduction automatique et teste les moyens de rendre les modèles de traduction automatique plus robustes.
"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","[0, 0, 1, 0, 0, 0, 0]",[],B1Lc-Gb0Z,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,"Nous apprenons des réseaux profonds d'unités à seuil dur en fixant les cibles des unités cachées par optimisation combinatoire et les poids par optimisation convexe, ce qui permet d'améliorer les performances sur ImageNet."
"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","[0, 0, 1, 0, 0, 0, 0]",[],B1Lc-Gb0Z,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,L'article explique et généralise les approches d'apprentissage des réseaux neuronaux à activation dure.
"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","[0, 0, 1, 0, 0, 0, 0]",[],B1Lc-Gb0Z,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,Cet article examine le problème de l'optimisation des réseaux profonds d'unités à seuil dur.
"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","[0, 0, 1, 0, 0, 0, 0]",[],B1Lc-Gb0Z,Deep Learning as a Mixed Convex-Combinatorial Optimization Problem,L'article aborde le problème de l'optimisation des réseaux neuronaux avec un seuil difficile et propose une nouvelle solution à ce problème avec une collection d'heuristiques/approximations.
"The robust and efficient recognition of visual relations in images is a hallmark of biological vision.Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations.Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs).The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity.We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations.Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.","[0, 0, 0, 0, 0, 1]",[],HymuJz-A-,Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks,"À l'aide d'un nouveau défi contrôlé de relations visuelles, nous montrons que les tâches de type identique-différent mettent à rude épreuve la capacité des CNN ; nous soutenons que les relations visuelles peuvent être mieux résolues en utilisant des stratégies mnémoniques d'attention."
"The robust and efficient recognition of visual relations in images is a hallmark of biological vision.Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations.Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs).The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity.We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations.Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.","[0, 0, 0, 0, 0, 1]",[],HymuJz-A-,Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks,Démontre que les réseaux neuronaux convolutionnels et relationnels ne parviennent pas à résoudre les problèmes de relations visuelles en entraînant les réseaux sur des données de relations visuelles générées artificiellement. 
"The robust and efficient recognition of visual relations in images is a hallmark of biological vision.Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations.Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs).The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity.We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations.Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning.","[0, 0, 0, 0, 0, 1]",[],HymuJz-A-,Not-So-CLEVR: Visual Relations Strain Feedforward Neural Networks,Cet article explore comment les CNN et les réseaux relationnels actuels ne parviennent pas à reconnaître les relations visuelles dans les images.
"Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker’s generalization on the unseen object moving patterns.To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.They are asymmetric in that the target is aware of the tracker, but not vice versa.Specifically, besides its own observation, the target is fed with the tracker’s observation and action, and learns to predict the tracker’s reward as an auxiliary task.We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkgYmhR9KX,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,"Nous proposons AD-VAT, où le traqueur et l'objet cible, considérés comme deux agents apprenants, sont des adversaires et peuvent s'améliorer mutuellement pendant l'apprentissage."
"Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker’s generalization on the unseen object moving patterns.To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.They are asymmetric in that the target is aware of the tracker, but not vice versa.Specifically, besides its own observation, the target is fed with the tracker’s observation and action, and learns to predict the tracker’s reward as an auxiliary task.We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkgYmhR9KX,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,Ce travail vise à résoudre le problème du suivi visuel actif à l'aide d'un mécanisme d'entraînement dans lequel le suiveur et la cible servent d'adversaires mutuels.
"Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker’s generalization on the unseen object moving patterns.To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.They are asymmetric in that the target is aware of the tracker, but not vice versa.Specifically, besides its own observation, the target is fed with the tracker’s observation and action, and learns to predict the tracker’s reward as an auxiliary task.We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkgYmhR9KX,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,Cet article présente une tâche simple de RL profond multi-agent où un tracker mobile essaie de suivre une cible mobile.
"Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker’s generalization on the unseen object moving patterns.To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.They are asymmetric in that the target is aware of the tracker, but not vice versa.Specifically, besides its own observation, the target is fed with the tracker’s observation and action, and learns to predict the tracker’s reward as an auxiliary task.We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS The code is available at https://github.com/zfw1226/active_tracking_rl","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],HkgYmhR9KX,AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking,"Propose une nouvelle fonction de récompense - ""somme nulle partielle"", qui encourage la concurrence entre le tracker et la cible uniquement lorsqu'ils sont proches et la pénalise lorsqu'ils sont trop éloignés."
"Identifying the hypernym relations that hold between words is a fundamental task in NLP.Word embedding methods have recently shown some capability to encode hypernymy.However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy.To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.","[0, 0, 0, 0, 1, 0, 0]",[],S1xf-W5paX,Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy,Nous avons présenté une méthode permettant d'apprendre conjointement un emboîtement hiérarchique de mots (HWE) à l'aide d'un corpus et d'une taxonomie pour identifier les relations d'hypernymie entre les mots.
"Identifying the hypernym relations that hold between words is a fundamental task in NLP.Word embedding methods have recently shown some capability to encode hypernymy.However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy.To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.","[0, 0, 0, 0, 1, 0, 0]",[],S1xf-W5paX,Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy,L'article présente une méthode d'apprentissage conjoint de l'intégration des mots à l'aide de statistiques de cooccurrence et de l'intégration d'informations hiérarchiques provenant de réseaux sémantiques.
"Identifying the hypernym relations that hold between words is a fundamental task in NLP.Word embedding methods have recently shown some capability to encode hypernymy.However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy.To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.","[0, 0, 0, 0, 1, 0, 0]",[],S1xf-W5paX,Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy,Cet article propose une méthode d'apprentissage conjoint des hypernoms à partir de textes bruts et de données de taxonomie supervisées. 
"Identifying the hypernym relations that hold between words is a fundamental task in NLP.Word embedding methods have recently shown some capability to encode hypernymy.However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.In this paper, we propose a method to learn a hierarchical word embedding in a speciﬁc order to capture the hypernymy.To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speciﬁc word embeddings on multiple benchmarks.","[0, 0, 0, 0, 1, 0, 0]",[],S1xf-W5paX,Joint Learning of Hierarchical Word Embeddings from a Corpus and a Taxonomy,"Cet article propose d'ajouter une mesure de différence d'""inclusion distributionnelle"" à l'objectif GloVE dans le but de représenter les relations hypernymiques."
"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.","[0, 0, 1, 0]",[],BJ8lbVAfz,Self-Organization adds application robustness to deep learners,intégration de l'auto-organisation et de l'apprentissage supervisé dans un réseau neuronal hiérarchique
"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.","[0, 0, 1, 0]",[],BJ8lbVAfz,Self-Organization adds application robustness to deep learners,"L'article traite de l'apprentissage dans un réseau neuronal à trois couches, dont la couche intermédiaire est organisée de manière topographique, et étudie l'interaction entre l'apprentissage non supervisé et l'apprentissage supervisé hiérarchique dans un contexte biologique."
"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.","[0, 0, 1, 0]",[],BJ8lbVAfz,Self-Organization adds application robustness to deep learners,"Une variante supervisée de la carte auto-organisée (SOM) de Kohonen, mais où la couche de sortie linéaire est remplacée par une couche softmax avec une erreur quadratique et une entropie croisée."
"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems.","[0, 0, 1, 0]",[],BJ8lbVAfz,Self-Organization adds application robustness to deep learners,"Propose un modèle utilisant des neurones cachés avec une fonction d'activation auto-organisatrice, dont les sorties alimentent un classificateur avec une fonction de sortie softmax. "
"Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.","[0, 1, 0, 0, 0, 0]",[],SJx94o0qYX,Precision Highway for Ultra Low-precision Quantization,autoroute de précision ; concept généralisé de flux d'informations de haute précision pour la quantification sur 4 bits 
"Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.","[0, 1, 0, 0, 0, 0]",[],SJx94o0qYX,Precision Highway for Ultra Low-precision Quantization,Étudie le problème de la quantification des réseaux neuronaux en utilisant une autoroute de précision de bout en bout pour réduire l'erreur de quantification accumulée et permettre une précision ultra-basse dans les réseaux neuronaux profonds. 
"Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.","[0, 1, 0, 0, 0, 0]",[],SJx94o0qYX,Precision Highway for Ultra Low-precision Quantization,Cet article étudie les méthodes permettant d'améliorer les performances des réseaux neuronaux quantifiés.
"Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.","[0, 1, 0, 0, 0, 0]",[],SJx94o0qYX,Precision Highway for Ultra Low-precision Quantization,"Cet article propose de conserver un flux d'activation/gradient élevé dans deux types de structures de réseaux, ResNet et LSTM."
"The vast majority of natural sensory data is temporally redundant.For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  This can be an obscene waste of energy.  We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ","[1, 0, 0, 0, 0, 0, 0, 0]",[],HkZy-bW0-,Temporally Efficient Deep Learning with Spikes,Un algorithme pour l'entraînement efficace de réseaux neuronaux sur des données temporellement redondantes.
"The vast majority of natural sensory data is temporally redundant.For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  This can be an obscene waste of energy.  We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ","[1, 0, 0, 0, 0, 0, 0, 0]",[],HkZy-bW0-,Temporally Efficient Deep Learning with Spikes,L'article décrit un schéma de codage neuronal pour l'apprentissage basé sur les pointes dans les réseaux neuronaux profonds.
"The vast majority of natural sensory data is temporally redundant.For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  This can be an obscene waste of energy.  We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ","[1, 0, 0, 0, 0, 0, 0, 0]",[],HkZy-bW0-,Temporally Efficient Deep Learning with Spikes,Cet article présente une méthode d'apprentissage basée sur les pointes qui vise à réduire le calcul nécessaire pendant l'apprentissage et le test lors de la classification de données temporelles redondantes.
"The vast majority of natural sensory data is temporally redundant.For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  This can be an obscene waste of energy.  We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  ","[1, 0, 0, 0, 0, 0, 0, 0]",[],HkZy-bW0-,Temporally Efficient Deep Learning with Spikes,"Cet article applique une version de codage prédictif du schéma de codage Sigma-Delta pour réduire la charge de calcul d'un réseau d'apprentissage profond, en combinant les trois composants d'une manière inédite."
"Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - βI(X;T).In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.We demonstrate the three caveats on the MNIST dataset.","[0, 0, 0, 1, 0, 0]",[],rke4HiAcY7,Caveats for information bottleneck in deterministic scenarios,Le goulot d'étranglement de l'information se comporte de manière surprenante lorsque la sortie est une fonction déterministe de l'entrée.
"Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - βI(X;T).In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.We demonstrate the three caveats on the MNIST dataset.","[0, 0, 0, 1, 0, 0]",[],rke4HiAcY7,Caveats for information bottleneck in deterministic scenarios,Affirme que la plupart des problèmes de classification réels présentent une telle relation déterministe entre les étiquettes de classe et les entrées X et explore plusieurs problèmes qui résultent de telles pathologies.
"Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - βI(X;T).In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.We demonstrate the three caveats on the MNIST dataset.","[0, 0, 0, 1, 0, 0]",[],rke4HiAcY7,Caveats for information bottleneck in deterministic scenarios,Exploration des questions qui se posent lors de l'application des concepts d'information bottlenext aux modèles déterministes d'apprentissage supervisé.
"Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate ""bottleneck"" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).The ""IB curve"" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the ""IB Lagrangian"", I(Y;T) - βI(X;T).In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of β; (2) there are ""uninteresting"" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.We demonstrate the three caveats on the MNIST dataset.","[0, 0, 0, 1, 0, 0]",[],rke4HiAcY7,Caveats for information bottleneck in deterministic scenarios,Les auteurs clarifient plusieurs comportements contre-intuitifs de la méthode du goulot d'information pour l'apprentissage supervisé d'une règle déterministe.
"We prove, under two sufficient conditions, that idealised models can have no adversarial examples.We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these.We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice.We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting.This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well.Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.","[1, 0, 0, 0, 0, 0]",[],B1eZRiC9YX,Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,"Nous prouvons que les réseaux neuronaux bayésiens idéalisés ne peuvent pas avoir d'exemples contradictoires, et nous donnons des preuves empiriques avec des BNN du monde réel."
"We prove, under two sufficient conditions, that idealised models can have no adversarial examples.We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these.We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice.We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting.This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well.Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.","[1, 0, 0, 0, 0, 0]",[],B1eZRiC9YX,Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,"L'article étudie la robustesse des classificateurs bayésiens et énonce deux conditions qui, selon eux, sont suffisantes pour que des ""modèles idéalisés"" sur des ""ensembles de données idéalisés"" ne présentent pas d'exemples contradictoires."
"We prove, under two sufficient conditions, that idealised models can have no adversarial examples.We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these.We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice.We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting.This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well.Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.","[1, 0, 0, 0, 0, 0]",[],B1eZRiC9YX,Sufficient Conditions for Robustness to Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,L'article pose une classe de classificateurs bayésiens discriminatifs qui n'ont pas d'exemples contradictoires.
"Deep neural networks are susceptible to adversarial attacks.In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task.These perturbations can thus be considered a program for the new task.We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.","[0, 0, 0, 1, 0, 0, 0]",[],Syx_Ss05tm,Adversarial Reprogramming of Neural Networks,Nous présentons la première instance d'attaques adversariales qui reprogramment le modèle cible pour qu'il exécute une tâche choisie par l'attaquant - sans que ce dernier ait besoin de spécifier ou de calculer la sortie souhaitée pour chaque entrée du temps de test.
"Deep neural networks are susceptible to adversarial attacks.In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task.These perturbations can thus be considered a program for the new task.We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.","[0, 0, 0, 1, 0, 0, 0]",[],Syx_Ss05tm,Adversarial Reprogramming of Neural Networks,Les auteurs présentent un nouveau schéma d'attaque contradictoire dans lequel un réseau neuronal est réaffecté pour accomplir une tâche différente de celle pour laquelle il a été formé à l'origine.
"Deep neural networks are susceptible to adversarial attacks.In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task.These perturbations can thus be considered a program for the new task.We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.","[0, 0, 0, 1, 0, 0, 0]",[],Syx_Ss05tm,Adversarial Reprogramming of Neural Networks,"Cet article propose une ""reprogrammation contradictoire"" de réseaux neuronaux fixes et bien entraînés et montre que la reprogrammation contradictoire est moins efficace sur les réseaux non entraînés."
"Deep neural networks are susceptible to adversarial attacks.In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary—even if the model was not trained to do this task.These perturbations can thus be considered a program for the new task.We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.","[0, 0, 0, 1, 0, 0, 0]",[],Syx_Ss05tm,Adversarial Reprogramming of Neural Networks,"L'article étend l'idée des ""attaques adverses"" dans l'apprentissage supervisé des réseaux neuronaux à une réaffectation complète de la solution d'un réseau entraîné."
"As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data.This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization.This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters.In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap.Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary.We find that it is necessary to use margin distributions at multiple layers of a deep network.On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap.In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HJlQfnCqKX,Predicting the Generalization Gap in Deep Networks with Margin Distributions,Nous développons un nouveau schéma pour prédire l'écart de généralisation dans les réseaux profonds avec une grande précision.
"As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data.This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization.This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters.In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap.Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary.We find that it is necessary to use margin distributions at multiple layers of a deep network.On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap.In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HJlQfnCqKX,Predicting the Generalization Gap in Deep Networks with Margin Distributions,Les auteurs suggèrent d'utiliser une marge géométrique et une distribution de la marge par couche pour prédire l'écart de généralisation.
"As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data.This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization.This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters.In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap.Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary.We find that it is necessary to use margin distributions at multiple layers of a deep network.On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap.In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.","[0, 0, 0, 0, 0, 0, 0, 0, 1]",[],HJlQfnCqKX,Predicting the Generalization Gap in Deep Networks with Margin Distributions,"Les résultats empiriques montrent un lien intéressant entre les statistiques de marge proposées et l'écart de généralisation, qui peut être utilisé pour fournir des indications normatives pour comprendre la généralisation dans les réseaux neuronaux profonds. "
"We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned.Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks.Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems.We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.While our focus is theoretical, we also present experiments that illustrate our theoretical findings.","[1, 0, 0, 0, 0]",[],rkMnHjC5YQ,Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps,Nous proposons un algorithme pour récupérer de manière prouvable les paramètres (poids de convolution et de sortie) d'un réseau convolutif avec des patchs qui se chevauchent.
"We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned.Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks.Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems.We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.While our focus is theoretical, we also present experiments that illustrate our theoretical findings.","[1, 0, 0, 0, 0]",[],rkMnHjC5YQ,Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps,Cet article étudie l'apprentissage théorique des réseaux de neurones convolutifs à une couche cachée. Il en résulte un algorithme d'apprentissage et des garanties prouvables à l'aide de cet algorithme.
"We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned.Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks.Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems.We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.While our focus is theoretical, we also present experiments that illustrate our theoretical findings.","[1, 0, 0, 0, 0]",[],rkMnHjC5YQ,Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps,Cet article présente un nouvel algorithme pour l'apprentissage d'un réseau neuronal à deux couches qui implique un seul filtre convolutif et un vecteur de poids pour différents emplacements.
"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data.Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem.A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping.Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training.We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.These arguments are supported by experimental results on several data sets.","[0, 0, 0, 1, 0, 0]",[],B1hYRMbCW,On the regularization of Wasserstein GANs,Un nouveau terme de régularisation peut améliorer votre formation de gans wasserstein
"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data.Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem.A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping.Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training.We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.These arguments are supported by experimental results on several data sets.","[0, 0, 0, 1, 0, 0]",[],B1hYRMbCW,On the regularization of Wasserstein GANs,Cet article propose un schéma de régularisation pour le GAN de Wasserstein basé sur la relaxation des contraintes sur la constante de Lipschitz de 1.
"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data.Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem.A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping.Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training.We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.These arguments are supported by experimental results on several data sets.","[0, 0, 0, 1, 0, 0]",[],B1hYRMbCW,On the regularization of Wasserstein GANs,"L'article traite de la régularisation/pénalisation dans l'ajustement des GANs, lorsqu'elle est basée sur une métrique de Wasserstein L_1."
"We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.From this theory, we obtain an easy-to-implement regularizer for the parameter updates.Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\'echet Inception Distance (FID) learning curves.","[1, 0, 0, 0]",[],Bye5OiR5F7,Wasserstein proximal of GANs,Nous proposons la méthode proximale de Wasserstein pour l'entraînement des GANs. 
"We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.From this theory, we obtain an easy-to-implement regularizer for the parameter updates.Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\'echet Inception Distance (FID) learning curves.","[1, 0, 0, 0]",[],Bye5OiR5F7,Wasserstein proximal of GANs,Propose une nouvelle procédure GAN qui prend en compte les points générés lors de l'itération précédente et met à jour le générateur à effectuer l fois.
"We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.From this theory, we obtain an easy-to-implement regularizer for the parameter updates.Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\'echet Inception Distance (FID) learning curves.","[1, 0, 0, 0]",[],Bye5OiR5F7,Wasserstein proximal of GANs,"Considère l'apprentissage par gradient naturel dans l'apprentissage GAN, où la structure riemannienne induite par la distance de Wasserstein-2 est employée."
"We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.From this theory, we obtain an easy-to-implement regularizer for the parameter updates.Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\'echet Inception Distance (FID) learning curves.","[1, 0, 0, 0]",[],Bye5OiR5F7,Wasserstein proximal of GANs,L'article entend utiliser le gradient naturel induit par la distance de Wasserstein-2 pour entraîner le générateur dans le GAN et les auteurs proposent l'opérateur proximal de Wasserstein comme régularisation.
"Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.In this paper, we develop a weighted mini-bucket approach for bounding the MEU. These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter.They can also be used as heuristic  functions to guide search, especially for planning such as MDPs and POMDPs.We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.","[1, 0, 0, 0, 0, 0, 0, 0]",[],r1ls4-DpvN,A Weighted Mini-Bucket Bound Heuristic for Solving Influence Diagrams,"Cet article présente une fonction heuristique basée sur l'élimination pour la prise de décision séquentielle, adaptée pour guider les algorithmes de recherche ET/OU pour la résolution des diagrammes d'influence."
"Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.In this paper, we develop a weighted mini-bucket approach for bounding the MEU. These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter.They can also be used as heuristic  functions to guide search, especially for planning such as MDPs and POMDPs.We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.","[1, 0, 0, 0, 0, 0, 0, 0]",[],r1ls4-DpvN,A Weighted Mini-Bucket Bound Heuristic for Solving Influence Diagrams,généralise l'heuristique d'inférence des minibuckets aux diagrammes d'influence.
"Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity.In contrast, standard NNs propagate only point estimates, discarding the uncertainty.Methods propagating also the variance have been proposed by several authors in different context.The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.We evaluate the accuracy of the approximation and suggest a simple calibration.Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkMuPjRcKQ,Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers,"Approximation de la moyenne et de la variance de la sortie du NN sur une entrée bruyante / un abandon / des paramètres incertains. Approximations analytiques pour les couches argmax, softmax et max."
"Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity.In contrast, standard NNs propagate only point estimates, discarding the uncertainty.Methods propagating also the variance have been proposed by several authors in different context.The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.We evaluate the accuracy of the approximation and suggest a simple calibration.Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkMuPjRcKQ,Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers,Les auteurs se concentrent sur le problème de la propagation de l'incertitude DNN
"Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity.In contrast, standard NNs propagate only point estimates, discarding the uncertainty.Methods propagating also the variance have been proposed by several authors in different context.The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.We evaluate the accuracy of the approximation and suggest a simple calibration.Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling.","[0, 0, 0, 0, 0, 1, 0, 0]",[],SkMuPjRcKQ,Feed-forward Propagation in Probabilistic Neural Networks with Categorical and Max Layers,"Cet article revisite la propagation feed-forward de la moyenne et de la variance dans les neurones, en abordant le problème de la propagation de l'incertitude à travers des couches de max-pooling et softmax."
"Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.Some of the most promising adversarial models today minimize a Wasserstein objective.It is smoother and more stable to optimize.In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJE6X305Fm,Don't let your Discriminator  be fooled,Un discriminateur qui n'est pas facilement trompé par un exemple contradictoire rend la formation du GAN plus robuste et conduit à un objectif plus lisse.
"Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.Some of the most promising adversarial models today minimize a Wasserstein objective.It is smoother and more stable to optimize.In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJE6X305Fm,Don't let your Discriminator  be fooled,Cet article propose une nouvelle façon de stabiliser le processus de formation du GAN en régularisant le discriminateur pour qu'il soit robuste aux exemples adverses.
"Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.Some of the most promising adversarial models today minimize a Wasserstein objective.It is smoother and more stable to optimize.In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJE6X305Fm,Don't let your Discriminator  be fooled,"L'article propose une méthode systématique d'entraînement des GANs avec des termes de régularisation de la robustesse, permettant un entraînement plus lisse des GANs. "
"Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.Some of the most promising adversarial models today minimize a Wasserstein objective.It is smoother and more stable to optimize.In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions.","[0, 0, 0, 0, 0, 1, 0, 0]",[],HJE6X305Fm,Don't let your Discriminator  be fooled,"Présente l'idée selon laquelle, en rendant un discriminateur robuste aux perturbations adverses, l'objectif du GAN peut être rendu lisse, ce qui donne de meilleurs résultats à la fois visuellement et en termes de FID."
"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","[0, 0, 0, 0, 1, 0]",[],By-IifZRW,Gaussian Process Neurons,Nous modélisons la fonction d'activation de chaque neurone comme un processus gaussien et l'apprenons en même temps que le poids avec l'inférence variationnelle.
"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","[0, 0, 0, 0, 1, 0]",[],By-IifZRW,Gaussian Process Neurons,Proposer de placer des prieurs de processus gaussiens sur la forme fonctionnelle de chaque fonction d'activation dans le réseau neuronal pour apprendre la forme des fonctions d'activation.
"Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice.In this paper, we bridge the gap between these good empirical results and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks.More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks ofbounded width and small depth can approximate a deep ReLU network in which the dense matrices areof low rank.Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks.We support our experimental results with thorough experiments on a large, real world video classification problem.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkeUG30cFQ,The Expressive Power of Deep Neural Networks with Circulant Matrices,Nous fournissons une étude théorique des propriétés des réseaux ReLU circulants-diagonaux profonds et démontrons qu'ils sont des approximateurs universels de largeur bornée.
"Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice.In this paper, we bridge the gap between these good empirical results and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks.More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks ofbounded width and small depth can approximate a deep ReLU network in which the dense matrices areof low rank.Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks.We support our experimental results with thorough experiments on a large, real world video classification problem.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkeUG30cFQ,The Expressive Power of Deep Neural Networks with Circulant Matrices,L'article propose d'utiliser des matrices circulantes et diagonales pour accélérer les calculs et réduire les besoins en mémoire dans les réseaux neuronaux.
"Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice.In this paper, we bridge the gap between these good empirical results and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks.More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks ofbounded width and small depth can approximate a deep ReLU network in which the dense matrices areof low rank.Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks.We support our experimental results with thorough experiments on a large, real world video classification problem.","[0, 0, 1, 0, 0, 0, 0, 0]",[],SkeUG30cFQ,The Expressive Power of Deep Neural Networks with Circulant Matrices,Cet article prouve que les réseaux ReLU diagonaux-circulants à largeur limitée (DC-ReLU) sont des approximateurs universels.
"Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.However, manual remote piloting of a drone is prone to errors.In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.","[0, 0, 0, 1, 0, 0]",[],BviYjfnIk,StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation,StarHopper est une nouvelle interface à écran tactile pour la navigation efficace et flexible des drones à caméra centrée sur l'objet.
"Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.However, manual remote piloting of a drone is prone to errors.In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.","[0, 0, 0, 1, 0, 0]",[],BviYjfnIk,StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation,"Les auteurs décrivent une nouvelle interface de contrôle de drone, StarHopper, qu'ils ont développée. Elle combine le pilotage automatisé et manuel en une nouvelle interface de navigation hybride et se débarrasse de l'hypothèse selon laquelle l'objet cible se trouve déjà dans le champ de vision du drone en utilisant une caméra aérienne supplémentaire."
"Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.However, manual remote piloting of a drone is prone to errors.In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.","[0, 0, 0, 1, 0, 0]",[],BviYjfnIk,StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation,"Cet article présente StarHopper, un système de navigation semi-automatique pour drones dans le contexte de l'inspection à distance."
"Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.However, manual remote piloting of a drone is prone to errors.In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.","[0, 0, 0, 1, 0, 0]",[],BviYjfnIk,StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation,"Présente StarHopper, une application qui utilise des techniques de vision par ordinateur avec une entrée tactile pour soutenir le pilotage de drones avec une approche centrée sur l'objet."
"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations.RNN can capture long-range dependency but is hard to parallelize and not time-efficient.CNN focuses on local dependency but does not perform well on some tasks.SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length.In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding.It requires as little memory as RNN but with all the merits of SAN.Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency.Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required.Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information.On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],H1cWzoxA-,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,"Un réseau d'auto-attention pour le codage de séquences sans RNN/CNN avec une faible consommation de mémoire, un calcul hautement parallélisable et des performances de pointe pour plusieurs tâches de TAL."
"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations.RNN can capture long-range dependency but is hard to parallelize and not time-efficient.CNN focuses on local dependency but does not perform well on some tasks.SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length.In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding.It requires as little memory as RNN but with all the merits of SAN.Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency.Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required.Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information.On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],H1cWzoxA-,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,Propose d'appliquer l'auto-attention à deux niveaux pour limiter le besoin de mémoire dans les modèles basés sur l'attention avec un impact négligeable sur la vitesse.
"Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations.RNN can capture long-range dependency but is hard to parallelize and not time-efficient.CNN focuses on local dependency but does not perform well on some tasks.SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length.In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding.It requires as little memory as RNN but with all the merits of SAN.Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency.Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required.Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information.On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]",[],H1cWzoxA-,Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling,Cet article présente le modèle d'auto-attention par blocs bidirectionnels comme un codeur polyvalent pour diverses tâches de modélisation de séquences en langage naturel.
"End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.","[0, 0, 0, 0, 1]",[],Syl7OsRqY7,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Un nouveau modèle de pointe pour la réponse aux questions à preuves multiples utilisant l'attention hiérarchique à gros grain et à grain fin.
"End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.","[0, 0, 0, 0, 1]",[],Syl7OsRqY7,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Propose une méthode d'AQ multi-sauts basée sur deux modules distincts (modules à grain grossier et à grain fin).
"End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.","[0, 0, 0, 0, 1]",[],Syl7OsRqY7,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Cet article propose une architecture intéressante de réseau de coattention à gros grain et à grain fin pour répondre à des questions à preuves multiples.
"End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.","[0, 0, 0, 0, 1]",[],Syl7OsRqY7,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,Se concentre sur l'AQ multi-choix et propose un cadre de notation grossier à fin.
"Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.","[0, 1, 0, 0, 0]",[],HyexAiA5Fm,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,Nous proposons une nouvelle méthodologie pour le transport optimal déséquilibré en utilisant des réseaux adversariaux génératifs.
"Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.","[0, 1, 0, 0, 0]",[],HyexAiA5Fm,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,Les auteurs considèrent le problème du transport optimal non équilibré entre deux mesures de masse totale différente en utilisant un algorithme stochastique min-max et une mise à l'échelle locale.
"Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.","[0, 1, 0, 0, 0]",[],HyexAiA5Fm,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,Les auteurs proposent une approche pour estimer le transport optimal non équilibré entre les mesures échantillonnées qui s'échelonne bien dans la dimension et dans le nombre d'échantillons.
"Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.","[0, 1, 0, 0, 0]",[],HyexAiA5Fm,Scalable Unbalanced Optimal Transport using Generative Adversarial Networks,L'article introduit une formulation statique pour le transport optimal déséquilibré en apprenant simultanément une carte de transport T et un facteur d'échelle xi.
"Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.","[0, 0, 1]",[],BJxbYoC9FQ,Classifier-agnostic saliency map extraction,Nous proposons une nouvelle méthode d'extraction de cartes de saillance qui permet d'extraire des cartes de meilleure qualité.
"Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.","[0, 0, 1]",[],BJxbYoC9FQ,Classifier-agnostic saliency map extraction,Propose une méthode agnostique de classification pour l'extraction de cartes de saillance.
"Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.","[0, 0, 1]",[],BJxbYoC9FQ,Classifier-agnostic saliency map extraction,Cet article présente un nouvel extracteur de carte de saillance qui semble améliorer les résultats de l'état de l'art.
"Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset.","[0, 0, 1]",[],BJxbYoC9FQ,Classifier-agnostic saliency map extraction,"Les auteurs soutiennent que lorsqu'une carte de saillance extraite dépend directement d'un modèle, elle peut ne pas être utile pour un classificateur différent, et proposent un schéma pour approcher la solution."
"Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs).However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images.To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration.The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances.To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances.We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances.Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases.Code and results are available in https://github.com/sangwoomo/instagan","[0, 0, 0, 1, 0, 0, 0, 0]",[],ryxwJhC9YX,InstaGAN: Instance-aware Image-to-Image Translation,Nous proposons une nouvelle méthode pour incorporer l'ensemble des attributs d'instance pour la traduction d'image à image.
"Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs).However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images.To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration.The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances.To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances.We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances.Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases.Code and results are available in https://github.com/sangwoomo/instagan","[0, 0, 0, 1, 0, 0, 0, 0]",[],ryxwJhC9YX,InstaGAN: Instance-aware Image-to-Image Translation,"Cet article propose une méthode -- InstaGAN -- qui s'appuie sur CycleGAN en prenant en compte des informations sur les instances sous la forme de masques de segmentation par instance, avec des résultats qui se comparent favorablement à CycleGAN et à d'autres lignes de base."
"Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs).However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images.To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration.The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances.To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances.We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances.Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases.Code and results are available in https://github.com/sangwoomo/instagan","[0, 0, 0, 1, 0, 0, 0, 0]",[],ryxwJhC9YX,InstaGAN: Instance-aware Image-to-Image Translation, Propose d'ajouter des masques de segmentation tenant compte des instances pour le problème de la traduction d'image à image non appariée.
"Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  In this paper, we provide a new explanation.By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions.We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,La carte paramètre-fonction des réseaux profonds est fortement biaisée ; cela peut expliquer pourquoi ils se généralisent. Nous utilisons PAC-Bayes et les processus gaussiens pour obtenir des limites non-vacuantes.
"Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  In this paper, we provide a new explanation.By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions.We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,"L'article étudie les capacités de généralisation des réseaux neuronaux profonds, à l'aide de la théorie de l'apprentissage PAC-Bayes et d'intuitions étayées empiriquement."
"Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  In this paper, we provide a new explanation.By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions.We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.","[0, 0, 0, 0, 0, 1, 0, 0, 0]",[],rye4g3AqFm,Deep learning generalizes because the parameter-function map is biased towards simple functions,"Cet article propose une explication des comportements de généralisation des grands réseaux neuronaux sur-paramétrés en affirmant que la carte paramètre-fonction des réseaux neuronaux est biaisée vers des fonctions ""simples"" et que le comportement de généralisation sera bon si le concept cible est également ""simple""."
"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters.In the variational E-step,the latent states are then optimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of theindividuals as the (log) joint probabilities given by the used generative model.As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches).Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood.In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SyjjD1WRb,Evolutionary Expectation Maximization for Generative Models with Binary Latents,"Nous présentons l'EM évolutionnaire comme un nouvel algorithme pour la formation non supervisée de modèles génératifs avec des variables latentes binaires, qui relie intimement l'EM variationnel à l'optimisation évolutionnaire."
"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters.In the variational E-step,the latent states are then optimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of theindividuals as the (log) joint probabilities given by the used generative model.As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches).Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood.In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SyjjD1WRb,Evolutionary Expectation Maximization for Generative Models with Binary Latents,L'article présente une combinaison de calcul évolutionnaire et de EM variationnelle pour les modèles avec des variables latentes binaires représentées par une approximation basée sur des particules.
"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters.In the variational E-step,the latent states are then optimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of theindividuals as the (log) joint probabilities given by the used generative model.As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches).Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood.In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],SyjjD1WRb,Evolutionary Expectation Maximization for Generative Models with Binary Latents,L'article tente d'intégrer étroitement les algorithmes d'apprentissage par maximisation des attentes aux algorithmes évolutionnaires.
"While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.Performing forward prediction on sequences of distributions has many important applications.However, there are two main challenges in designing a network model for this task.First, neural networks are unable to encode distributions compactly as each node encodes just a real value.A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJlp8sA5Y7,An Efficient Network for Predicting Time-Varying Distributions,Nous proposons un modèle de réseau récurrent efficace pour la prédiction prospective sur des distributions variant dans le temps.
"While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.Performing forward prediction on sequences of distributions has many important applications.However, there are two main challenges in designing a network model for this task.First, neural networks are unable to encode distributions compactly as each node encodes just a real value.A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJlp8sA5Y7,An Efficient Network for Predicting Time-Varying Distributions,Cet article propose une méthode de création de réseaux neuronaux qui fait correspondre des distributions historiques à des distributions et applique la méthode à plusieurs tâches de prédiction de distribution.
"While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.Performing forward prediction on sequences of distributions has many important applications.However, there are two main challenges in designing a network model for this task.First, neural networks are unable to encode distributions compactly as each node encodes just a real value.A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJlp8sA5Y7,An Efficient Network for Predicting Time-Varying Distributions,Propose un réseau de régression de distribution récurrent qui utilise une architecture récurrente sur un modèle précédent de réseau de régression de distribution.
"While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.Performing forward prediction on sequences of distributions has many important applications.However, there are two main challenges in designing a network model for this task.First, neural networks are unable to encode distributions compactly as each node encodes just a real value.A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact.","[0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SJlp8sA5Y7,An Efficient Network for Predicting Time-Varying Distributions,Cet article traite de la régression sur les distributions de probabilité en étudiant les distributions variant dans le temps dans le cadre d'un réseau neuronal récurrent.
"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront.In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","[0, 0, 0, 1]",[],rJXMpikCZ,Graph Attention Networks,"Une nouvelle approche du traitement des données structurées en graphes par des réseaux neuronaux, en tirant parti de l'attention portée au voisinage d'un nœud. Obtient des résultats de pointe sur des tâches de réseaux de citations transductives et une tâche d'interaction protéine-protéine inductive."
"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront.In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","[0, 0, 0, 1]",[],rJXMpikCZ,Graph Attention Networks,"Cet article propose une nouvelle méthode de classification des nœuds d'un graphe, qui peut être utilisée dans des scénarios semi-supervisés et sur un graphe complètement nouveau. "
"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront.In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","[0, 0, 0, 1]",[],rJXMpikCZ,Graph Attention Networks,"L'article présente une architecture de réseau neuronal permettant d'opérer sur des données structurées en graphes, appelée Graph Attention Networks."
"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront.In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","[0, 0, 0, 1]",[],rJXMpikCZ,Graph Attention Networks,Fournit une discussion juste et presque complète de l'état de l'art des approches pour apprendre des représentations vectorielles pour les nœuds d'un graphe.
"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints.Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics.Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task.In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network.In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model.In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer.The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network.Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network.Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network.We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1hcZZ-AW,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,Une nouvelle approche basée sur l'apprentissage par renforcement pour comprimer les réseaux neuronaux profonds avec distillation des connaissances.
"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints.Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics.Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task.In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network.In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model.In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer.The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network.Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network.Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network.We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1hcZZ-AW,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,Cet article propose d'utiliser l'apprentissage par renforcement au lieu d'heuristiques prédéfinies pour déterminer la structure du modèle compressé dans le processus de distillation des connaissances.
"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints.Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics.Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task.In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network.In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model.In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer.The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network.Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network.Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network.We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]",[],B1hcZZ-AW,N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning,"Introduit une méthode de compression de réseau à réseau fondée sur des principes, qui utilise les gradients de politique pour optimiser deux politiques qui compriment un enseignant fort en un modèle d'étudiant fort mais plus petit."
"Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss.However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples.In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset.Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.","[1, 0, 0, 0, 0]",[],HJxyAjRcFX,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,Nous prouvons que l'effondrement des modes dans les GANs conditionnels est largement attribué à une inadéquation entre la perte de reconstruction et la perte de GAN et nous introduisons un ensemble de nouvelles fonctions de perte comme alternatives à la perte de reconstruction.
"Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss.However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples.In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset.Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.","[1, 0, 0, 0, 0]",[],HJxyAjRcFX,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,L'article propose une modification de l'objectif traditionnel du GAN conditionnel afin de promouvoir la génération diversifiée et multimodale d'images. 
"Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss.However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples.In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset.Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.","[1, 0, 0, 0, 0]",[],HJxyAjRcFX,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,Cet article propose une alternative aux erreurs L1/L2 qui sont utilisées pour augmenter les pertes adverses lors de l'entraînement des GANs conditionnels.
"Generative models are important tools to capture and investigate the properties of complex empirical data.Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data.Does learning the parameters of both architectures obey the same rules?We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference.In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.","[1, 0, 0, 0, 0, 0]",[],SySisz-CW,On the difference between building and extracting patterns: a causal analysis of deep generative models.,Nous utilisons l'inférence causale pour caractériser l'architecture des modèles génératifs.
"Generative models are important tools to capture and investigate the properties of complex empirical data.Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data.Does learning the parameters of both architectures obey the same rules?We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference.In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.","[1, 0, 0, 0, 0, 0]",[],SySisz-CW,On the difference between building and extracting patterns: a causal analysis of deep generative models.,"Cet article examine la nature des filtres convolutifs dans l'encodeur et le décodeur d'un VAE, et dans un générateur et un discriminateur d'un GAN."
"Generative models are important tools to capture and investigate the properties of complex empirical data.Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data.Does learning the parameters of both architectures obey the same rules?We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference.In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.","[1, 0, 0, 0, 0, 0]",[],SySisz-CW,On the difference between building and extracting patterns: a causal analysis of deep generative models.,Ce travail exploite le principe de causalité pour quantifier comment les poids des couches successives s'adaptent les uns aux autres.
"Many deep reinforcement learning approaches use graphical state representations,this means visually distinct games that share the same underlying structure cannoteffectively share knowledge.This paper outlines a new approach for learningunderlying game state embeddings irrespective of the visual rendering of the gamestate.We utilise approaches from multi-task learning and domain adaption inorder to place visually distinct game states on a shared embedding manifold.Wepresent our results in the context of deep reinforcement learning agents.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJB7fkWR-,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,Une approche pour apprendre un espace d'intégration partagé entre des jeux visuellement distincts.
"Many deep reinforcement learning approaches use graphical state representations,this means visually distinct games that share the same underlying structure cannoteffectively share knowledge.This paper outlines a new approach for learningunderlying game state embeddings irrespective of the visual rendering of the gamestate.We utilise approaches from multi-task learning and domain adaption inorder to place visually distinct game states on a shared embedding manifold.Wepresent our results in the context of deep reinforcement learning agents.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJB7fkWR-,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,"Une nouvelle approche pour l'apprentissage de la structure sous-jacente de jeux visuellement distincts combinant des couches convolutionnelles pour le traitement des images d'entrée, la critique asynchrone de l'acteur pour l'apprentissage par renforcement profond et une approche contradictoire pour forcer la représentation d'intégration à être indépendante de la représentation visuelle des jeux."
"Many deep reinforcement learning approaches use graphical state representations,this means visually distinct games that share the same underlying structure cannoteffectively share knowledge.This paper outlines a new approach for learningunderlying game state embeddings irrespective of the visual rendering of the gamestate.We utilise approaches from multi-task learning and domain adaption inorder to place visually distinct game states on a shared embedding manifold.Wepresent our results in the context of deep reinforcement learning agents.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJB7fkWR-,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,Présente une méthode pour apprendre une politique sur des jeux visuellement distincts en adaptant l'apprentissage par renforcement profond.
"Many deep reinforcement learning approaches use graphical state representations,this means visually distinct games that share the same underlying structure cannoteffectively share knowledge.This paper outlines a new approach for learningunderlying game state embeddings irrespective of the visual rendering of the gamestate.We utilise approaches from multi-task learning and domain adaption inorder to place visually distinct game states on a shared embedding manifold.Wepresent our results in the context of deep reinforcement learning agents.","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],BJB7fkWR-,Domain Adaptation for Deep Reinforcement Learning in Visually Distinct Games,Cet article présente une architecture d'agent qui utilise une représentation partagée pour entraîner plusieurs tâches avec des statistiques visuelles différentes au niveau des sprites.
"We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$.Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data.This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.Our results apply to increasing activations whose derivatives are bounded away from zero.The analysis is based oni) an SGD convergence result with nonlinear activations andii) careful statistical characterization of the state vector.Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkeMHjR9Ym,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,Nous étudions l'équation d'état d'un réseau neuronal récurrent. Nous montrons que le SGD peut apprendre efficacement la dynamique inconnue à partir de quelques observations d'entrée/sortie sous des hypothèses appropriées.
"We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$.Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data.This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.Our results apply to increasing activations whose derivatives are bounded away from zero.The analysis is based oni) an SGD convergence result with nonlinear activations andii) careful statistical characterization of the state vector.Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkeMHjR9Ym,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,"L'article étudie les systèmes dynamiques à temps discret avec une équation d'état non linéaire, prouvant que l'exécution de SGD sur une trajectoire de longueur fixe donne une convergence logarithmique."
"We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$.Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data.This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.Our results apply to increasing activations whose derivatives are bounded away from zero.The analysis is based oni) an SGD convergence result with nonlinear activations andii) careful statistical characterization of the state vector.Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkeMHjR9Ym,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,Ce travail considère le problème de l'apprentissage d'un système dynamique non linéaire dans lequel la sortie est égale à l'état. 
"We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$.Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data.This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.Our results apply to increasing activations whose derivatives are bounded away from zero.The analysis is based oni) an SGD convergence result with nonlinear activations andii) careful statistical characterization of the state vector.Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rkeMHjR9Ym,Stochastic Gradient Descent Learns State Equations with Nonlinear Activations,Cet article étudie la capacité de SGD à apprendre la dynamique d'un système linéaire et l'activation non linéaire.
"Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation.The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform.In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.To make this, we propose several novel methods for learning neuron manifold from DNN model.Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data.Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.","[0, 0, 0, 1, 0, 0]",[],SJlYcoCcKX,KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD,Une nouvelle méthode de distillation des connaissances pour l'apprentissage par transfert
"Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation.The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform.In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.To make this, we propose several novel methods for learning neuron manifold from DNN model.Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data.Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.","[0, 0, 0, 1, 0, 0]",[],SJlYcoCcKX,KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD,Ce travail introduit une méthode de distillation des connaissances utilisant le concept de collecteur de neurones proposé. 
"Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation.The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform.In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.To make this, we propose several novel methods for learning neuron manifold from DNN model.Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data.Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features.","[0, 0, 0, 1, 0, 0]",[],SJlYcoCcKX,KNOWLEDGE DISTILL VIA LEARNING NEURON MANIFOLD,Propose une méthode de distillation des connaissances dans laquelle le collecteur neuronal est considéré comme la connaissance transférée.
"We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization.At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models.Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively.We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters.Lastly we visualize and discuss the learned features of slimmable networks.Code and models are available at: https://github.com/JiahuiYu/slimmable_networks","[1, 0, 0, 0, 0, 0, 0]",[],H1gMCsAqY7,Slimmable Neural Networks,"Nous présentons une méthode simple et générale pour former un seul réseau neuronal exécutable à différentes largeurs (nombre de canaux dans une couche), permettant des compromis instantanés et adaptatifs entre précision et efficacité au moment de l'exécution."
"We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization.At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models.Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively.We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters.Lastly we visualize and discuss the learned features of slimmable networks.Code and models are available at: https://github.com/JiahuiYu/slimmable_networks","[1, 0, 0, 0, 0, 0, 0]",[],H1gMCsAqY7,Slimmable Neural Networks,"L'article propose de combiner des modèles de taille différente en un seul réseau partagé, ce qui améliore considérablement les performances de détection."
"We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization.At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models.Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively.We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters.Lastly we visualize and discuss the learned features of slimmable networks.Code and models are available at: https://github.com/JiahuiYu/slimmable_networks","[1, 0, 0, 0, 0, 0, 0]",[],H1gMCsAqY7,Slimmable Neural Networks,Cet article entraîne un seul exécutable réseau à différentes largeurs.
"Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval.Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model.In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.","[0, 0, 1]",[],Skvd-myR-,Learning Non-Metric Visual Similarity for Image Retrieval,Réseau de similarité pour apprendre une estimation non métrique de la similarité visuelle entre une paire d'images
"Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval.Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model.In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.","[0, 0, 1]",[],Skvd-myR-,Learning Non-Metric Visual Similarity for Image Retrieval,Les auteurs proposent une mesure de similarité d'apprentissage pour la similarité visuelle et obtiennent par ce biais une amélioration des ensembles de données très connus d'Oxford et de Paris pour la récupération d'images.
"Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval.Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model.In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets.","[0, 0, 1]",[],Skvd-myR-,Learning Non-Metric Visual Similarity for Image Retrieval,L'article soutient qu'il est plus approprié d'utiliser des distances non métriques plutôt que des distances métriques.
"Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.However, it is often the case that data are abundant in some domains but scarce in others.Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain.In general, this requires learning plausible mappings between domains.CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint.However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data.In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction.We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised.In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models.Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation.In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model.Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,Un nouvel apprentissage contradictoire cyclique augmenté d'un modèle de tâche auxiliaire qui améliore les performances d'adaptation au domaine dans des situations supervisées et non supervisées à faibles ressources. 
"Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.However, it is often the case that data are abundant in some domains but scarce in others.Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain.In general, this requires learning plausible mappings between domains.CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint.However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data.In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction.We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised.In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models.Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation.In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model.Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,Propose une extension des méthodes d'adaptation adversatives cohérentes avec le cycle afin de s'attaquer à l'adaptation au domaine lorsque des données cibles supervisées limitées sont disponibles.
"Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.However, it is often the case that data are abundant in some domains but scarce in others.Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain.In general, this requires learning plausible mappings between domains.CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint.However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data.In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction.We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised.In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models.Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation.In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model.Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices.","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]",[],B1G9doA9F7,Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,Cet article introduit une approche d'adaptation au domaine basée sur l'idée du GAN cyclique et propose deux algorithmes différents.
"Nodes residing in different parts of a graph can have similar structural roles within their local network topology.The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs.However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node.Here we develop GraphWave, a method that represents each node’s local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns.We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network.Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features.We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.","[0, 0, 0, 1, 0, 0, 0]",[],rJR2ylbRb,Spectral Graph Wavelets for Structural Role Similarity in Networks,Nous développons une méthode d'apprentissage des signatures structurelles dans les réseaux basée sur la diffusion des ondelettes spectrales de graphes.
"Nodes residing in different parts of a graph can have similar structural roles within their local network topology.The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs.However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node.Here we develop GraphWave, a method that represents each node’s local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns.We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network.Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features.We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.","[0, 0, 0, 1, 0, 0, 0]",[],rJR2ylbRb,Spectral Graph Wavelets for Structural Role Similarity in Networks,Utilisation des modèles de diffusion spectrale par ondelettes du voisinage local d'un nœud pour intégrer le nœud dans un espace de faible dimension.
"Nodes residing in different parts of a graph can have similar structural roles within their local network topology.The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs.However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node.Here we develop GraphWave, a method that represents each node’s local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns.We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network.Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features.We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines.","[0, 0, 0, 1, 0, 0, 0]",[],rJR2ylbRb,Spectral Graph Wavelets for Structural Role Similarity in Networks,L'article présente une méthode de comparaison des nœuds d'un graphe basée sur l'analyse en ondelettes du laplacien du graphe. 
"Driving simulators play an important role in vehicle research.However, existing virtual reality simulators do not give users a true sense of presence.UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.It is powered by SUMO and Unity.UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management.We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.We present a user study that subjectively measures user's sense of presence in UniNet.Our findings suggest that our novel mixed reality system does increase this sensation.","[0, 0, 1, 0, 0, 0, 0, 0]",[],4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,Un simulateur de conduite en réalité mixte utilisant des caméras stéréo et la RV par transparence a été évalué dans le cadre d'une étude auprès de 24 participants.
"Driving simulators play an important role in vehicle research.However, existing virtual reality simulators do not give users a true sense of presence.UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.It is powered by SUMO and Unity.UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management.We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.We present a user study that subjectively measures user's sense of presence in UniNet.Our findings suggest that our novel mixed reality system does increase this sensation.","[0, 0, 1, 0, 0, 0, 0, 0]",[],4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,Propose un système complexe pour la simulation de conduite.
"Driving simulators play an important role in vehicle research.However, existing virtual reality simulators do not give users a true sense of presence.UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.It is powered by SUMO and Unity.UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management.We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.We present a user study that subjectively measures user's sense of presence in UniNet.Our findings suggest that our novel mixed reality system does increase this sensation.","[0, 0, 1, 0, 0, 0, 0, 0]",[],4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,Cet article présente un simulateur de conduite à réalité mixte permettant d'améliorer la sensation de présence.
"Driving simulators play an important role in vehicle research.However, existing virtual reality simulators do not give users a true sense of presence.UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.It is powered by SUMO and Unity.UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management.We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.We present a user study that subjectively measures user's sense of presence in UniNet.Our findings suggest that our novel mixed reality system does increase this sensation.","[0, 0, 1, 0, 0, 0, 0, 0]",[],4ZO8BVlix-,UniNet: A Mixed Reality Driving Simulator,"Propose un simulateur de conduite en réalité mixte qui intègre la génération de trafic et revendique une ""présence"" accrue grâce à un système de RM."
We consider the problem of improving kernel approximation via feature maps.These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.,"[1, 0, 0, 0, 0]",[],H1U_af-0-,Quadrature-based features for kernel approximation,Règles de quadrature pour l'approximation par noyau.
We consider the problem of improving kernel approximation via feature maps.These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.,"[1, 0, 0, 0, 0]",[],H1U_af-0-,Quadrature-based features for kernel approximation,L'article propose d'améliorer l'approximation du noyau des caractéristiques aléatoires en utilisant des règles de quadrature comme les règles sphériques-radiales stochastiques.
We consider the problem of improving kernel approximation via feature maps.These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.,"[1, 0, 0, 0, 0]",[],H1U_af-0-,Quadrature-based features for kernel approximation,Les auteurs proposent une nouvelle version de l'approche des cartes de caractéristiques aléatoires pour résoudre de manière approximative les problèmes de noyaux à grande échelle.
We consider the problem of improving kernel approximation via feature maps.These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.,"[1, 0, 0, 0, 0]",[],H1U_af-0-,Quadrature-based features for kernel approximation,"Cet article montre que les techniques dues à Genz & Monahan (1998) peuvent être utilisées pour obtenir une faible erreur d'approximation des noyaux dans le cadre de la caractéristique de fourier aléatoire, une nouvelle façon d'appliquer les règles de quadrature pour améliorer l'approximation des noyaux."
"Human world knowledge is both structured and flexible.When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts.Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context.Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects.To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts.Using this dataset, we developed neural listener and speaker models with strong capacity for generalization.By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities.We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkgZ3oR9FX,Learning to Refer to 3D Objects with Natural Language,"Comment construire des haut-parleurs/auditeurs neuronaux qui apprennent les caractéristiques fines des objets 3D, à partir du langage référentiel."
"Human world knowledge is both structured and flexible.When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts.Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context.Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects.To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts.Using this dataset, we developed neural listener and speaker models with strong capacity for generalization.By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities.We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object.","[0, 0, 0, 1, 0, 0, 0, 0]",[],rkgZ3oR9FX,Learning to Refer to 3D Objects with Natural Language,"Les auteurs présentent une étude sur l'apprentissage de la référence aux objets 3D, en collectant un ensemble de données d'expressions référentielles et en formant plusieurs modèles en expérimentant un certain nombre de choix architecturaux."
"Object-based factorizations provide a useful level of abstraction for interacting with the world.Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.","[0, 0, 1, 0, 0, 0]",[],HJx9EhC9tQ, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,"Nous présentons un cadre pour l'apprentissage de représentations centrées sur l'objet, adaptées à la planification dans des tâches qui nécessitent une compréhension de la physique."
"Object-based factorizations provide a useful level of abstraction for interacting with the world.Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.","[0, 0, 1, 0, 0, 0]",[],HJx9EhC9tQ, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,L'article présente une plateforme permettant de prédire les images d'objets interagissant entre eux sous l'effet des forces gravitationnelles.
"Object-based factorizations provide a useful level of abstraction for interacting with the world.Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.","[0, 0, 1, 0, 0, 0]",[],HJx9EhC9tQ, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,"L'article présente une méthode qui apprend à reproduire des ""tours de blocs"" à partir d'une image donnée."
"Object-based factorizations provide a useful level of abstraction for interacting with the world.Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.","[0, 0, 1, 0, 0, 0]",[],HJx9EhC9tQ, Reasoning About Physical Interactions with Object-Oriented Prediction and Planning,Propose une méthode qui apprend à raisonner sur l'interaction physique de différents objets sans supervision des propriétés des objets.
"We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.","[0, 0, 0, 0, 1]",[],BJk7Gf-CZ,Global Optimality Conditions for Deep Neural Networks,"Nous fournissons des conditions nécessaires et suffisantes vérifiables efficacement pour l'optimalité globale des réseaux neuronaux linéaires profonds, avec quelques extensions initiales à des paramètres non linéaires."
"We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.","[0, 0, 0, 0, 1]",[],BJk7Gf-CZ,Global Optimality Conditions for Deep Neural Networks,L'article donne des conditions pour l'optimalité globale de la fonction de perte des réseaux neuronaux linéaires profonds.
"We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.","[0, 0, 0, 0, 1]",[],BJk7Gf-CZ,Global Optimality Conditions for Deep Neural Networks,L'article donne des résultats théoriques concernant l'existence de minima locaux dans la fonction objectif des réseaux neuronaux profonds.
"We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.","[0, 0, 0, 0, 1]",[],BJk7Gf-CZ,Global Optimality Conditions for Deep Neural Networks,Étudie certaines propriétés théoriques des réseaux linéaires profonds.
"Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.The summarised information can be used to represent time series features.In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.The fixed-length vector can therefore represent features only in the selected dimensions.In addition, we propose using rolling fixed window approach to generate samples.The change of time series features over time can be summarised as a smooth trajectory path.The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1cLblgCZ,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,Utilisation d'un modèle auto-encodeur récurrent pour extraire des caractéristiques multidimensionnelles de séries temporelles
"Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.The summarised information can be used to represent time series features.In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.The fixed-length vector can therefore represent features only in the selected dimensions.In addition, we propose using rolling fixed window approach to generate samples.The change of time series features over time can be summarised as a smooth trajectory path.The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1cLblgCZ,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,Ce document décrit une application de l'autoencodeur récurrent pour analyser des séries temporelles multidimensionnelles.
"Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.The summarised information can be used to represent time series features.In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.The fixed-length vector can therefore represent features only in the selected dimensions.In addition, we propose using rolling fixed window approach to generate samples.The change of time series features over time can be summarised as a smooth trajectory path.The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1cLblgCZ,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,"L'article décrit un modèle d'auto-encodeur séquence à séquence qui est utilisé pour apprendre des représentations de séquences, montrant que pour leur application, de meilleures performances sont obtenues lorsque le réseau est seulement entraîné à reconstruire un sous-ensemble des mesures de données. "
"Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.The summarised information can be used to represent time series features.In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.The fixed-length vector can therefore represent features only in the selected dimensions.In addition, we propose using rolling fixed window approach to generate samples.The change of time series features over time can be summarised as a smooth trajectory path.The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system.","[0, 1, 0, 0, 0, 0, 0, 0]",[],r1cLblgCZ,Recurrent Auto-Encoder Model for Multidimensional Time Series Representation,Propose une stratégie inspirée du modèle d'auto-encodeur récurrent de sorte que le regroupement de données multidimensionnelles de séries temporelles puisse être effectué sur la base de vecteurs de contexte.
"We view molecule optimization as a graph-to-graph translation problem.The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.A key challenge is therefore to model diverse translation outputs.Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. ","[0, 0, 0, 0, 1, 0, 0]",[],B1xJAsA5F7,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,Nous présentons un cadre d'encodage-décodage de graphe à graphe pour l'apprentissage de diverses traductions de graphes.
"We view molecule optimization as a graph-to-graph translation problem.The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.A key challenge is therefore to model diverse translation outputs.Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. ","[0, 0, 0, 0, 1, 0, 0]",[],B1xJAsA5F7,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,"Propose un modèle de traduction de graphe en graphe pour l'optimisation des molécules, inspiré de l'analyse des paires moléculaires appariées."
"We view molecule optimization as a graph-to-graph translation problem.The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.A key challenge is therefore to model diverse translation outputs.Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. ","[0, 0, 0, 0, 1, 0, 0]",[],B1xJAsA5F7,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,Extension de la JT-VAE au scénario de traduction de graphe à graphe en ajoutant la variable latente pour capturer la multi-modalité et une régularisation adversariale dans l'espace latent.
"We view molecule optimization as a graph-to-graph translation problem.The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.A key challenge is therefore to model diverse translation outputs.Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. ","[0, 0, 0, 0, 1, 0, 0]",[],B1xJAsA5F7,Learning Multimodal Graph-to-Graph Translation for Molecule Optimization,"Propose un système assez complexe, impliquant de nombreux choix et composants différents, pour obtenir des compendiums chimiques aux propriétés améliorées à partir de corpus donnés."
"Partial differential equations (PDEs) are widely used across the physical and computational sciences.Decades of research and engineering went into designing fast iterative solution methods.Existing solvers are general purpose, but may be sub-optimal for specific classes of problems.In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain.We achieve this goal by learning to modify the updates of an existing solver using a deep neural network.Crucially, our approach is proven to preserve strong correctness and convergence guarantees.After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.","[0, 0, 0, 1, 0, 0, 0]",[],rklaWn0qK7,Learning Neural PDE Solvers with Convergence Guarantees,Nous apprenons un solveur neuronal rapide pour les EDP qui a des garanties de convergence.
"Partial differential equations (PDEs) are widely used across the physical and computational sciences.Decades of research and engineering went into designing fast iterative solution methods.Existing solvers are general purpose, but may be sub-optimal for specific classes of problems.In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain.We achieve this goal by learning to modify the updates of an existing solver using a deep neural network.Crucially, our approach is proven to preserve strong correctness and convergence guarantees.After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.","[0, 0, 0, 1, 0, 0, 0]",[],rklaWn0qK7,Learning Neural PDE Solvers with Convergence Guarantees,Développe une méthode pour accélérer la méthode des différences finies dans la résolution des EDP et propose un cadre révisé pour l'itération du point fixe après discrétisation.
"Partial differential equations (PDEs) are widely used across the physical and computational sciences.Decades of research and engineering went into designing fast iterative solution methods.Existing solvers are general purpose, but may be sub-optimal for specific classes of problems.In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain.We achieve this goal by learning to modify the updates of an existing solver using a deep neural network.Crucially, our approach is proven to preserve strong correctness and convergence guarantees.After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.","[0, 0, 0, 1, 0, 0, 0]",[],rklaWn0qK7,Learning Neural PDE Solvers with Convergence Guarantees,Les auteurs proposent une méthode linéaire pour accélérer les solveurs d'EDP.
"Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.","[0, 1, 0, 0, 0, 0]",[],rkxacs0qY7,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Nous réalisons une inférence variationnelle fonctionnelle sur les processus stochastiques définis par les réseaux neuronaux bayésiens.
"Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.","[0, 1, 0, 0, 0, 0]",[],rkxacs0qY7,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Ajustement d'approximations variationnelles de réseaux neuronaux bayésiens sous forme fonctionnelle et en considérant l'appariement à un antécédent de processus stochastique implicitement via des échantillons.
"Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.","[0, 1, 0, 0, 0, 0]",[],rkxacs0qY7,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Présente un nouvel objectif ELBO pour l'entraînement des BNNs qui permet d'encoder des prieurs plus significatifs dans le modèle plutôt que les prieurs de poids moins informatifs caractéristiques de la littérature.
"Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.","[0, 1, 0, 0, 0, 0]",[],rkxacs0qY7,FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS,Présente un nouvel algorithme d'inférence variationnelle pour les modèles de réseaux neuronaux bayésiens où la priorité est spécifiée de manière fonctionnelle plutôt que par une priorité sur les poids. 
"Words are not created equal.In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal.In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.This connection allows us to introduce a novel principled hypernymy score for word embeddings.Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds.We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry.Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Ske5r3AqK7,Poincare Glove: Hyperbolic Word Embeddings,Nous intégrons les mots dans l'espace hyperbolique et faisons le lien avec les intégrations gaussiennes de mots.
"Words are not created equal.In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal.In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.This connection allows us to introduce a novel principled hypernymy score for word embeddings.Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds.We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry.Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Ske5r3AqK7,Poincare Glove: Hyperbolic Word Embeddings,Cet article adapte l'encastrement de mots Glove à un espace hyperbolique donné par le modèle du demi-plan de Poincaré.
"Words are not created equal.In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal.In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.This connection allows us to introduce a novel principled hypernymy score for word embeddings.Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds.We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry.Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy.","[0, 0, 1, 0, 0, 0, 0, 0]",[],Ske5r3AqK7,Poincare Glove: Hyperbolic Word Embeddings,"Cet article propose une approche pour mettre en œuvre un modèle d'incorporation de mots hyperbolique basé sur GLOVE, qui est optimisé via les méthodes d'optimisation riemannienne."
"Answering questions about a text frequently requires aggregating information from multiple places in that text.End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.However, with this supervision, it can perform well.On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B1lf43A5Y7,How to learn (and how not to learn) multi-hop reasoning with memory networks,"Les réseaux de mémoire n'apprennent pas le raisonnement multi-sauts, sauf si nous les supervisons."
"Answering questions about a text frequently requires aggregating information from multiple places in that text.End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.However, with this supervision, it can perform well.On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B1lf43A5Y7,How to learn (and how not to learn) multi-hop reasoning with memory networks,Le raisonnement multi-sauts n'est pas facile à apprendre directement et nécessite une supervision directe. Le fait de bien réussir sur WikiHop ne signifie pas nécessairement que le modèle apprend réellement à sauter.
"Answering questions about a text frequently requires aggregating information from multiple places in that text.End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.However, with this supervision, it can perform well.On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B1lf43A5Y7,How to learn (and how not to learn) multi-hop reasoning with memory networks,L'article propose d'étudier le problème bien connu de l'apprentissage des réseaux de mémoire et plus précisément la difficulté de la supervision de l'apprentissage de l'attention avec de tels modèles.
"Answering questions about a text frequently requires aggregating information from multiple places in that text.End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.However, with this supervision, it can perform well.On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.We hypothesize that many ""multi-hop"" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.","[0, 0, 0, 0, 0, 0, 0, 1]",[],B1lf43A5Y7,How to learn (and how not to learn) multi-hop reasoning with memory networks,Cet article soutient que le réseau de mémoire ne parvient pas à apprendre un raisonnement multi-sauts raisonnable.
"Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood.We compute deep convolutional network generators by inverting a fixed embedding operator.Therefore, they do not require to be optimized with a discriminator or an encoder.The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images.This embedding is computed with a wavelet Scattering transform.Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.","[0, 0, 1, 0, 0, 0]",[],r1NYjfbR-,Generative networks as inverse problems with Scattering transforms,Nous introduisons des réseaux génératifs qui n'ont pas besoin d'être appris avec un discriminateur ou un encodeur ; ils sont obtenus en inversant un opérateur d'encastrement spécial défini par une transformée de diffusion en ondelettes.
"Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood.We compute deep convolutional network generators by inverting a fixed embedding operator.Therefore, they do not require to be optimized with a discriminator or an encoder.The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images.This embedding is computed with a wavelet Scattering transform.Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.","[0, 0, 1, 0, 0, 0]",[],r1NYjfbR-,Generative networks as inverse problems with Scattering transforms,Présente les transformées de diffusion en tant que modèles génératifs d'images dans le contexte des réseaux adversariens génératifs et suggère pourquoi elles pourraient être considérées comme des transformées de gaussianisation avec une perte d'information et une inversion contrôlées. 
"Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood.We compute deep convolutional network generators by inverting a fixed embedding operator.Therefore, they do not require to be optimized with a discriminator or an encoder.The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images.This embedding is computed with a wavelet Scattering transform.Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.","[0, 0, 1, 0, 0, 0]",[],r1NYjfbR-,Generative networks as inverse problems with Scattering transforms,L'article propose un modèle génératif pour les images qui ne nécessite pas l'apprentissage d'un discriminateur (comme dans les GAN) ou d'un encastrement appris.
"Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token.Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input.We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.","[0, 0, 0, 1, 0, 0, 0]",[],B1xf9jAqFQ,Neural Speed Reading with Structural-Jump-LSTM,Nous proposons un nouveau modèle de lecture rapide neuronale qui utilise la structure de ponctuation inhérente d'un texte pour définir un comportement efficace de saut et d'évitement.
"Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token.Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input.We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.","[0, 0, 0, 1, 0, 0, 0]",[],B1xf9jAqFQ,Neural Speed Reading with Structural-Jump-LSTM,L'article propose un modèle Structural-Jump-LSTM pour accélérer la lecture automatique avec deux agents au lieu d'un.
"Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token.Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input.We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.","[0, 0, 0, 1, 0, 0, 0]",[],B1xf9jAqFQ,Neural Speed Reading with Structural-Jump-LSTM,Propose un nouveau modèle de lecture rapide neuronale dans lequel le nouveau lecteur a la capacité de sauter un mot ou une séquence de mots.
"Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token.Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input.We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.","[0, 0, 0, 1, 0, 0, 0]",[],B1xf9jAqFQ,Neural Speed Reading with Structural-Jump-LSTM,"L'article propose une méthode de lecture rapide utilisant des actions de saut et d'évitement, montrant que la méthode proposée est aussi précise que le LSTM mais utilise beaucoup moins de calcul."
"One of the key challenges of session-based recommender systems is to enhance users’ purchase intentions.In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP).In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning.Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).Speciﬁcally, IRN enables the agent to explore its environment and learn predictive representations via three key components.The imagination core generates predicted trajectories, i.e., imagined items that users may purchase.The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards.To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms.Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines.Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can ""guess what you like"" via internal planning.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SkfTIj0cKX,Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction,Nous proposons l'architecture IRN pour augmenter la récompense d'achat éparse et retardée pour la recommandation basée sur la session.
"One of the key challenges of session-based recommender systems is to enhance users’ purchase intentions.In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP).In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning.Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).Speciﬁcally, IRN enables the agent to explore its environment and learn predictive representations via three key components.The imagination core generates predicted trajectories, i.e., imagined items that users may purchase.The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards.To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms.Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines.Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can ""guess what you like"" via internal planning.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SkfTIj0cKX,Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction,Cet article propose d'améliorer les performances des systèmes de recommandation par l'apprentissage par renforcement en utilisant un réseau de reconstruction de l'imagination.
"One of the key challenges of session-based recommender systems is to enhance users’ purchase intentions.In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP).In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning.Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).Speciﬁcally, IRN enables the agent to explore its environment and learn predictive representations via three key components.The imagination core generates predicted trajectories, i.e., imagined items that users may purchase.The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards.To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms.Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines.Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can ""guess what you like"" via internal planning.","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]",[],SkfTIj0cKX,Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction,L'article présente une approche de recommandation basée sur la session en se concentrant sur les achats des utilisateurs plutôt que sur les clics. 
"The question why deep learning algorithms generalize so well has attracted increasingresearch interest.However, most of the well-established approaches,such as hypothesis capacity, stability or sparseness, have not provided completeexplanations (Zhang et al., 2016; Kawaguchi et al., 2017).In this work, we focuson the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesiswill not change much due to perturbations of its training examples, then itwill also generalize well.As most deep learning algorithms are stochastic (e.g.,Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustnessarguments of Xu & Mannor, and introduce a new approach – ensemblerobustness – that concerns the robustness of a population of hypotheses.Throughthe lens of ensemble robustness, we reveal that a stochastic learning algorithm cangeneralize well as long as its sensitiveness to adversarial perturbations is boundedin average over training examples.Moreover, an algorithm may be sensitive tosome adversarial examples (Goodfellow et al., 2015) but still generalize well.Tosupport our claims, we provide extensive simulations for different deep learningalgorithms and different network architectures exhibiting a strong correlation betweenensemble robustness and the ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],r1YUtYx0-,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,Explication théorique et empirique de la généralisation des algorithmes stochastiques d'apprentissage profond par la robustesse d'ensemble
"The question why deep learning algorithms generalize so well has attracted increasingresearch interest.However, most of the well-established approaches,such as hypothesis capacity, stability or sparseness, have not provided completeexplanations (Zhang et al., 2016; Kawaguchi et al., 2017).In this work, we focuson the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesiswill not change much due to perturbations of its training examples, then itwill also generalize well.As most deep learning algorithms are stochastic (e.g.,Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustnessarguments of Xu & Mannor, and introduce a new approach – ensemblerobustness – that concerns the robustness of a population of hypotheses.Throughthe lens of ensemble robustness, we reveal that a stochastic learning algorithm cangeneralize well as long as its sensitiveness to adversarial perturbations is boundedin average over training examples.Moreover, an algorithm may be sensitive tosome adversarial examples (Goodfellow et al., 2015) but still generalize well.Tosupport our claims, we provide extensive simulations for different deep learningalgorithms and different network architectures exhibiting a strong correlation betweenensemble robustness and the ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],r1YUtYx0-,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,Cet article présente une adaptation de la robustesse algorithmique de Xu&Mannor'12 et présente des limites d'apprentissage et une expérience montrant la corrélation entre la robustesse empirique de l'ensemble et l'erreur de généralisation. 
"The question why deep learning algorithms generalize so well has attracted increasingresearch interest.However, most of the well-established approaches,such as hypothesis capacity, stability or sparseness, have not provided completeexplanations (Zhang et al., 2016; Kawaguchi et al., 2017).In this work, we focuson the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesiswill not change much due to perturbations of its training examples, then itwill also generalize well.As most deep learning algorithms are stochastic (e.g.,Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustnessarguments of Xu & Mannor, and introduce a new approach – ensemblerobustness – that concerns the robustness of a population of hypotheses.Throughthe lens of ensemble robustness, we reveal that a stochastic learning algorithm cangeneralize well as long as its sensitiveness to adversarial perturbations is boundedin average over training examples.Moreover, an algorithm may be sensitive tosome adversarial examples (Goodfellow et al., 2015) but still generalize well.Tosupport our claims, we provide extensive simulations for different deep learningalgorithms and different network architectures exhibiting a strong correlation betweenensemble robustness and the ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],r1YUtYx0-,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,Propose une étude de la capacité de généralisation des algorithmes d'apprentissage profond en utilisant une extension de la notion de stabilité appelée robustesse d'ensemble et donne des limites sur l'erreur de généralisation d'un algorithme aléatoire en termes de paramètre de stabilité et fournit une étude empirique tentant de connecter la théorie à la pratique.
"The question why deep learning algorithms generalize so well has attracted increasingresearch interest.However, most of the well-established approaches,such as hypothesis capacity, stability or sparseness, have not provided completeexplanations (Zhang et al., 2016; Kawaguchi et al., 2017).In this work, we focuson the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesiswill not change much due to perturbations of its training examples, then itwill also generalize well.As most deep learning algorithms are stochastic (e.g.,Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustnessarguments of Xu & Mannor, and introduce a new approach – ensemblerobustness – that concerns the robustness of a population of hypotheses.Throughthe lens of ensemble robustness, we reveal that a stochastic learning algorithm cangeneralize well as long as its sensitiveness to adversarial perturbations is boundedin average over training examples.Moreover, an algorithm may be sensitive tosome adversarial examples (Goodfellow et al., 2015) but still generalize well.Tosupport our claims, we provide extensive simulations for different deep learningalgorithms and different network architectures exhibiting a strong correlation betweenensemble robustness and the ability to generalize.","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]",[],r1YUtYx0-,Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms,L'article a étudié la capacité de généralisation des algorithmes d'apprentissage du point de vue de la robustesse dans un contexte d'apprentissage profond.
"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training.Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","[0, 0, 0, 0, 1, 0, 0]",[],r1wEFyWCW,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,Apprentissage en quelques clics PixelCNN
"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training.Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","[0, 0, 0, 0, 1, 0, 0]",[],r1wEFyWCW,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,"L'article propose d'utiliser l'estimation de la densité lorsque la disponibilité des données de formation est faible, en utilisant un modèle de méta-apprentissage."
"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training.Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","[0, 0, 0, 0, 1, 0, 0]",[],r1wEFyWCW,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,"Cet article examine le problème de l'estimation de la densité d'un ou de plusieurs tirages, en utilisant des techniques de méta-apprentissage qui ont été appliquées à l'apprentissage supervisé d'un ou de plusieurs tirages."
"Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  However, such models require many thousands of gradient-based weight updates and unique image examples for training.Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.","[0, 0, 0, 0, 1, 0, 0]",[],r1wEFyWCW,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,L'article se concentre sur l'apprentissage en quelques coups avec l'estimation de densité autorégressive et améliore PixelCNN avec l'attention neuronale et les techniques de méta-apprentissage.
"Neural networks exhibit good generalization behavior in theover-parameterized regime, where the number of network parametersexceeds the number of observations.Nonetheless,current generalization bounds for neural networks fail to explain thisphenomenon.In an attempt to bridge this gap, we study the problem oflearning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function.In the case where the network has LeakyReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.Specifically, we prove convergence rates of SGD to a globalminimum and provide generalization guarantees for this global minimumthat are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model.This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,Nous montrons que SGD apprend des réseaux neuronaux surparamétrés à deux couches avec des activations Leaky ReLU qui se généralisent de manière prouvable sur des données linéairement séparables.
"Neural networks exhibit good generalization behavior in theover-parameterized regime, where the number of network parametersexceeds the number of observations.Nonetheless,current generalization bounds for neural networks fail to explain thisphenomenon.In an attempt to bridge this gap, we study the problem oflearning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function.In the case where the network has LeakyReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.Specifically, we prove convergence rates of SGD to a globalminimum and provide generalization guarantees for this global minimumthat are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model.This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,L'article étudie des modèles surparamétrés capables d'apprendre des solutions bien généralisées en utilisant un réseau à une couche cachée avec une couche de sortie fixe.
"Neural networks exhibit good generalization behavior in theover-parameterized regime, where the number of network parametersexceeds the number of observations.Nonetheless,current generalization bounds for neural networks fail to explain thisphenomenon.In an attempt to bridge this gap, we study the problem oflearning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function.In the case where the network has LeakyReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.Specifically, we prove convergence rates of SGD to a globalminimum and provide generalization guarantees for this global minimumthat are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model.This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],rJ33wwxRb,SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data,"Cet article montre que, sur des données linéairement séparées, la SGD sur un réseau surparamétré peut toujours conduire à un classificateur qui se généralise de manière prouvable."
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}.These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.We propose to learn about decision states from prior experience.By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.We find that this simple mechanism effectively identifies decision states, even in partially observed settings.In effect, the model learns the sensory cues that correlate with potential subgoals.In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rJg8yhAqKm,InfoBot: Transfer and Exploration via the Information Bottleneck,L'entraînement des agents avec des goulots d'étranglement en matière d'information sur les politiques d'objectifs favorise le transfert et permet d'obtenir une prime d'exploration puissante.
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}.These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.We propose to learn about decision states from prior experience.By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.We find that this simple mechanism effectively identifies decision states, even in partially observed settings.In effect, the model learns the sensory cues that correlate with potential subgoals.In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rJg8yhAqKm,InfoBot: Transfer and Exploration via the Information Bottleneck,Propose de régulariser les pertes RL standard avec l'information mutuelle conditionnelle négative pour la recherche de politique dans un cadre RL multi-objectifs.
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}.These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.We propose to learn about decision states from prior experience.By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.We find that this simple mechanism effectively identifies decision states, even in partially observed settings.In effect, the model learns the sensory cues that correlate with potential subgoals.In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rJg8yhAqKm,InfoBot: Transfer and Exploration via the Information Bottleneck,Cet article propose le concept d'état de décision et propose une régularisation de divergence KL pour apprendre la structure des tâches et utiliser cette information pour encourager la politique à visiter les états de décision.
"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}.These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.We propose to learn about decision states from prior experience.By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.We find that this simple mechanism effectively identifies decision states, even in partially observed settings.In effect, the model learns the sensory cues that correlate with potential subgoals.In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.","[0, 0, 0, 0, 1, 0, 0, 0]",[],rJg8yhAqKm,InfoBot: Transfer and Exploration via the Information Bottleneck,L'article propose une méthode de régularisation des politiques conditionnées par un objectif avec un terme d'information mutuelle. 
"Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.This allows us to estimate a descent direction which can then be passed to a first-order optimizer.We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).We provide a demo of Guided ES at: redacted URL","[0, 0, 0, 0, 0, 1, 0, 0]",[],B1xFxh0cKX,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search,"Nous proposons une méthode d'optimisation pour les cas où seuls des gradients biaisés sont disponibles - nous définissons un nouvel estimateur de gradient pour ce scénario, nous dérivons le biais et la variance de cet estimateur, et nous l'appliquons à des exemples de problèmes."
"Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.This allows us to estimate a descent direction which can then be passed to a first-order optimizer.We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).We provide a demo of Guided ES at: redacted URL","[0, 0, 0, 0, 0, 1, 0, 0]",[],B1xFxh0cKX,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search,Les auteurs proposent une approche qui combine la recherche aléatoire avec l'information sur le gradient de substitution et présentent une discussion sur le compromis variance-bias ainsi qu'une discussion sur l'optimisation des hyperparamètres.
"Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.This allows us to estimate a descent direction which can then be passed to a first-order optimizer.We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).We provide a demo of Guided ES at: redacted URL","[0, 0, 0, 0, 0, 1, 0, 0]",[],B1xFxh0cKX,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search, L'article propose une méthode pour améliorer la recherche aléatoire en construisant un sous-espace des k gradients de substitution précédents.
"Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.This allows us to estimate a descent direction which can then be passed to a first-order optimizer.We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).We provide a demo of Guided ES at: redacted URL","[0, 0, 0, 0, 0, 1, 0, 0]",[],B1xFxh0cKX,Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search,Cet article tente d'accélérer l'évolution du type OpenAI en introduisant une distribution non isotrophe avec une matrice de covariance de la forme I + UU^t et des informations externes telles qu'un gradient de substitution pour déterminer U
"Point clouds are an important type of geometric data and have widespread use in computer graphics and vision.However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space.Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation.This paper studies the unsupervised problem of a generative model exploiting graph convolution.We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator.The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry.We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.","[0, 0, 0, 0, 1, 0, 0]",[],SJeXSo09FQ,Learning Localized Generative Models for 3D Point Clouds via Graph Convolution,Un GAN utilisant des opérations de convolution de graphes avec des graphes calculés dynamiquement à partir de caractéristiques cachées.
"Point clouds are an important type of geometric data and have widespread use in computer graphics and vision.However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space.Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation.This paper studies the unsupervised problem of a generative model exploiting graph convolution.We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator.The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry.We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.","[0, 0, 0, 0, 1, 0, 0]",[],SJeXSo09FQ,Learning Localized Generative Models for 3D Point Clouds via Graph Convolution,"L'article propose une version des GANs spécifiquement conçue pour générer des nuages de points, l'opération de suréchantillonnage constituant la principale contribution de ce travail."
"Point clouds are an important type of geometric data and have widespread use in computer graphics and vision.However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space.Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation.This paper studies the unsupervised problem of a generative model exploiting graph convolution.We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator.The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry.We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.","[0, 0, 0, 0, 1, 0, 0]",[],SJeXSo09FQ,Learning Localized Generative Models for 3D Point Clouds via Graph Convolution,Cet article propose des GAN à convolution graphique pour les nuages de points 3D irréguliers qui apprennent le domaine et les caractéristiques en même temps.
"Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples.However, mislabeled examples are to hard avoid in extremely large datasets.We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics.We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses.This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead.Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.","[0, 0, 0, 0, 1, 0]",[],HyGDdsCcFQ,Better Generalization with On-the-fly Dataset Denoising,"Nous présentons un algorithme rapide et facile à mettre en œuvre, qui est robuste au bruit des données."
"Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples.However, mislabeled examples are to hard avoid in extremely large datasets.We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics.We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses.This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead.Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.","[0, 0, 0, 0, 1, 0]",[],HyGDdsCcFQ,Better Generalization with On-the-fly Dataset Denoising,L'article vise à éliminer les exemples potentiels avec le bruit de l'étiquette en écartant ceux qui présentent de grandes pertes dans la procédure de formation.
"Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1lTEh09FQ,Combinatorial Attacks on Binarized Neural Networks,Les attaques basées sur le gradient contre les réseaux neuronaux binarisés ne sont pas efficaces en raison de la non-différentiabilité de ces réseaux. Notre algorithme IPROP résout ce problème en utilisant l'optimisation des nombres entiers.
"Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1lTEh09FQ,Combinatorial Attacks on Binarized Neural Networks,Propose un nouvel algorithme de style de propagation de cible pour générer des attaques adverses fortes sur les réseaux neuronaux binarisés.
"Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1lTEh09FQ,Combinatorial Attacks on Binarized Neural Networks,Cet article propose un nouvel algorithme d'attaque basé sur MILP sur les réseaux neuronaux binaires.
"Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks"" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP.","[0, 0, 0, 0, 1, 0, 0, 0, 0]",[],S1lTEh09FQ,Combinatorial Attacks on Binarized Neural Networks,"Cet article présente un algorithme permettant de trouver des attaques adverses contre des réseaux neuronaux binaires. Cet algorithme trouve itérativement les représentations souhaitées couche par couche, du sommet à l'entrée, et est plus efficace que la résolution complète de la programmation linéaire mixte en nombres entiers (MILP)."
"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.These results constitute a new state-of-the-art in their respective settings.","[0, 1, 0, 0, 0, 0, 0]",[],SklckhR5Ym,Improved Language Modeling by Decoding the Past,Le décodage du dernier token du contexte à l'aide de la distribution prédite du token suivant agit comme un régularisateur et améliore la modélisation du langage.
"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.These results constitute a new state-of-the-art in their respective settings.","[0, 1, 0, 0, 0, 0, 0]",[],SklckhR5Ym,Improved Language Modeling by Decoding the Past,Les auteurs introduisent l'idée d'un décodage passé à des fins de régularisation pour améliorer la perplexité sur Penn Treebank.
"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.These results constitute a new state-of-the-art in their respective settings.","[0, 1, 0, 0, 0, 0, 0]",[],SklckhR5Ym,Improved Language Modeling by Decoding the Past,"Propose un terme de perte supplémentaire à utiliser lors de la formation d'un LSTM LM et montre qu'en ajoutant ce terme de perte, ils peuvent atteindre la perplexité SOTA sur un certain nombre de repères LM."
"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.These results constitute a new state-of-the-art in their respective settings.","[0, 1, 0, 0, 0, 0, 0]",[],SklckhR5Ym,Improved Language Modeling by Decoding the Past,Suggère une nouvelle technique de régularisation qui peut être ajoutée à celles utilisées dans le modèle AWD-LSTM de Merity et al. (2017) avec une faible surcharge.
"The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.Specifically, we assume that the instances are sampled from a Markov chain.Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rJ695PxRW,Discovering Order in Unordered Datasets: Generative Markov Networks,Proposer d'observer les ordres implicites dans les ensembles de données dans une optique de modèle génératif.
"The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.Specifically, we assume that the instances are sampled from a Markov chain.Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rJ695PxRW,Discovering Order in Unordered Datasets: Generative Markov Networks,Les auteurs traitent du problème de l'ordre implicite dans un ensemble de données et de la difficulté de le récupérer. Ils proposent d'apprendre un modèle sans métrique de distance qui suppose une chaîne de Markov comme mécanisme génératif des données. 
"The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.Specifically, we assume that the instances are sampled from a Markov chain.Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rJ695PxRW,Discovering Order in Unordered Datasets: Generative Markov Networks,L'article propose des réseaux de Markov génératifs - une approche basée sur l'apprentissage profond pour modéliser les séquences et découvrir l'ordre dans les ensembles de données.
"The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.Specifically, we assume that the instances are sampled from a Markov chain.Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.","[0, 0, 0, 0, 0, 0, 1, 0, 0]",[],rJ695PxRW,Discovering Order in Unordered Datasets: Generative Markov Networks,Propose d'apprendre l'ordre d'un échantillon de données non ordonné par l'apprentissage d'une chaîne de Markov.
"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","[1, 0, 0, 0]",[],B1KJJf-R-,Neural Program Search: Solving Data Processing Tasks from Description and Examples,Synthèse de programmes à partir d'une description en langage naturel et d'exemples d'entrée/sortie via la recherche d'arbres sur le modèle Seq2Tree.
"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","[1, 0, 0, 0]",[],B1KJJf-R-,Neural Program Search: Solving Data Processing Tasks from Description and Examples,"Présente un modèle seq2Tree pour traduire un énoncé de problème en langage naturel en programme fonctionnel correspondant en DSL, qui a montré une amélioration par rapport à l'approche de base seq2seq."
"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","[1, 0, 0, 0]",[],B1KJJf-R-,Neural Program Search: Solving Data Processing Tasks from Description and Examples,Cet article aborde le problème de la synthèse de programmes lorsqu'on dispose d'une description du problème et d'un petit nombre d'exemples d'entrée-sortie.
"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.","[1, 0, 0, 0]",[],B1KJJf-R-,Neural Program Search: Solving Data Processing Tasks from Description and Examples,L'article présente une technique de synthèse de programmes impliquant une grammaire restreinte de problèmes qui est analysée par faisceau à l'aide d'un réseau encodeur-décodeur attentionnel.
"Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks.The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable).In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions.This is a very mild condition satisfied even by neural networks with a single neuron.Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics.When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set.When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.Our analysis sheds lights on understanding the practical performance of GANs.","[0, 0, 0, 0, 0, 1, 0, 0]",[],Hk9Xc_lR-,On the Discrimination-Generalization Tradeoff in GANs,Cet article étudie les propriétés de discrimination et de généralisation des GAN lorsque l'ensemble des discriminateurs est une classe de fonctions restreinte comme les réseaux neuronaux.
"Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks.The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable).In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions.This is a very mild condition satisfied even by neural networks with a single neuron.Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics.When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set.When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.Our analysis sheds lights on understanding the practical performance of GANs.","[0, 0, 0, 0, 0, 1, 0, 0]",[],Hk9Xc_lR-,On the Discrimination-Generalization Tradeoff in GANs,Équilibre les capacités des classes de générateurs et de discriminateurs dans les GAN en garantissant que les IPM induits sont des métriques et non des pseudo-métriques.
"Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks.The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable).In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions.This is a very mild condition satisfied even by neural networks with a single neuron.Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics.When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set.When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.Our analysis sheds lights on understanding the practical performance of GANs.","[0, 0, 0, 0, 0, 1, 0, 0]",[],Hk9Xc_lR-,On the Discrimination-Generalization Tradeoff in GANs,Cet article fournit une analyse mathématique du rôle de la taille de l'ensemble adversaire/discriminateur dans les GAN.
"Normalization layers are a staple in state-of-the-art deep neural network architectures.They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.","[0, 0, 0, 0, 1, 0]",[],H1gsz30cKX,Fixup Initialization: Residual Learning Without Normalization,Tout ce dont vous avez besoin pour former des réseaux résiduels profonds est une bonne initialisation ; les couches de normalisation ne sont pas nécessaires.
"Normalization layers are a staple in state-of-the-art deep neural network architectures.They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.","[0, 0, 0, 0, 1, 0]",[],H1gsz30cKX,Fixup Initialization: Residual Learning Without Normalization,Une méthode est présentée pour l'initialisation et la normalisation des réseaux résiduels profonds. Cette méthode est basée sur des observations de l'explosion vers l'avant et vers l'arrière dans de tels réseaux. Les performances de la méthode sont comparables aux meilleurs résultats obtenus par d'autres réseaux avec une normalisation plus explicite.
"Normalization layers are a staple in state-of-the-art deep neural network architectures.They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.","[0, 0, 0, 0, 1, 0]",[],H1gsz30cKX,Fixup Initialization: Residual Learning Without Normalization,"Les auteurs proposent une nouvelle façon d'initialiser les réseaux résiduels, qui est motivée par la nécessité d'éviter les gradients explosifs/évanouissants."
"Normalization layers are a staple in state-of-the-art deep neural network architectures.They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.","[0, 0, 0, 0, 1, 0]",[],H1gsz30cKX,Fixup Initialization: Residual Learning Without Normalization,Propose une nouvelle méthode d'initialisation utilisée pour entraîner des RedNets très profonds sans utiliser de norme de lot.
"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult.In a such situation, learning a metric of a sequence from data is one possible solution.The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning.In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training.The partial reward function is a reward function for a partial sequence of a certain length.SeqGAN employs a reward function for completed sequence only.By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole.In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence.Expert-based reward function training is not a kind of GAN frameworks.This makes the optimization of the generator easier.We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE.Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1kP7vlRb,Toward learning better metrics for sequence generation training with policy gradient,"Cet article vise à apprendre une meilleure métrique pour l'apprentissage non supervisé, tel que la génération de texte, et montre une amélioration significative par rapport à SeqGAN."
"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult.In a such situation, learning a metric of a sequence from data is one possible solution.The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning.In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training.The partial reward function is a reward function for a partial sequence of a certain length.SeqGAN employs a reward function for completed sequence only.By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole.In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence.Expert-based reward function training is not a kind of GAN frameworks.This makes the optimization of the generator easier.We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE.Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1kP7vlRb,Toward learning better metrics for sequence generation training with policy gradient,"Décrit une approche pour générer des séquences temporelles en apprenant des valeurs état-action, où l'état est la séquence générée jusqu'à présent, et l'action est le choix de la valeur suivante. "
"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult.In a such situation, learning a metric of a sequence from data is one possible solution.The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning.In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training.The partial reward function is a reward function for a partial sequence of a certain length.SeqGAN employs a reward function for completed sequence only.By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole.In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence.Expert-based reward function training is not a kind of GAN frameworks.This makes the optimization of the generator easier.We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE.Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",[],r1kP7vlRb,Toward learning better metrics for sequence generation training with policy gradient,"Cet article examine le problème de l'amélioration de la génération de séquences par l'apprentissage de meilleures métriques, en particulier le problème du biais d'exposition."
"One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ","[1, 0, 0, 0]",[],rJTGkKxAZ,Learning Generative Models with Locally Disentangled Latent Factors,"Décomposer la tâche d'apprentissage d'un modèle génératif en apprenant des facteurs latents démêlés pour des sous-ensembles de données, puis en apprenant le joint sur ces facteurs latents.  "
"One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ","[1, 0, 0, 0]",[],rJTGkKxAZ,Learning Generative Models with Locally Disentangled Latent Factors,"Facteurs localement désenchevêtrés pour le modèle génératif hiérarchique à variables latentes, qui peut être considéré comme une variante hiérarchique de l'inférence adversariale apprise."
"One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ","[1, 0, 0, 0]",[],rJTGkKxAZ,Learning Generative Models with Locally Disentangled Latent Factors,L'article étudie le potentiel des modèles de variables latentes hiérarchiques pour générer des images et des séquences d'images et propose d'entraîner plusieurs modèles ALI empilés les uns sur les autres pour créer une représentation hiérarchique des données.
"One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  ","[1, 0, 0, 0]",[],rJTGkKxAZ,Learning Generative Models with Locally Disentangled Latent Factors,L'article vise à apprendre les hiérarchies pour l'entraînement du GAN dans un programme d'optimisation hiérarchique directement au lieu d'être conçu par un humain.
"Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.In this paper, we propose a new way to leverage visual knowledge for sentence representations.Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.","[0, 1, 0, 0, 0]",[],BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,Nous proposons un modèle conjoint pour incorporer les connaissances visuelles dans les représentations de phrases.
"Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.In this paper, we propose a new way to leverage visual knowledge for sentence representations.Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.","[0, 1, 0, 0, 0]",[],BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,L'article propose une méthode permettant d'utiliser des vidéos accompagnées de légendes pour améliorer l'intégration des phrases.
"Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.In this paper, we propose a new way to leverage visual knowledge for sentence representations.Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.","[0, 1, 0, 0, 0]",[],BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,"Cette soumission propose un modèle pour l'apprentissage de phrases dont les représentations sont fondées, sur la base de données vidéo associées."
"Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.In this paper, we propose a new way to leverage visual knowledge for sentence representations.Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.","[0, 1, 0, 0, 0]",[],BJe8niAqKX,Learning Grounded Sentence Representations by Jointly Using Video and Text Information,Propose une méthode pour améliorer les incorporations de phrases basées sur le texte grâce à un cadre multimodal conjoint.
