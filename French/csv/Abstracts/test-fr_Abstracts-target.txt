FearNet est un réseau de neurones à mémoire efficace, inspiré de la formation de la mémoire dans le cerveau des mammifères, capable d'apprendre des classes de manière incrémentielle sans oubli catastrophique.
Cet article présente une nouvelle solution à un problème de classification incrémentielle basée sur un système à double mémoire. 
L'apprentissage multi-vues améliore l'apprentissage non supervisé de la représentation des phrases
L'approche utilise des codeurs différents et complémentaires de la phrase d'entrée et la maximisation du consensus.
L'article présente un cadre multi-vues pour améliorer la représentation des phrases dans les tâches NLP en utilisant des architectures objectives génératives et discriminatives.
Cet article montre que les cadres multi-vues sont plus efficaces que l'utilisation d'encodeurs individuels pour l'apprentissage des représentations de phrases.
Nous montrons comment des objets discrets peuvent être appris de manière non supervisée à partir de pixels, et comment effectuer un apprentissage par renforcement en utilisant cette représentation d'objets.
Une méthode pour apprendre des représentations d'objets à partir de pixels pour faire de l'apprentissage par renforcement. 
L'article propose une architecture neuronale pour mettre en correspondance des flux vidéo avec une collection discrète d'objets, sans annotations humaines, en utilisant une perte de reconstruction de pixels non supervisée. 
Un ensemble de données à grande échelle pour l'entraînement des modèles d'attention pour la reconnaissance d'objets permet une reconnaissance d'objets plus précise, plus interprétable et plus proche de l'homme.
Les gains récents en matière de reconnaissance visuelle proviennent de l'utilisation des mécanismes d'attention visuelle dans les réseaux convolutifs profonds, qui apprennent où se concentrer grâce à une forme faible de supervision basée sur les étiquettes de classes d'images.
Présente une nouvelle approche de l'attention dans laquelle un grand ensemble de données sur l'attention est collecté et utilisé pour entraîner un NN de manière supervisée afin d'exploiter l'attention humaine autodéclarée.
Cet article propose une nouvelle approche pour utiliser des signaux plus informatifs, plus précisément les régions que les humains jugent importantes sur les images, pour améliorer les réseaux neuronaux convolutifs profonds.
Nous avons proposé une méthode de défense efficace en termes de temps contre les attaques adverses à une étape et itératives.
Proposer une nouvelle méthode efficace en termes de calcul, appelée e2SAD, qui génère des ensembles de deux échantillons adverses d'entraînement pour chaque échantillon d'entraînement propre.
L'article présente une méthode de défense contradictoire en deux étapes, qui consiste à générer deux exemples contradictoires par échantillon propre et à les inclure dans la boucle d'apprentissage réelle afin d'atteindre la robustesse et de prétendre qu'elle peut surpasser les méthodes itératives plus coûteuses.
L'article présente une approche en deux étapes pour générer des exemples contradictoires forts à un coût bien moindre par rapport aux récentes attaques contradictoires itératives à plusieurs étapes.
Une comparaison de cinq architectures de réseaux neuronaux profonds pour la détection de noms de domaine malveillants montre étonnamment peu de différences.
Les auteurs proposent d'utiliser cinq architectures profondes pour la tâche de cybersécurité consistant à détecter les algorithmes de génération de domaine.
Applique plusieurs architectures NN pour classer les URL entre celles qui sont bénignes et celles qui sont liées à des logiciels malveillants.
Cet article propose de reconnaître automatiquement les noms de domaine comme malveillants ou bénins par des réseaux profonds entraînés à classer directement la séquence de caractères comme telle.
Les méthodes d'apprentissage adversarial encouragent les modèles NLI à ignorer les biais spécifiques aux ensembles de données et aident les modèles à se transférer entre les ensembles de données.
L'article propose une configuration contradictoire pour atténuer les artefacts d'annotation dans les données d'inférence en langage naturel.
Cet article présente une méthode pour éliminer les biais d'un modèle d'implication textuelle par le biais d'un objectif de formation contradictoire. 
Une nouvelle approche de pointe pour l'intégration de graphes de connaissances.
Présente une fonction de notation de prédiction de liens neuronaux qui peut déduire des modèles de symétrie, d'anti-symétrie, d'inversion et de composition de relations dans une base de connaissances.
Cet article propose une approche de l'intégration des graphes de connaissances en modélisant les relations comme des rotations dans l'espace vectoriel complexe.
propose une méthode d'intégration des graphes à utiliser pour la prédiction des liens.
Nous avons modifié le CNN en utilisant des HyperNetworks et avons observé une meilleure robustesse contre les exemples adverses.
Amélioration de la robustesse et de la fiabilité des réseaux neuronaux profonds à convolution par l'utilisation de noyaux de convolution dépendant des données
Nous proposons une approche de méta-apprentissage pour la classification de quelques images qui permet d'obtenir de bonnes performances à grande vitesse par rétro-propagation à travers la solution de solveurs rapides, tels que la régression ridge ou la régression logistique.
L'article propose un algorithme de méta-apprentissage qui revient à fixer les caractéristiques (c'est-à-dire toutes les couches cachées d'un NN profond), et à traiter chaque tâche comme ayant sa propre couche finale qui pourrait être une régression ridge ou une régression logistique.
Cet article propose une approche de méta-apprentissage pour le problème de la classification en quelques coups, ils utilisent une méthode basée sur la paramétrisation de l'apprenant pour chaque tâche par un solveur à forme fermée.
Nous offrons une nouvelle perspective sur la formation d'un modèle d'apprentissage automatique à partir de zéro dans un cadre d'étiquettes hiérarchiques, c'est-à-dire en le considérant comme une communication bidirectionnelle entre l'homme et les algorithmes, et nous étudions comment nous pouvons à la fois mesurer et améliorer l'efficacité. 
Introduit un nouveau cadre d'apprentissage actif dans lequel l'oracle propose une étiquette partielle ou faible au lieu de demander l'étiquette d'un exemple particulier, ce qui simplifie la recherche d'informations.
Cet article propose une méthode d'apprentissage actif avec rétroaction partielle qui surpasse les lignes de base existantes avec un budget limité.
L'article considère un problème de classification multiclasse dans lequel les étiquettes sont regroupées dans un nombre donné M de sous-ensembles, qui contiennent toutes les étiquettes individuelles comme singletons.
Nous montrons que les espaces de Wasserstein sont de bonnes cibles pour l'intégration de données à structure sémantique complexe.
Apprend les encastrements dans un espace discret de distributions de probabilité, en utilisant une version minimisée et régularisée des distances de Wasserstein.
L'article décrit une nouvelle méthode d'incorporation qui incorpore les données dans l'espace des mesures de probabilité doté de la distance de Wasserstein. 
L'article propose d'intégrer les données dans des espaces de Wasserstein de faible dimension, qui peuvent capturer la structure sous-jacente des données de manière plus précise.
Un algorithme de regroupement qui effectue une réduction non linéaire de la dimension et un regroupement en optimisant un objectif global continu.
Présente un algorithme de regroupement en résolvant conjointement l'auto-codeur profond et le regroupement comme un objectif global continu, montrant de meilleurs résultats que les schémas de regroupement de l'état de l'art.
Le clustering continu profond est une méthode de clustering qui intègre l'objectif de l'autoencodeur avec l'objectif du clustering puis s'entraîne en utilisant SGD.
Pour accélérer le calcul des réseaux neuronaux convolutifs, nous proposons une nouvelle technique d'élagage en deux étapes qui permet d'obtenir une plus grande sparsité des poids dans le domaine de Winograd sans modifier la structure du réseau.
Propose un cadre d'élagage spatial-Winograd qui permet de conserver les poids élagués du domaine spatial dans le domaine Winograd et d'améliorer la sparsité du domaine Winograd.
Propose deux techniques d'élagage des couches convolutives qui utilisent l'algorithme de Winograd.
Nous proposons un modèle bayésien non paramétrique pour l'apprentissage fédéré avec des réseaux neuronaux.
Utilise le processus bêta pour faire une correspondance neuronale fédérée.
L'article considère l'apprentissage fédéré des réseaux neuronaux, où les données sont distribuées sur plusieurs machines et la répartition des points de données est potentiellement inhomogène et déséquilibrée.
Méthode générale pour entraîner des noyaux MCMC expressifs paramétrés avec des réseaux neuronaux profonds. Étant donné une distribution cible p, notre méthode fournit un échantillonneur à mélange rapide, capable d'explorer efficacement l'espace d'état.
Propose une HMC généralisée en modifiant l'intégrateur saute-mouton à l'aide de réseaux neuronaux pour que l'échantillonneur converge et mélange rapidement. 
Nous montrons que des défaillances rares mais catastrophiques peuvent être entièrement manquées par les tests aléatoires, ce qui pose des problèmes pour un déploiement sûr. L'approche que nous proposons pour les tests contradictoires résout ce problème.
Propose une méthode qui apprend un prédicteur de probabilité de défaillance pour un agent appris, ce qui permet de prédire quels états initiaux provoquent la défaillance d'un système.
Cet article propose une approche d'échantillonnage par importance pour l'échantillonnage des cas de défaillance pour les algorithmes RL, basée sur une fonction apprise via un réseau neuronal sur les défaillances qui se produisent pendant la formation de l'agent.
Cet article propose une approche contradictoire pour identifier les cas de défaillance catastrophique dans l'apprentissage par renforcement.
Pour résoudre le problème de l'effondrement postérieur dans les VAE, nous proposons une procédure de formation nouvelle mais simple qui optimise de manière agressive le réseau d'inférence avec davantage de mises à jour. Cette nouvelle procédure de formation atténue l'effondrement postérieur et permet d'obtenir un meilleur modèle VAE. 
Examine le phénomène d'effondrement postérieur, montrant qu'un entraînement accru du réseau d'inférence peut réduire le problème et conduire à de meilleurs optima.
Les auteurs proposent de modifier la procédure de formation des VAE uniquement comme solution à l'effondrement postérieur, sans toucher au modèle ni à l'objectif.
Découvrez de manière générative de nouvelles paires d'entités significatives ayant une certaine relation médicale en apprenant purement à partir des paires d'entités significatives existantes, sans avoir besoin d'un corpus de texte supplémentaire pour l'extraction discriminante.
Présente un auto-codeur variationnel pour générer des paires d'entités à partir d'une relation dans un contexte médical.
Dans le contexte médical, cet article décrit le problème classique de la "complétion de la base de connaissances" à partir de données structurées uniquement.
Nous analysons de près la fonction objectif du VAE et tirons de nouvelles conclusions qui conduisent à des améliorations simples.
Propose une méthode VAE en deux étapes pour générer des échantillons de haute qualité et éviter le flou.
Ce document analyse les VAE gaussiens.
L'article fournit un certain nombre de résultats théoriques sur les Auto-Encodeurs Variationnels Gaussiens "vanille", qui sont ensuite utilisés pour construire un nouvel algorithme appelé "VAE à 2 étapes".
Un algorithme de réglage des hyperparamètres utilisant l'analyse de Fourier discrète et la détection comprimée.
Étudie le problème de l'optimisation des hyperparamètres en supposant que la fonction inconnue peut être approximée, en montrant que la minimisation approximative peut être effectuée sur l'hypercube booléen.
L'article explore l'optimisation des hyperparamètres en supposant une structure dans la fonction inconnue qui relie les hyperparamètres à la précision de la classification.
Une nouvelle méthode d'inférence de permutations par descente de gradient, avec des applications à l'inférence de correspondances latentes et à l'apprentissage supervisé de permutations avec des réseaux neuronaux.
L'article utilise une approximation finie de l'opérateur Sinkhorn pour décrire comment construire un réseau neuronal pour l'apprentissage à partir de données de formation à valeur de permutation. 
L'article propose une nouvelle méthode d'approximation du poids maximal discret pour l'apprentissage des permutations latentes.
Un nouveau cadre de recherche d'architecture neuronale différentiable pour la quantification mixte des ConvNets.
Les auteurs présentent une nouvelle méthode de recherche d'architecture neuronale qui sélectionne la quantification de précision des poids à chaque couche du réseau neuronal, et l'utilisent dans le contexte de la compression de réseau.
L'article présente une nouvelle approche de la quantification du réseau en quantifiant différentes couches avec des largeurs de bits différentes et introduit un nouveau cadre de recherche d'architecture neuronale différentiable.
Fonction de perte lisse pour la minimisation de l'erreur top-k
Propose d'utiliser la perte top-k avec des modèles profonds pour résoudre le problème de la confusion des classes avec des classes similaires présentes ou absentes de l'ensemble de données d'entraînement.
Lisse les pertes top-k.
Cet article introduit une fonction de perte de substitution lisse pour le SVM top-k, dans le but de brancher le SVM aux réseaux neuronaux profonds.
Nous présentons Mol-CycleGAN - un nouveau modèle génératif pour l'optimisation des molécules afin d'améliorer la conception des médicaments.
L'article présente une approche d'optimisation des propriétés moléculaires basée sur l'application des CycleGAN aux auto-codeurs variationnels pour les molécules et utilise un VAE spécifique au domaine appelé Junction Tree VAE (JT-VAE).
Cet article utilise un autoencodeur variationnel pour apprendre une fonction de traduction, de l'ensemble des molécules sans la propriété concernée à l'ensemble des molécules avec la propriété. 
Nous prenons la reconnaissance des visages comme point de rupture et proposons la distillation de modèles avec transfert de connaissances de la classification des visages à l'alignement et à la vérification.
Cet article propose de transférer le classificateur du modèle de classification des visages à la tâche d'alignement et de vérification.
Le manuscrit présente des expériences sur la distillation des connaissances d'un modèle de classification des visages vers des modèles d'étudiants pour l'alignement et la vérification des visages.
Amélioration de 35% des recommandations basées sur la session avec des RNN (GRU4Rec) en utilisant des fonctions de perte et un échantillonnage nouvellement conçus.
Cet article analyse les fonctions de perte existantes pour les recommandations basées sur les sessions et propose deux nouvelles fonctions de perte qui ajoutent une pondération aux fonctions de perte existantes basées sur le classement.
présente des modifications par rapport aux travaux antérieurs pour la recommandation basée sur la session en utilisant le RNN en pondérant les exemples négatifs par leur "pertinence".
Cet article discute des problèmes d'optimisation des fonctions de perte dans GRU4Rec, propose des astuces d'optimisation et suggère une version améliorée.
Nous développons une approche d'apprentissage tout au long de la vie pour l'apprentissage par transfert basée sur la théorie PAC-Bayes, dans laquelle les prieurs sont ajustés au fur et à mesure que de nouvelles tâches sont rencontrées, facilitant ainsi l'apprentissage de nouvelles tâches.
Une nouvelle limite de risque PAC-Bayes qui sert de fonction objective pour l'apprentissage automatique multi-tâches, et un algorithme pour minimiser une version simplifiée de cette fonction objective.
Étend les limites PAC-Bayes existantes à l'apprentissage multi-tâches, afin de permettre l'adaptation de l'antériorité à différentes tâches.
Une version adaptée d'Adam pour l'entraînement de DNNs, qui comble le fossé de généralisation entre Adam et SGD.
Propose une variante de l'algorithme d'optimisation ADAM qui normalise les poids de chaque unité cachée en utilisant la normalisation par lots.
Extension de l'algorithme d'optimisation d'Adam pour préserver la direction de mise à jour en adaptant le taux d'apprentissage pour les poids entrants dans une unité cachée en utilisant conjointement la norme L2 du vecteur gradient.
Nous montrons comment nous pouvons utiliser la représentation successeur pour découvrir des options propres dans des domaines stochastiques, à partir de pixels bruts. Les options propres sont des options apprises pour naviguer dans les dimensions latentes d'une représentation apprise.
Étend l'idée des options propres aux domaines avec des transitions stochastiques et où les caractéristiques de l'état sont apprises.
Montre l'équivalence entre les fonctions de valeur proto et les représentations de successeur et dérive l'idée d'options propres comme mécanisme de découverte d'options.
Cet article fait suite aux travaux antérieurs de Machado et al. (2017) montrant comment les fonctions de proto-valeur peuvent être utilisées pour définir des options appelées " options propres ".
Nous fournissons des limites supérieures améliorées pour le nombre de régions linéaires utilisées dans l'expressivité du réseau, et un algorithme très efficace (par rapport au comptage exact) pour obtenir des limites inférieures probabilistes sur le nombre réel de régions linéaires.
Contribue à l'étude du nombre de régions linéaires dans les réseaux neuronaux RELU en utilisant un algorithme de comptage probabiliste approximatif et une analyse.
S'appuie sur des travaux antérieurs étudiant le comptage des régions linéaires dans les réseaux neuronaux profonds et améliore la limite supérieure proposée précédemment en modifiant la contrainte de dimensionnalité.
L'article traite de l'expressivité d'un réseau neuronal linéaire par morceaux, caractérisée par le nombre de régions linéaires de la fonction modélisée, et tire parti des algorithmes probabilistes pour calculer les limites plus rapidement et prouver des limites plus strictes.
Une mémoire de travail d'inspiration biologique pouvant être intégrée dans des modèles d'attention visuelle récurrente pour des performances de pointe
Introduit une nouvelle architecture de réseau inspirée de la mémoire de travail visuelle attentive et l'applique à des tâches de classification et l'utilise comme modèle génératif
L'article ajoute au modèle d'attention récurrent un nouveau modèle de mémoire de travail de Hebb-Rosenblatt et obtient des résultats compétitifs sur MNIST.
L'article utilise l'encodage automatique variationnel et le conditionnement de réseau pour le transfert de timbres musicaux, nous développons et généralisons notre architecture pour les transferts d'instruments entre plusieurs, avec des visualisations et des évaluations.
propose un auto-encodeur variationnel modulé pour effectuer le transfert de timbres musicaux en remplaçant le critère de traduction contradictoire habituel par un écart moyen maximal.
Décrit un modèle many-to-many pour le transfert de timbre musical qui s'appuie sur les développements récents en matière de transfert de domaine et de style.
Propose un modèle hybride basé sur le VAE pour effectuer le transfert de timbre sur des enregistrements d'instruments de musique.
Nous étudions le comportement des autoencodeurs multicouches de vanille liés à des poids sous l'hypothèse de poids aléatoires. Grâce à une caractérisation exacte dans la limite des grandes dimensions, notre analyse révèle des phénomènes intéressants de transition de phase.
Une analyse théorique des auto-codeurs avec poids liés entre codeur et décodeur (weight-tied) via l'analyse du champ moyen.
Analyse les performances des auto-encodeurs liés pondérés en s'appuyant sur les progrès récents réalisés dans l'analyse des problèmes de statistiques à haute dimension et, en particulier, sur l'algorithme de passage de messages.
Cet article étudie les auto-codeurs sous plusieurs hypothèses, et souligne que ce modèle d'auto-codeur aléatoire peut être analysé de manière élégante et rigoureuse avec des équations unidimensionnelles.
Inspirés par des travaux antérieurs sur les autoencodeurs Sliced-Wasserstein (SWAE) et le lissage à noyau, nous construisons un nouveau modèle génératif : l'autoencodeur Cramer-Wold (CWAE).
Cet article propose une variante du WAE basée sur une nouvelle distance statistique entre la distribution des données codées et la distribution antérieure latente.
Introduit une variation des AudoEncoders de Wasserstein qui est une nouvelle architecture d'auto-encodeur régularisé qui propose un choix spécifique de la pénalité de divergence.
Cet article propose un auto-codeur de Cramer-Wold, qui utilise la distance de Cramer-Wold entre deux distributions sur la base du théorème de Cramer-Wold.
Nous utilisons un discriminateur GAN pour réaliser un schéma d'échantillonnage de rejet approximatif sur la sortie du générateur GAN.
 Propose un algorithme d'échantillonnage par rejet pour l'échantillonnage du générateur GAN.
Cet article a proposé un schéma d'échantillonnage de rejet post-traitement pour les GAN, nommé Discriminator Rejection Sampling, pour aider à filtrer les â€˜bonsâ€™ échantillons du générateur de GANsâ€™.
Une méthode simple et rapide pour extraire les caractéristiques visuelles des réseaux de neurones convolutifs
propose un moyen rapide d'apprendre des caractéristiques convolutionnelles qui peuvent ensuite être utilisées avec n'importe quel classificateur en utilisant un nombre réduit d'épocs d'entraînement et des délais spécifiques de la vitesse d'apprentissage.
Utilisez un schéma de décroissance du taux d'apprentissage qui est fixe par rapport au nombre d'époques utilisées dans la formation et extrayez la sortie de l'avant-dernière couche comme caractéristiques pour former un classificateur conventionnel.
Nous fournissons de nouvelles perspectives et interprétations des RNN du point de vue des opérateurs spline max-affine.
Réécrit les équations du RNN d'Elman en termes d'opérateurs spline max-affine.
Fournir une nouvelle approche pour comprendre les RNN utilisant des opérateurs spline max-affine (MASO) en les réécrivant avec des activations affines et convexes par morceaux MASO.
Les auteurs s'appuient sur l'interpétation par l'opérateur spline max-affine d'une classe importante de réseaux profonds, en se concentrant sur les réseaux neuronaux récurrents qui utilisent le bruit dans l'état caché initial comme régularisation.
Nous adaptons les Projecteurs de Théorèmes Neuraux à de grands ensembles de données, améliorons le processus d'apprentissage des règles, et l'étendons pour raisonner conjointement sur du texte et des bases de connaissances.
propose une extension du système de vérificateurs de théorèmes neuronaux qui répond aux principaux problèmes de ce modèle en réduisant la complexité temporelle et spatiale de celui-ci
met à l'échelle les PNT en utilisant la recherche approximative du plus proche voisin sur les faits et les règles pendant l'unification et suggère de paramétrer les prédicats en utilisant l'attention sur les prédicats connus
améliore l'approche du vérificateur de théorèmes neuronal proposée précédemment en utilisant la recherche du plus proche voisin.
Généralisation des relations apprises entre des paires d'images à l'aide d'un petit nombre de données d'entraînement à des types d'images inédites en utilisant un modèle de systèmes dynamiques explicables, le Reservoir Computing, et une technique d'apprentissage biologiquement plausible basée sur des analogies.
Revendique les résultats de la "combinaison de transformations" dans le contexte de la RC en utilisant un réseau d'écho-état avec des acctivations tanh standard, à la différence que les poids récurrents ne sont pas formés.
Nouvelle méthode de classification de différentes distorsions des données MNIST
L'article utilise un réseau à état d'écho pour apprendre à classer les transformations d'images entre paires d'images dans l'une des cinq classes.
Nous présentons Generative Adversarial Privacy and Fairness (GAPF), un cadre axé sur les données pour l'apprentissage de représentations privées et équitables avec des garanties certifiées de confidentialité et d'équité.
Cet article utilise un modèle GAN pour donner un aperçu des travaux liés à l'apprentissage par représentation privée/équitable (PRL).
Cet article présente une approche basée sur les adversaires pour des représentations privées et équitables par une distorsion apprise des données qui minimise la dépendance aux variables sensibles alors que le degré de distorsion est contraint.
Les auteurs décrivent un cadre permettant d'apprendre une représentation de parité démographique qui peut être utilisée pour entraîner certains classificateurs.
Nous présentons des mesures et une attaque optimale pour évaluer les modèles qui se défendent contre les exemples adverses en utilisant le seuillage de confiance.
Cet article présente une famille d'attaques contre les algorithmes de seuillage de confiance, en se concentrant principalement sur les méthodologies d'évaluation.
Propose une méthode d'évaluation des modèles de défense par seuillage de confiance et une approche pour générer des exemples adverses en choisissant la mauvaise classe avec le plus de confiance lors d'attaques ciblées.
L'article présente une méthodologie d'évaluation des attaques contre les méthodes de seuillage de confiance et propose un nouveau type d'attaque.
une nouvelle méthode d'apprentissage avec récompense clairsemée utilisant le ré-étiquetage de la récompense contradictoire
propose d'utiliser un cadre multi-agents compétitif pour encourager l'exploration et montre que CER + HER > HER ~ CER
Proposer une nouvelle méthode d'apprentissage à partir de récompenses éparses dans des contextes d'apprentissage par renforcement sans modèle et densifier les récompenses.
Pour résoudre les problèmes de récompenses éparses et encourager l'exploration dans les algorithmes de RL, les auteurs proposent une stratégie de ré-étiquetage appelée Competitive Experience Reply (CER).
La construction d'un modèle TTS avec des VAE à mélange gaussien permet un contrôle fin du style d'élocution, des conditions de bruit, etc.
Décrit le modèle GAN conditionné pour générer des spectres Mel conditionnés par le locuteur en augmentant l'espace z correspondant à l'identification.
Cet article propose un modèle de variable latente à deux couches pour obtenir une représentation latente démêlée, facilitant ainsi le contrôle fin de divers attributs.
Cet article propose un modèle qui peut contrôler les attributs non annotés tels que le style de parole, l'accent, le bruit de fond, etc.
Permettre aux modèles de réponse aux questions visuelles de compter en traitant les propositions d'objets qui se chevauchent.
Cet article propose une architecture de réseau conçue à la main sur un graphe de propositions d'objets pour effectuer une suppression douce non maximale afin d'obtenir le nombre d'objets.
Se concentre sur un problème de comptage dans la réponse à une question visuelle en utilisant le mécanisme d'attention et propose un composant de comptage différentiable qui compte explicitement le nombre d'objets.
Cet article aborde le problème du comptage des objets dans les réponses aux questions visuelles, il propose plusieurs heuristiques pour trouver le comptage correct.
Une approche simple et sans apprentissage pour l'incorporation de phrases avec des performances compétitives par rapport aux modèles sophistiqués nécessitant une grande quantité de données d'apprentissage ou un temps d'apprentissage prolongé.
Présentation d'une nouvelle méthode sans formation pour générer l'intégration des phrases avec une analyse systématique.
propose une nouvelle méthode basée sur la géométrie pour l'incorporation de phrases à partir de vecteurs d'incorporation de mots en quantifiant la nouveauté, l'importance et l'unicité de corpus de chaque mot
Cet article explore l'intégration de phrases basée sur la décomposition orthogonale de l'espace couvert par les intégrations de mots.
Nous proposons de nouvelles extensions des réseaux prototypiques qui sont augmentés de la capacité d'utiliser des exemples non étiquetés lors de la production de prototypes.
Cet article est une extension d'un réseau prototypique qui envisage d'utiliser les exemples non étiquetés disponibles pour aider à former chaque épisode.
Étudie le problème de la classification semi-supervisée de quelques images en étendant les réseaux prototypiques à l'apprentissage semi-supervisé avec des exemples de classes de distracteurs.
Étend le réseau prototypique au cadre semi-supervisé en mettant à jour les prototypes à l'aide de pseudo-étiquettes attribuées, en traitant les distracteurs et en évaluant les échantillons en fonction de la distance aux prototypes originaux.
Nous prouvons théoriquement que les interpolations linéaires ne sont pas adaptées à l'analyse des modèles génératifs implicites entraînés. 
étudie le problème du moment où l'interpolant linéaire entre deux variables aléatoires suit la même distribution, lié à la distribution préalable d'un modèle génératif implicite
Ce travail demande comment interpoler dans l'espace latent étant donné un modèle de variable latente.
Détection d'un nodule pulmonaire à partir de données de projection plutôt que d'images.
Les DNN sont utilisés pour la détection des nodules pulmonaires par patchs dans les données de projection CT.
Modélisation conjointe de la reconstruction de la tomographie assistée par ordinateur et de la détection des lésions pulmonaires par l'apprentissage du mappage du sinogramme brut aux sorties de détection de bout en bout
Présente une formation de bout en bout d'une architecture CNN qui combine le traitement du signal d'une image CT et l'analyse d'image.
Nous évaluons quantitativement et qualitativement les méthodes de navigation basées sur l'apprentissage par renforcement profond dans diverses conditions pour répondre à la question de savoir dans quelle mesure elles sont proches de remplacer les planificateurs de chemin et les algorithmes de cartographie classiques.
Évaluer un modèle RL profond sur des labyrinthes d'entraînement en mesurant la latence répétée vers le but et la comparaison avec le chemin le plus court.
Nous évaluons l'apprentissage de modèles de bruit hétéroscédastiques à l'aide de différents filtres de Bayes différentiables.
propose d'apprendre des modèles de bruit hétéroscédastiques à partir de données en optimisant la probabilité de prédiction de bout en bout par le biais de filtres bayésiens différentiables et de deux versions différentes du filtre de Kalman non centré.
Revoit les filtres de Bayes et évalue l'avantage d'entraîner les modèles d'observation et de bruit de processus tout en gardant tous les autres modèles fixes.
Cet article présente une méthode pour apprendre et utiliser le bruit dépendant de l'état et de l'observation dans les algorithmes traditionnels de filtrage bayésien. L'approche consiste à construire un modèle de réseau neuronal qui prend en entrée les données d'observation brutes et produit une représentation compacte et une covariance diagonale associée.
Nous repensons la manière dont l'information peut être exploitée plus efficacement dans le graphe de connaissances afin d'améliorer les performances dans la tâche d'apprentissage à zéro coup et proposons un module de propagation de graphe dense (DGP) à cette fin.
Les auteurs proposent une solution au problème du lissage excessif dans les réseaux de convulsions graphiques en permettant une propagation dense entre tous les nœuds apparentés, pondérée par la distance mutuelle.
propose un nouveau réseau de neurones convolutifs à graphes pour résoudre le problème de la classification à zéro coup en utilisant les structures relationnelles entre les classes comme entrée des réseaux convolutifs à graphes pour apprendre des classificateurs de classes non vues.
Une segmentation sémantique basée sur les capsules, dans laquelle les probabilités des étiquettes de classe sont retracées dans le pipeline des capsules. 
Les auteurs présentent un mécanisme de traçage pour associer le niveau le plus bas des Capsules à leurs classes respectives.
propose une couche de traçage pour les réseaux de capsules afin de réaliser une segmentation sémantique et utilise explicitement la relation partie-entière dans les couches de capsules
Propose une méthode de traçage basée sur le concept CapsNet de Sabour pour effectuer une segmentation sémantique en parallèle à la classification.
Nous considérons le SGD comme une trajectoire dans l'espace des mesures de probabilité, nous montrons son lien avec les processus de Markov, nous proposons un modèle de Markov simple de l'apprentissage du SGD, et nous le comparons expérimentalement au SGD utilisant des quantités théoriques d'information. 
construit une chaîne de Markov qui suit un chemin court dans la métrique TV sur P et montre que les trajectoires de SGD et \alpha-SMLC ont une entropie conditionnelle similaire
Etude de la trajectoire de H(\hat{y}) par rapport à H(\hat{y}|y) sur le plan d'information pour les méthodes de descente de gradient stochastique pour l'entraînement des réseaux de neurones
Décrit le SGD du point de vue de la distribution p(y',y) où y est le vrai label de classe (éventuellement corrompu) et y' une prédiction du modèle.
Cet article propose une méthode pour automatiser la conception de la proposition MCMC à gradient stochastique en utilisant une approche de méta apprentissage. 
Présente une approche de méta-apprentissage pour concevoir automatiquement l'échantillonneur MCMC basé sur la dynamique hamiltonienne afin qu'il se mélange plus rapidement sur des problèmes similaires aux problèmes d'entraînement.
Paramétrage des matrices de diffusion et de curl par des réseaux neuronaux et méta-apprentissage et optimisation d'un algorithme sg-mcmc. 
Les techniques d'évaluation de la qualité des images améliorent la formation et l'évaluation des réseaux adversariaux génératifs basés sur l'énergie.
propose une formulation basée sur l'énergie pour le modeal BEGAN et le modifie pour inclure un terme basé sur l'évaluation de la qualité de l'image
Propose de nouvelles fonctions d'énergie dans le cadre de BEGAN (cadre GAN à équilibre frontal), notamment le score l_1, le score de similarité de gradient de magnitude et le score de chrominance.
Nous présentons une variante simple de l'optimisation du momentum qui est capable de surpasser le momentum classique, Nesterov et Adam sur des tâches d'apprentissage profond avec un réglage minimal des hyperparamètres.
Introduit une variante de la quantité de mouvement qui agrège plusieurs vitesses avec des coefficients d'amortissement différents, ce qui réduit considérablement l'oscillation.
Proposition d'une méthode de momentum agrégé pour l'optimisation basée sur le gradient en utilisant plusieurs vecteurs de vitesse avec différents facteurs d'amortissement au lieu d'un seul vecteur de vitesse pour améliorer la stabilité.
Les auteurs combinent plusieurs étapes de mise à jour pour obtenir un momentum agrégé et démontrent que cette méthode est plus stable que les autres méthodes de momentum.
Nous introduisons l'unité actualisée récurrente qui applique l'attention à une séquence de n'importe quelle longueur en temps linéaire.
Cet article propose l'attention récurrente actualisée (RDA), une extension de la moyenne pondérée récurrente (RWA) en ajoutant un facteur d'actualisation.
Étend la moyenne de poids récurrente pour surmonter les limites de la méthode originale tout en conservant ses avantages et propose la méthode d'utilisation des réseaux Elman comme RNN de base.
Il est possible d'apprendre une distribution gaussienne centrée sur zéro sur les poids d'un réseau neuronal en apprenant uniquement les variances, et cela fonctionne étonnamment bien.
Cet article étudie les effets de la moyenne du postérieur variationnel et propose une couche de variance, qui utilise uniquement la variance pour stocker l'information.
étudie les réseaux neuronaux de variance qui se rapprochent du postérieur des réseaux neuronaux bayésiens avec des distributions gaussiennes à moyenne nulle.
Nous créons un réseau de réseaux de convolution graphique, en alimentant chacun une puissance différente de la matrice d'adjacence, en combinant toute leur représentation dans un sous-réseau de classification, ce qui permet d'atteindre l'état de l'art en matière de classification semi-supervisée des nœuds.
Propose un nouveau réseau de GCN avec deux approches : une couche entièrement connectée au-dessus des caractéristiques empilées et un mécanisme d'attention qui utilise un poids scalaire par GCN.
présente un réseau de réseaux convolutifs de graphes qui utilise des statistiques de marche aléatoire pour extraire des informations des voisins proches et éloignés dans le graphe.
Un algorithme d'élagage rapide pour les couches DNN entièrement connectées avec une analyse théorique de la dégradation de l'erreur de généralisation.
Présente un algorithme d'élagage bon marché pour les couches denses des DNN.
Propose une solution au problème de l'élagage des DNN en posant la fonction objectif de Net-trim comme une fonction Différence de convexité (DC).
Nous proposons un nouveau réseau temporel hybride qui atteint des performances de pointe pour la segmentation d'actions vidéo sur trois ensembles de données publiques.
Aborde le problème de la segmentation d'actions dans des vidéos longues, jusqu'à 10 minutes, en utilisant une architecture d'encodeur-décodeur convolutif temporel.
Propose une combinaison de réseaux convolutifs et récurrents temporels pour la segmentation d'actions vidéo.
Cet article propose de transférer les connaissances d'un modèle profond à un modèle peu profond en imitant les caractéristiques étape par étape.
Explique une méthode de transfert de connaissances étape par étape en utilisant différentes structures de réseaux.
Cet article propose de diviser un réseau en plusieurs parties et de distiller chaque partie séquentiellement afin d'améliorer les performances de la distillation dans les réseaux profonds d'enseignants.
Des améliorations de la robustesse des adversaires, ainsi que des garanties de robustesse démontrables, sont obtenues en augmentant l'entraînement des adversaires avec une régularisation Lipschitz traçable.
Étude de l'augmentation de la perte d'apprentissage par un terme supplémentaire de régularisation du gradient afin d'améliorer la robustesse des modèles face à des exemples contradictoires.
Utilise une astuce pour simplifier la perte de l'adversaire par une perte dans laquelle la perturbation de l'adversaire apparaît sous une forme fermée.
Un modèle combinant élimination et sélection pour répondre à des questions à choix multiples
Donne une élaboration sur le lecteur d'attention graduée ajoutant des portes basées sur l'élimination des réponses dans la compréhension de lecture à choix multiple.
Cet article propose l'utilisation d'une porte d'élimination dans les architectures de modèles pour les tâches de compréhension de la lecture, mais n'obtient pas de résultats à la pointe de la technologie.
Cet article propose un nouveau modèle de compréhension de lecture à choix multiples basé sur l'idée que certaines options devraient être éliminées pour déduire de meilleures représentations du passage/de la question.
Nous avons proposé un nouveau cadre de raisonnement récursif probabiliste (PR2) pour les tâches d'apprentissage par renforcement profond multi-agents.
propose une nouvelle approche pour une formation entièrement décentralisée dans l'apprentissage par renforcement multi-agent.
Aborde le problème de doter les agents RL de capacités de raisonnement récursif dans un cadre multi-agents, en se basant sur l'hypothèse que le raisonnement récursif leur est bénéfique pour converger vers des équilibres non triviaux.
L'article présente une méthode de formation décentralisée pour l'apprentissage par renforcement multi-agent, où les agents déduisent les politiques des autres agents et utilisent les modèles déduits pour prendre des décisions. 
Un nouvel algorithme pour réduire le surcoût de communication de l'apprentissage profond distribué en distinguant les gradients â€˜unambiguâ€™.
Proposition d'une méthode de compression du gradient basée sur la variance pour réduire les frais de communication de l'apprentissage profond distribué.
propose une nouvelle méthode de compression des mises à jour du gradient pour les SGD distribués afin d'accélérer l'exécution globale.
Présente la méthode de compression du gradient basée sur la variance pour une formation distribuée efficace des réseaux neuronaux et la mesure de l'ambiguïté.
Une nouvelle technique non supervisée d'adaptation au domaine profond qui unifie efficacement l'alignement des corrélations et la minimisation de l'entropie.
Améliore l'approche d'alignement par corrélation pour l'adaptation au domaine en remplaçant la distance euclidienne par la distance géodésique log-euclidienne entre deux matrices de covariance, et en sélectionnant automatiquement le coût d'équilibrage par l'entropie sur le domaine cible.
Proposition d'alignement de corrélation à entropie minimale, un algorithme d'adaptation de domaine non supervisé qui relie les méthodes de minimisation d'entropie et d'alignement de corrélation.
Nous proposons une variante de l'algorithme de rétropropagation, dans laquelle les gradients sont protégés par des conceptors contre la dégradation des tâches apprises précédemment.
Cet article applique la notion de concepteurs, une forme de régularisateur, pour empêcher l'oubli dans l'apprentissage continu dans la formation de réseaux neuronaux sur des tâches séquentielles.
Présente une méthode d'apprentissage de nouvelles tâches, sans interférence avec les tâches précédentes, à l'aide de concepteurs.
Analysez la raison pour laquelle les modèles génératifs à réponse neuronale préfèrent les réponses universelles ; proposez une méthode pour l'éviter.
Étudie le problème des réponses universelles dont souffrent les modèles de génération neuronale Seq2Seq.
L'article cherche à améliorer la tâche de génération de réponses neuronales en minimisant les réponses communes par une modification de la fonction de perte et en présentant les réponses communes/universelles pendant la phase de formation.
Nous tirons parti de la structure syntaxique du code source pour générer des séquences en langage naturel.
Présente une méthode pour générer des séquences à partir d'un code en analysant et en produisant un arbre syntaxique.
Cet article présente un codage basé sur l'AST pour le code de programmation et montre son efficacité dans les tâches de résumé de code extrême et de sous-titrage de code.
Cet article présente un nouveau modèle code-séquence qui exploite la structure syntaxique des langages de programmation pour coder des extraits de code source et les décoder ensuite en langage naturel.
Nous améliorons les CNN avec un nouveau mécanisme d'attention pour une reconnaissance fine. Des performances supérieures sont obtenues sur 5 jeux de données.
Décrit un nouveau mécanisme attentionnel appliqué à la reconnaissance à grain fin qui améliore constamment la précision de reconnaissance de la ligne de base.
Cet article propose un mécanisme d'attention à action directe pour la classification d'images à grain fin.
Cet article présente un mécanisme d'attention intéressant pour la classification d'images à grain fin.
Nous remplaçons les convolutions normales par des convolutions adaptatives pour améliorer le générateur de GANs.
propose de remplacer les convolutions dans le générateur par un bloc de convolution adaptatif qui apprend à générer les poids de convolution et les biais des opérations de suréchantillonnage de manière adaptative par emplacement de pixel.
Utilise la convolution adaptative dans le contexte des GAN avec un bloc appelé AdaConvBlock qui remplace la convolution ordinaire, ce qui donne plus de contexte local par poids de noyau de sorte qu'il peut générer des objets localement flexibles.
Nous réalisons des expériences à grande échelle pour montrer qu'une simple variante en ligne de la distillation peut nous aider à étendre la formation de réseaux neuronaux distribués à un plus grand nombre de machines.
Proposition d'une méthode permettant d'étendre la formation distribuée au-delà des limites actuelles de la descente de gradient stochastique en mini-batch.
Proposition d'une méthode de distillation en ligne appelée co-distillation, appliquée à l'échelle, où deux modèles différents sont entraînés à faire correspondre les prédictions de l'autre modèle en plus de minimiser sa propre perte.
Une technique de distillation en ligne est introduite pour accélérer les algorithmes traditionnels de formation de réseaux neuronaux distribués à grande échelle.
Nous présentons un algorithme pour accélérer l'entraînement des SVM sur des ensembles de données massifs en construisant des représentations compactes qui fournissent une inférence efficace et approximative.
étudie l'approche du coreset pour les SVM et vise à échantillonner un petit ensemble de points pondérés de telle sorte que la fonction de perte sur les points se rapproche de manière prouvable de celle sur l'ensemble du jeu de données.
L'article propose une construction de Coreset basée sur l'échantillonnage d'importance pour représenter de grandes données d'entraînement pour les SVM.
Nous prouvons un taux de convergence non convexe pour la méthode du gradient stochastique du signe. L'algorithme a des liens avec des algorithmes comme Adam et Rprop, ainsi que des schémas de quantification du gradient utilisés dans l'apprentissage automatique distribué.
Fourni une analyse de convergence de l'algorithme Sign SGD pour les cas non-covexes
L'article explore un algorithme qui utilise le signe des gradients au lieu des gradients réels pour former des modèles profonds.
Nous présentons un intergiciel transparent pour l'accélération des réseaux neuronaux, avec son propre moteur de compilation, qui atteint une vitesse de 11,8 fois sur les CPU et de 2,3 fois sur les GPU.
Cet article propose une couche intergicielle transparente pour l'accélération des réseaux neuronaux et obtient quelques résultats d'accélération sur des architectures de base de CPU et de GPU.
Nous proposons une convolution conique et la 2D-DFT pour encoder l'équivariance de rotation dans un réseau neuronal.
Dans le contexte de la classification d'images, l'article propose une architecture de réseau neuronal convolutif avec des cartes de caractéristiques équivoques en rotation qui sont finalement rendues invariantes en rotation en utilisant la magnitude de la transformée de Fourier discrète (DFT) 2D.
Les auteurs proposent un réseau neuronal invariant en rotation en combinant convolution conique et 2D-DFT.
Nous présentons une nouvelle structure de type "feed-forward" pour générer des métamères visuels.
propose un modèle NeuroFovea pour la génération de métamères de point de fixation en utilisant une approche de transfert de style via une architecture de style Encoder-Decoder.
Une analyse du métamérisme et un modèle capable de produire rapidement des métamères utiles pour la psychophysique expérimentale et d'autres domaines.
L'article propose une mÃ©thode rapide pour gÃ©nÃ©rer des mÃ©tamÃ?res visuels â€" des images physiquement diffÃ©rentes qui ne peuvent pas Ãªtre distinguÃ©es d'un original â€" par le biais d'un transfert de style fovÃ©ral, rapide et arbitraire.
Nous montrons que l'entraînement des réseaux relu feedforward avec un régularisateur faible donne une marge maximale et nous analysons les implications de ce résultat.
étudie la théorie de la marge pour les ensembles neuronaux et montre que la marge maximale augmente de façon monotone avec la taille du réseau
Cet article étudie le biais implicite des minimiseurs d'une perte d'entropie croisée régularisée d'un réseau à deux couches avec des activations ReLU, en obtenant une limite supérieure de généralisation qui n'augmente pas avec la taille du réseau.
Une architecture distribuée pour l'apprentissage par renforcement profond à l'échelle, utilisant la génération de données en parallèle pour améliorer l'état de l'art sur le benchmark Arcade Learning Environment en une fraction du temps d'entraînement de l'horloge murale des approches précédentes.
Examine un système Deep RL distirbué dans lequel les expériences, plutôt que les gradients, sont partagées entre les travaux parallèles et l'apprenant centralisé.
Une approche parallèle de l'apprentissage du DQN, basée sur l'idée que plusieurs acteurs collectent des données en parallèle tandis qu'un seul apprenant entraîne le modèle à partir d'expériences échantillonnées dans la mémoire centrale.
Cet article propose une architecture distribuée pour l'apprentissage par renforcement profond à l'échelle, en se concentrant sur l'ajout de la parallélisation dans l'algorithme de l'acteur dans le cadre de la répétition d'expérience priorisée.
Architectures neuronales fournissant des représentations de signaux observés de manière irrégulière qui permettent de manière prouvée la reconstruction du signal.
prouve que les réseaux neuronaux convolutifs avec la fonction d'activation Leaky ReLU sont des cadres non linéaires, avec des résultats similaires pour les séries chronologiques non uniformément échantillonnées
Cet article considère les réseaux neuronaux sur les séries temporelles et montre que les premiers filtres convolutifs peuvent être choisis pour représenter une transformée en ondelettes discrète.
Mécanismes d'attention basés sur les phrases pour attribuer l'attention sur les phrases, en réalisant des alignements d'attention de jeton à phrase, de phrase à jeton, de phrase à phrase, en plus des attentions existantes de jeton à jeton.
L'article présente un mécanisme d'attention qui calcule une somme pondérée non seulement sur des tokens uniques mais aussi sur des ngrammes (phrases).
Les réseaux profonds sont plus susceptibles de se tromper en toute confiance lorsqu'ils sont testés sur des données inattendues. Nous proposons une méthodologie expérimentale pour étudier le problème, et deux méthodes pour réduire les erreurs de confiance sur des distributions d'entrée inconnues.
Propose deux idées pour réduire les prédictions erronées trop confiantes : la "distillation G" de l'ensemble d'am avec des données supplémentaires non supervisées et la réduction de la confiance en la nouveauté à l'aide d'un détecteur de nouveauté.
Les auteurs proposent deux méthodes pour estimer la confiance de la classification sur de nouvelles distributions de données non vues. La première idée consiste à utiliser des méthodes d'ensemble comme approche de base pour aider à identifier les cas incertains, puis à utiliser des méthodes de distillation pour réduire l'ensemble en un seul modèle imitant le comportement de l'ensemble. La deuxième idée est d'utiliser un classificateur détecteur de nouveauté et de pondérer la sortie du réseau par le score de nouveauté.
Nous décrivons un algorithme d'optimisation pratique pour les réseaux neuronaux profonds qui fonctionne plus rapidement et génère de meilleurs modèles par rapport aux algorithmes largement utilisés.
propose un nouvel algorithme qui prétend utiliser le hessien de manière implicite et qui s'inspire des séries de puissance.
Présente un nouvel algorithme d'ordre 2 qui utilise implicitement l'information sur la courbure et montre l'intuition derrière les schémas d'approximation dans les algorithmes et valide l'heuristique dans diverses expériences.
Nous introduisons l'approximation par largeur de bit fractionnelle et montrons qu'elle présente des avantages significatifs.
Propose une méthode pour faire varier le degré de quantification dans un réseau neuronal pendant la phase de propagation vers l'avant.
Maintenir la précision d'un mot net de 2bits tout en utilisant des poids de moins de 2bits
Le remplacement de la moyenne est une méthode efficace pour améliorer la perte après élagage et les fonctions de notation basées sur l'approximation de Taylor fonctionnent mieux avec les valeurs absolues. 
Propose une amélioration simple des méthodes d'élagage des unités utilisant le "remplacement moyen".
Cet article présente une stratégie d'élagage par remplacement de la moyenne et utilise le développement de Taylor à valeur absolue comme fonction de notation pour l'élagage.
 Évitez l'effondrement postérieur en limitant le taux.
Présente une approche visant à empêcher l'effondrement de la postériorité dans les VAE en limitant la famille de l'approximation variationnelle de la postériorité.
Cet article introduit une contrainte sur la famille des a posteriori variationnels de sorte que le terme KL peut être contrôlé pour lutter contre l'effondrement des a posteriori dans les modèles génératifs profonds tels que les VAE.
Nous avons développé un momentum adaptatif par lot qui peut atteindre une perte plus faible par rapport aux méthodes par mini-batch après avoir analysé les mêmes époques de données, et il est plus robuste contre une grande taille de pas.
Cet article aborde le problème du réglage automatique de la taille des lots pendant la formation d'apprentissage profond, et prétend étendre le SGD adaptatif par lot à la dynamique adaptative et adopter les algorithmes aux problèmes de réseaux neuronaux complexes.
L'article propose de généraliser un algorithme qui exécute le SGD avec des tailles de lot adaptatives en ajoutant un momentum à la fonction d'utilité.
Nous utilisons les méta-gradients pour attaquer la procédure de formation des réseaux neuronaux profonds pour les graphes.
Etudie le problème de l'apprentissage d'un meilleur paramètre de graphe empoisonné qui peut maximiser la perte d'un réseau neuronal de graphe. 
Un algorithme pour modifier la structure du graphe en ajoutant/supprimant des arêtes de manière à dégrader la performance globale de la classification des nœuds, et l'idée d'utiliser le méta-apprentissage pour résoudre le problème d'optimisation à deux niveaux.
Nous montrons que les modèles structurés modulaires sont les meilleurs en termes de généralisation systématique et que leurs versions de bout en bout ne généralisent pas aussi bien.
Cet article évalue la généralisation systémique entre les réseaux neuronaux modulaires et les modèles génériques par l'introduction d'un nouvel ensemble de données de raisonnement spatial.
Une évaluation empirique ciblée de la généralisation dans les modèles de raisonnement visuel, axée sur le problème de la reconnaissance de triples (objet, relation, objet) dans des scènes synthétiques comportant des lettres et des chiffres.
Les modèles relationnels prospectifs pour l'apprentissage multi-agents font des prédictions précises du comportement futur des agents, ils produisent des représentations interprétables et peuvent être utilisés à l'intérieur des agents.
Une façon de réduire la variance dans l'apprentissage sans modèle en ayant un modèle explicite, qui utilise une architecture de type réseau de convoyage de graphes, des actions que les autres agents prendront. 
Prédire le comportement des multi-agents à l'aide d'un modèle relationnel prospectif avec une composante récurrente, en surpassant deux lignes de base et deux ablations.
La solution normalisée de la descente du gradient sur la régression logistique (ou une perte décroissante similaire) converge lentement vers la solution de la marge max L2 sur des données séparables.
L'article offre une preuve formelle que la descente du gradient sur la perte logistique converge très lentement vers la solution SVM dure dans le cas où les données sont linéairement séparables. 
Cet article se concentre sur la caractérisation du comportement de la minimisation de la perte logarithmique sur des données linéairement séparables, et montre que la perte logarithmique, minimisée par descente de gradient, conduit à la convergence vers la solution de marge maximale.
En s'appuyant sur des travaux antérieurs sur la généralisation des domaines, nous espérons produire un classificateur qui se généralisera à des domaines inconnus, même si les identifiants de domaine ne sont pas disponibles pendant la formation.
Une approche de généralisation de domaine pour révéler l'information sémantique basée sur un schéma de projection linéaire des couches de sortie du CNN et du NGLCM.
L'article propose une approche non supervisée pour identifier les caractéristiques d'image qui ne sont pas significatives pour les tâches de classification d'image.
Nous proposons une méthode d'apprentissage du camouflage physique des véhicules pour attaquer de manière adversative les détecteurs d'objets dans la nature. Nous trouvons notre camouflage efficace et transférable.
Les auteurs étudient le problème de l'apprentissage d'un motif de camouflage qui, appliqué à un véhicule simulé, empêchera un détecteur d'objets de le détecter.
Cet article vise l'apprentissage contradictoire pour la détection des voitures interférentes en apprenant des motifs de camouflage.
Nous combinons des arbres de décision différentiables avec des autoencodeurs variationnels supervisés pour améliorer l'interprétabilité de la classification. 
Cet article propose un modèle hybride d'un autoencodeur variationnel composé d'un arbre de décision différentiable et d'un schéma d'apprentissage associé. Les expériences démontrent les performances de classification de l'arbre, les performances de la vraisemblance logarithmique négative et l'interprétabilité de l'espace latent.
L'article tente de construire un classificateur interprétable et précis en combinant un VAE supervisé et un arbre de décision différentiable.
Une approche pratique et garantie pour l'entraînement de classifieurs efficaces en présence de décalages d'étiquettes entre les ensembles de données source et cible.
Les auteurs proposent un nouvel algorithme pour améliorer la stabilité de la procédure d'estimation de la pondération de l'importance des classes avec une procédure en deux étapes.
Les auteurs considèrent le problème de l'apprentissage en cas de décalage d'étiquettes, où les proportions d'étiquettes diffèrent alors que les conditionnels sont égaux, et proposent un estimateur amélioré avec régularisation.
Approche visant à améliorer la précision de la classification sur les classes de la queue.
L'objectif principal de cet article est d'apprendre un classificateur ConvNet qui est plus performant pour les classes situées dans la queue de la distribution d'occurrence des classes.
Proposition d'un cadre bayésien avec un modèle de mélange gaussien pour résoudre un problème dans les applications de classification, à savoir que le nombre de données d'entraînement provenant de différentes classes n'est pas équilibré.
Nous présentons une méthode permettant de synthétiser des états d'intérêt pour les agents d'apprentissage par renforcement afin d'analyser leur comportement. 
Cet article propose un modèle génératif d'observations visuelles en RL qui est capable de générer des observations d'intérêt.
Une approche pour visualiser les états d'intérêt qui implique un auto-codeur variationnel qui apprend à reconstruire l'espace d'état et une étape d'optimisation qui trouve des paramètres de conditionnement pour générer des images synthétiques.
Un réseau neuronal profond d'abstention formé avec une nouvelle fonction de perte qui apprend des représentations pour savoir quand s'abstenir permettant un apprentissage robuste en présence de différents types de bruit.
Une nouvelle fonction de perte pour l'entraînement d'un réseau neuronal profond qui peut s'abstenir, avec des performances examinées sous différents angles : en existence de bruit structuré, en existence de bruit non structuré, et détection de monde ouvert.
Ce manuscrit introduit des classificateurs profonds d'abstention qui modifient la perte d'entropie croisée multiclasse avec une perte d'abstention, qui est ensuite appliquée aux tâches de classification d'images perturbées.
Une technique de régularisation pour l'apprentissage TD qui évite la sur-généralisation temporelle, en particulier dans les réseaux profonds.
Une variation de l'apprentissage par différence temporelle pour le cas de l'approximation de fonction qui tente de résoudre le problème de la sur-généralisation à travers les états temporellement successifs.
L'article présente HR-TD, une variante de l'algorithme TD(0), destinée à améliorer le problème de sur-généralisation dans les TD classiques.
Nous présentons un nouveau noyau CNN pour les grilles non structurées pour les signaux sphériques, et montrons un gain significatif de précision et d'efficacité des paramètres sur des tâches telles que la classification 3D et la segmentation d'images omnidirectionnelles.
Une méthode efficace permettant l'apprentissage profond sur des données sphériques qui atteint des chiffres compétitifs/de pointe avec beaucoup moins de paramètres que les approches populaires.
L'article propose un nouveau noyau convolutionnel pour le CNN sur les grilles non structurées et formule la convolution par une combinaison linéaire d'opérateurs différentiels.
Dans les tâches de prédiction visuelle, laisser votre modèle prédictif choisir les moments à prédire a deux effets : (i) améliorer la qualité de la prédiction, et (ii) conduire à des prédictions sémantiquement cohérentes "d'état de goulot d'étranglement", qui sont utiles pour la planification.
Procédé sur la prédiction d'images dans une vidéo, l'approche comprenant que la prédiction de la cible est flottante, résolue par un minimum sur l'erreur de prédiction.
Reformule la tâche de prédiction/interpolation vidéo de manière à ce qu'un prédicteur ne soit pas obligé de générer des images à des intervalles de temps fixes, mais qu'il soit entraîné à générer des images qui se produisent à n'importe quel moment dans le futur.
Nous avons utilisé un LSTM pour détecter quand un smartphone entre dans un bâtiment. Ensuite, nous prédisons le niveau de l'étage de l'appareil en utilisant les données des capteurs à bord du smartphone.
L'article présente un système permettant d'estimer le niveau d'un étage par le biais des données des capteurs de leur appareil mobile en utilisant un LSTM et les changements de la pression barométrique.
Proposition d'une méthode en deux étapes pour déterminer à quel étage se trouve un téléphone mobile à l'intérieur d'un grand immeuble.
Combiner la représentation des objectifs du langage avec des reconstitutions d'expériences a posteriori.
Cet article considère l'hypothèse implicite dans le retour d'expérience a posteriori, à savoir qu'il est possible d'accéder à une correspondance entre les états et les objectifs, et propose une représentation des objectifs en langage naturel.
Cette soumission utilise le cadre Hindsight Experience Replay avec des objectifs en langage naturel pour améliorer l'efficacité de l'échantillonnage des modèles de suivi des instructions.
Nous proposons un codebook conjoint et un schéma de factorisation pour améliorer la mise en commun du second ordre.
Cet article présente un moyen de combiner les représentations factorisées du second ordre existantes avec une affectation difficile de type codebook.
Proposition d'une nouvelle représentation bilinéaire basée sur un modèle de livre de codes, et une formulation efficace dans laquelle les projections basées sur le livre de codes sont factorisées via une projection partagée pour réduire davantage la taille des paramètres.
Nous proposons et appliquons une méthodologie de méta-apprentissage basée sur la supervision faible, pour combiner l'apprentissage semi-supervisé et d'ensemble sur la tâche d'extraction de relations biomédicales.
Une méthode semi-supervisée pour la classification des relations, qui forme plusieurs apprenants de base en utilisant un petit ensemble de données étiquetées et applique certains d'entre eux pour annoter des exemples non étiquetés pour l'apprentissage semi-supervisé.
Cet article aborde le problème de la génération de données d'entraînement pour l'extraction de relations biologiques, et utilise les prédictions de données étiquetées par des classificateurs faibles comme données d'entraînement supplémentaires pour un algorithme de méta apprentissage.
Cet article propose une combinaison d'apprentissage semi-supervisé et d'apprentissage d'ensemble pour l'extraction d'informations. Des expériences ont été menées sur une tâche d'extraction de relations biomédicales.
Une classe de réseaux qui génèrent des modèles simples à la volée (appelés explications) qui agissent comme un régularisateur et permettent des diagnostics et une interprétabilité cohérents des modèles.
Les auteurs affirment que l'art antérieur intègre directement les réseaux neuronaux dans les modèles graphiques en tant que composants, ce qui rend les modèles ininterprétables.
Proposition d'une combinaison de réseaux neuronaux et de modèles graphiques en utilisant un réseau neuronal profond pour prédire les paramètres d'un modèle graphique.
Nous proposons un algorithme d'apprentissage par imitation sans modèle qui est capable de réduire le nombre d'interactions avec l'environnement par rapport à l'algorithme d'apprentissage par imitation le plus avancé, à savoir GAIL.
Propose d'étendre l'algorithme déterministe de gradient de politique pour apprendre à partir de démonstrations, tout en le combinant avec un type d'estimation de densité de l'expert.
Cet article considère le problème de l'apprentissage par imitation sans modèle et propose une extension de l'algorithme génératif d'apprentissage par imitation adversariale en remplaçant la politique stochastique de l'apprenant par une politique déterministe.
L'article combine l'IRL, la formation contradictoire et les idées des gradients de politique déterministe dans le but de réduire la complexité de l'échantillon.
Graphique CNN à faible complexité de calcul (sans approximation) avec une meilleure précision de classification.
Propose une nouvelle approche CNN pour la classification des graphes en utilisant un filtre basé sur des marches sortantes de longueur croissante pour incorporer des informations provenant de sommets plus éloignés en une étape de propagation.
Proposition d'une nouvelle architecture de réseau neuronal pour la classification semi-supervisée de graphes, basée sur des filtres polynomiaux de graphes et leur utilisation sur des couches successives de réseau neuronal avec des fonctions d'activation ReLU.
L'article présente le GCN adaptatif à la topologie pour généraliser les réseaux convolutifs aux données structurées par des graphes.
Nous montrons que l'oubli catastrophique se produit dans ce qui est considéré comme une tâche unique et nous constatons que les exemples qui ne sont pas sujets à l'oubli peuvent être retirés de l'ensemble d'entraînement sans perte de généralisation.
Étudie le comportement d'oubli des exemples de formation pendant le SGD, et montre qu'il existe des "exemples de soutien" dans la formation des réseaux neuronaux à travers différentes architectures de réseau.
Cet article analyse la mesure dans laquelle les réseaux apprennent à classer correctement des exemples spécifiques, puis oublient ces exemples au cours de la formation.
L'article étudie si certains exemples dans la formation des réseaux neuronaux sont plus difficiles à apprendre que d'autres. Ces exemples sont oubliés et réappris plusieurs fois au cours de l'apprentissage.
Une approche non supervisée pour apprendre des représentations démêlées d'objets entièrement à partir de vidéos monoculaires non étiquetées.
Conçoit une représentation des caractéristiques à partir de séquences vidéo capturées dans une scène depuis différents points de vue.
Proposition d'une méthode d'apprentissage de représentation non supervisée pour les entrées visuelles qui incorpore une approche d'apprentissage métrique rapprochant les paires de parcelles d'images les plus proches dans l'espace d'intégration tout en écartant les autres paires.
Cet article explore l'apprentissage auto-supervisé des représentations d'objets, avec l'idée principale d'encourager les objets ayant des caractéristiques similaires à être davantage â€˜attraitsâ€™ les uns des autres.
Nous entraînons avec des récompenses de vecteurs alignés sur l'état un agent prédisant les changements d'état à partir de distributions d'actions, en utilisant une nouvelle technique d'apprentissage par renforcement inspirée de la régression quantile.
Présente un algorithme qui vise à accélérer l'apprentissage par renforcement dans les situations où la récompense est alignée avec l'espace d'état. 
Cet article traite de la RL dans l'espace d'action continu, en utilisant une politique reparamétrée et un nouvel objectif de formation basé sur les vecteurs.
Ce travail propose de mélanger la RL distributionnelle avec un réseau chargé de modéliser l'évolution du monde en termes de quantiles, en revendiquant des améliorations de l'efficacité de l'échantillon.
Nous proposons la mise à jour arrière épisodique, un nouvel algorithme d'apprentissage par renforcement profond qui échantillonne les transitions épisode par épisode et met à jour les valeurs de manière récursive et à rebours pour obtenir un apprentissage rapide et stable.
Propose un nouveau DQN où les cibles sont calculées sur un épisode complet par une mise à jour en arrière (de la fin au début) pour une propagation plus rapide des récompenses à la fin de l'épisode.
Les auteurs proposent de modifier l'algorithme DQN en appliquant l'opérateur max de Bellman de manière récursive sur une trajectoire avec une certaine décroissance pour éviter l'accumulation d'erreurs avec le max imbriqué.
Dans les réseaux à Q profond, la mise à jour des valeurs de Q commence à la fin de l'épisode afin de faciliter la propagation rapide des récompenses le long de l'épisode.
Dans ce travail, nous présentons une nouvelle architecture de réseau neuronal profond siamois capable d'apprendre efficacement des données en présence de multiples événements indésirables.
Cet article introduit les réseaux neuronaux siamois dans le cadre des risques concurrents en optimisant l'indice c directement.
Les auteurs abordent les problèmes d'estimation du risque dans un contexte d'analyse de survie avec des risques concurrents et proposent d'optimiser directement l'indice de discrimination dépendant du temps en utilisant un réseau de survie siamois.
Nous présentons un modèle de réseau de pointeurs basé sur le type ainsi qu'une méthode de perte basée sur la valeur pour entraîner efficacement un modèle neuronal afin de traduire le langage naturel en SQL.
L'article prétend développer une nouvelle méthode pour convertir les requêtes en langage naturel en SQL en utilisant une grammaire pour guider le décodage et en utilisant une nouvelle fonction de perte pour le mécanisme de pointeur/copie.
Un estimateur de gradient sans biais et à faible variance pour les modèles de variables latentes discrètes.
Propose une nouvelle technique de réduction de la variance à utiliser lors du calcul d'un gradient de perte attendue où l'attente est par rapport à des variables aléatoires binaires indépendantes.
Un algorithme combinant Rao-Blackwellization et nombres aléatoires communs pour réduire la variance de l'estimateur du gradient de la fonction de score dans le cas particulier des réseaux binaires stochastiques.
Un estimateur sans biais et à faible variance augment-REINFORCE-merge (ARM) pour le calcul et la rétropropagation des gradients dans les réseaux neuronaux binaires.
Nous prouvons que la SGD locale parallèle atteint une vitesse linéaire avec beaucoup moins de communication que la SGD parallèle par mini-lots.
Fournit une preuve de convergence pour le SGD local, et prouve que le SGD local peut fournir les mêmes gains de vitesse que le minibatch, mais peut être capable de communiquer beaucoup moins.
Cet article présente une analyse de la SGD locale et des limites sur la fréquence à laquelle les estimateurs obtenus en exécutant la SGD doivent être moyennés afin de produire des accélérations de parallélisation linéaires.
Les auteurs analysent l'algorithme SGD local, où $K$ chaînes parallèles de SGD sont exécutées, et les itérés sont occasionnellement synchronisés entre les machines en faisant la moyenne
Perception compacte du processus dynamique
Etudie le problème de la représentation compacte du modèle d'un système dynamique complexe tout en préservant l'information en utilisant une méthode de goulot d'information.
Cet article étudie la dynamique linéaire gaussienne et propose un algorithme pour calculer la hiérarchie des goulets d'information (IBH).
RNN dense qui possède des connexions complètes de chaque état caché à plusieurs états cachés précédents de toutes les couches directement.
Propose une nouvelle architecture RNN qui modélise mieux les dépendances à long terme, peut apprendre la représentation multi-échelle des données séquentielles et contourne le problème des gradients en utilisant des unités de déclenchement paramétrées.
Cet article propose une architecture RNN dense entièrement connectée avec des connexions gated à chaque couche et des connexions aux couches précédentes, et ses résultats sur la tâche de modélisation au niveau du personnage de la PTB.
Les SD-GAN démêlent les codes latents en fonction des points communs connus dans un ensemble de données (par exemple, des photographies représentant la même personne).
Cet article étudie le problème de la génération d'images contrôlées et propose un algorithme qui produit une paire d'images ayant la même identité.
Cet article propose le SD-GAN, une méthode d'entraînement des GAN permettant de démêler les informations d'identité et de non-identité dans le vecteur latent d'entrée Z.
Nous proposons un cadre d'apprentissage pour les traductions interdomaines qui est exactement cohérent avec les cycles et qui peut être appris par apprentissage contradictoire, par estimation du maximum de vraisemblance ou par une méthode hybride.
Propose AlignFlow, un moyen efficace de mettre en œuvre le principe de cohérence des cycles en utilisant des flux inversibles.
Modèles de flux pour la traduction d'image à image non appariée
Dans un contexte de synthèse de programme où l'entrée est un ensemble d'exemples, nous réduisons le coût en calculant un sous-ensemble d'exemples représentatifs.
Propose une méthode d'identification d'exemples représentatifs pour la synthèse de programmes afin d'augmenter l'évolutivité des solutions de programmation par contraintes existantes.
Une méthode pour choisir un sous-ensemble d'exemples sur lequel exécuter un solveur de contraintes afin de résoudre des problèmes de synthèse de programmes.
Cet article propose une méthode pour accélérer les synthétiseurs de programmes à usage général.
Nous présentons les réseaux relationnels récurrents, un module de réseau neuronal puissant et général pour le raisonnement relationnel, et l'utilisons pour résoudre 96,6 % des Sudokus les plus difficiles et 19/20 des tâches BaBi.
Introduction d'un réseau relationnel récurrent (RRN) qui peut être ajouté à n'importe quel réseau neuronal pour ajouter une capacité de raisonnement relationnel.
Présentation d'un réseau neuronal profond pour la prédiction structurée qui obtient des performances de pointe sur les puzzles Soduku et la tâche BaBi.
Cet article décrit une méthode appelée réseau relationnel pour ajouter une capacité de raisonnement relationnel aux réseaux neuronaux profonds.
Trois classes de prieurs sont tout ce dont vous avez besoin pour entraîner des modèles profonds à partir de données U uniquement, alors que deux ne devraient pas suffire.
Propose un estimateur sans biais qui permet d'entraîner des modèles avec une faible supervision sur deux ensembles de données non étiquetées avec des prieurs de classe connus et discute des propriétés théoriques des estimateurs.
Une méthodologie pour l'entraînement de n'importe quel classificateur binaire à partir de données non étiquetées seulement, et une méthode de minimisation du risque empirique pour deux ensembles de données non étiquetées où les priorités de classe sont données.
L'agrégation des preuves de classe à partir de nombreux petits patchs d'image suffit à résoudre ImageNet, donne des modèles plus interprétables et peut expliquer certains aspects de la prise de décision des DNN populaires.
Cet article propose une architecture de réseau neuronal nouvelle et compacte qui utilise les informations contenues dans les caractéristiques des sacs de mots. L'algorithme proposé n'utilise que les informations du patch de manière indépendante et effectue un vote majoritaire en utilisant des patchs classés de manière indépendante.
 Les méthodes actuelles de mutation somatique ne fonctionnent pas avec les biopsies liquides (c'est-à-dire le séquençage à faible couverture), nous appliquons une architecture CNN à une représentation unique d'une lecture et de son ailgnment, nous montrons une amélioration significative par rapport aux méthodes précédentes dans le cadre de la faible fréquence.
Propose une solution basée sur CNN appelée Kittyhawk pour l'appel de mutation somatique à des fréquences alléliques ultra basses.
Un nouvel algorithme pour détecter les mutations cancéreuses à partir du séquençage de l'ADN libre de cellules qui identifiera le contexte de séquence qui caractérise les erreurs de séquençage des véritables mutations.
Cet article propose un cadre d'apprentissage profond pour prédire les mutations somatiques à des fréquences extrêmement faibles, ce qui se produit lors de la détection de tumeurs à partir d'ADN libre.
L'article présente un nouveau corpus de référence (gold-standard corpus) de littérature scientifique biomédicale annotée manuellement avec des mentions de concepts UMLS.
Détaille la construction d'un ensemble de données annotées manuellement, couvrant des concepts biomédicaux, qui est plus grand et couvert par une ontologie plus vaste que les ensembles de données précédents.
Cet article utilise MedMentions, un modèle semi-Markov de TaggerOne pour la reconnaissance et la liaison de concepts de bout en bout sur un ensemble de résumés Pubmed pour étiqueter les articles avec des concepts/entités biomédicaux.
Nous proposons une méthode de clustering profond où, au lieu d'un centroïde, chaque cluster est représenté par un autoencodeur.
Présente le clustering profond basé sur un mélange d'autoencodeurs, où les points de données sont alloués à un cluster en fonction de l'erreur de représentation si le réseau autoencodeur était utilisé pour le représenter.
Une approche de regroupement profond qui utilise un cadre d'autoencodeur pour apprendre une intégration à faible dimension des données simultanément au regroupement des données à l'aide d'un réseau neuronal profond.
Une méthode de clustering profond qui représente chaque cluster avec différents auto-encodeurs, fonctionne de bout en bout et peut également être utilisée pour regrouper de nouvelles données entrantes sans refaire toute la procédure de clustering.
Nous définissons une nouvelle métrique de probabilité intégrale (IPM de Sobolev) et montrons comment elle peut être utilisée pour l'entraînement de GANs pour la génération de textes et l'apprentissage semi-supervisé.
Suggère un nouveau schéma de régularisation pour les GANs basé sur une norme de Sobolev, mesurant les déviations entre les normes L2 des dérivés.
Les auteurs proposent un autre type de GAN utilisant la configuration typique d'un GAN mais avec une classe de fonctions différente, et produisent une recette pour l'entraînement des GANs avec ce type de classe de fonctions.
L'article propose une pénalité de gradient différente pour les critiques GAN qui oblige la norme quadratique attendue du gradient à être égale à 1.
Nous proposons une nouvelle approche pour entraîner les GAN avec un mélange de générateurs afin de surmonter le problème de l'effondrement des modes.
Aborder le problème de l'effondrement des modes dans les GAN en utilisant une distribution de mélange sous contrainte pour le générateur et un classificateur auxiliaire qui prédit la composante de mélange source.
L'article propose un mélange de générateurs pour entraîner les GAN sans coût de calcul supplémentaire.
Les auteurs présentent l'utilisation du MGAN, qui vise à surmonter le problème de l'effondrement du modèle par des générateurs de mélange, et qui permet d'obtenir des résultats de pointe.
Nous présentons la plateforme BabyAI pour étudier l'efficacité des données de l'apprentissage des langues avec un humain dans la boucle.
Présente une plateforme de recherche avec un robot dans la boucle pour apprendre à exécuter des instructions linguistiques dans lesquelles le langage a des structures de composition.
Présentation d'une plateforme d'apprentissage des langues fondée sur des bases solides, qui remplace tout humain dans la boucle par un enseignant heuristique et utilise une langue synthétique cartographiée dans un monde quadrillé en 2D.
Un antécédent k-means combiné à une régularisation L1 donne des résultats de compression de pointe.
Cet article explore la compression et la liaison des paramètres souples des DNNs/CNNs.
La méthode SVRG échoue dans les problèmes modernes d'apprentissage profond.
Cet article présente une analyse des méthodes de style SVRG, montrant que l'abandon, la norme de lot, l'augmentation des données (culture aléatoire/rotation/translations) ont tendance à augmenter le biais et/ou la variance des mises à jour.
Cet article étudie l'applicabilité de la SVGD aux réseaux neuronaux modernes et montre que l'application naïve de la SVGD échoue généralement.
Une méthode pour appliquer l'apprentissage profond à des surfaces 3D en utilisant leurs descripteurs sphériques et la convolution anisotropique alt-az sur 2-sphères.
Présente un schéma de convolution anisotrope polaire sur une sphère unitaire en remplaçant la translation du filtre par sa rotation.
Cet article explore l'apprentissage profond des formes 3D en utilisant la convolution anisotropique alt-az à 2 sphères.
Entraînement de réseaux binaires/ternaires par reparamétrisation locale avec l'approximation CLT
Entraîne les réseaux de distribution de poids binaires et ternaires en utilisant la rétropropagation pour échantillonner les pré-activations des neurones avec l'astuce de reparamétrisation.
Cet article propose d'utiliser des paramètres stochastiques en combinaison avec l'astuce de reparamétrisation locale pour entraîner des réseaux neuronaux avec des poids binaires ou ternaires, ce qui conduit à des résultats de pointe.
La distillation de complétion optimale (OCD) est une procédure d'entraînement pour l'optimisation des modèles de séquence à séquence basée sur la distance d'édition qui permet d'atteindre l'état de l'art dans les tâches de reconnaissance de la parole de bout en bout.
Approche alternative à l'entraînement des modèles seq2seq utilisant un programme dynamique pour calculer les continuations optimales des préfixes prédits
Un algorithme d'apprentissage pour les modèles auto-régressifs qui ne nécessite pas de pré-entraînement MLE et peut directement optimiser à partir de l'échantillonnage.
L'article examine un défaut des modèles de séquence à séquence entraînés en utilisant l'estimation du maximum de vraisemblance et propose une approche basée sur les distances d'édition et l'utilisation implicite de séquences d'étiquettes données pendant l'entraînement.
un modèle conjoint et une méthode de sparsification du gradient pour l'apprentissage fédéré
Applique le dropout variationnel pour réduire le coût de communication de la formation distribuée des réseaux neuronaux, et fait des expériences sur les jeux de données mnist, cifar10 et svhn. 
Les auteurs proposent un algorithme qui réduit les coûts de communication dans l'apprentissage fédéré en envoyant des gradients épars du dispositif au serveur et inversement.
Combine l'algorithme d'optimisation distribuée avec le dropout variationnel pour sparifier les gradients envoyés au serveur maître par les apprenants locaux.
Nous prouvons une théorie de boosting multiclasse pour les architectures ResNet qui crée simultanément une nouvelle technique de boosting multiclasse et fournit un nouvel algorithme pour les architectures de style ResNet.
Présente un algorithme de type boosting pour la formation de réseaux résiduels profonds, une analyse de convergence pour l'erreur de formation et une analyse de la capacité de généralisation.
Une méthode d'apprentissage pour ResNet utilisant le cadre de boosting qui décompose l'apprentissage de réseaux complexes et utilise moins de coûts de calcul.
Les auteurs proposent le ResNet profond comme algorithme de boosting, et affirment qu'il est plus efficace que la rétropropagation standard de bout en bout.
L'article analyse le paysage d'optimisation des réseaux neuronaux à une couche cachée et conçoit un nouvel objectif qui ne présente pas de minimum local parasite. 
Cet article étudie le problème de l'apprentissage des réseaux neuronaux à une couche cachée, établit un lien entre la perte de population par les moindres carrés et les polynômes d'Hermite, et propose une nouvelle fonction de perte.
Une méthode de factorisation tensorielle pour l'apprentissage d'un réseau neuronal à une couche cachée.
Un corpus ouvert d'extraction d'information et son analyse approfondie
Construit un nouveau corpus pour l'extraction d'informations qui est plus grand que les corpus publics précédents et contient des informations qui n'existent pas dans les corpus actuels.
Présente un ensemble de données de triples open-IE qui ont été collectées à partir de Wikipedia à l'aide d'un système d'extraction récent. 
L'article décrit la création d'un corpus Open IE sur la Wikipédia anglaise par une méthode automatique.
Nous définissons un DSL flexible pour la génération d'architectures RNN qui autorise des RNN de taille et de complexité variables et proposons une fonction de classement qui représente les RNN sous forme de réseaux neuronaux récursifs, en simulant leurs performances pour décider des architectures les plus prometteuses.
Présente une nouvelle méthode pour générer des architectures RNNs en utilisant un langage spécifique au domaine pour deux types de générateurs (aléatoire et basé sur RL) ainsi qu'une fonction de classement et un évaluateur.
Cet article présente la recherche de bonnes architectures de cellules RNN comme un problème d'optimisation en boîte noire où les exemples sont représentés comme un arbre d'opérateurs et notés sur la base de fonctions apprises ou générées par un agent RL.
Cet article étudie une stratégie de méta-apprentissage pour la recherche automatique d'architecture dans le contexte des RNN en utilisant un DSL qui spécifie les opérations récurrentes des RNN.
Nous appliquons la formation et l'inférence avec seulement des nombres entiers de faible largeur dans les DNNs.
Une méthode appelée WAGE qui quantifie tous les opérandes et opérateurs dans un réseau neuronal afin de réduire le nombre de bits pour la représentation dans un réseau.
Les auteurs proposent de discrétiser les poids, les activations, les gradients et les erreurs des réseaux neuronaux, tant au moment de la formation que du test.
Dans cet article, nous développons des méthodes de sparification rapide sans réentraînement qui peuvent être déployées pour la sparification à la volée des CNN dans de nombreux contextes industriels.
Cet article propose des approches pour l'élagage des CNN sans recyclage en introduisant trois schémas pour déterminer les seuils des poids d'élagage.
Cet article décrit une méthode de sparsification des CNN sans réapprentissage.
Nous proposons que la formation avec des ensembles croissants, étape par étape, offre une optimisation pour les réseaux neuronaux.
Les auteurs comparent l'apprentissage par programme à l'apprentissage dans un ordre aléatoire avec des étapes qui ajoutent un nouvel échantillon d'exemples à l'ensemble précédemment construit au hasard.
Cet article étudie l'influence de l'ordre dans le curriculum et l'apprentissage autogéré, et montre que, dans une certaine mesure, l'ordre des instances de formation n'est pas important.
Une méthode de traduction d'image à image qui ajoute à une image le contenu d'une autre, créant ainsi une nouvelle image.
Ce document aborde la tâche du transfert de contenu, avec la noblesse étant sur la perte.
Un ensemble de données pour tester le raisonnement mathématique (et la généralisation algébrique), et des résultats sur les modèles actuels de séquence à séquence.
Présente un nouvel ensemble de données synthétiques pour évaluer la capacité de raisonnement mathématique des modèles de séquence à séquence, et l'utilise pour évaluer plusieurs modèles.
Modèle pour la résolution de problèmes mathématiques de base.
Cet article présente des paramétrages efficaces et économiques des réseaux de neurones convolutifs motivés par des équations différentielles partielles. 
Présente quatre alternatives "à faible coût" à l'opération de convolution standard qui peuvent être utilisées à la place de cette dernière afin de réduire leur complexité de calcul.
Cet article présente des méthodes permettant de réduire le coût de calcul des implémentations CNN, et présente de nouvelles paramétrisations des architectures de type CNN qui limitent le couplage des paramètres.
L'article propose une perspective basée sur les EDP pour comprendre et paramétrer les CNN.
Utiliser la théorie du taux de distorsion pour déterminer dans quelle mesure un modèle de variable latente peut être amélioré.
Aborde les problèmes d'optimisation de l'antériorité dans le modèle de variable latente et la sélection de la fonction de vraisemblance en proposant des critères basés sur une limite inférieure de la log-vraisemblance négative.
Présente un théorème qui donne une limite inférieure à la log-vraisemblance négative de la distorsion de taux pour la modélisation à variables latentes.
Les auteurs soutiennent que la théorie de la distorsion du taux pour la compression avec perte fournit une boîte à outils naturelle pour étudier les modèles de variables latentes propose une limite inférieure.
Nous ignorons les non-linéarités et ne calculons pas les gradients dans le passage en arrière afin d'économiser le calcul et de garantir que les gradients circulent toujours. 
L'auteur a proposé des algorithmes de rétropropagation linéaire pour assurer le flux des gradients pour toutes les parties pendant la rétropropagation.
Comprendre l'auto-codeur discret VQ-VAE de manière systématique en utilisant EM et l'utiliser pour concevoir un modèle de traduction non-autogressif correspondant à une base autorégressive forte.
Cet article introduit une nouvelle façon d'interpréter le VQ-VAE et propose un nouvel algorithme de formation basé sur le regroupement EM doux.
L'article présente un point de vue alternatif sur la procédure de formation pour le VQ-VAE en utilisant l'algorithme EM doux.
Cet article présente un réseau neuronal profond intégrant une fonction de perte en ce qui concerne la distribution optimale de la marge, qui atténue le problème de l'ajustement excessif sur le plan théorique et empirique.
Présente une limite PAC-Bayes pour une perte de marge
Nous cherchons à comprendre les représentations apprises dans les réseaux comprimés par le biais d'un régime expérimental que nous appelons le triage des réseaux profonds.
Compare diverses méthodes d'initialisation et d'entraînement pour transférer les connaissances d'un réseau VGG à un réseau d'étudiants plus petit en remplaçant les blocs de couches par des couches uniques.
Ce document présente cinq méthodes pour effectuer le triage ou la compression des couches de blocs pour les réseaux profonds.
L'article propose une méthode de compression d'un bloc de couches dans un NN qui évalue plusieurs sous-approches différentes.
La preuve empirique d'un nouveau phénomène nécessite de nouveaux éclairages théoriques et est pertinente pour les discussions actives dans la littérature sur le SGD et la généralisation de la compréhension.
L'article traite d'un phénomène selon lequel la formation de réseaux neuronaux dans des contextes très spécifiques peut tirer un grand profit d'un programme comprenant des taux d'apprentissage élevés.
Les auteurs analysent la formation des réseaux résiduels en utilisant de grands taux d'apprentissage cycliques, et démontrent une convergence rapide avec les taux d'apprentissage cycliques et des preuves que les grands taux d'apprentissage agissent comme une régularisation.
Nous proposons une méthode pour la construction de réseaux infiniment larges arbitrairement profonds, sur la base de laquelle nous dérivons un nouveau schéma d'initialisation des poids pour les réseaux finis et démontrons sa performance compétitive.
Propose une approche d'initialisation des poids pour permettre des réseaux infiniment profonds et de largeur infinie, avec des résultats expérimentaux sur de petits ensembles de données.
Propose des réseaux neuronaux profonds de largeur infinie.
Nous avons dérivé des règles d'apprentissage de la plasticité synaptique biologiquement plausibles pour un réseau neuronal récurrent permettant de stocker des représentations de stimulus. 
Modèle de réseau neuronal constitué de neurones connectés de manière récurrente et d'une ou plusieurs redoutes qui vise à conserver une certaine sortie dans le temps.
Cet article présente un mécanisme de mémoire auto-organisée dans un modèle neuronal, et introduit une fonction objective qui minimise les changements dans le signal à mémoriser.
Pour comprendre l'entraînement du GAN, nous définissons une dynamique simple du GAN, et montrons les différences quantitatives entre les mises à jour optimales et de premier ordre dans ce modèle.
Les auteurs étudient l'impact des GAN dans des contextes où, à chaque itération, le discriminateur s'entraîne jusqu'à convergence et le générateur se met à jour avec des étapes de gradient, ou lorsque quelques étapes de gradient sont effectuées pour le discriminateur et le générateur.
Cet article étudie la dynamique de l'apprentissage contradictoire de GANs sur un modèle de mélange gaussien.
Nous proposons une méthode basée sur le gradient pour transférer des connaissances provenant de sources multiples dans différents domaines et tâches.
Cet article propose de combiner les gradients des domaines sources pour faciliter l'apprentissage dans le domaine cible. 
La première formulation bayésienne variationnelle de l'inférence phylogénétique, un problème d'inférence difficile sur des structures dont les composantes discrètes et continues sont entremêlées.
Explore une solution d'inférence approximative au problème de l'inférence bayésienne des arbres phylogénétiques en tirant parti des réseaux bayésiens subsplit récemment proposés et des estimateurs de gradient modernes pour VI.
Propose une approche variationnelle de l'inférence bayésienne postérieure dans les arbres phylogénétiques.
Il s'agit d'une architecture neuronale hybride permettant d'accélérer le modèle autorégressif. 
Il en ressort que pour augmenter la taille du modèle sans augmenter le temps d'inférence pour la prédiction séquentielle, il faut utiliser un modèle qui prédit plusieurs pas de temps à la fois.
Cet article présente HybridNet, un système neuronal de synthèse de la parole et d'autres sons qui combine le modèle WaveNet avec un LSTM dans le but d'offrir un modèle permettant une génération audio plus rapide en temps d'inférence.
Interprétation en identifiant les caractéristiques apprises par le modèle qui servent d'indicateurs pour la tâche d'intérêt. Expliquer les décisions du modèle en mettant en évidence la réponse de ces caractéristiques dans les données de test. Évaluer objectivement les explications à l'aide d'un ensemble de données contrôlé.
Cet article propose une méthode pour produire des explications visuelles pour les sorties des réseaux neuronaux profonds et publie un nouvel ensemble de données synthétiques.
Une méthode pour les réseaux neuronaux profonds qui identifie automatiquement les caractéristiques pertinentes de l'ensemble des classes, soutenant l'interprétation et l'explication sans dépendre d'annotations supplémentaires.
Un cadre pour l'apprentissage efficace de représentations de phrases de haute qualité.
Propose un algorithme plus rapide pour l'apprentissage de représentations de phrases de type SkipThought à partir de corpus de phrases ordonnées, qui remplace le décodeur au niveau des mots par une perte de classification contrastive.
Cet article propose un cadre pour l'apprentissage non supervisé des représentations de phrases en maximisant un modèle de la probabilité des vraies phrases contextuelles par rapport aux phrases candidates aléatoires.
Nous dérivons une pénalité de norme sur la sortie du réseau neuronal du point de vue du goulot d'étranglement de l'information.
Met en avant la pénalité de la norme d'activation, une régularisation de type L_2 sur les activations, en la dérivant du principe du goulot d'étranglement de l'information.
Cet article crée une correspondance entre les pénalités de la norme d'activation et le cadre du goulot d'étranglement de l'information en utilisant le cadre du décrochage variationnel.
Une méthode entièrement non supervisée, pour intégrer naturellement la réduction de la dimensionnalité et le regroupement temporel dans un cadre unique d'apprentissage de bout en bout.
Propose un algorithme qui intègre un auto-codeur avec le regroupement de données de séries temporelles en utilisant une structure de réseau qui convient aux données de séries temporelles.
Un algorithme pour effectuer conjointement une réduction de la dimensionnalité et un regroupement temporel dans un contexte d'apprentissage profond, en utilisant un autoencodeur et un objectif de regroupement.
Les auteurs ont proposé une méthode non supervisée de regroupement de séries temporelles construite avec des réseaux neuronaux profonds et équipée d'un encodeur-décodeur et d'un mode de regroupement pour raccourcir les séries temporelles, extraire les caractéristiques temporelles locales et obtenir les représentations encodées.
Un réseau de neurones à mémoire augmentée qui résout le problème du nombre limité de classes en tirant parti de la hiérarchie des classes dans l'apprentissage supervisé et le méta-apprentissage.
Cet article présente des méthodes pour ajouter un biais inductif à un classificateur par le biais d'une prédiction grossière à fine le long d'une hiérarchie de classes et l'apprentissage d'un classificateur KNN basé sur la mémoire qui garde la trace des instances mal étiquetées pendant l'apprentissage.
Cet article formule le problème de la classification de plusieurs classes à partir d'une perspective d'apprentissage supervisé et d'une perspective de méta-apprentissage.
Un nouveau composant de perte qui force le réseau à apprendre une représentation qui est bien adaptée au regroupement pendant la formation pour une tâche de classification.
Cet article propose deux termes de régularisation basés sur une perte charnière composée sur la divergence KL entre deux arguments d'entrée normalisés par softmax afin d'encourager l'apprentissage de représentations démêlées.
Proposition de deux régularisateurs destinés à rendre les représentations apprises dans l'avant-dernière couche d'un classificateur plus conformes à la structure inhérente des données.
Nous montrons comment obtenir de bonnes représentations du point de vue de la recherche de similiarité.
Etudie l'impact du changement de la partie classification d'image au-dessus du DNN sur la capacité d'indexer les descripteurs avec un algorithme LSH ou kd-tree.
Propose d'utiliser la perte d'entropie croisée softmax pour apprendre un réseau qui tente de réduire les angles entre les entrées et les vecteurs de classe correspondants dans un cadre supervisé en utilisant.
Nous présentons une technique qui permet l'apprentissage de réseaux neuronaux quantifiés basé sur le gradient.
Propose une méthode unifiée et générale d'entraînement des réseaux neuronaux avec des poids et des activations synaptiques quantifiés à précision réduite.
Une nouvelle approche de la quantification des activations qui est à la pointe de l'art ou compétitive sur plusieurs problèmes d'images réelles.
Procédé d'apprentissage de réseaux neuronaux avec des poids et des activations quantifiés par quantification stochastique des valeurs et remplacement de la distribution catégorielle résultante par une relaxation continue.
Nous montrons que le problème d'effondrement des modes dans les GANs peut s'expliquer par un manque de partage d'informations entre les observations d'un lot d'entraînement, et nous proposons un cadre basé sur la distribution pour le partage global d'informations entre les gradients qui conduit à un entraînement contradictoire plus stable et plus efficace.
Propose de remplacer les discriminateurs à échantillon unique dans la formation adversariale par des discriminateurs qui opèrent explicitement sur des distributions d'exemples.
Théorie sur les tests à deux échantillons et la MMD et comment les intégrer avantageusement dans le cadre du GAN.
Nous avons conçu un cadre de bout en bout utilisant le modèle de séquence à séquence pour effectuer la normalisation des noms chimiques.
Standardise les noms non systématiques dans l'extraction d'informations chimiques en créant un corpus parallèle de noms non systématiques et systématiques et en construisant un modèle seq2seq.
Ce travail présente une méthode permettant de traduire les noms non systématiques des composés chimiques en leurs équivalents systématiques en utilisant une combinaison de mécanismes
Le SGD est orienté au début de la formation vers une région dans laquelle son pas est trop grand par rapport à la courbure, ce qui a un impact sur le reste de la formation. 
Analyse la relation entre la convergence/généralisation et la mise à jour sur les plus grands vecteurs propres du Hessien des pertes empiriques des DNNs.
Ce travail étudie la relation entre la taille de l'étape SGD et la courbure de la surface de perte.
Nous présentons un nouvel algorithme d'apprentissage par renforcement, qui prédit des actions multiples et en tire des échantillons.
Ce travail introduit un mélange uniforme de politiques déterministes, et constate que cette paramétrisation des politiques stochastiques surpasse DDPG sur plusieurs repères de gymnastique OpenAI.
Les auteurs étudient une méthode permettant d'améliorer les performances des réseaux formés avec DDPG, et montrent une amélioration des performances sur un grand nombre d'environnements de contrôle continu standard.
Conscients des inconvénients de l'application du dropout original sur DenseNet, nous concevons la méthode de dropout à partir de trois aspects, dont l'idée pourrait également être appliquée à d'autres modèles CNN.
Application de différentes structures et planifications de décrochage binaire dans le but spécifique de régulariser l'architecture DenseNet.
Propose une technique de pré-dropout pour densenet qui implémente le dropout avant la fonction d'activation non-linéaire.
Guider les modèles profonds conscients des relations vers un meilleur apprentissage avec des connaissances humaines.
Ce travail propose une variante du réseau de colonnes basée sur l'injection de conseils humains en modifiant les calculs dans le réseau.
Une méthode pour incorporer des conseils humains à l'apprentissage profond en étendant Column Network, un réseau neuronal à graphes pour la classification collective.
Les récents succès des réseaux de neurones binaires peuvent être compris à partir de la géométrie des vecteurs binaires de haute dimension.
Étudie numériquement et théoriquement les raisons du succès empirique des réseaux neuronaux binarisés.
Ce document analyse l'efficacité des réseaux neuronaux binaires et explique pourquoi la binarisation permet de préserver les performances du modèle.
Après avoir prouvé qu'un neurone agit comme un résolveur de problème inverse pour la superrésolution et qu'un réseau de neurones est garanti pour fournir une solution, nous avons proposé une architecture de réseau double qui est plus rapide que l'état de l'art.
Discute de l'utilisation des réseaux neuronaux pour la super-résolution.
Une nouvelle architecture pour la résolution de tâches de super-résolution d'images, et une analyse visant à établir une connexion entre les CNN pour la résolution de super-résolution et la résolution de problèmes inverses régularisés de faible densité.
Modèle dynamique qui apprend des stratégies de division et de conquête par supervision faible.
Propose d'ajouter un nouveau biais inductif à l'architecture des réseaux neuronaux en utilisant une stratégie de division et de conquête.
Cet article étudie les problèmes qui peuvent être résolus à l'aide d'une approche de programmation dynamique, et propose une architecture de réseau neuronal pour résoudre ces problèmes qui battent les lignes de base de séquence à séquence.
L'article propose une architecture de réseau unique qui peut apprendre des stratégies de division et de conquête pour résoudre des tâches algorithmiques.
un gradient de type Rep pour les distributions continues/discrètes non reparamétrisables ; généralisé ensuite aux modèles probabilistes profonds, donnant lieu à une rétro-propagation statistique
Présente un estimateur de gradient pour les objectifs basés sur l'espérance qui est sans biais, a une faible variance et s'applique aux variables aléatoires continues et discrètes.
Une méthode améliorée pour calculer les dérivés de l'espérance, et un nouvel estimateur à gradient de faible variance qui permet l'apprentissage de modèles génératifs dans lesquels les observations ou les variables latentes sont discrètes.
Conçoit un gradient de faible variance pour des distributions associées à des variables aléatoires continues ou discrètes.
Nous montrons que les paysages des paramètres et des coûts des hyperparamètres des NN peuvent être générés sous forme d'états quantiques à l'aide d'un seul circuit quantique et qu'ils peuvent être utilisés pour la formation et la méta-formation.
Décrit une méthode permettant de quantifier un cadre d'apprentissage profond en considérant la forme à deux états d'une sphère de Bloch/qubit et en créant un réseau neuronal binaire quantique.
Cet article propose l'amplification d'amplitude quantique, un nouvel algorithme pour la formation et la sélection de modèles dans les réseaux de neurones binaires.
Propose une nouvelle idée de sortie d'un état quantique qui représente un paysage complet des coûts de tous les paramètres pour un réseau neuronal binaire donné, en construisant un réseau neuronal binaire quantique (QBNN).
Une méthode générale pour l'entraînement d'un classificateur robuste certifié sensible aux coûts contre les perturbations adverses
Calcule et intègre les coûts des attaques adverses dans l'objectif d'optimisation afin d'obtenir un modèle qui soit sensible aux coûts et robuste contre les attaques adverses. 
S'appuie sur les travaux semestriels de Dalvi et al. et étend l'approche de la robustesse certifiable avec une matrice de coûts qui spécifie pour chaque paire de classes source-cible si le modèle doit être robuste aux exemples adverses.
Utilisation de triplets pour apprendre une métrique de comparaison des réponses neuronales et améliorer les performances d'une prothèse.
Les auteurs développent de nouvelles métriques de distance de train d'épis, incluant des réseaux neuronaux et des métriques quadratiques. Ils montrent que ces métriques sont plus performantes que la distance naïve de Hamming et qu'elles capturent implicitement une certaine structure dans le code neuronal.
En vue d'améliorer les prothèses neuronales, les auteurs proposent d'apprendre une métrique entre les réponses neuronales en optimisant une forme quadratique ou un réseau neuronal profond.
Cet article présente une méthode permettant de générer des questions (indices) et des requêtes (suggestions) afin d'aider les utilisateurs à réaliser des cartes mentales.
Présente un outil d'aide à la cartographie de l'esprit grâce à des suggestions de contexte liées aux nœuds existants et à des questions qui développent des branches moins développées.
Cet article présente une approche pour aider les personnes à effectuer des tâches de mindmapping, en concevant une interface et des caractéristiques algorithmiques pour soutenir le mindmapping, et contribue à une étude d'évaluation.
Détection d'échantillons hors distribution à l'aide de statistiques de caractéristiques d'ordre inférieur sans qu'il soit nécessaire de modifier le DNN sous-jacent.
Présente un algorithme pour détecter les échantillons hors distribution en utilisant l'estimation courante de la moyenne et de la variance dans les couches BatchNorm pour construire des représentations de caractéristiques qui sont ensuite introduites dans un classificateur linéaire.
Une approche pour détecter les échantillons hors distribution dans laquelle les auteurs proposent d'utiliser la régression logistique sur les statistiques simples de chaque couche de normalisation de lot du CNN.
L'article suggère d'utiliser les scores Z pour comparer les échantillons d'ID et d'OOD afin d'évaluer ce que les réseaux profonds essaient de faire.
Nous proposons une nouvelle méthode nommée Maximal Divergence Sequential Auto-Encoder qui exploite la représentation de l'AutoEncoder variationnel pour la détection des vulnérabilités du code binaire.
Cet article propose une architecture basée sur un autoencodeur variationnel pour les incorporations de code dans le cadre de la détection des vulnérabilités des logiciels binaires. Les incorporations apprises sont plus efficaces pour distinguer les codes binaires vulnérables et non vulnérables que les codes de base.
Cet article propose un modèle d'extraction automatique de caractéristiques pour la détection de vulnérabilités à l'aide d'une technique d'apprentissage profond. 
Le calcul de l'attention sur la base de la distribution postérieure conduit à une attention plus significative et à de meilleures performances.
Cet article propose un modèle de séquence à séquence où l'attention est traitée comme une variable latente, et dérive de nouvelles procédures d'inférence pour ce modèle, obtenant des améliorations dans les tâches de traduction automatique et de génération d'inflexions morphologiques.
Cet article présente un nouveau modèle d'attention postérieure pour les problèmes seq2seq.
Compression des modèles DNN entraînés en minimisant leur complexité tout en limitant leur perte.
Cet article propose une méthode de compression des réseaux neuronaux profonds sous contraintes de précision.
Cet article présente une méthode d'encodage k-means contrainte par la valeur de perte pour la compression de réseau et développe un algorithme itératif pour l'optimisation du modèle.
Nous développons une technique permettant de visualiser les mécanismes d'attention dans des réseaux neuronaux arbitraires. 
Propose d'apprendre un réseau d'attention latente qui peut aider à visualiser la structure interne d'un réseau neuronal profond.
Les auteurs de cet article proposent un schéma de visualisation de boîtes noires basé sur les données. 
Nous étudions une variété d'algorithmes RL pour la génération de molécules et définissons de nouveaux benchmarks (qui seront publiés sous la forme d'un Gym OpenAI). Nous avons constaté que le PPO et un algorithme MLE d'ascension de colline fonctionnent le mieux.
Examine l'évaluation des modèles pour la génération de molécules en proposant 19 points de référence, en élargissant les petits ensembles de données à un grand ensemble de données normalisées et en explorant comment appliquer les techniques de RL pour la conception de molécules.
Cet article montre que les méthodes RL les plus sophistiquées sont moins efficaces que la simple technique d'ascension de collines, l'OPP faisant exception, lors de la modélisation et de la synthèse de molécules.
La capacité la plus robuste de raisonnement analogique est induite lorsque les réseaux apprennent des analogies en contrastant des structures relationnelles abstraites dans leurs domaines d'entrée.
L'article étudie la capacité d'un réseau neuronal à apprendre l'analogie, en montrant qu'un réseau neuronal simple est capable de résoudre certains problèmes d'analogie.
Cet article décrit une approche pour former des réseaux de neurones pour des tâches de raisonnement analogique, en considérant spécifiquement l'analogie visuelle et les analogies symboliques.
Un modèle de conversation neuronale orienté vers le but par l'autoproduction
Un modèle de jeu autonome pour la génération de dialogues orientés vers les objectifs, visant à renforcer le couplage entre la récompense de la tâche et le modèle de langage.
Cet article décrit une méthode d'amélioration d'un système de dialogue orienté vers un objectif en utilisant le selfplay. 
complétion de requêtes de recherche en temps réel à l'aide de modèles de langage LSTM au niveau des caractères
Cet article présente des méthodes de complétion de requêtes qui incluent la correction des préfixes, ainsi que certains détails d'ingénierie pour répondre à des exigences particulières de latence sur un CPU.
Les auteurs proposent un algorithme pour résoudre le problème de complétion des requêtes avec correction des erreurs. Ils adoptent une modélisation basée sur les RNN au niveau des caractères et optimisent la partie inférence pour atteindre les objectifs en temps réel.
Dans cet article, nous prouvons la convergence vers la criticité de RMSProp (stochastique et déterministe) et de ADAM déterministe pour des objectifs lisses non convexes et nous démontrons une sensibilité intéressante de beta_1 pour ADAM sur les autoencodeurs. 
Cet article présente une analyse de convergence de RMSProp et ADAM dans le cas de fonctions lisses non convexes.
Il est essentiel de concevoir des mécanismes de défense non supervisés contre les attaques adverses pour garantir la généralisation de la défense. 
Cet article présente une méthode de détection des exemples contradictoires dans un contexte de classification par apprentissage profond.
Cet article présente une méthode non supervisée de détection des exemples défavorables de réseaux neuronaux.
Recherche d'une architecture neuronale sans proxy pour l'apprentissage direct d'architectures sur une tâche cible à grande échelle (ImageNet) tout en réduisant le coût au même niveau que la formation normale.
Cet article aborde le problème de la recherche d'architecture, et cherche spécifiquement à le faire sans avoir à s'entraîner sur des tâches "proxy" où le problème est simplifié par une optimisation, une complexité architecturale ou une taille d'ensemble de données plus limitées.
Un nouveau cadre basé sur l'inférence variationnelle pour la détection de hors distribution
Décrit une approche probabiliste pour quantifier l'incertitude dans les tâches de classification DNN qui surpasse les autres méthodes SOTA dans la tâche de détection de la non-répartition.
Un nouveau cadre pour la détection de la non-répartition, basé sur l'inférence variaitonale et une distribution de Dirichlet antérieure, qui présente les résultats de l'état de l'art sur plusieurs ensembles de données.
Une détection de la distribution hors norme via une nouvelle méthode d'approximation de la distribution de confiance de la probabilité de classification utilisant l'inférence variationnelle de la distribution de Dirichlet.
Nous apprenons une représentation de l'espace d'action d'un agent à partir d'observations visuelles pures. Nous utilisons une approche récurrente à variables latentes avec une nouvelle perte de composabilité.
Propose un modèle compositionnel à variables latentes pour apprendre des modèles qui prédisent ce qui va se passer ensuite dans des scénarios où les étiquettes d'action ne sont pas disponibles en abondance.
Une approche basée sur l'IB variationnelle pour apprendre des représentations d'actions directement à partir de vidéos d'actions en cours, ce qui permet d'obtenir une meilleure efficacité des méthodes d'apprentissage ultérieures tout en nécessitant une quantité moindre de vidéos d'étiquettes d'actions.
Cet article propose une approche de la prédiction vidéo qui trouve de manière autonome un espace d'action codant les différences entre les images suivantes.
L'apprentissage par renforcement peut être utilisé pour entraîner les agents à négocier la formation d'équipes dans de nombreux protocoles de négociation.
Cet article étudie la RL profonde multi-agents dans des contextes où tous les agents doivent coopérer pour accomplir une tâche (par exemple, recherche et sauvetage, jeux vidéo multi-joueurs), et utilise des jeux simples de vote pondéré coopératif pour étudier l'efficacité de la RL profonde et pour comparer les solutions trouvées par la RL profonde à une solution équitable.
Une approche d'apprentissage par renforcement pour négocier des coalitions dans des contextes de théorie des jeux coopératifs qui peut être utilisée dans les cas où des simulations d'entraînement illimitées sont disponibles.
Méthodes non supervisées pour la recherche, l'analyse et le contrôle des neurones importants en NMT
Cet article présente des approches non supervisées pour découvrir les neurones importants dans les systèmes neuronaux de traduction automatique et analyse les propriétés linguistiques contrôlées par ces neurones.
Méthodes non supervisées de classement des neurones dans la traduction automatique où les neurones importants sont ainsi identifiés et utilisés pour contrôler la sortie de la TA.
Nous proposons un cadre DRL qui dissocie les connaissances spécifiques à la tâche et à l'environnement.
Les auteurs proposent de décomposer l'apprentissage par renforcement en une fonction PATH et une fonction GOAL.
Une architecture modulaire visant à séparer les connaissances spécifiques à l'environnement et les connaissances spécifiques à la tâche en différents modules, au même titre que l'A3C standard pour un large éventail de tâches.
pix2scene : une approche générative profonde pour la modélisation implicite des propriétés géométriques d'une scène 3D à partir d'images
Exploration de l'explication de scènes avec des surfels dans un modèle de reconnaissance neuronal, et démonstration des résultats sur la reconstruction et la synthèse d'images, et la rotation de formes mentales.
Les auteurs présentent une méthode permettant de créer un modèle de scène 3D à partir d'une image 2D et d'une pose de caméra en utilisant un modèle auto-superficiel.
L'identification des relations qui relient les mots est importante pour diverses tâches de TAL. Nous modélisons la représentation des relations comme un problème d'apprentissage supervisé et apprenons des opérateurs paramétrés qui font correspondre des encastrements de mots pré-entraînés à des représentations de relations.
Cet article présente une nouvelle méthode pour représenter les relations lexicales sous forme de vecteurs en utilisant uniquement des encastrements de mots pré-entraînés et une nouvelle fonction de perte opérant sur des paires de paires de mots.
Une nouvelle solution au problème de la composition des relations lorsque vous disposez déjà d'embeddings de mots/entités pré-entraînés et que vous souhaitez uniquement apprendre à composer des représentations de relations.
Nous proposons d'entraîner deux copies identiques d'un réseau neuronal récurrent (qui partagent des paramètres) avec des masques d'abandon différents tout en minimisant la différence entre leurs prédictions (pré-softmax).
Présente le décrochage fraternel comme une amélioration par rapport au décrochage linéaire par espérance en termes de convergence, et démontre l'utilité du décrochage fraternel sur un certain nombre de tâches et d'ensembles de données.
Apprentissage de la pondération et des déformations des ensembles de données spatio-temporelles pour des approximations très efficaces du comportement des liquides.
Un modèle basé sur un réseau neuronal est utilisé pour interpoler des simulations pour de nouvelles conditions de scène à partir de surfaces implicites 4D densément enregistrées pour une scène structurée.
Cet article présente une approche d'apprentissage profond couplé pour générer des données réalistes de simulation de liquide qui peuvent être utiles pour des applications d'aide à la décision en temps réel.
Cet article présente une approche d'apprentissage profond pour la simulation physique qui combine deux réseaux pour synthétiser des données 4D représentant des simulations physiques 3D.
Nous construisons et évaluons des réseaux neuronaux invariants en fonction de la couleur sur un nouvel ensemble de données réalistes.
Propose une méthode pour rendre les réseaux de neurones pour la reconnaissance d'images invariants en couleur et l'évalue sur le jeu de données cifar 10.
Les auteurs étudient une couche d'entrée modifiée qui permet d'obtenir des réseaux invariants en couleur et montrent que certaines couches d'entrée invariantes en couleur peuvent améliorer la précision pour des images de test dont la distribution des couleurs est différente de celle des images d'apprentissage.
Les auteurs testent un CNN sur des images dont les canaux de couleur ont été modifiés pour être invariants aux permutations, les performances n'étant pas trop dégradées. 
Nous analysons comment le degré de chevauchement entre les champs réceptifs d'un réseau convolutif affecte son pouvoir expressif.
L'article étudie le pouvoir expressif fourni par le "chevauchement" dans les couches de convolution des DNN en considérant des activations linéaires avec mise en commun des produits.
Cet article analyse l'expressivité des circuits arithmétiques convolutifs et montre qu'un nombre exponentiellement grand de ConvAC non chevauchants est nécessaire pour approximer le tenseur de grille d'un ConvAC chevauchant.
Un algorithme théorique pour tester l'optimalité locale et extraire les directions de descente aux points non différentiables des risques empiriques des réseaux ReLU à une couche cachée.
Propose un algorithme pour vérifier si un point donné est un point stationnaire généralisé du second ordre.
Un algorithme théorique, impliquant la résolution de programmes quadratiques convexes et non convexes, pour vérifier l'optimalité locale et échapper aux selles lors de l'entraînement de réseaux ReLU à deux couches.
L'auteur propose une méthode pour vérifier si un point est un point stationnaire ou non et ensuite classer les points stationnaires comme étant des points stationnaires locaux ou de second ordre.
Une nouvelle perte basée sur des négatifs relativement durs qui permet d'obtenir des performances de pointe dans la recherche de légendes d'images.
Apprentissage de l'intégration conjointe de phrases et d'images à l'aide de la perte de triplets appliquée aux négatifs les plus durs au lieu de faire la moyenne de tous les triplets.
Nous utilisons le principe de minimisation alternée pour fournir une nouvelle technique efficace d'entraînement des autoencodeurs profonds.
Cadre de minimisation alternée pour l'entraînement des réseaux auto-codeurs et codeurs-décodeurs
Les auteurs explorent une approche d'optimisation alternée pour l'entraînement des codeurs automatiques, en traitant chaque couche comme un modèle linéaire généralisé, et suggèrent d'utiliser le GD normalisé stochastique comme algorithme de minimisation dans chaque phase.
Apprentissage par transfert pour l'estimation des effets causaux à l'aide de réseaux neuronaux.
Développe des algorithmes pour estimer l'effet de traitement moyen conditionnel par ensemble de données auxiliaires dans différents environnements, à la fois avec et sans apprenant de base.
Les auteurs proposent des méthodes pour aborder une nouvelle tâche d'apprentissage par transfert pour l'estimation de la fonction CATE, et les évaluent en utilisant un cadre synthétique et un ensemble de données expérimentales du monde réel.
Utilisation de la régression par réseaux neuronaux et comparaison des cadres d'apprentissage par transfert pour estimer un effet de traitement moyen conditionnel sous des hypothèses d'ignorabilité stricte
Nous présentons LeMoNADe, une méthode de détection de motifs appris de bout en bout fonctionnant directement sur des vidéos d'imagerie calcique.
Cet article propose un modèle de type VAE pour l'identification de motifs à partir de vidéos d'imagerie calcique, qui s'appuie sur des variables de Bernouli et nécessite une astuce Gumbel-softmax pour l'inférence.
Nous proposons un cadre pour apprendre une bonne politique par apprentissage par imitation à partir d'un ensemble de démonstrations bruyantes via le méta-entraînement d'un évaluateur d'adéquation des démonstrations.
Contribue à un algorithme basé sur MAML pour l'apprentissage par imitation qui détermine automatiquement si les démonstrations fournies sont "appropriées".
Procédé d'apprentissage par imitation à partir d'un ensemble de démonstrations comprenant des comportements inutiles, qui sélectionne les démonstrations utiles en fonction des gains de performance qu'elles procurent au moment du méta-apprentissage.
Nous introduisons des modèles génératifs implicites causaux, qui peuvent échantillonner à partir de distributions conditionnelles et interventionnelles, et proposons également deux nouveaux GAN conditionnels que nous utilisons pour les entraîner.
Procédé de combinaison d'un graphe occasionnel, décrivant la structure de dépendance des étiquettes, avec deux architectures GAN conditionnelles qui génèrent des images en conditionnant l'étiquette binaire.
Les auteurs abordent la question de l'apprentissage d'un modèle causal entre les variables de l'image et l'image elle-même à partir de données d'observation, lorsqu'il existe une structure causale entre les étiquettes de l'image.
Nous prouvons que le NCE est auto-normalisé et le démontrons sur des ensembles de données.
Présente une preuve de l'auto-normalisation de NCE en tant que résultat d'être une approximation de matrice de bas-rang de la matrice de probabilités conditionnelles normalisées.
Cet article examine le problème des modèles auto-normalisants et explique le mécanisme d'auto-normalisation en interprétant les NCE en termes de factorisation matricielle.
L'enrichissement des incorporations de mots avec des informations sur les affects améliore leurs performances dans les tâches de prédiction des sentiments.
Propose d'utiliser des lexiques affectés pour améliorer les incorporations de mots afin de surpasser les standards Word2vec et Glove.
Cet article propose d'intégrer des informations provenant d'une ressource sémantique quantifiant l'affect des mots dans un algorithme d'incorporation de mots basé sur le texte afin que les modèles de langage reflètent mieux les phénomènes sémantiques et pragmatiques.
Cet article introduit des modifications aux fonctions de perte word2vec et GloVe afin d'incorporer des lexiques d'affects pour faciliter l'apprentissage d'incorporations de mots sensibles aux affects.
Pour l'intégration non supervisée et inductive des réseaux, nous proposons une nouvelle approche pour explorer les voisins les plus pertinents et préserver les connaissances acquises précédemment sur les nœuds en utilisant l'architecture de bi-attention et en introduisant un biais global, respectivement.
Ce document propose une extension de GraphSAGE en utilisant une matrice de biais d'intégration globale dans les fonctions d'agrégation locales et une méthode d'échantillonnage des nœuds intéressants.
Nous soutenons que la généralisation de l'intégration des graphes linéaires n'est pas due à la contrainte de dimensionnalité mais plutôt à la petite norme des vecteurs d'intégration.
Les auteurs montrent que l'erreur de généralisation des méthodes d'intégration de graphes linéaires est limitée par la norme des vecteurs d'intégration plutôt que par des contraintes de dimensionnalité.
Les auteurs proposent une limite théorique sur la performance de généralisation de l'apprentissage d'intégrations de graphes et soutiennent que la norme des coordonnées détermine le succès de la représentation apprise.
Mélangez SGD et momentum (ou faites quelque chose de similaire avec Adam) pour un grand profit.
L'article propose des modifications simples à SGD et Adam, appelées variantes QH, qui peuvent récupérer la méthode parentale et une foule d'autres astuces d'optimisation.
Une variante du momentum classique qui prend une moyenne pondérée du momentum et de la mise à jour du gradient, et une évaluation de ses relations avec d'autres schémas d'optimisation basés sur le momentum.
Une nouvelle façon de généraliser les rendements lambda en permettant à l'agent RL de décider du poids qu'il veut accorder à chacun des rendements à n étapes.
Étend l'algorithme A3C avec des retours lambda, et propose une approche pour apprendre les poids des retours.
Les auteurs présentent les retours autodidactiques basés sur la confiance, une méthode d'apprentissage profond RL permettant d'ajuster les poids d'un vecteur d'admissibilité dans l'estimation de valeur de type TD(lambda) afin de favoriser des estimations plus stables de l'état.
nous avons proposé un nouveau modèle de conduite autonome composé d'un module de perception pour voir et penser et d'un module de conduite pour se comporter afin d'acquérir une meilleure capacité de généralisation et d'explication des accidents.
Présente une architecture d'apprentissage multitâche pour l'estimation des cartes de profondeur et de segmentation et la prédiction de la conduite à l'aide d'un module de perception et d'un module de décision de conduite.
Une méthode pour une architecture modifiée de bout en bout qui a une meilleure capacité de généralisation et d'explication, qui est plus robuste à un cadre de test différent, et qui a une sortie de décodeur qui peut aider à déboguer le modèle.
Les auteurs présentent un réseau de neurones convolutifs multi-tâches pour la conduite de bout en bout et fournissent des évaluations avec le simulateur open source CARLA montrant une meilleure performance de généralisation dans de nouvelles conditions de conduite que les lignes de base.
Un cadre générique pour adapter les techniques existantes d'intégration de graphes à de grands graphes.
Cet article propose un cadre d'intégration multi-niveaux à appliquer en plus des méthodes d'intégration de réseau existantes afin de s'adapter plus rapidement aux réseaux à grande échelle.
Les auteurs proposent un cadre en trois étapes pour l'intégration de graphes à grande échelle avec une qualité d'intégration améliorée.
Une nouvelle méthode pour augmenter la résistance des OCSVM contre les attaques d'intégrité ciblées par des transformations non linéaires sélectives des données vers des dimensions inférieures.
Les auteurs proposent une défense contre les attaques contre la sécurité des détecteurs d'anomalies basés sur des SVM à une classe.
Cet article explore la façon dont les projections aléatoires peuvent être utilisées pour rendre l'OCSVM robuste aux données d'entraînement perturbées de façon adversariale.
Apprendre une meilleure représentation des réseaux neuronaux grâce au principe du goulot d'étranglement de l'information.
Propose une méthode d'apprentissage basée sur le cadre du goulot d'étranglement de l'information, où les couches cachées des réseaux profonds compriment l'entrée X tout en conservant suffisamment d'informations pour prédire la sortie Y.
Cet article présente une nouvelle méthode de formation des réseaux neuronaux stochastiques suivant un cadre de pertinence/compression de l'information similaire au goulot d'étranglement de l'information.
Nous proposons un estimateur de l'écart moyen maximal, approprié lorsqu'une distribution cible n'est accessible que par une procédure de sélection d'échantillon biaisée, et nous montrons qu'il peut être utilisé dans un réseau génératif pour corriger ce biais.
Propose un estimateur pondéré par l'importance de la MMD pour estimer la MMD entre des distributions basées sur des échantillons biaisés selon un schéma inconnu connu ou estimé.
Les auteurs abordent le problème du biais de sélection de l'échantillon dans les MMD-GAN et proposent une estimation de la MMD entre deux distributions en utilisant la divergence moyenne maximale pondérée.
Cet article présente une modification de l'objectif utilisé pour entraîner les réseaux génératifs avec un adversaire MMD. 
Utilisation de la régression bayésienne pour estimer la postériorité des fonctions Q et déploiement de l'échantillonnage de Thompson comme stratégie d'exploration ciblée avec un compromis efficace entre l'exploration et l'exploitation.
Les auteurs proposent un nouvel algorithme d'exploration en RL profond où ils appliquent une régression linéaire bayésienne avec les caractéristiques de la dernière couche d'un réseau DQN pour estimer la fonction Q de chaque action.
Les auteurs décrivent comment utiliser les réseaux neuronaux bayésiens avec l'échantillonnage de Thompson pour une exploration efficace dans le cadre du q-learning et proposent une approche qui surpasse les approches d'exploration epsilon-greedy.
PolyCNN n'a besoin d'apprendre qu'un seul filtre convolutif d'ensemencement à chaque couche. Il s'agit d'une variante efficace du CNN traditionnel, avec des performances équivalentes.
Tentatives de réduction du nombre de paramètres du modèle CNN en utilisant la transformation polynomiale des filtres pour créer un gonflement des réponses des filtres.
Les auteurs proposent une architecture de partage de poids pour réduire le nombre de paramètres des réseaux neuronaux convolutionnels avec filtres d'ensemencement.
Dans cet article, nous proposons KL-CPD, un nouveau cadre d'apprentissage à noyau pour la CPD des séries temporelles qui optimise une limite inférieure de la puissance de test via un modèle génératif auxiliaire comme substitut de la distribution anormale. 
Décrit une nouvelle approche pour optimiser le choix du noyau en vue d'une puissance d'essai accrue et montre qu'elle offre des améliorations par rapport aux autres solutions.
Regrouper avant de classer ; utiliser des étiquettes faibles pour améliorer la classification 
Propose l'utilisation d'une fonction de perte basée sur le regroupement à plusieurs niveaux d'un réseau profond ainsi que l'utilisation de la structure hiérarchique de l'espace des étiquettes pour former de meilleures représentations.
Cet article utilise des informations hiérarchiques sur les étiquettes pour imposer des pertes supplémentaires aux représentations intermédiaires dans la formation des réseaux neuronaux.
La minimisation du regret basée sur l'avantage est un nouvel algorithme d'apprentissage par renforcement profond qui est particulièrement efficace pour les tâches partiellement observables, comme la navigation à la première personne dans Doom et Minecraft.
Cet article présente les concepts de minimisation du regret contrefactuel dans le domaine de la RL profonde et un algorithme appelé ARM qui peut mieux gérer l'observabilité partielle.
L'article présente une variante de l'algorithme policy-gradient inspirée de la théorie des jeux et basée sur l'idée de minimisation du regret contre-factuel. Il affirme que cette approche peut traiter le domaine observable partiel mieux que les méthodes standard.
un modèle d'apprentissage multi-tâches profond adaptant la représentation tensorielle en anneau
Une variante de la formulation de l'anneau tensoriel pour l'apprentissage multitâche en partageant certains des noyaux TT pour l'apprentissage de la "tâche commune" tout en apprenant des noyaux TT individuels pour chaque tâche séparée.
Un modèle de régression qui apprend les distributions conditionnelles d'un processus stochastique, en incorporant l'attention dans les processus neuronaux.
Propose de résoudre le problème de l'underfitting dans la méthode du processus neuronal en ajoutant un mécanisme d'attention au chemin déterministe.
Une extension au cadre des processus neuronaux qui ajoute un mécanisme de conditionnement basé sur l'attention, permettant au modèle de mieux capturer les dépendances dans l'ensemble de conditionnement.
Les auteurs étendent les processus neuronaux en incorporant l'auto-attention pour enrichir les caractéristiques des points de contexte et l'attention croisée pour produire une représentation spécifique à la requête. Ils résolvent le problème de sous-adaptation des PNA et montrent que les PNA convergent mieux et plus rapidement que les PNA.
Résoudre le problème du damier dans la couche déconvolutionnelle en construisant des dépendances entre les pixels.
Ce travail propose des couches de déconvolution de pixels pour les réseaux neuronaux convolutifs comme moyen d'atténuer l'effet de damier.
Une nouvelle technique pour généraliser les opérations de déconvolution utilisées dans les architectures CNN standard, qui propose de faire une prédiction séquentielle des caractéristiques des pixels adjacents, ce qui permet d'obtenir des sorties plus lisses spatialement pour les couches de déconvolution.
Nous quantifions et élaguons les poids des réseaux neuronaux en utilisant l'inférence bayésienne variationnelle avec un antécédent multimodal induisant la sparsité.
Propose d'utiliser un mélange de spike propto continu 1/abs comme prior pour un réseau neuronal bayésien et démontre la bonne performance avec des convnets relativement sparifiés pour minist et cifar-10.
Cet article présente une approche bayésienne variationnelle permettant de quantifier les poids des réseaux neuronaux à des valeurs ternaires après la formation, et ce de manière raisonnée.
Nous mettons en œuvre une approche d'élagage des poids des DNN qui permet d'obtenir les taux d'élagage les plus élevés.
Cet article se concentre sur l'élagage des poids pour la compression des réseaux neuronaux, atteignant un taux de compression de 30x pour AlexNet et VGG pour ImageNet.
Une technique d'élagage progressif qui impose une contrainte de sparsité structurelle sur le paramètre de poids et réécrit l'optimisation comme un cadre ADMM, obtenant une précision plus élevée que la descente de gradient projetée.
Cet article présente une nouvelle architecture d'apprentissage profond pour résoudre le problème de l'apprentissage supervisé avec des séries temporelles multivariées éparses et irrégulièrement échantillonnées.
Propose un cadre pour faire des prédictions sur des données de séries temporelles éparses et irrégulièrement échantillonnées en utilisant un module d'interpolation qui modélise les valeurs manquantes en utilisant l'interpolation lisse, l'interpolation non lisse et l'intensité. 
Résout le problème de l'apprentissage supervisé avec des séries temporelles multivariées éparses et irrégulièrement échantillonnées en utilisant un réseau d'interpolation semi-paramétrique suivi d'un réseau de prédiction.
Fonction de perte invariante par permutation pour la prédiction d'ensembles de points.
Propose une nouvelle perte pour l'enregistrement des points (alignement de deux ensembles de points) avec une propriété invariante de permutation préférable. 
Cet article introduit une nouvelle fonction de distance entre des ensembles de points, applique deux autres distances de permutation dans une tâche de détection d'objets de bout en bout, et montre qu'en deux dimensions, tous les minima locaux de la perte holographique sont des minima globaux.
Propose des fonctions de perte invariantes par permutation qui dépendent de la distance des ensembles.
Nous présentons un modèle hiérarchique pour le placement efficace, de bout en bout, de graphes de calcul sur des dispositifs matériels.
Propose d'apprendre conjointement des groupes d'opérateurs à colocaliser et de placer les groupes appris sur des dispositifs pour distribuer les opérations d'apprentissage profond via l'apprentissage par renforcement.
Les auteurs visent un réseau entièrement connecté pour remplacer l'étape de co-localisation dans une méthode d'auto-placement proposée pour accélérer le temps d'exécution d'un modèle TensorFlow.
Propose un algorithme de placement de périphériques pour placer les opérations de tensorflow sur des périphériques.
Nous proposons une méthode d'utilisation des propriétés de groupe pour apprendre une représentation du mouvement sans étiquettes et démontrons l'utilisation de cette méthode pour représenter le mouvement 2D et 3D.
Propose d'apprendre le groupe de mouvement rigide à partir d'une représentation latente de séquences d'images sans avoir besoin d'étiquettes explicites et démontre expérimentalement la méthode sur des séquences de chiffres MINST et le jeu de données KITTI.
Cet article propose une approche pour apprendre les caractéristiques du mouvement vidéo de manière non supervisée, en utilisant des contraintes pour optimiser le réseau neuronal afin de produire des caractéristiques qui peuvent être utilisées pour régresser l'odométrie.
Cet article propose une nouvelle couche convolutive qui fonctionne dans un espace de Hilbert à noyau reproducteur continu.
Projeter des exemples dans un espace de Hilbert RK et effectuer une convolution et un filtrage dans cet espace.
Cet article formule une variante des réseaux neuronaux convolutifs qui modélise à la fois les activations et les filtres comme des fonctions continues composées de bases de noyaux.
Les CNN entraînés par ImageNet sont biaisés vers la texture des objets (au lieu de la forme comme les humains). En surmontant cette différence majeure entre la vision humaine et la vision artificielle, on obtient de meilleures performances de détection et une robustesse inédite aux distorsions de l'image.
L'utilisation d'un stylisme d'image pour augmenter les données d'entraînement des CNN entraînés par ImageNet afin que les réseaux résultants soient plus conformes aux jugements humains.
Cet article étudie les CNN comme AlexNet, VGG, GoogleNet et ResNet50, montre que ces modèles sont biaisés vers la texture lorsqu'ils sont entraînés sur ImageNet, et propose un nouveau jeu de données ImageNet.
Nous évaluons l'efficacité de l'exécution de tâches discriminantes auxiliaires par-dessus les statistiques de la distribution postérieure apprises par les autoencodeurs variationnels pour renforcer la dépendance du locuteur.
Proposer un modèle d'autoencodeur pour apprendre une représentation pour la vérification du locuteur en utilisant des fenêtres d'analyse de courte durée.
Une version modifiée du modèle d'autoencodeur variationnel qui aborde le problème de la reconnaissance du locuteur dans le contexte de segments de courte durée.
L'inférence variationnelle est biaisée, débarrassons-nous-en.
Présente l'inférence variationnelle jackknife, une méthode permettant de débiaiser les objectifs de Monte Carlo tels que l'auto-encodeur pondéré par l'importance.
Les auteurs analysent le biais et la variance de la limite IWAE et dérivent une approche jacknife pour estimer les moments comme moyen de débiaiser IWAE pour les échantillons finis pondérés par l'importance.
Un cadre qui fournit une politique pour le changement de voie autonome en apprenant à prendre des décisions tactiques de haut niveau avec un apprentissage par renforcement profond, et en maintenant une intégration étroite avec un contrôleur de bas niveau pour prendre des actions de bas niveau.
Il propose une nouvelle stratégie d'apprentissage, le Q-masking, qui consiste à coupler un contrôleur de bas niveau défini avec une politique de prise de décision tactique de haut niveau.
Cet article propose une approche d'apprentissage Q profond au problème du changement de voie en utilisant le " Q-masking ", qui réduit l'espace d'action en fonction des contraintes ou des connaissances préalables.
Les auteurs proposent une méthode qui utilise une politique de haut niveau basée sur l'apprentissage Q, combinée à un masque contextuel dérivé des contraintes de sécurité et des contrôleurs de bas niveau, qui empêche certaines actions d'être sélectionnées dans certains états. 
Recherche automatique de conception robotique à l'aide de réseaux de neurones à graphes
Propose une approche pour la conception automatique de robots basée sur l'évolution des graphes neuronaux. Les expériences démontrent que l'optimisation du contrôleur et du matériel est meilleure que l'optimisation du contrôleur seul.
Les auteurs proposent un schéma basé sur une représentation graphique de la structure du robot, et un réseau grapho-neural comme contrôleurs pour optimiser les structures des robots, combinés à leurs contrôleurs.  
Nous démontrons un autoencodeur pour les graphes.
Apprendre à générer des graphes à l'aide de méthodes d'apprentissage profond en "une seule fois", en produisant directement les probabilités d'existence des nœuds et des arêtes, ainsi que les vecteurs d'attributs des nœuds.
Un codeur automatique variationnel pour générer des graphiques
Nous proposons un nouvel algorithme pour l'apprentissage des LSTM en apprenant vers des portes à valeurs binaires, dont nous avons montré qu'il possède de nombreuses propriétés intéressantes.
Proposer une nouvelle fonction "gate" pour le LSTM afin d'activer les valeurs des gates vers 0 ou 1. 
L'article vise à pousser les portes LSTM à être binaires en employant la récente astuce Gumbel-Softmax pour obtenir une distribution catégorielle entraînable de bout en bout.
Amélioration des recommandations à l'aide d'une modélisation sensible au temps avec des réseaux neuronaux dans plusieurs catégories de produits sur un site Web de vente au détail
Ce document propose une nouvelle méthode de recommandation basée sur les réseaux neuronaux.
Les auteurs décrivent une procédure de construction de leur système de recommandation de production à partir de zéro et intègrent la décroissance temporelle des achats dans le cadre d'apprentissage.
Coupler le cadre de restauration d'image basé sur le GAN avec un autre réseau spécifique à la tâche pour générer une image réaliste tout en préservant les caractéristiques spécifiques à la tâche.
Une nouvelle méthode de couplage Task-GAN de l'image qui couple le GAN et un réseau spécifique à la tâche, ce qui permet d'éviter l'hallucination ou l'effondrement du mode.
Les auteurs proposent d'augmenter la restauration d'images basée sur le GAN avec une autre branche spécifique à la tâche, telle que les tâches de classification, pour une amélioration supplémentaire.
Un réseau neuronal profond formé de bout en bout qui exploite le modèle de mélange gaussien pour effectuer une estimation de la densité et une détection non supervisée des anomalies dans un espace à faible dimension appris par un auto-codeur profond.
L'article présente un cadre d'apprentissage profond conjoint pour la réduction de la dimension et le regroupement, qui permet une détection compétitive des anomalies.
Une nouvelle technique de détection des anomalies où les étapes de réduction de la dimension et d'estimation de la densité sont optimisées conjointement.
Nous avons proposé des réseaux de sous-espaces projectifs pour l'apprentissage en quelques coups et l'apprentissage semi-supervisé en quelques coups.
Cet article propose une nouvelle approche basée sur l'encastrement pour le problème de l'apprentissage en petit nombre et une extension de ce modèle au cadre de l'apprentissage semi-supervisé en petit nombre.
Nouvelle méthode de classification entièrement et semi-supervisée de quelques images basée sur l'apprentissage d'un encastrement général, puis sur l'apprentissage d'un sous-espace de celui-ci pour chaque classe.
Nous étudions la prise en compte des contingences et les aspects contrôlables dans l'exploration et obtenons des performances de pointe sur Montezuma's Revenge sans démonstration d'expert.
Cet article étudie le problème de l'extraction d'une représentation d'état significative pour aider à l'exploration lorsqu'on est confronté à une tâche de récompense éparse, en identifiant les caractéristiques contrôlables (apprises) de l'état.
Cet article propose l'idée novatrice d'utiliser la conscience des contingences pour faciliter l'exploration dans les tâches d'apprentissage par renforcement à récompense éparse, en obtenant des résultats à la pointe de la technologie.
Nous avons proposé un algorithme supervisé, le DNA-GAN, pour démêler les attributs multiples des images.
Cet article étudie le problème de la génération d'images conditionnées par attributs à l'aide de réseaux adversaires génératifs, et propose de générer des images à partir d'attributs et de codes latents comme représentation de haut niveau.
Cet article propose une nouvelle méthode pour démêler les différents attributs des images à l'aide d'un nouveau GAN à structure ADN.
Cet article présente une nouvelle technique de modélisation générative à variables latentes qui permet de représenter des informations globales dans une variable latente et des informations locales dans une autre variable latente.
L'article présente un VAE qui utilise des étiquettes pour séparer la représentation apprise en une partie invariante et une partie covariante.
Nous abordons le problème de l'apprentissage actif comme un problème de sélection de l'ensemble de base et nous montrons que cette approche est particulièrement utile dans le cadre de l'apprentissage actif par lots, ce qui est crucial lors de la formation des CNN.
Les auteurs proposent un algorithme d'apprentissage actif agnostique pour la classification multi-classes.
L'article propose un algorithme d'apprentissage actif en mode batch pour le CNN en tant que problème de noyau qui surpasse l'échantillonnage aléatoire et l'échantillonnage d'incertitude.
étudie l'apprentissage actif pour les réseaux neuronaux convolutifs, formule le problème de l'apprentissage actif en tant que sélection de l'ensemble de base et présente une nouvelle stratégie.
Un algorithme simple pour améliorer l'optimisation et la gestion des dépendances à long terme dans les LSTM.
L'article présente un algorithme stochastique simple appelé h-detach, spécifique à l'optimisation des LSTM et destiné à résoudre ce problème.
propose une modification simple du processus d'apprentissage du LSTM pour faciliter la propogation du gradient le long des états de la cellule, ou le "chemin temporel linéaire".
Un entraînement adéquat des CNN avec la classe dustbin augmente leur robustesse aux attaques adverses et leur capacité à traiter des échantillons hors distribution.
Cet article propose d'ajouter un label supplémentaire pour la détection des échantillons OOD et des exemples adverses dans les modèles CNN.
L'article propose une classe supplémentaire qui incorpore des images de distribution naturelle et des images interpolées pour les échantillons adverses et de distribution dans les CNN.
Dans un réseau neuronal convolutif profond formé avec un niveau suffisant d'augmentation des données, optimisé par SGD, les régularisateurs explicites (décroissance des poids et abandon) pourraient ne pas apporter d'amélioration supplémentaire de la généralisation.
Cet article propose l'augmentation des données comme alternative aux techniques de régularisation couramment utilisées, et montre que pour quelques modèles/tâches de référence, les mêmes performances de généralisation peuvent être obtenues en utilisant uniquement l'augmentation des données.
Cet article présente une étude systématique de l'augmentation des données dans la classification d'images avec des réseaux neuronaux profonds, suggérant que l'augmentation des données peut répliquer certains régularisateurs communs comme la décroissance du poids et l'abandon.
Dans ce travail, nous présentons Gedit, un système de gestes sur le clavier pour une édition de texte mobile pratique.
Rapporte la conception et l'évaluation des techniques d'interaction de Gedit.
Présente un nouvel ensemble de gestes tactiles permettant d'effectuer une transition transparente entre la saisie et l'édition de texte sur les appareils mobiles.
Nous prouvons que le DNN est une solution approximée de manière récursive au principe d'entropie maximale.
Présente une dérivation qui lie un DNN à l'application récursive de l'ajustement du modèle d'entropie maximale.
Cet article vise à fournir une vue de l'apprentissage profond du point de vue du principe d'entropie maximale.
Nous présentons une nouvelle approche de l'apprentissage par renforcement qui exploite une fonction de récompense intrinsèque indépendante de la tâche, entraînée sur des mesures de pouls périphériques qui sont corrélées aux réponses du système nerveux autonome humain. 
Propose un cadre d'apprentissage par renforcement basé sur la réaction émotionnelle humaine dans le contexte de la conduite autonome.
Les auteurs proposent d'utiliser des signaux, tels que les réponses viscérales autonomes de base qui influencent la prise de décision, dans le cadre du RL en augmentant les fonctions de récompense du RL avec un modèle appris directement des réponses du système nerveux humain.
propose d'utiliser les signaux physiologiques pour améliorer les performances des algorithmes d'apprentissage par renforcement et construire une fonction de récompense intrinsèque moins éparse en mesurant l'amplitude du pouls cardiaque
Les CNN sont-ils robustes ou fragiles au bruit des étiquettes ? En pratique, robustes.
Les auteurs ont testé la robustesse des CNN au bruit des étiquettes en utilisant l'arbre ImageNet 1k de WordNet.
Une analyse de la performance des modèles de réseaux neuronaux convolutifs lorsqu'un bruit dépendant de la classe et un bruit indépendant de la classe sont introduits.
Démontre que les CNN sont plus robustes au bruit des étiquettes pertinentes pour la classe et soutient que le bruit du monde réel devrait être pertinent pour la classe.
Synthèse audio de haute qualité avec les GAN
Propose une approche qui utilise le cadre GAN pour générer de l'audio en modélisant les magnitudes des logs et les fréquences instantanées avec une résolution fréquentielle suffisante dans le domaine spectral. 
Une stratégie pour générer des échantillons audio à partir du bruit avec des GANs, avec des modifications de l'architecture et de la représentation nécessaires pour générer un audio convaincant qui contient un code latent interprétable.
Présente une idée simple pour mieux représenter les données audio afin de pouvoir appliquer des modèles convolutifs tels que les réseaux adversariens génératifs.
Optimisation des graphes avec filtrage du signal dans le domaine des sommets.
L'article étudie l'apprentissage de la matrice d'adjacence d'un graphe avec un graphe non orienté faiblement connecté avec des poids d'arêtes non négatifs en utilisant un algorithme de descente de sous-gradients projetés.
Développe un nouveau schéma de rétropropagation sur la matrice d'adjacence d'un graphe de réseau neuronal.
Cet article décrit un outil de création 3D pour la RA dans les chaînes de montage de l'industrie 4.0.
L'article traite de la manière dont les outils de création de RA soutiennent la formation des systèmes de chaîne de montage et propose une approche
Un système de guidage en RA pour les chaînes de montage industrielles qui permet la création sur place de contenu en RA.
Présente un système qui permet de former plus efficacement les ouvriers d'une usine en utilisant un système de réalité augmentée. 
Nous montrons qu'avec un choix approprié de pas, l'algorithme itératif du premier ordre largement utilisé pour l'entraînement des GANs converge en fait vers une solution stationnaire avec un taux sous-linéaire.
Cet article utilise les GAN et l'apprentissage multi-tâches pour fournir une garantie de convergence pour les algorithmes primal-dual sur certains problèmes min-max.
Analyse la dynamique d'apprentissage des GAN en formulant le problème comme un problème d'optimisation primal-dual en supposant une classe limitée de modèles.
Nous montrons comment utiliser la RL profonde pour construire des agents capables de résoudre des dilemmes sociaux au-delà des jeux matriciels.
Apprendre à jouer à des jeux à somme générale à deux joueurs avec état à information imparfaite
Spécifie une stratégie de déclenchement (CCC) et l'algorithme correspondant, démontrant la convergence vers des résultats efficaces dans les dilemmes sociaux sans que les agents aient besoin d'observer les actions des autres.
L'article propose et analyse deux schémas de quantification pour la communication des gradients stochastiques dans l'apprentissage distribué qui réduiraient les coûts de communication par rapport à l'état de l'art tout en maintenant la même précision.  
Les auteurs proposent d'appliquer une quantification en escalier aux gradients stochastiques calculés par le processus d'apprentissage, ce qui permet d'améliorer l'erreur de quantification et d'obtenir des résultats supérieurs à ceux obtenus par les méthodes de référence. Ils proposent également un schéma imbriqué pour réduire le coût de communication.
Les auteurs établissent un lien entre la réduction de la communication dans l'optimisation distribuée et la quantification en escalier et développent deux nouveaux algorithmes d'entraînement distribués où la surcharge de communication est considérablement réduite.
Entraînement conjoint d'un réseau de génération de bruits adverses et d'un réseau de classification afin de fournir une meilleure robustesse aux attaques adverses.
Une solution GAN pour les modèles profonds de classification, face aux attaques de type boîte blanche et boîte noire, qui produit des modèles robustes. 
L'article propose un mécanisme défensif contre les attaques adverses en utilisant des GANs avec des perturbations générées utilisées comme exemples adverses et un discriminateur utilisé pour les distinguer.
Utilisation de méthodes d'ensemble comme défense contre les perturbations adverses contre les réseaux neuronaux profonds.
Cet article propose d'utiliser l'ensemblisme comme un mécanisme de défense contradictoire.
Nous avons étudié de manière empirique la robustesse de différents ensembles de réseaux neuronaux profonds aux deux types d'attaques, FGSM et BIM, sur deux ensembles de données populaires, MNIST et CIFAR10.
Proposition de la méthode de génération de phrases basée sur la fusion entre les informations textuelles et les informations visuelles associées aux informations textuelles.
Ce travail décrit un modèle d'apprentissage profond pour les systèmes de dialogue qui tire parti des informations visuelles.
Cet article propose un nouvel ensemble de données pour le dialogue ancré et fait une observation computationnelle selon laquelle il pourrait aider à raisonner sur la vision même lors d'un dialogue basé sur le texte.
propose d'augmenter les approches traditionnelles de génération de phrases/dialogue basées sur le texte en incorporant des informations visuelles, en collectant un ensemble de données comprenant à la fois du texte et des images ou des vidéos associées.
nous avons proposé un nouveau réseau convolutif récurrent contextuel avec une propriété robuste d'apprentissage visuel 
Cet article présente la connexion par rétroaction pour améliorer l'apprentissage des caractéristiques en incorporant des informations contextuelles.
L'article propose d'ajouter des connexions "récurrentes" dans un réseau de convolution avec un mécanisme de déclenchement.
Nous montrons que l'entraînement d'un réseau profond à l'aide de la normalisation par lots est équivalent à une inférence approximative dans les modèles bayésiens, et nous démontrons comment cette découverte nous permet de faire des estimations utiles de l'incertitude du modèle dans les réseaux conventionnels.
Cet article propose d'utiliser la normalisation par lot au moment du test pour obtenir l'incertitude prédictive, et montre que la prédiction Monte Carlo au moment du test en utilisant la norme par lot est meilleure que l'abandon.
Propose que la procédure de régularisation appelée normalisation par lots peut être comprise comme une inférence bayésienne approximative, qui fonctionne de manière similaire à l'abandon MC en termes d'estimations de l'incertitude qu'elle produit.
Nous améliorons l'élimination des gradients (une technique qui consiste à n'échanger que les gradients importants lors de la formation distribuée) en incorporant les gradients locaux lors de la mise à jour des paramètres afin de réduire la perte de qualité et d'améliorer le temps de formation.
Cet article propose 3 modes de combinaison des gradients locaux et globaux pour mieux utiliser un plus grand nombre de nœuds de calcul.
Examine le problème de la réduction des besoins en communication pour la mise en œuvre des techniques d'optimisation distribuées, en particulier la SGD.
Exploration à l'aide de la RL distributionnelle et de la variance tronquée.
Présente une méthode RL pour gérer les compromis d'exploration-exploitation via des techniques UCB.
Une méthode pour utiliser la distribution apprise par la régression quantile DQN pour l'exploration, au lieu de la stratégie epsilon-greedy habituelle.
Proposition de nouveaux algorithmes (QUCB et QUCB+) pour gérer le compromis d'exploration dans les bandits à plusieurs bras et plus généralement dans l'apprentissage par renforcement.
Motivés par les théories du langage et de la communication, nous présentons des auto-codeurs communautaires, dans lesquels plusieurs codeurs et décodeurs apprennent collectivement des représentations structurées et réutilisables.
Les auteurs abordent le problème de l'apprentissage de la représentation, visent à construire une représentation réutilisable et structurée, soutiennent que la co-adaptation entre le codeur et le décodeur dans l'EA traditionnelle produit une représentation médiocre, et introduisent des auto-encodeurs basés sur la communauté.
L'article présente un cadre d'auto-codage basé sur la communauté pour traiter la co-adaptation des codeurs et des décodeurs et vise à construire de meilleures représentations.
Nous présentons MetaMimic, un algorithme qui prend en entrée un ensemble de données de démonstration et produit (i) une politique d'imitation haute-fidélité à un coup et (ii) une politique de tâche inconditionnelle.
L'article examine le problème de l'imitation en une seule fois avec une grande précision d'imitation, en étendant DDPGfD pour utiliser uniquement les trajectoires d'état.
Cet article propose une approche pour l'imitation à un coup avec une grande précision, et aborde le problème commun de l'exploration dans l'apprentissage par imitation.
Présente une méthode RL pour l'apprentissage à partir d'une démonstration vidéo sans accès à des actions expertes.
Nous présentons une nouvelle méthode de normalisation pour les réseaux neuronaux profonds qui est robuste aux multi-modalités dans les distributions de caractéristiques intermédiaires.
Méthode de normalisation qui apprend la distribution multimodale dans l'espace des caractéristiques.
Propose une généralisation de la normalisation des lots en supposant que les statistiques des activations unitaires sur les lots et sur les dimensions spatiales ne sont pas unimodales.
Nous avons proposé une méthode basée sur la distillation des connaissances pour améliorer la précision de la traduction automatique neuronale multilingue.
Un modèle de traduction automatique neuronal multilingue multiple qui forme d'abord des modèles distincts pour chaque paire de langues, puis effectue une distillation.
L'article vise à former un modèle de traduction automatique en augmentant la perte d'entropie croisée standard avec une composante de distillation basée sur des modèles d'enseignants individuels (une seule paire de langues).
Nous étudions les différents types de connaissances préalables qui aident l'apprentissage humain et nous constatons que les connaissances préalables générales sur les objets jouent le rôle le plus critique pour guider le jeu humain.
Les auteurs étudient, par le biais d'expériences, les aspects des prieurs humains qui sont les plus importants pour l'apprentissage par renforcement dans les jeux vidéo.
Les auteurs présentent une étude des prieurs employés par les humains dans les jeux vidéo et démontrent l'existence d'une taxonomie de caractéristiques qui affectent à des degrés divers la capacité à accomplir des tâches dans le jeu.
Poussés par le besoin de méthodes d'optimisation des hyperparamètres parallélisables et en boucle ouverte, nous proposons l'utilisation de processus ponctuels k-déterminants dans l'optimisation des hyperparamètres par recherche aléatoire.
Propose d'utiliser le k-DPP pour sélectionner les points candidats dans les recherches d'hyperparamètres.
Les auteurs proposent k-DPP comme méthode en boucle ouverte pour l'optimisation des hyperparamètres et fournissent son étude empirique et sa comparaison avec d'autres méthodes.
Examine la recherche non séquentielle et non informée d'hyperparamètres à l'aide de processus ponctuels déterminants, qui sont des distributions de probabilité sur des sous-ensembles d'un ensemble de base ayant la propriété que les sous-ensembles comportant des éléments plus "divers" ont une probabilité plus élevée.
Dans l'apprentissage par transfert inductif, le réglage fin des réseaux convolutifs pré-entraînés est nettement plus performant que l'apprentissage à partir de zéro.
Aborde le problème de l'apprentissage par transfert dans les réseaux profonds et propose d'avoir un terme de régularisation qui pénalise la divergence par rapport à l'initialisation.
propose une analyse des différentes techniques de régularisation adaptative pour l'apprentissage par transfert profond, en se concentrant spécifiquement sur l'utilisation d'une condition L@-SP.
Pour des raisons d'efficacité, nous nous intéressons aux réseaux neuronaux avec des couches de produits internes en diagonale de bloc.
Cet article propose que les couches internes d'un réseau neuronal soient des blocs diagonaux et explique que les matrices de blocs diagonaux sont plus efficaces que l'élagage et que les couches de blocs diagonaux conduisent à des réseaux plus efficaces.
Remplacement des couches entièrement connectées par des couches entièrement connectées en diagonale de bloc
Nous proposons une nouvelle technique de normalisation des poids appelée normalisation spectrale pour stabiliser l'apprentissage du discriminateur des GANs.
Cet article utilise la régularisation spectrale pour normaliser les objectifs du GAN, et le GAN qui en résulte, appelé SN-GAN, garantit essentiellement la propriété de Lipschitz du discriminateur.
Cet article propose la "normalisation spectrale", ce qui constitue une belle avancée dans l'amélioration de l'entraînement des GANs.
Les politiques de transition permettent aux agents de composer des compétences complexes en reliant de manière fluide des compétences primitives précédemment acquises.
Propose un schéma de transition vers des états de stratage favorables à l'exécution d'options données dans des domaines continus. Ce schéma utilise deux processus d'apprentissage réalisés simultanément.
Présente une méthode d'apprentissage des politiques de transition d'une tâche à l'autre dans le but d'accomplir des tâches complexes en utilisant un estimateur de proximité d'état pour récompenser la politique de transition.
propose un nouveau schéma d'apprentissage avec une fonction de récompense auxiliaire apprise pour optimiser les politiques de transition qui relient l'état final d'une macro-action/option précédente aux bons états d'initiation de la macro-action/option suivante.
Nous classons les caractéristiques dynamiques qu'une et deux cellules GRU peuvent et ne peuvent pas capturer en temps continu, et nous vérifions nos résultats expérimentalement avec la prédiction de séries temporelles à k étapes. 
Les auteurs analysent les GRU avec des tailles cachées de un et deux comme des systèmes dynamiques à temps continu, affirmant que la puissance expressive de la représentation de l'état caché peut fournir une connaissance préalable de la performance d'un GRU sur un ensemble de données donné.
Cet article analyse les GRU du point de vue des systèmes dynamiques et montre que les GRU 2d peuvent être entraînés à adopter une variété de points fixes et peuvent se rapprocher des attracteurs linéaires, mais ne peuvent pas imiter un attracteur annulaire.
Convertit les équations GRU en temps continu et utilise la théorie et les expériences pour étudier les réseaux GRU à 1 et 2 dimensions et présenter toutes les variétés de topologie dynamique disponibles dans ces systèmes.
Les entrées différenciées entraînent une différenciation fonctionnelle du réseau, et l'interaction des fonctions de perte entre les réseaux peut affecter le processus d'optimisation.
Une modification du réseau original du sablier pour l'estimation de la pose unique qui apporte des améliorations par rapport à la ligne de base originale.
Les auteurs étendent un réseau en sablier empilé avec des modules inception-resnet-A et proposent une approche multi-échelle pour l'estimation de la pose humaine dans des images fixes RVB.
Pour former un encastrement de phrases à partir de documents techniques, notre approche tient compte de la structure du document pour trouver un contexte plus large et gérer les mots hors vocabulaire.
Présente des idées pour améliorer l'intégration des phrases en s'appuyant sur davantage de contexte.
Apprendre des représentations de phrases avec des informations sur les dépendances des phrases
Élargit l'idée de former une représentation non supervisée des phrases utilisée dans l'approche SkipThough en utilisant un ensemble plus large d'éléments pour former la représentation d'une phrase.
Nous explorons la structure des fonctions de perte neuronales, et l'effet des paysages de perte sur la généralisation, en utilisant une gamme de méthodes de visualisation.
Cet article propose une méthode pour visualiser la fonction de perte d'un NN et donne un aperçu de la capacité de formation et de généralisation des NN.
Étudie la non-convexité de la surface de perte et des chemins d'optimisation.
Nous démontrons qu'en exploitant un codage de sortie à plusieurs voies, plutôt que le codage à un coup largement utilisé, nous pouvons rendre les modèles profonds plus robustes aux attaques adverses.
Cet article propose de remplacer la couche finale d'entropie croisée formée sur des étiquettes à un coup dans les classificateurs en codant chaque étiquette comme un vecteur à haute dimension et en formant le classificateur pour minimiser la distance L2 par rapport au codage de la classe correcte.
Les auteurs proposent une nouvelle méthode de lutte contre les attaques adverses qui présente des gains significatifs par rapport aux lignes de base.
Nous présentons le premier modèle NMT avec un décodage entièrement parallèle, réduisant la latence d'inférence de 10x.
Ce travail propose un décodeur non autorégressif pour le cadre codeur-décodeur dans lequel la décision de générer un mot ne dépend pas de la décision antérieure des mots générés.
Cet article décrit une approche de décodage non autorégressif pour la traduction automatique neuronale avec la possibilité d'un décodage plus parallèle qui peut entraîner une accélération significative.
propose l'introduction d'un ensemble de variables latentes pour représenter la fertilité de chaque mot source afin de rendre la génération de la phrase cible non autorégressive.
Nous démontrons une méthode certifiable, entraînable et évolutive de défense contre les exemples adverses.
Propose une nouvelle défense contre les attaques de sécurité sur les réseaux neuronaux avec le modèle d'attaque qui produit un certificat de sécurité sur l'algorithme.
Déduit une limite supérieure de la perturbation contradictoire pour les réseaux neuronaux à une couche cachée.
nous proposons un régularisateur qui améliore les performances de classification des réseaux de neurones
les auteurs proposent d'entraîner un modèle à partir d'un point de maximisation de l'information mutuelle entre les prédictions et les véritables sorties, avec un terme de régularisation qui minimise les informations non pertinentes pendant l'apprentissage.
propose de décomposer les paramètres en une carte de caractéristiques inversible F et une transformation linéaire w dans la dernière couche pour maximiser l'information mutuelle I(Y, \hat{T}) tout en limitant les informations non pertinentes
Cet article présente un nouveau cadre de modélisation générative qui évite l'effondrement des variables latentes et clarifie l'utilisation de certains facteurs ad hoc dans la formation des auto-codeurs variationnels.
L'article propose de résoudre le problème d'un codeur automatique variationnel ignorant les variables latentes.
Cet article propose d'ajouter un autoencodeur stochastique au modèle VAE original pour résoudre le problème du décodeur LSTM d'un modèle de langage qui pourrait être trop puissant pour ignorer les informations de la variable latente.
Cet article présente AutoGen, qui combine un auto-codeur variationnel génératif avec un modèle de reconstruction haute-fidélité basé sur l'auto-codeur pour mieux utiliser la représentation latente.
 Cet article étudie le problème de la division de domaine en segmentant les instances tirées de différentes distributions probabilistes.  
Cet article traite du problème de la reconnaissance de la nouveauté dans l'apprentissage par ensemble ouvert et l'apprentissage généralisé par zéro et propose une solution possible.
Une approche de la séparation des domaines basée sur le bootstrapping pour identifier les seuils de coupure de similarité pour les classes connues, suivi d'un test de Kolmogorov-Smirnoff pour affiner les zones d'intra-distribution bootstrappées.
propose d'introduire un nouveau domaine, le domaine incertain, afin de mieux gérer la division entre les domaines vus/non vus dans l'apprentissage par ensembles ouverts et l'apprentissage généralisé par le zéro.
La SGD effectue implicitement une inférence variationnelle ; le bruit de gradient est fortement non isotrope, de sorte que la SGD ne converge même pas vers les points critiques de la perte originale.
Cet article propose une analyse variationnelle du SGD en tant que processus de non-équilibre.
Cet article traite de la fonction objective régularisée minimisée par le SGD standard dans le contexte des réseaux neuronaux, et fournit une perspective d'inférence variationnelle en utilisant l'équation de Fokker-Planck.
Développe une théorie pour étudier l'impact du bruit de gradient stochastique pour le SGD, en particulier pour les modèles de réseaux neuronaux profonds.
Les agents peuvent apprendre à imiter uniquement des démonstrations visuelles (sans actions) au moment du test après avoir appris de leur propre expérience sans aucune forme de supervision au moment de la formation.
Cet article propose une approche de l'apprentissage visuel par l'apprentissage de fonctions d'habileté paramétriques.
Un article sur l'imitation d'une tâche présentée juste pendant l'inférence, où l'apprentissage est effectué de manière auto-supervisée et où, pendant la formation, l'agent explore des tâches connexes mais différentes.
propose une méthode permettant de contourner le problème de la démonstration coûteuse d'un expert en utilisant l'exploration aléatoire d'un agent pour apprendre des compétences généralisables qui peuvent être appliquées sans formation préalable spécifique.
L'article fournit une description d'une procédure pour améliorer le modèle d'espace vectoriel des mots avec une évaluation des modèles Paragram et GloVe pour les repères de similarité.
Cet article propose un nouvel algorithme qui ajuste les vecteurs de mots de GloVe et utilise ensuite une fonction de similarité non euclidienne entre eux.
Les auteurs présentent des observations sur les faiblesses des modèles d'espace vectoriel existants et énumèrent une approche en 6 étapes pour affiner les vecteurs de mots existants
Nous proposons une nouvelle méthode de quantification et l'appliquons à la quantification des RNN pour la compression et l'accélération.
Cet article propose une méthode de quantification multi-bits pour les réseaux neuronaux récurrents.
Une technique pour quantifier les matrices de poids des réseaux neuronaux, et une procédure d'optimisation alternée pour estimer l'ensemble des k vecteurs et coefficients binaires qui représentent le mieux le vecteur original.
Nous remplaçons les couches entièrement connectées d'un réseau neuronal par l'ansatz de renormalisation de l'intrication à plusieurs échelles, un type d'opération quantique qui décrit les corrélations à longue portée. 
Dans cet article, les auteurs suggèrent d'utiliser la technique de tensorisation MERA pour compresser les réseaux neuronaux.
Une nouvelle paramétrisation des cartes linéaires pour l'utilisation des réseaux neuronaux, utilisant une factorisation hiérarchique de la carte linéaire qui réduit le nombre de paramètres tout en permettant de modéliser des interactions relativement complexes.
Études sur la compression des couches d'anticipation à l'aide de décompositions tensorielles de faible rang et exploration d'une décomposition en arbre
Nous présentons une analyse unique d'un réseau neuronal formé pour éliminer la redondance et identifier la structure optimale du réseau.
Cet article propose un ensemble d'heuristiques pour identifier une bonne architecture de réseau neuronal, basée sur l'ACP des activations des unités sur l'ensemble de données.
Cet article présente un cadre pour l'optimisation des architectures de réseaux neuronaux par l'identification des filtres redondants entre les couches.
Nous réalisons la première analyse de sécurité approfondie des attaques par empreintes digitales de DNN qui exploitent les canaux latéraux du cache, ce qui représente une étape vers la compréhension de la vulnérabilité des DNN aux attaques par canaux latéraux.
Cet article examine le problème de la prise d'empreintes digitales des architectures de réseaux neuronaux à l'aide de canaux secondaires de cache, et discute des défenses de sécurité par l'obscurité.
Cet article réalise des attaques de type "cache side-channel" pour extraire les attributs d'un modèle victime et en déduire son architecture. Il montre également qu'elles permettent d'obtenir une précision de classification presque parfaite.
Nous proposons la formation par objectifs complémentaires (COT), un nouveau paradigme de formation qui optimise à la fois les objectifs primaires et complémentaires pour apprendre efficacement les paramètres des réseaux neuronaux.
Envisage d'augmenter l'objectif d'entropie croisée avec une maximisation de l'objectif "complémentaire", qui vise à neutraliser les probabilités prédites de classes autres que les étiquettes de vérité terrain.
Les auteurs proposent un objectif secondaire pour la minimisation de softmax basé sur l'évaluation des informations recueillies auprès des classes incorrectes, ce qui conduit à une nouvelle approche de formation.
Traite de l'entraînement des réseaux neuronaux pour les tâches de classification ou de génération de séquences en utilisant la perte d'entropie transversale.
Estimation de l'incertitude en un seul passage vers l'avant sans paramètres supplémentaires pouvant être appris.
Une nouvelle méthode de calcul des estimations de l'incertitude de sortie dans les DNN pour les problèmes de classification qui correspond aux méthodes de pointe pour l'estimation de l'incertitude et les surpasse dans les tâches de détection de la non-répartition.
Les auteurs présentent le softmax inhibé, une modification du softmax par l'ajout d'une activation constante qui fournit une mesure de l'incertitude. 
Nous avons créé un système riche en fonctionnalités pour l'apprentissage profond avec des entrées cryptées, produisant des sorties cryptées, préservant la confidentialité.
Un cadre pour l'inférence de modèles d'apprentissage profond privés utilisant des schémas FHE qui supportent l'amorçage rapide et peuvent donc réduire le temps de calcul.
L'article présente un moyen d'évaluer un réseau neuronal de manière sécurisée en utilisant le cryptage homomorphique.
Nous présentons un système appelé GamePad pour explorer l'application des méthodes d'apprentissage automatique à la preuve de théorèmes dans l'assistant de preuve Coq.
Cet article décrit un système d'application de l'apprentissage automatique à la démonstration interactive de théorèmes, se concentre sur les tâches de prédiction de tactique et d'évaluation de position, et montre qu'un modèle neuronal surpasse un SVM pour ces deux tâches.
Propose que des techniques d'apprentissage automatique soient utilisées pour aider à construire des preuves dans le proverbe de théorème Coq.
Dans cet article, nous avons étudié l'entraînement efficace de réseaux quantifiés par le poids avec gradient quantifié dans un environnement distribué, à la fois théoriquement et empiriquement.
Cet article étudie les propriétés de convergence de la quantification de poids tenant compte des pertes avec différentes précisions de gradient dans l'environnement distribué, et fournit une analyse de convergence pour la quantification de poids avec des gradients à précision complète, quantifiés et quantifiés coupés.
Les auteurs proposent une analyse de l'effet de la quantification simultanée des poids et des gradients dans l'apprentissage d'un modèle paramétré dans un environnement distribué entièrement synchronisé.
Une stratégie de régularisation pour améliorer les performances de l'apprentissage séquentiel
Une nouvelle approche, basée sur la régularisation, du problème de l'apprentissage séquentiel en utilisant un modèle de taille fixe qui ajoute des termes supplémentaires à la perte, encourageant la sparsité de la représentation et combattant l'oubli catastrophique.
Cet article traite du problème de l'oubli catastrophique dans l'apprentissage tout au long de la vie en proposant des stratégies d'apprentissage régularisées.
Un réseau neuronal synaptique avec graphe de synapses et apprentissage qui présente la caractéristique de conjugaison topologique et de distribution de Bose-Einstein dans l'espace de surprise.  
Les auteurs proposent un réseau neuronal hybride composé d'un graphe de synapses qui peut être intégré dans un réseau neuronal standard.
Présente un modèle de réseau neuronal d'inspiration biologique basé sur les canaux ioniques excitateurs et inhibiteurs des membranes de cellules réelles.
Modèles d'intégration de graphes généralisés
Une approche généralisée d'intégration de graphes de connaissances qui apprend les intégrations sur la base de trois objectifs simultanés différents, et dont les performances sont équivalentes ou même supérieures à celles des approches existantes de l'état de l'art.
S'attaque à la tâche d'apprentissage d'encastrements de graphes multi-relationnels à l'aide d'un réseau de neurones.
Propose une nouvelle méthode, GEN, pour calculer les enchâssements de graphes multirelations, en particulier que les soi-disant E-Cells et R-Cells peuvent répondre à des requêtes de la forme (h,r, ?),(?r,t), et (h, ?,t)
L'apprentissage curriculaire Minimax est une méthode d'apprentissage automatique impliquant l'augmentation de la dureté souhaitable et la réduction programmée de la diversité.
 Une approche d'apprentissage de programmes d'études utilisant une fonction d'ensemble submodulaire qui capture la diversité des exemples choisis pendant la formation. 
L'article présente l'apprentissage curriculaire MiniMax comme une approche permettant de former de manière adaptative des modèles en leur fournissant différents sous-ensembles de données. 
Modèles implicites appliqués à la causalité et à la génétique
Les auteurs proposent d'utiliser le modèle implicite pour s'attaquer au problème des associations à l'échelle du génome.
Cet article propose des solutions aux problèmes que posent, dans les études d'association à l'échelle du génome, la confusion due à la structure de la population et la présence potentielle d'interactions non linéaires entre différentes parties du génome, et établit un pont entre la génétique statistique et la ML.
Présente un modèle génératif non linéaire pour GWAS qui modélise la structure de la population, où les non-linéarités sont modélisées à l'aide de réseaux neuronaux comme approximateurs de fonctions non linéaires et où l'inférence est effectuée à l'aide d'une inférence variationnelle sans vraisemblance.
Apprentissage en quelques clics en exploitant la relation au niveau de l'objet pour apprendre la relation au niveau de l'image (similarité).
Cet article traite du problème de l'apprentissage en quelques clics en proposant une approche basée sur l'intégration qui apprend à comparer les caractéristiques de niveau objet entre les exemples de l'ensemble de support et de l'ensemble de requête.
propose une méthode d'apprentissage en quelques clics qui exploite la relation au niveau de l'objet entre différentes images sur la base de la recherche de voisins proches et concatène les cartes de caractéristiques de deux images d'entrée en une seule carte de caractéristiques.
Les chercheurs qui explorent les techniques de traitement du langage naturel appliquées au code source n'utilisent aucune forme d'enchâssement pré-formé, nous montrons qu'ils devraient le faire.
Cet article cherche à comprendre si le pré-entraînement des mots incorporés pour le code du langage de programmation en utilisant des modèles de langage de type NLP a un impact sur la tâche de résumé de code extrême.
Ce travail montre comment le pré-entraînement des vecteurs de mots à l'aide de corpus de code conduit à des représentations qui sont plus appropriées que les représentations initialisées et entraînées de manière aléatoire pour la prédiction des noms de fonctions/méthodes.
Nous résolvons le Rubik's Cube avec de l'apprentissage par renforcement pur.
Solution pour résoudre le Rubik cube en utilisant l'apprentissage par renforcement (RL) avec la recherche d'arbre de Monte-Carlo (MCTS) par itération autodidactique. 
Ce travail résout le Rubik's Cube en utilisant une méthode d'itération de politique approximative appelée itération autodidactique, en surmontant le problème des récompenses éparses en créant son propre système de récompenses.
Présente un algorithme RL profond pour résoudre le Rubik's cube qui gère l'énorme espace d'état et la récompense très éparse du Rubik's cube.
Nous décrivons un modèle différentiable de bout en bout pour l'AQ qui apprend à représenter des portions de texte dans la question comme des dénotations dans le graphe de connaissances, en apprenant à la fois des modules neuronaux pour la composition et la structure syntaxique de la phrase.
Cet article présente un modèle de réponse aux questions visuelles qui peut apprendre à la fois les paramètres et les prédicteurs de structure pour un réseau neuronal modulaire, sans structures supervisées ou assistance d'un analyseur syntaxique.
propose d'entraîner un modèle de réponse aux questions à partir des réponses uniquement et d'une base de données en apprenant des arbres latents qui capturent la syntaxe et apprennent la sémantique des mots.
Nous présentons une nouvelle infrastructure de compilateur qui répond aux lacunes des cadres d'apprentissage profond existants.
Proposition de passer de la génération de code ad hoc dans les moteurs d'apprentissage profond aux meilleures pratiques en matière de compilateurs et de langages.
Cet article présente un cadre de compilateur qui permet de définir des langages spécifiques à un domaine pour les systèmes d'apprentissage profond, et définit des étapes de compilation qui peuvent tirer parti des optimisations standard et des optimisations spécialisées pour les réseaux neuronaux.
Cet article introduit un DLVM pour tirer parti des aspects de compilation d'un compilateur tensoriel
Architecture basée sur l'attention pour l'ancrage du langage via l'apprentissage par renforcement dans un nouvel environnement de grille 2D personnalisable.  
L'article aborde le problème de la navigation à partir d'une instruction et propose une approche permettant de combiner les informations textuelles et visuelles via un mécanisme d'attention.
Cet article considère le problème de suivre des instructions en langage naturel en ayant une vue à la première personne d'un environnement a priori inconnu, et propose une méthode d'architecture neuronale.
Étudie le problème de la navigation vers un objet cible dans un environnement quadrillé en 2D en suivant une description donnée en langage naturel et en recevant des informations visuelles sous forme de pixels bruts.
Une architecture simple composée de convolutions et d'attention permet d'obtenir des résultats comparables à ceux des modèles récurrents les mieux documentés.
Une méthode rapide et performante d'augmentation des données basée sur la paraphrase et un modèle non récurrent de compréhension de la lecture utilisant uniquement des convolutions et l'attention.
Cet article propose d'appliquer des modules CNN + auto-attention au lieu de LSTM et d'améliorer la formation du modèle RC avec des paraphrases de passages générées par un modèle neuronal de paraphrase afin d'améliorer la performance RC.
Cet article présente un modèle de compréhension de la lecture utilisant des convolutions et l'attention et propose d'augmenter les données d'entraînement supplémentaires par une paraphrase basée sur une traduction automatique neuronale standard.
Nous présentons les CNN sphériques, un réseau convolutif pour les signaux sphériques, et l'appliquons à la reconnaissance de modèles 3D et à la régression de l'énergie moléculaire.
L'article propose un cadre pour la construction de réseaux convolutifs sphériques basé sur une nouvelle synthèse de plusieurs concepts existants.
Cet article se concentre sur la manière d'étendre les réseaux neuronaux convolutifs pour qu'ils aient une invariance sphérique intégrée, et adapte les outils de l'analyse harmonique non abélienne pour atteindre cet objectif.
Les auteurs développent un nouveau schéma pour représenter les données sphériques à partir de la base.
Une méthode pour effectuer une conception automatisée sur des objets du monde réel tels que des dissipateurs thermiques et des profils d'aile qui utilise des réseaux neuronaux et la descente de gradient.
Réseau neuronal (paramétrage et prédiction) et descente de gradient (rétropropogation) pour concevoir automatiquement des tâches d'ingénierie. 
Cet article présente l'utilisation d'un réseau profond pour approximer le comportement d'un système physique complexe, puis la conception de dispositifs optimaux en optimisant ce réseau par rapport à ses entrées.
 Nous proposons une version duale de la distance adversariale logistique pour l'alignement des caractéristiques et nous montrons qu'elle produit des itérations de gradient plus stables que l'objectif min-max.
L'article traite de la correction des GAN au niveau du calcul.
Cet article étudie une formulation double d'une perte contradictoire basée sur une limite supérieure de la perte logistique, et transforme le problème standard min max de la formation contradictoire en un seul problème de minimisation.
Propose de reformuler l'objectif du point selle du GAN (pour un discriminateur de régression logistique) comme un problème de minimisation en dualisant l'objectif du maximum de vraisemblance pour la régression logistique régularisée.
mise en œuvre de réseaux neuronaux binaires à la pointe de la performance de calcul
L'article présente une bibliothèque écrite en C/CUDA qui comporte toutes les fonctionnalités nécessaires à la propagation vers l'avant des BCNN.
Cet article s'appuie sur Binary-NET et l'étend aux architectures CNN, fournit des optimisations qui améliorent la vitesse de la passe avant, et fournit un code optimisé pour Binary CNN.
Nous proposons un algorithme de sélection de sous-ensembles qui peut être entraîné avec des méthodes basées sur le gradient tout en obtenant des performances quasi optimales par optimisation submodulaire.
Propose un modèle basé sur un réseau de neurones qui intègre une fonction submodulaire en combinant une technique d'optimisation basée sur le gradient avec un cadre submodulaire appelé "Differentiable Greedy Network" (DGN).
propose un réseau neuronal qui vise à sélectionner un sous-ensemble d'éléments (par exemple, la sélection de k phrases qui sont principalement liées à une demande à partir d'un ensemble de documents récupérés).
Nous introduisons l'apprentissage de représentation hiérarchiquement groupé (HCRL), qui optimise simultanément l'apprentissage de représentation et le regroupement hiérarchique dans l'espace d'intégration.
L'article propose d'utiliser le CRP imbriqué comme un modèle de regroupement plutôt que comme un modèle thématique.
Présente une nouvelle méthode de regroupement hiérarchique sur un espace d'intégration où l'espace d'intégration et le regroupement hiérarchique sont appris simultanément.
Nous développons un cadre d'augmentation de l'apprentissage non supervisé statistique-géométrique pour les réseaux neuronaux profonds afin de les rendre robustes aux attaques adverses.
Transformation des réseaux neuronaux profonds traditionnels en calssifieurs robustes adversaires à l'aide de GRN.
propose une défense basée sur des distributions de caractéristiques conditionnelles à la classe pour transformer les réseaux de neurones profonds en classificateurs robustes.
Rendre plus efficace l'apprentissage par renforcement profond dans de grands espaces état-action en utilisant l'exploration structurée avec des politiques hiérarchiques profondes.
Une méthode pour coordonner le comportement d'un agent en utilisant des politiques qui ont une structure latente partagée, une méthode d'optimisation variationnelle des politiques pour optimiser les politiques coordonnées, et une dérivation de la mise à jour variationnelle et hiérarchique des auteurs.
Cet article propose une innovation algorithmique consistant en des variables latentes hiérarchiques pour l'exploration coordonnée dans des environnements multi-agents.
Nous fournissons de nombreuses informations sur la généralisation des réseaux neuronaux à partir du cas linéaire théoriquement traitable.
Les auteurs étudient un modèle simple de réseaux linéaires pour comprendre l'apprentissage par généralisation et transfert.
La normalisation par lots maintient la variance du gradient tout au long de la formation, ce qui stabilise l'optimisation.
Ce document analyse l'effet de la normalisation des lots sur la rétropropagation par gradient dans les réseaux résiduels.
Les jugements comportementaux humains sont utilisés pour obtenir des représentations éparses et interprétables d'objets qui se généralisent à d'autres tâches.
Cet article décrit une expérience à grande échelle sur les représentations objet/sémantique humaines et un modèle de ces représentations.
Cet article développe un nouveau système de représentation d'objets à partir d'un entraînement sur des données collectées à partir de jugements humains impairs d'images.
Une nouvelle approche pour apprendre un espace sémantique clairsemé, positif et interprétable qui maximise les jugements de similarité humains en s'entraînant pour maximiser spécifiquement la prédiction des jugements de similarité humains.
Nous proposons un agent qui se place entre l'utilisateur et un système de réponse aux questions de type boîte noire et qui apprend à reformuler les questions pour obtenir les meilleures réponses possibles.
Cet article propose une réponse active aux questions via une approche d'apprentissage par renforcement qui apprend à reformuler les questions de manière à fournir les meilleures réponses possibles.
décrit clairement comment les chercheurs ont conçu et formé activement deux modèles pour la reformulation des questions et la sélection des réponses pendant les épisodes de réponse aux questions
Apprentissage de prieurs pour les autoencodeurs adversariaux
Propose une extension simple des auto-encodeurs adversariaux pour la génération d'images conditionnelles.
Se concentre sur les auto-codeurs adversaires et introduit un réseau de générateurs de codes pour transformer une antériorité simple en une antériorité qui, avec le générateur, peut mieux s'adapter à la distribution des données.
Génération de texte à l'aide d'enchâssements de phrases à partir de vecteurs Skip-Thought avec l'aide de réseaux adversariaux génératifs.
Décrit l'application des réseaux adversariens génératifs pour la modélisation des données textuelles à l'aide de vecteurs de pensées et d'expériences avec différentes saveurs de GAN pour deux ensembles de données différents.
Présente une estimation du gradient en ligne, sans biais et facile à mettre en œuvre pour les modèles récurrents.
Les auteurs présentent une nouvelle approche de l'apprentissage en ligne des paramètres des réseaux neuronaux récurrents à partir de longues séquences qui surmonte l'imitation de la rétropropagation tronquée dans le temps.
Cet article aborde l'apprentissage en ligne des RNN de manière raisonnée, et propose une modification de RTRL et l'utilisation d'une approche prospective pour le calcul du gradient.
Super résolution des étiquettes grossières en étiquettes au niveau du pixel, appliquée à l'imagerie aérienne et aux scans médicaux.
Procédé de superrésolution d'étiquettes de segmentation grossières à basse résolution si la distribution conjointe des étiquettes à basse et haute résolution est connue.
Nous proposons une méthode pour aligner les caractéristiques latentes apprises à partir de différents ensembles de données en utilisant les corrélations harmoniques.
Propose d'utiliser les correspondances des caractéristiques pour préformer l'alignement des collecteurs entre les lots de données provenant des mêmes échantillons afin d'éviter la collecte de mesures bruitées.
L'évolution de la forme du corps des agents contrôlés par RL améliore leurs performances (et favorise l'apprentissage).
Algorithme PEOM qui incorpore la valeur de Shapley pour accélérer l'évolution en identifiant la contribution de chaque partie du corps.
Associer la récompense à la motivation intrinsèque pour éviter les états catastrophiques et atténuer l'oubli catastrophique.
Un algorithme RL qui combine l'algorithme DQN avec un modèle de peur entraîné en parallèle pour prédire les états catastrophiques.
L'article étudie l'oubli catastrophique en RL, en mettant l'accent sur les tâches où un DQN est capable d'apprendre à éviter les événements catastrophiques tant qu'il évite l'oubli.
Un nouvel opérateur de convolution pour l'apprentissage automatique de la représentation à l'intérieur de la boule unitaire
Ce travail est lié aux récents articles sur les réseaux sphériques CNN et SE(n) équivariants et étend les idées précédentes aux données volumétriques dans la boule unitaire.
Propose d'utiliser des convolutions volumétriques sur des réseaux de convolutions afin d'apprendre la balle unitaire et discute de la méthodologie et des résultats du processus.
Nous formons des politiques d'apprentissage par renforcement en utilisant l'augmentation de la récompense, l'apprentissage par programme et le méta-apprentissage pour naviguer avec succès dans les pages Web.
Développe une méthode d'apprentissage par curriculum pour former un agent RL à la navigation sur le web, basée sur l'idée de décomposer une instruction en de multiples sous-instructions.
Classification interlinguistique des textes par codage universel
Cet article propose une approche de la classification de textes multilingues par l'utilisation de corpus comparables.
Apprentissage d'enchâssements interlinguistiques et apprentissage d'un classificateur utilisant des données étiquetées dans la langue source pour aborder l'apprentissage d'un catégoriseur de texte interlinguistique sans informations étiquetées dans la langue cible.
Dans ce travail, nous proposons des auto-encodeurs récursifs intérieurs-extérieurs profonds (DIORA), une méthode entièrement non supervisée pour découvrir la syntaxe tout en apprenant simultanément des représentations pour les constituants découverts. 
Un modèle neuronal d'arbre latent entraîné avec un objectif d'auto-encodage qui atteint l'état de l'art sur l'analyse syntaxique non supervisée des constituants et capture la structure syntaxique mieux que d'autres modèles d'arbre latent.
L'article propose un modèle pour l'analyse syntaxique non supervisée des dépendances (induction d'arbres latents) qui est basé sur une combinaison de l'algorithme inside-outside avec la modélisation neuronale (auto-encodeurs récursifs). 
Nous étudions le biais de l'objectif de méta-optimisation à court terme.
Cet article propose un modèle et un problème simplifiés pour démontrer le biais à court terme de la méta-optimisation du taux d'apprentissage.
Cet article étudie la question de la rétropropagation tronquée pour la méta-optimisation par le biais d'un certain nombre d'expériences sur un problème fictif.
une manière hiérarchique et compositionnelle de générer des légendes
Cet article présente une méthode plus interprétable pour le sous-titrage des images.
Nous développons une nouvelle mesure de complexité topologique pour les réseaux neuronaux profonds et démontrons qu'elle capture leurs propriétés saillantes.
Cet article propose la notion de persistance neuronale, une mesure topologique permettant d'attribuer des scores aux couches entièrement connectées d'un réseau neuronal.
L'article propose d'analyser la complexité d'un réseau neuronal en utilisant son homologie persistante zéro-ième.
L'examen des frontières de décision autour d'une entrée vous donne plus d'informations qu'un petit voisinage fixe.
Les auteurs présentent une nouvelle attaque pour générer des exemples contradictoires. Ils attaquent les classificateurs créés par la classification aléatoire de petites perturbations L2.
Une nouvelle approche pour générer des attaques adverses contre un réseau neuronal, et une méthode pour défendre un réseau neuronal contre ces attaques.
Nous entraînons un réseau neuronal à produire des poids approximativement optimaux en fonction des hyperparamètres.
Hyper-réseaux pour l'optimisation des hyper-paramètres dans les réseaux neuronaux.
Estimation de la matrice de covariance des actifs financiers avec des modèles à variables latentes à processus gaussien
Illustre comment le modèle de processus gaussien à variables latentes (GP-LVM) peut remplacer les modèles factoriels linéaires classiques pour l'estimation des matrices de covariance dans les problèmes d'optimisation de portefeuille.
Cet article utilise des GPLVM standard pour modéliser la structure de covariance et une représentation d'espace latent des séries chronologiques financières du S&P500, afin d'optimiser les portefeuilles et de prédire les valeurs manquantes.
Ce document propose d'utiliser un GPLVM pour modéliser les rendements financiers.
Nous introduisons l'apprentissage méta-adversarial, une nouvelle technique pour régulariser les GAN, et proposons une méthode d'apprentissage en contrôlant explicitement la distribution de la sortie du discriminateur.
L'article propose un apprentissage contradictoire de régularisation de la variance pour l'entraînement des GAN afin de garantir que le gradient du générateur ne disparaisse pas.
Un agent d'apprentissage par renforcement profond avec un bruit paramétrique ajouté à ses poids peut être utilisé pour faciliter une exploration efficace.
Cet article présente les NoisyNets, des réseaux neuronaux dont les paramètres sont perturbés par une fonction de bruit paramétrique, qui obtiennent une amélioration substantielle des performances par rapport aux algorithmes d'apprentissage par renforcement profond de base.
Nouvelle méthode d'exploration pour les RL profonds en injectant du bruit dans les poids des réseaux profonds, le bruit pouvant prendre différentes formes.
"Active Neural Localizer", un réseau neuronal entièrement différentiable qui apprend à localiser efficacement en utilisant l'apprentissage par renforcement profond.
Cet article formule le problème de la localisation sur une carte connue à l'aide d'un réseau de croyances comme un problème de RL où le but de l'agent est de minimiser le nombre d'étapes pour se localiser.
Il s'agit d'un article clair et intéressant qui construit un réseau paramétré pour sélectionner les actions d'un robot dans un environnement simulé.
Nous développons un algorithme de formation pour les modèles de traduction automatique non autorégressifs, qui permet d'obtenir une précision comparable à celle des modèles de base fortement autorégressifs, mais qui est plus rapide d'un ordre de grandeur dans l'inférence.  
Distille des connaissances à partir des états cachés intermédiaires et des poids d'attention pour améliorer la traduction automatique neuronale non autorégressive.
Propose de tirer parti d'un modèle autorégressif bien entraîné pour informer les états cachés et l'alignement des mots des modèles non autorégressifs de traduction automatique neuronale.
En utilisant des opérations morphologiques (dilatation et érosion), nous avons défini une classe de réseau qui peut approximer toute fonction continue. 
Cet article propose de remplacer les unités standard RELU/tanh par une combinaison d'opérations de dilatation et d'érosion, en observant que le nouvel opérateur crée plus d'hyperplans et a plus de pouvoir expressif.
Les auteurs présentent Morph-Net, un réseau neuronal à une seule couche où la mise en correspondance est effectuée à l'aide de la dilatation et de l'érosion morphologiques.
Ce travail fait progresser la compression des DNN au-delà des poids vers les activations en intégrant l'élagage des activations avec l'élagage des poids. 
Une méthode de compression intégrale du modèle qui gère à la fois l'élagage des poids et des activations, ce qui permet un calcul plus efficace du réseau et une réduction effective du nombre de multiplications et d'accumulations.
Cet article présente une nouvelle approche pour réduire le coût de calcul des réseaux neuronaux profonds en intégrant l'élagage des activations avec l'élagage des poids et montre que les techniques courantes d'élagage exclusif des poids augmentent le nombre d'activations non nulles après ReLU.
Nous proposons une méthode simple pour entraîner les encodeurs automatiques variationnels (VAE) avec des représentations latentes discrètes, en utilisant l'échantillonnage par importance.
Introduction d'une distribution d'échantillonnage par importance et utilisation d'échantillons de la distribution pour calculer l'estimation du gradient pondéré par importance
Cet article propose d'utiliser l'échantillonnage important pour optimiser le VAE avec des variables latentes discrètes.
Un nouvel algorithme SGD asynchrone distribué qui atteint une précision de pointe sur les architectures existantes sans réglage ou surcharge supplémentaire.
Propose une amélioration des approches ASGD existantes à l'échelle moyenne en utilisant le momentum avec SGD pour la formation asynchrone à travers un pool de travailleurs distribués.
Cet article aborde le problème de la stagnation du gradient par rapport aux performances parallèles dans l'apprentissage profond distribué, et propose une approche pour estimer les futurs paramètres du modèle aux esclaves afin de réduire les effets de latence de communication.
Nous proposons un nouvel algorithme d'apprentissage des réseaux neuronaux profonds, qui débloque la dépendance des couches de la rétropropagation.
Un paradigme de formation alternatif pour les DNI dans lequel le module auxiliaire est formé pour se rapprocher directement de la sortie finale du modèle original, offrant des avantages secondaires.
Décrit une méthode d'entraînement de réseaux neuronaux sans verrouillage des mises à jour.
Fournit une version non biaisée de la rétropropagation tronquée en échantillonnant les longueurs de troncature et en les repondérant en conséquence.
Propose des méthodes de détermination stochastique des points de troncature dans la rétro-propagation dans le temps.
Une nouvelle approximation de la rétropropagation dans le temps pour surmonter les charges de calcul et de mémoire qui surviennent lorsqu'on doit apprendre à partir de longues séquences.
attaque non ciblée et ciblée sur GCN en ajoutant de faux nœuds
Les auteurs proposent une nouvelle technique contradictoire permettant d'ajouter de "faux" nœuds pour tromper un classificateur basé sur le GCN.
Apprentissage par transfert pour les séquences via l'apprentissage de l'alignement des informations au niveau cellulaire entre les domaines.
L'article propose d'utiliser le RNN/LSTM avec alignement de collocation comme méthode d'apprentissage de la représentation pour l'apprentissage par transfert/adaptation de domaine en PNL.
Nous formulons l'incertitude du modèle dans l'apprentissage par renforcement sous la forme d'un processus de décision de Markov adaptatif de Bayes continu et présentons une méthode d'optimisation de politique bayésienne pratique et évolutive.
L'utilisation d'une approche bayésienne permet d'obtenir un meilleur compromis entre l'exploration et l'exploitation en RL.
Nous soutenons que les benchmarks GAN doivent exiger un grand échantillon du modèle pour pénaliser la mémorisation et nous cherchons à savoir si les divergences des réseaux neuronaux ont cette propriété.
Les auteurs proposent un critère d'évaluation de la qualité des échantillons produits par un Réseau Adversarial Génératif.
génération de dialogue dans un domaine ouvert avec des actes de dialogue
Les auteurs utilisent une technique de supervision à distance pour ajouter des balises d'actes de dialogue comme facteur de conditionnement pour générer des réponses dans des dialogues à domaine ouvert.
L'article décrit une technique permettant d'incorporer des actes de dialogue dans des agents conversationnels neuronaux.
Notre hypothèse est qu'étant donné deux domaines, la cartographie la moins complexe qui présente une faible divergence se rapproche de la cartographie cible.
L'article aborde le problème de l'apprentissage de mappings entre différents domaines sans aucune supervision, en énonçant trois conjectures.
Démontre qu'en apprentissage non supervisé sur des données non alignées, il est possible d'apprendre la correspondance entre domaines en utilisant uniquement le GAN sans perte de reconstruction.
Nous affinons les résultats de sur-approximation des vérificateurs incomplets en utilisant des solveurs MILP pour prouver des propriétés de robustesse supérieures à l'état de l'art. 
Présente un vérificateur qui permet d'améliorer la précision des vérificateurs incomplets et l'évolutivité des vérificateurs complets en utilisant la surparamétrisation, la programmation linéaire en nombres entiers mixtes et la relaxation de la programmation linéaire.
Une stratégie mixte pour obtenir une meilleure précision sur les vérifications de la robustesse des réseaux neuronaux feed-forward avec des fonctions d'activation linéaires par morceaux, en obtenant une meilleure précision que les vérificateurs incomplets et plus d'évolutivité que les vérificateurs complets.
Les HMMs sont-ils un cas particulier des RNNs ? Nous étudions une série de transformations architecturales entre les HMM et les RNN, à la fois par des dérivations théoriques et par une hybridation empirique, et nous fournissons de nouvelles perspectives.
Cet article examine si les HMM sont un cas particulier de RNN en utilisant la modélisation du langage et le balisage POS.
Nous proposons une nouvelle méthode de régularisation qui pénalise la covariance entre les dimensions des couches cachées d'un réseau.
Cet article présente un mécanisme de régularisation qui pénalise la covariance entre toutes les dimensions de la représentation latente d'un réseau neuronal afin de démêler la représentation latente.
Le système proposé imite le processus de classification médiatisé par une série de prélèvements à une composante.
Une méthode pour augmenter la précision des réseaux profonds sur des tâches de classification multi-classes, apparemment par une réduction de la classification multi-classes en classification binaire.
Une nouvelle procédure de classification basée sur le discernement, la réponse maximale et la vérification multiple pour améliorer la précision des réseaux médiocres et améliorer les réseaux à action directe.
L'expérience montre que les modèles plus grands s'entraînent en moins d'étapes d'entraînement, car tous les facteurs de la traversée de l'espace de poids s'améliorent.
Cet article montre que les RNN plus larges améliorent la vitesse de convergence lorsqu'ils sont appliqués à des problèmes de PNL et, par extension, l'effet de l'augmentation des largeurs dans les réseaux neuronaux profonds sur la convergence de l'optimisation.
Cet article caractérise l'impact de la sur-paramétrisation sur le nombre d'itérations nécessaires à un algorithme pour converger, et présente d'autres observations empiriques sur les effets de la sur-paramétrisation dans la formation des réseaux neuronaux.
Réseaux de pointeurs à têtes multiples pour l'apprentissage conjoint afin de localiser et de réparer les bogues d'utilisation de la variable
Propose un modèle basé sur LSTM avec des pointeurs pour décomposer le problème de VarMisuse en plusieurs étapes.
Cet article présente un modèle basé sur LSTM pour la détection et la réparation du bug VarMisuse, et démontre des améliorations significatives par rapport aux approches précédentes sur plusieurs ensembles de données.
Classification de type humain avec des CNN
L'article valide l'idée que les réseaux neuronaux convolutifs profonds pourraient apprendre à regrouper les données d'entrée mieux que d'autres méthodes de regroupement en notant leur capacité à interpréter le contexte de chaque point d'entrée grâce à un large champ de vision.
Ce travail combine l'apprentissage profond pour la représentation des caractéristiques avec la tâche de regroupement non supervisé de type humain.
Nous développons deux algorithmes à complexité linéaire pour l'interprétation de modèles diagnostiques basés sur la valeur de Shapley, dans le cas où la contribution des caractéristiques à la cible est bien approchée par une factorisation structurée en graphes.
L'article propose deux approximations de la valeur de Shapley utilisée pour générer des scores de caractéristiques pour l'interprétabilité.
Cet article propose deux méthodes de notation de l'importance des caractéristiques par instance en utilisant les valeurs de Shapely, et fournit deux méthodes efficaces de calcul des valeurs de Shapely approximatives lorsqu'il existe une structure connue des caractéristiques.
Des codes locaux ont été trouvés dans les réseaux neuronaux à action directe.
Une méthode pour déterminer dans quelle mesure les neurones individuels dans une couche cachée d'un MLP codent un code localiste, qui est étudié pour différentes représentations d'entrée.
Étudie le développement de représentations localistes dans les couches cachées des réseaux neuronaux à action directe.
Extension de la modélisation relationnelle pour prendre en charge les données multimodales à l'aide d'encodeurs neuronaux.
Cet article propose d'effectuer la prédiction de liens dans les bases de connaissances en complétant les entités originales par des informations multimodales, et présente un modèle capable d'encoder toutes sortes d'informations lors de la notation des triples.
L'article porte sur l'intégration d'informations provenant de différentes modalités dans les approches de prédiction de liens.
Proposition d'une nouvelle méthode intégrant l'échantillonnage SG-MCMC, l'antériorité de groupe clairsemé et l'élagage de réseau pour apprendre l'ensemble structuré clairsemé (SSE) avec des performances améliorées et un coût considérablement réduit par rapport aux méthodes traditionnelles. 
Les auteurs proposent une procédure permettant de générer un ensemble de modèles structurés épars.
Un nouveau cadre pour la formation de réseaux neuronaux d'ensemble qui utilise les méthodes SG-MCMC dans le cadre de l'apprentissage profond, puis augmente l'efficacité du calcul par la sparsité+élagage de groupe.
Cet article explore l'utilisation de FNN et de LSTM pour rendre la moyenne des modèles bayésiens plus facilement calculable et améliorer la performance moyenne des modèles.
Nouveau cadre pour le méta-apprentissage qui unifie et étend une large classe de méthodes d'apprentissage few-shot existantes. Il permet d'obtenir d'excellentes performances dans les tests d'apprentissage en quelques points sans nécessiter d'inférence itérative au moment du test.   
Ce travail s'attaque à l'apprentissage en quelques coups du point de vue de l'inférence probabiliste, en atteignant l'état de l'art malgré une configuration plus simple que celle de nombreux concurrents.
Définition d'une perte softmax partiellement mutuelle exclusive pour les données positives et mise en œuvre d'un schéma d'échantillonnage basé sur la coopération.
Cet article présente l'échantillonnage par importance coopératif afin de résoudre le problème de l'hypothèse mutuellement exclusive du softmax traditionnel qui est biaisé lorsque les échantillons négatifs ne sont pas explicitement définis.
Cet article propose des méthodes PMES pour assouplir l'hypothèse de résultat exclusif dans la perte softmax, démontrant ainsi le mérite empirique de l'amélioration des modèles d'intégration de type word2vec.
Cadre "enseignant-élève" pour une classification vidéo efficace utilisant moins de trames 
L'article propose une idée pour distiller à partir d'un modèle de classification vidéo complet un petit modèle qui ne reçoit qu'un nombre réduit de trames.
Les auteurs présentent un réseau professeur-étudiant pour résoudre le problème de la classification des vidéos, en proposant des algorithmes d'apprentissage en série et en parallèle visant à réduire les coûts de calcul.
Une vision statistique unifiée de la vaste classe des modèles génératifs profonds 
L'article développe un cadre interprétant les algorithmes GAN comme effectuant une forme d'inférence variationnelle sur un modèle génératif reconstruisant une variable indicatrice de l'appartenance d'un échantillon à la vraie distribution générative des données.
une méthode combinant l'apprentissage de listes de règles et l'apprentissage de prototypes 
Présente un nouveau cadre de prédiction interprétable, qui combine l'apprentissage basé sur des règles, l'apprentissage par prototype et les NN, particulièrement applicable aux données longitudinales.
Cet article vise à remédier au manque d'interprétabilité des modèles d'apprentissage profond et propose l'apprentissage par prototype via des listes de règles (PEARL), qui combine l'apprentissage par règle et l'apprentissage par prototype pour obtenir une classification plus précise et simplifier la tâche d'interprétabilité.
Cet article propose un nouveau réseau adversarial génératif qui est plus stable, plus efficace et qui produit de meilleures images que celles du statu quo. 
Ce document combine Fisher-GAN et Deli-GAN.
Cet article combine Deli-GAN, qui a une distribution préalable de mélange dans l'espace latent, et Fisher GAN, qui utilise Fisher IPM au lieu de JSD comme objectif.
Nous présentons une architecture de réseau multi-capteurs modulaire avec un mécanisme attentionnel qui permet une sélection dynamique des capteurs sur des données bruyantes du monde réel provenant de CHiME-3.
Une architecture neuronale générique capable d'apprendre l'attention qui doit être portée aux différents canaux d'entrée en fonction de la qualité relative de chaque capteur par rapport aux autres.
 Examine l'utilisation de l'attention pour la sélection des capteurs ou des canaux, avec des résultats sur TIDIGITS et GRID montrant un avantage de l'attention sur la concaténation des caractéristiques.
Pour permettre l'entraînement des DNN dans le nuage tout en protégeant simultanément la confidentialité des données, nous proposons d'exploiter les représentations intermédiaires des données, ce qui est réalisé en divisant les DNN et en les déployant séparément sur des plateformes locales et dans le nuage.
Cet article propose une technique pour privatiser les données en apprenant une représentation des caractéristiques qui est difficile à utiliser pour la reconstruction d'images, mais utile pour la classification des images.
GANs récurrents conditionnels pour la génération de séquences médicales à valeur réelle, présentant de nouvelles approches d'évaluation et une analyse empirique de la confidentialité.
propose d'utiliser des données synthétiques générées par des GAN pour remplacer les données personnelles identifiables dans l'entraînement des modèles ML pour les applications sensibles à la vie privée.
Les auteurs proposent une nouvelle architecture GAN récurrente qui génère des séquences de domaine continues, et l'évaluent sur plusieurs tâches synthétiques et une tâche de données chronologiques ICU.
Propose d'utiliser les RGAN et les RCGAN pour générer des séquences synthétiques de données réelles.
Nos études et modèles empiriques fournissent de nouvelles informations précieuses pour les concepteurs qui souhaitent comprendre et contrôler la manière dont les effets d'accentuation seront perçus par les utilisateurs.
Cet article examine quelle mise en évidence visuelle est perçue plus rapidement dans la visualisation de données et comment les différentes méthodes de mise en évidence se comparent les unes aux autres.
Deux études sur l'efficacité des effets d'accentuation, l'une évaluant les niveaux de différences utiles, et l'autre plus appliquée utilisant des visualisations réelles différentes pour une investigation plus écologiquement valide.
Une architecture de raisonnement simple basée sur le réseau de mémoire (MemNN) et le réseau de relations (RN), réduisant la complexité temporelle par rapport au RN et permettant d'obtenir des résultats à la pointe de la technologie pour l'assurance qualité basée sur l'histoire bAbI et le dialogue bAbI.
Introduit le réseau de mémoire connexe (RMN), une amélioration des réseaux de relations (RN).
Nous montrons que la division d'un réseau neuronal en branches parallèles améliore les performances et qu'un couplage approprié des branches améliore encore plus les performances.
Ce travail propose une reconfiguration du modèle CNN existant à la pointe de la technologie en utilisant une nouvelle architecture de branchement, avec de meilleures performances.
Cet article montre les avantages de l'ensemblisme couplé en termes d'économie de paramètres.
Présente une architecture de réseau profond qui traite les données à l'aide de plusieurs branches parallèles et combine les données postérieures de ces branches pour calculer les scores finaux.
Combinaison de l'injection de bruit, de la quantification graduelle et de l'apprentissage par blocage d'activation pour obtenir une quantification de pointe à 3, 4 et 5 bits.
Propose d'injecter du bruit pendant la formation et de fixer les valeurs des paramètres d'une couche ainsi que la sortie d'activation dans la quantification du réseau neuronal.
Procédé de quantification de réseaux neuronaux profonds pour la classification et la régression, utilisant l'injection de bruit, le bridage avec des activations maximales apprises, et la quantification progressive par blocs pour obtenir des performances égales ou supérieures aux méthodes de pointe.
Nous proposons Leap, un cadre qui transfère les connaissances entre les processus d'apprentissage en minimisant la distance attendue que le processus de formation parcourt sur la surface de perte d'une tâche.
L'article propose un nouvel objectif de méta-apprentissage visant à surpasser les approches de pointe lorsqu'il s'agit de collections de tâches qui présentent une diversité inter-tâches substantielle.
Une alternative à l'apprentissage par transfert qui apprend plus rapidement, nécessite beaucoup moins de paramètres (3-13 %), obtient généralement de meilleurs résultats et préserve précisément les performances sur les anciennes tâches.
Modules de contrôle pour l'apprentissage par incrémentation sur des ensembles de données de classification d'images
Nous présentons une technique générale permettant l'inférence à faible précision de 8 bits des réseaux neuronaux convolutifs. 
Cet article conçoit un système pour quantifier automatiquement les modèles CNN prétraités.
Nous proposons d'incorporer des biais inductifs et des opérations provenant de la géométrie hyperbolique pour améliorer le mécanisme d'attention des réseaux neuronaux.
Cet article remplace la similarité du produit scalaire utilisée dans les mécanismes d'attention par la distance hyperbolique négative et l'applique au modèle Transformer existant, aux réseaux d'attention graphique et aux réseaux de relations.
Les auteurs proposent une nouvelle approche pour améliorer l'attention relationnelle en changeant les fonctions de correspondance et d'agrégation pour utiliser une géométrie hyperbolique. 
Cet article démontre comment la théorie de contrôle de l'infinité H peut aider à mieux concevoir des politiques profondes et robustes pour les moteurs de robots.
Propose d'incorporer des éléments de contrôle robuste dans la recherche sur les politiques guidées afin de concevoir une méthode qui résiste aux perturbations et à l'inadéquation des modèles.
L'article présente une méthode d'évaluation de la sensibilité et de la robustesse des politiques de RL profond, et propose une approche de jeu dynamique pour l'apprentissage de politiques robustes.
Analyse de la vulnérabilité des classificateurs aux perturbations universelles et relation avec la courbure de la frontière de décision.
L'article fournit une analyse intéressante liant la géométrie des limites de décision du classificateur à de petites perturbations adverses universelles.
Cet article traite des perturbations universelles - des perturbations qui peuvent induire en erreur un classificateur entraîné si elles sont ajoutées à la plupart des points de données d'entrée.
L'article développe des modèles qui tentent d'expliquer l'existence de perturbations universelles qui trompent les réseaux neuronaux.
Nous proposons une méthode de méta-apprentissage pour la correction interactive des politiques en langage naturel.
Cet article présente un cadre de méta-apprentissage qui montre comment apprendre de nouvelles tâches dans une configuration interactive. Chaque tâche est apprise par une configuration d'apprentissage par renforcement, puis la tâche est mise à jour en observant de nouvelles instructions.
Cet article apprend aux agents à accomplir des tâches via des instructions en langage naturel dans un processus itératif.
Nous étudions la modularité des modèles génératifs profonds.
L'article fournit un moyen d'étudier la structure modulaire du modèle génératif profond, avec le concept clé de distribution sur les canaux des architectures de générateur.
Nous présentons Seq2SQL, qui traduit les questions en requêtes SQL en utilisant les récompenses de l'exécution des requêtes en ligne, et WikiSQL, un ensemble de données de tables/questions/requêtes SQL plusieurs fois plus grand que les ensembles de données existants.
Un nouvel ensemble de données d'analyse sémantique qui se concentre sur la génération de SQL à partir du langage naturel en utilisant un modèle basé sur l'apprentissage par renforcement.
La modélisation du bruit à l'entrée pendant la formation discriminante améliore la robustesse des adversaires. Proposer une métrique d'évaluation basée sur l'ACP pour la robustesse des adversaires.
Cet article propose, ExL, une méthode d'apprentissage contradictoire utilisant un bruit multiplié qui s'avère utile pour se défendre contre les attaques de type boîte noire sur trois ensembles de données.
Cet article inclut un bruit multiplicatif N dans les données d'entraînement afin d'obtenir une robustesse à l'adversité, lors de l'entraînement à la fois sur les paramètres du modèle theta et sur le bruit lui-même.
Une méthode pour répondre à la question "pourquoi pas la classe B ?" pour expliquer les réseaux profonds
L'article propose une approche visant à fournir des explications visuelles contrastives pour les réseaux neuronaux profonds.
Nous analysons l'inversibilité des réseaux neuronaux profonds en étudiant les préimages des couches ReLU et la stabilité de l'inverse.
Cet article étudie le volume de préimage de l'activation d'un réseau ReLU à une certaine couche, et il s'appuie sur la linéarité par morceaux de la fonction forward d'un réseau ReLU. 
Cet article présente une analyse de l'invariance inverse des réseaux ReLU et fournit des limites supérieures sur les valeurs singulières d'un réseau de train.
L'entraînement d'ensembles par des adversaires offre une robustesse aux exemples d'adversité supérieure à celle observée dans les modèles entraînés par des adversaires et les ensembles entraînés indépendamment de ceux-ci.
 Propose d'entraîner un ensemble de modèles conjointement, où à chaque étape temporelle, un ensemble d'exemples qui sont contradictoires pour l'ensemble lui-même est incorporé dans l'apprentissage.
les réseaux de routage : un nouveau type de réseau neuronal qui apprend à router son entrée de manière adaptative pour l'apprentissage multitâche.
L'article suggère d'utiliser un réseau modulaire avec un contrôleur qui prend des décisions, à chaque pas de temps, concernant le prochain nodule à appliquer.
L'article présente une nouvelle formulation pour l'apprentissage de l'architecture optimale d'un réseau neuronal dans un cadre d'apprentissage multi-tâche en utilisant l'apprentissage par renforcement multi-agent pour trouver une politique, et montre une amélioration par rapport aux architectures codées en dur avec des couches partagées.
Nous montrons comment optimiser la norme L_0 attendue des modèles paramétriques avec la descente de gradient et introduisons une nouvelle distribution qui facilite le hard gating.
Les auteurs présentent une approche basée sur le gradient pour minimiser une fonction objective avec une pénalité clairsemée L0 afin de faciliter l'apprentissage de réseaux neuronaux clairsemés.
Nous proposons une nouvelle architecture de réseau neuronal graphique interprétable basée sur l'attention qui surpasse les réseaux neuronaux graphiques actuels dans des ensembles de données de référence standard.
Les auteurs proposent deux extensions des GCN, en supprimant les non-linéarités intermédiaires du calcul des GCN et en ajoutant un mécanisme d'attention dans la couche d'agrégation.
L'article propose un algorithme d'apprentissage semi-supervisé pour la classification des nœuds de graphe, inspiré des réseaux neuronaux graphiques.
Un cadre pour la formation de modèles génératifs basés sur des autoencodeurs, avec des pertes non contradictoires et des architectures de réseaux neuronaux non restreintes.
Cet article utilise des autoencodeurs pour effectuer une correspondance de distribution dans un espace à haute dimension.
Les espaces d'intégration de collecteurs de produits à courbure hétérogène donnent des représentations améliorées par rapport aux espaces d'intégration traditionnels pour une variété de structures.
Propose une méthode de réduction de la dimensionnalité qui incorpore les données dans un collecteur produit de collecteurs sphériques, euclidiens et hyperboliques. L'algorithme est basé sur la correspondance entre les distances géodésiques sur le collecteur produit et les distances des graphes.
Nous intégrons des méthodes symboliques (déductives) et statistiques (basées sur les neurones) pour permettre la synthèse de programmes en temps réel avec une généralisation presque parfaite à partir d'un exemple d'entrée-sortie.
L'article présente une approche de type "branch-and-bound" pour apprendre de bons programmes, dans laquelle un LSTM est utilisé pour prédire quelles branches de l'arbre de recherche devraient conduire à de bons programmes.
Proposition d'un système qui synthétise des programmes à partir d'un seul exemple et dont la généralisation est meilleure que l'état de l'art antérieur.
Nous explorons l'intersection des VAE et du codage clairsemé.
Cet article propose une extension des VAE avec des prieurs et des a posteriori épars pour apprendre des représentations interprétables éparses.
L'élimination progressive et raisonnée des connexions sautées évite la dégradation des réseaux profonds à action directe.
Les auteurs présentent une nouvelle stratégie de formation, VAN, pour la formation de réseaux feed-forward très profonds sans saut de connexion.
L'article présente une architecture qui interpole linéairement entre les ResNets et les réseaux profonds vanille sans sauter de connexions.
Compression de réseaux neuronaux profonds déployés sur un dispositif embarqué. 
Les auteurs présentent un algorithme de formation basé sur le SVRG régularisé l-1 qui est capable de forcer de nombreux poids du réseau à être 0.
Ce travail permet de réduire les besoins en mémoire.
Un algorithme d'apprentissage basé sur le codage prédictif pour construire des modèles de réseaux neuronaux profonds du cerveau
L'article considère l'apprentissage d'un réseau neuronal génératif en utilisant une configuration de codage prédictif.
La reconnaissance d'instances d'objets avec des autoencodeurs adversariaux a été réalisée avec une nouvelle cible "image mentale" qui est une représentation canonique de l'image d'entrée.
L'article propose une méthode d'apprentissage de caractéristiques pour la reconnaissance d'objets qui est invariante à diverses transformations de l'objet, notamment la pose de l'objet.
Cet article s'est penché sur la reconnaissance de quelques clichés par le biais d'une image mentale générée comme représentation intermédiaire de l'image d'entrée.
Combiner la logique temporelle avec l'apprentissage par renforcement hiérarchique pour la composition des compétences
L'article propose une stratégie pour construire un PDM produit à partir d'un PDM original et de l'automate associé à une formule LTL.
Propose de joindre la logique temporelle à l'apprentissage par renforcement hiérarchique pour simplifier la composition des compétences.
Nous proposons un schéma de quantification pour les poids et les activations des réseaux neuronaux profonds. Ce schéma réduit considérablement l'empreinte mémoire et accélère l'inférence.
Compression de modèles CNN et accélération de l'inférence par quantification.
Lorsqu'un robot est déployé dans un environnement dans lequel des humains ont agi, l'état de l'environnement est déjà optimisé en fonction de ce que les humains veulent, et nous pouvons nous en servir pour déduire les préférences humaines.
Les auteurs proposent d'augmenter la fonction de récompense explicite d'un agent RL avec des récompenses/coûts auxiliaires déduits de l'état initial et d'un modèle de la dynamique de l'état.
Ce travail propose une façon de déduire l'information implicite dans l'état initial en utilisant l'IRL et de combiner la récompense déduite avec une récompense spécifiée.
Catégorisation systématique des méthodes de régularisation pour l'apprentissage profond, révélant leurs similitudes.
Tente de construire une taxonomie pour les techniques de régularisation employées dans l'apprentissage profond.
Nous prouvons l'efficacité exponentielle des réseaux neuronaux de type récurrent par rapport aux réseaux peu profonds.
Les auteurs comparent la complexité des réseaux à train tensoriel avec les réseaux structurés par décomposition CP
Un nouveau traitement probabiliste pour le GAN avec une garantie théorique.
Cet article propose un GAN bayésien qui a des garanties théoriques de convergence vers la distribution réelle et met les vraisemblances sur le générateur et le discriminateur avec des logarithmes proportionnels aux fonctions objectives traditionnelles du GAN.
Défense contre les perturbations adverses des réseaux neuronaux à partir de l'hypothèse du collecteur 
Le manuscrit propose deux fonctions objectives basées sur l'hypothèse du collecteur comme mécanismes de défense contre les exemples contradictoires.
Défense contre les attaques adverses basées sur l'hypothèse du manifold des données naturelles
recherche d'une architecture neuronale à un seul coup via l'optimisation parcimonieuse directe
Présente une méthode de recherche d'architecture où les connexions sont supprimées avec une régularisation éparse.
Cet article propose l'optimisation spartiate directe, qui est une méthode permettant d'obtenir des architectures neuronales sur des problèmes spécifiques, à un coût de calcul raisonnable.
Cet article propose une méthode de recherche d'une architecture neuronale basée sur une optimisation directe et éparse.
Obtient une précision de pointe pour les réseaux quantifiés et peu profonds en tirant parti de la distillation. 
propose des modèles de petite taille et peu coûteux en combinant distillation et quantification pour les expériences de vision et de traduction automatique neuronale
Cet article présente un cadre d'utilisation du modèle de l'enseignant pour aider à la compression du modèle d'apprentissage profond dans le contexte de la compression de modèle.
améliorer la NMT avec des arbres latents
Cet article décrit une méthode permettant d'induire des structures de dépendance côté source au service de la traduction automatique neuronale.
Apprenez en travaillant à rebours à partir d'une seule démonstration, même inefficace, et demandez progressivement à l'agent de faire une plus grande partie de la résolution lui-même.
Cet article présente une méthode permettant d'accroître l'efficacité des méthodes RL à récompense clairsemée par un cursus à rebours sur des démonstrations d'experts. 
L'article présente une stratégie pour résoudre des tâches de récompense éparses avec RL en échantillonnant les états initiaux à partir de démonstrations.
Mémoire externe pour l'apprentissage par renforcement en ligne basée sur l'estimation des gradients sur une nouvelle technique d'échantillonnage des réservoirs.
L'article propose une approche modifiée de la RL, où une "mémoire épisodique" supplémentaire est conservée par l'agent et utilise un "réseau d'interrogation" qui se base sur l'état actuel.
Nous réalisons une décomposition biais-variance pour les machines de Boltzmann en utilisant une formulation géométrique de l'information.
L'objectif de cet article est d'analyser l'efficacité et la généralisation de l'apprentissage profond en présentant une analyse théorique de la décomposition biais-variance pour les modèles hiérarchiques, en particulier les machines de Boltzmann.  
L'article arrive à la conclusion principale qu'il est possible de réduire à la fois le biais et la variance dans un modèle hiérarchique.
Combinaison de l'élagage du réseau et des noyaux persistants dans une mise en œuvre pratique, rapide et précise du réseau.
Cet article présente les RNN persistants épars, un mécanisme permettant d'ajouter l'élagage aux travaux existants consistant à stocker les poids des RNN sur une puce.
Nous présentons une nouvelle méthode d'élagage et un format de matrice clairsemé pour permettre un taux de compression d'index élevé et un processus de décodage d'index parallèle.
Les auteurs utilisent le codage de Viterbi pour compresser de façon spectaculaire l'index de la matrice éparse d'un réseau élagué, réduisant ainsi l'une des principales surcharges de mémoire et accélérant l'inférence dans le cadre parallèle.
Un nouveau réseau de politiques hiérarchiques qui peut réutiliser des compétences précédemment acquises en même temps que de nouvelles compétences et en tant que sous-composantes de celles-ci, en découvrant les relations sous-jacentes entre les compétences.
Cet article vise à apprendre des politiques hiérarchiques en utilisant une structure de politique récursive régulée par une grammaire temporelle stochastique.
Cet article propose une approche de l'apprentissage des politiques hiérarchiques dans un contexte d'apprentissage tout au long de la vie en empilant les politiques et en utilisant ensuite une politique explicite de "basculement".
Nous proposons d'utiliser la projection de formules algébriques vectorielles explicites comme moyen alternatif de visualiser les espaces d'encastrement spécifiquement adaptés aux tâches d'analyse orientées vers un but précis et elle surpasse t-SNE dans notre étude auprès des utilisateurs.
Analyse des psaces d'encastrement de manière non paramétrique (basée sur des exemples)
Nous proposons une approche fondée sur des principes qui confère aux classificateurs la capacité de résister à des variations plus importantes entre les données d'entraînement et les données d'essai de manière intelligente et efficace.
Utilisation de l'apprentissage introspectif pour gérer les variations de données au moment du test
Cet article suggère l'utilisation de réseaux de transformation appris, intégrés dans des réseaux introspectifs pour améliorer les performances de classification avec des exemples synthétisés.
La discrétisation de l'entrée conduit à la robustesse contre les exemples adverses.
Les auteurs présentent une étude approfondie de la discrétisation/quantification de l'entrée comme défense contre les exemples adverses.
nous avons prouvé l'existence de limites indépendantes de la dimension pour les algorithmes d'apprentissage de faible précision.
Cet article discute des conditions dans lesquelles la convergence des modèles d'entraînement avec des poids de faible précision ne dépend pas de la dimension du modèle.
Modifications du MAML et du RL2 qui devraient permettre une meilleure exploration. 
L'article propose une astuce pour étendre les fonctions objectives afin de piloter l'exploration dans le méta-RL, en plus de deux algorithmes récents de méta-RL
Un modèle symbolique neuronal probabiliste avec un espace de programme latent, pour des réponses aux questions plus interprétables
Cet article propose un modèle de variable latente discrète et structurée pour la réponse à des questions visuelles qui implique une généralisation et un raisonnement compositionnels avec un gain significatif de performance et de capacité.
Nous utilisons la vérification formelle pour évaluer l'efficacité des techniques de recherche d'exemples contradictoires ou de défense contre les exemples contradictoires.
Cet article propose une méthode pour calculer des exemples contradictoires avec une distance minimale par rapport aux entrées originales.
Les auteurs proposent d'utiliser des exemples à distance minimale prouvée comme outil pour évaluer la robustesse d'un réseau entraîné.
L'article décrit une méthode pour générer des exemples contradictoires qui ont une distance minimale par rapport à l'exemple d'entraînement utilisé pour les générer.
Le récupérateur de paragraphes et le lecteur automatique interagissent grâce à l'apprentissage par renforcement, ce qui permet d'obtenir de grandes améliorations sur des ensembles de données en domaine ouvert.
L'article présente un nouveau cadre d'interaction bidirectionnelle entre le récupérateur de documents et le lecteur pour répondre à des questions dans un domaine ouvert, avec l'idée d'un "état du lecteur" du lecteur au récupérateur.
L'article propose un modèle de lecture automatique extractive multi-documents composé de 3 parties distinctes et d'un algorithme.
Présente une nouvelle architecture qui exploite le pouvoir de globalisation de l'information des u-nets dans des réseaux plus profonds et qui donne de bons résultats dans toutes les tâches sans aucun artifice.
Une architecture de réseau pour la segmentation sémantique d'images, basée sur la composition d'une pile d'architectures U-Net de base, qui réduit le nombre de paramètres et améliore les résultats.
Cette étude propose une architecture U-Net empilée pour la segmentation d'images.
Nous proposons un réseau neuronal capable de générer des questions spécifiques à un sujet.
Présente une approche basée sur un réseau neuronal pour générer des questions spécifiques à un sujet, en partant du principe que les questions thématiques sont plus significatives dans les applications pratiques.
propose une méthode de génération basée sur les thèmes en utilisant un LSTM pour extraire les thèmes en utilisant une technique de codage en deux étapes.
Nous mettons en œuvre un réseau d'adaptation de domaine contradictoire pour stabiliser une interface cerveau-machine fixe contre les changements progressifs des signaux neuronaux enregistrés.
Décrit une nouvelle approche pour l'interface cerveau-machine implantée afin de résoudre les problèmes de calibration et de décalage des covariables. 
Les auteurs définissent un IMC qui utilise un auto-codeur et abordent ensuite le problème de la dérive des données dans l'IMC.
Analyser et comprendre comment les agents des réseaux neuronaux apprennent à comprendre un langage simple et ancré dans le sol.
Les auteurs relient les méthodes expérimentales psychologiques à la compréhension de la façon dont la boîte noire des méthodes d'apprentissage profond résout les problèmes.
Cet article présente une analyse des agents qui apprennent le langage de base par apprentissage par renforcement dans un environnement simple qui combine des instructions verbales et des informations visuelles.
Apprendre les parties d'un objet, sa structure hiérarchique et sa dynamique en observant son mouvement.
Propose un modèle d'apprentissage non supervisé qui apprend à démêler les objets en parties, à prédire la structure hiérarchique des parties et, sur la base des parties démêlées et de la hiérarchie, à prédire le mouvement.
Nous développons une compréhension des techniques efficaces en termes de ressources sur la super-résolution.
L'article propose une évaluation empirique détaillée des compromis obtenus par différents réseaux neuronaux convolutifs sur le problème de la super résolution.
Cet article propose d'améliorer l'efficacité des ressources du système pour les réseaux à super résolution.
Nous étudions les réseaux ReLU dans le domaine de Fourier et démontrons un comportement particulier.
Analyse de Fourier du réseau ReLU, qui montre qu'il est biaisé vers l'apprentissage des basses fréquences. 
Cet article présente des contributions théoriques et empiriques sur le thème des coefficients de Fourier des réseaux neuronaux.
L'article propose d'utiliser des distributions de probabilité au lieu de points pour les tâches d'intégration d'instances telles que la reconnaissance et la vérification.
L'article propose une alternative à l'incorporation de points actuelle et une technique pour les former.
L'article propose un modèle utilisant des inscriptions incertaines pour étendre l'apprentissage profond aux applications bayésiennes.
Nous proposons des couches de contraction tensorielle et de régression tensorielle à faible rang afin de préserver et d'exploiter la structure multi-linéaire dans l'ensemble du réseau, ce qui permet de réaliser d'énormes économies d'espace avec peu ou pas d'impact sur les performances.
Cet article propose de nouvelles architectures de couches de réseaux neuronaux utilisant une représentation à faible rang des tenseurs.
Cet article intègre la décomposition tensorielle et la régression tensorielle dans le CNN en utilisant une nouvelle couche de régression tensorielle.
Nous utilisons des dictionnaires bilingues pour augmenter les données de la traduction automatique neuronale.
Cet article étudie l'utilisation de dictionnaires bilingues pour créer des sources synthétiques de données monolingues côté cible afin d'améliorer les modèles NMT entraînés avec de petites quantités de données parallèles.
Nous proposons un nouveau modèle de curiosité basé sur la mémoire épisodique et les idées d'atteignabilité qui nous permet de surmonter les problèmes connus de "couch-potato" des travaux antérieurs.
Propose de donner des bonus d'exploration dans les algorithmes RL en donnant des bonus plus importants aux observations qui sont plus éloignées dans les étapes de l'environnement.
Les auteurs proposent un bonus d'exploration destiné à faciliter les problèmes de récompense éparse en RL et envisagent de nombreuses expériences sur des environnements 3D complexes.
Nous introduisons un nouveau jeu de données d'implications logiques dans le but de mesurer la capacité des modèles à capturer et à exploiter la structure des expressions logiques dans le cadre d'une tâche de prédiction d'implication.
L'article propose un nouveau modèle pour utiliser des modèles profonds afin de détecter l'implication logique comme un produit de fonctions continues sur des mondes possibles.
Propose un nouveau modèle conçu pour l'apprentissage automatique avec prédiction de l'implication logique.
L'article porte sur une nouvelle méthode d'apprentissage incrémental efficace sur le plan énergétique.
Propose une procédure d'apprentissage incrémental comme apprentissage par transfert.
L'article présente une méthode pour former des réseaux neuronaux convolutifs profonds de manière incrémentielle, dans laquelle les données sont disponibles par petits lots sur une période donnée.
Présente une approche de l'apprentissage incrémental par classe à l'aide de réseaux profonds en proposant trois stratégies d'apprentissage différentes dans l'approche finale/meilleure.
utiliser le balayage parallèle pour paralléliser les réseaux neuronaux récurrents linéaires. entraîner le modèle sur une longueur de 1 million de dépendances
Propose d'accélérer le RNN en appliquant la méthode de Blelloch.
Les auteurs proposent un algorithme parallèle pour les RNN de substitution linéaire, qui produit des accélérations par rapport aux implémentations existantes de Quasi-RNN, SRU et LSTM.
GAN en langage naturel pour remplir les blancs
Cet article propose de générer du texte à l'aide de GANs.
Génération d'échantillons de texte à l'aide de GAN et d'un mécanisme permettant de compléter les mots manquants en fonction du texte environnant.
Comparaison des représentations psychophysiques et des représentations de texture codées par CNN dans une application de détection de nouveauté par réseau neuronal à une classe.
Cet article se concentre sur la détection de la nouveauté et montre que les représentations psychophysiques peuvent surpasser les caractéristiques du codeur VGG dans certaines parties de cette tâche.
Cet article considère la détection des anomalies dans les textures et propose une fonction de perte originale.
Propose l'entraînement de deux détecteurs d'anomalies à partir de trois modèles différents pour détecter les anomalies perceptives dans les textures visuelles.
Nous proposons un nouveau type d'approche de régularisation qui encourage le non-recouvrement dans l'apprentissage de la représentation, dans le but d'améliorer l'interprétabilité et de réduire le surajustement.
L'article introduit un régularisateur matriciel pour induire simultanément la sparsité et l'orthogonalité approximative.
L'article étudie une méthode de régularisation pour promouvoir l'éparpillement et réduire le chevauchement entre les supports des vecteurs de poids dans les représentations apprises afin d'améliorer l'interprétabilité et d'éviter le surajustement.
L'article propose une nouvelle approche de régularisation qui encourage simultanément les vecteurs de poids (W) à être clairsemés et orthogonaux les uns par rapport aux autres.
Il prouve que les mécanismes de déclenchement sont invariants par rapport aux transformations temporelles. Introduit et teste une nouvelle initialisation pour les LSTMs à partir de cette idée.
L'article établit un lien entre le concept de réseau récurrent et son effet sur la façon dont le réseau réagit aux transformations temporelles, et l'utilise pour développer un schéma simple d'initialisation du biais.
Nous proposons et vérifions l'efficacité de l'apprentissage pour enseigner, un nouveau cadre pour guider automatiquement le processus d'apprentissage automatique.
Cet article se concentre sur l'"enseignement automatique" et propose de tirer parti de l'apprentissage par renforcement en définissant la récompense comme la vitesse d'apprentissage de l'apprenant et en utilisant le gradient de politique pour mettre à jour les paramètres de l'enseignant.
Les auteurs définissent un modèle d'apprentissage profond composé de quatre éléments : un modèle d'élève, un modèle d'enseignant, une fonction de perte et un ensemble de données. 
Suggère un cadre "apprendre à enseigner", correspondant à des choix sur les données présentées à l'apprenant.
Une perte différentiable pour les contraintes logiques pour l'entraînement et l'interrogation des réseaux de neurones.
Un cadre pour transformer les requêtes sur les paramètres et les paires d'entrée et de sortie des réseaux neuronaux en fonctions de perte différentiables et un langage déclaratif associé pour spécifier ces requêtes.
Cet article aborde le problème de la combinaison des approches logiques avec les réseaux neuronaux en traduisant une formule logique en une fonction de perte non négative pour un réseau neuronal.
Approche basée sur les algorithmes génétiques pour l'optimisation des politiques de réseaux neuronaux profonds
Les auteurs présentent un algorithme d'entraînement d'ensembles de réseaux de politiques qui mélange régulièrement les différentes politiques de l'ensemble.
Cet article propose une méthode d'optimisation des politiques inspirée des algorithmes génétiques, qui imite les opérateurs de mutation et de croisement sur les réseaux de politiques.
Un cadre de principe pour la quantification de modèles utilisant la méthode du gradient proximal, avec une évaluation empirique et des analyses de convergence théoriques.
Propose la méthode ProxQuant pour entraîner les réseaux neuronaux avec des poids quantifiés.
Propose de résoudre les réseaux binaires et ses variantes en utilisant la descente proximale du gradient.
Les "mauvais" minima locaux disparaissent dans un réseau neuronal multicouche : une preuve avec des hypothèses plus raisonnables qu'auparavant
Dans les réseaux à une seule couche cachée, le volume des minima locaux sous-optimaux diminue de manière exponentielle par rapport aux minima globaux.
Cet article vise à répondre à la question de savoir pourquoi les algorithmes standard basés sur le SGD sur les réseaux neuronaux convergent vers de "bonnes" solutions.
Nous proposons une méthode d'attaque invariante de l'attention pour générer des exemples adverses plus transférables pour les attaques de type boîte noire, qui peuvent tromper les défenses de pointe avec un taux de réussite élevé.
L'article propose une nouvelle façon de surmonter les défenses de l'état de l'art contre les attaques adverses sur CNN.
Cet article suggère que le "déplacement de l'attention" est une propriété clé qui explique l'échec du transfert des attaques adverses et propose une méthode d'attaque invariante en fonction de l'attention.
Comment former 100 000 classes sur un seul GPU ?
Propose une méthode de hachage efficace MACH pour l'approximation softmax dans le contexte d'un grand espace de sortie, qui économise à la fois la mémoire et le calcul.
Une méthode de classification pour les problèmes impliquant un grand nombre de classes dans un cadre multi-classes démontrée sur les jeux de données ODP et Imagenet-21K
L'article présente un schéma basé sur le hachage pour réduire la mémoire et le temps de calcul pour la classification à K voies lorsque K est grand.
Nous présentons une méthode générale d'estimation sans biais des gradients de fonctions boîte noire de variables aléatoires. Nous appliquons cette méthode à l'inférence variationnelle discrète et à l'apprentissage par renforcement. 
Suggère une nouvelle approche de la descente de gradient pour l'optimisation de la boîte noire ou l'entraînement de modèles à variables latentes discrètes.
Nous proposons un estimateur de la taille du support de la distribution apprise des GANs pour montrer qu'ils souffrent effectivement de l'effondrement des modes, et nous prouvons que les GANs codeurs-décodeurs n'évitent pas non plus ce problème.
L'article tente d'estimer expérimentalement la taille du support des solutions produites par des GANs typiques. 
Cet article propose un nouveau test astucieux basé sur le paradoxe d'anniversaire pour mesurer la diversité dans un échantillon généré. Les résultats de l'expérience sont interprétés comme signifiant que l'effondrement des modes est fort dans un certain nombre de modèles génératifs de pointe.
L'article utilise le paradoxe de l'anniversaire pour montrer que certaines architectures GAN génèrent des distributions avec un support assez faible.
Nous proposons un cadre pour générer des adversaires naturels contre les classificateurs boîte noire pour les domaines visuels et textuels, en effectuant la recherche d'adversaires dans l'espace sémantique latent.
Propose une méthode pour la création d'exemples d'adversaires sémantiques.
propose un cadre pour générer des exemples contradictoires naturels en recherchant des adversaires dans un espace latent de représentation de données denses et continues. 
Nous étendons la méthode K-FAC aux RNN en développant une nouvelle famille d'approximations de Fisher.
Les auteurs étendent la méthode K-FAC aux RNN et présentent 3 façons d'approximer F, en montrant des résultats d'optimisation sur 3 ensembles de données, qui surclassent ADAM à la fois en nombre de mises à jour et en temps de calcul.
Propose d'étendre la méthode d'optimisation de la courbure appropriée du facteur Kronecker au cadre des réseaux neuronaux récurrents.
Les auteurs présentent une méthode de second ordre spécialement conçue pour les RNN.
Le modèle génératif pour les noyaux des réseaux neuronaux convolutifs, qui agit comme une distribution préalable lors de la formation sur de nouveaux ensembles de données.
Une méthode de modélisation des réseaux neuronaux convolutifs utilisant une méthode de Bayes.
Propose le "deep weight prior" : l'idée est d'obtenir un prior sur un ensemble de données auxiliaire et ensuite d'utiliser ce prior sur les filtres CNN pour démarrer l'inférence pour un ensemble de données d'intérêt.
Cet article explore l'apprentissage de priorités informatives pour les modèles de réseaux neuronaux convolutifs avec des domaines de problèmes similaires en utilisant des autoencodeurs pour obtenir une priorité expressive sur les poids filtrés des réseaux formés.
Nous avons appliqué des techniques d'apprentissage profond à la segmentation d'images hyperspectrales et à l'échantillonnage itératif de caractéristiques.
Proposition d'un schéma gourmand pour sélectionner un sous-ensemble de caractéristiques spectrales hautement corrélées dans une tâche de classification.
Cet article explore l'utilisation des réseaux neuronaux pour la classification et la segmentation de l'imagerie hyperspectrale (HSI) des cellules.
Classification des cellules et mise en œuvre de la segmentation cellulaire basée sur des techniques d'apprentissage profond avec réduction des caractéristiques d'entrée.
Nous avons créé un nouvel ensemble de données pour l'interprétation des données sur les parcelles et proposons également une ligne de base pour la même chose.
Les auteurs proposent un pipeline pour résoudre le problème DIP impliquant l'apprentissage à partir de jeux de données contenant des triplets de la forme {plot, question, réponse}.
Propose un algorithme capable d'interpréter les données présentées dans les graphiques scientifiques.
Amélioration des réseaux neuronaux récurrents à état prédictif par des caractéristiques aléatoires orthogonales
Propose d'améliorer les performances des réseaux neuronaux récurrents à états prédictifs en considérant des caractéristiques aléatoires orthogonales.
Cet article aborde le problème de la formation des réseaux de neurones récurrents à état prédictif et apporte deux contributions.
Nous proposons l'apprentissage pondéré par la fidélité, une approche maître-élève semi-supervisée pour la formation de réseaux neuronaux utilisant des données faiblement étiquetées.
Cet article propose une approche pour l'apprentissage avec une faible supervision en utilisant un ensemble de données propres et un ensemble de données bruitées et en supposant un enseignant et des réseaux d'étudiants.
L'article vise à former des modèles de réseaux neuronaux profonds avec peu d'échantillons d'entraînement étiquetés.
Les auteurs proposent une approche pour la formation de modèles d'apprentissage profond pour les situations où il n'y a pas assez de données annotées fiables.
Nous avons proposé deux nouvelles approches, la régression inverse incrémentale en tranches et la régression inverse incrémentale en tranches avec chevauchement, pour mettre en œuvre la réduction de dimension supervisée de manière à l'apprendre en ligne.
Etudie le problème de la réduction de la dimension suffisante et propose un algorithme de régression inverse incrémentielle en tranches.
Cet article propose un algorithme d'apprentissage en ligne pour la réduction supervisée de la dimension, appelé régression inverse tranchée incrémentielle.
Nous proposons un modèle d'apprentissage permettant aux DNN d'apprendre avec seulement 2 bits/poids, ce qui est particulièrement utile pour l'apprentissage sur appareil.
Propose une méthode pour discrétiser un NN de manière incrémentale afin d'améliorer la mémoire et les performances.
L'apprentissage des opérateurs de transport sur les collecteurs constitue une représentation précieuse pour effectuer des tâches telles que l'apprentissage par transfert.
Utilise un cadre d'apprentissage par dictionnaire pour apprendre des opérateurs de transport multiples sur des chiffres USPS augmentés.
L'article considère le cadre de l'apprentissage de l'opérateur de transport multiple de Culpepper et Olshausen (2009), et l'interprète comme l'obtention d'une estimation MAP sous un modèle génératif probabiliste.
Nous présentons un modèle neuronal variationnel pour l'apprentissage de concepts visuels compositionnels guidés par le langage.
Propose une nouvelle architecture de réseau neuronal qui apprend les concepts d'objet en combinant un bêta-VAE et un SCAN.
Cet article présente un modèle basé sur les VAE pour la traduction entre les images et le texte. Leur représentation latente est bien adaptée à l'application d'opérations symboliques, ce qui leur donne un langage plus expressif pour l'échantillonnage des images à partir du texte. 
Cet article propose un nouveau modèle appelé SCAN (Symbol-Concept Association Network) pour l'apprentissage hiérarchique des concepts et permet la généralisation à de nouveaux concepts composés à partir de concepts existants en utilisant des opérateurs logiques.
Latent Topic Conversational Model, un hybride de seq2seq et de modèle thématique neuronal pour générer des réponses plus diverses et plus intéressantes.
Cet article propose la combinaison du modèle thématique et du modèle conversationnel seq2seq.
Nous proposons un modèle conversationnel avec des informations topiques en combinant le modèle seq2seq avec des modèles thématiques neuronaux et nous montrons que le modèle proposé surpasse le modèle de base seq2seq et d'autres variantes de modèles à variables latentes de seq2seq.
Cet article aborde la question de la topicalité durable dans les modèles de conversation et propose un modèle qui est une combinaison d'un modèle thématique neuronal et d'un système de dialogue basé sur seq2seq. 
Le problème de l'analyse des graphes est transformé en un problème d'analyse des nuages de points. 
Propose un réseau GNN profond pour les problèmes de classification de graphes en utilisant leur couche adaptative de mise en commun des graphes.
Les auteurs proposent une méthode d'apprentissage de représentations pour les graphes
Nous proposons de générer des exemples contradictoires basés sur des réseaux génératifs contradictoires dans des contextes de boîte noire et de boîte semi-blanche.
Décrit AdvGAN, un GAN conditionnel avec perte adversariale, et évalue AdvGAN en boîte semi-blanche et boîte noire, en rapportant les résultats de l'état de l'art.
Cet article propose un moyen de générer des exemples contradictoires qui trompent les systèmes de classification et remporte le défi mnist de MadryLab.
Cet article démontre comment former des autoencodeurs profonds de bout en bout pour obtenir des résultats SoA sur un ensemble de données Netflix fractionnées dans le temps.
Cet article présente un modèle d'autoencodeur profond pour la prédiction de classement qui surpasse les autres approches de pointe sur le jeu de données de prix Netflix. 
Propose d'utiliser un AE profond pour effectuer des tâches de prédiction de classement dans les systèmes de recommandation.
Les auteurs présentent un modèle pour des recommandations Netflix plus précises, démontrant qu'un autoencodeur profond peut surpasser les modèles plus complexes basés sur les RNN qui ont des informations temporelles. 
Nous présentons le transformateur universel, un modèle de séquence récurrent parallèle en temps auto-attentif qui surpasse les transformateurs et les LSTM dans un large éventail de tâches de séquence à séquence, y compris la traduction automatique.
Propose un nouveau modèle UT, basé sur le modèle Transformer, avec récurrence ajoutée et arrêt dynamique de la récurrence.
Cet article étend Transformer en appliquant récursivement un bloc d'auto-attention à têtes multiples, plutôt que d'empiler plusieurs blocs dans le Transformer vanille.
L'article développe un cadre d'apprentissage continu interprétable où les explications des tâches terminées sont utilisées pour renforcer l'attention de l'apprenant pendant les tâches futures, et où une métrique d'explication est également proposée. 
Les auteurs proposent un cadre pour l'apprentissage continu basé sur des explications pour les classifications effectuées des tâches apprises précédemment.
Cet article propose une extension du cadre d'apprentissage continu en utilisant l'apprentissage continu variationnel existant comme méthode de base avec le poids de la preuve.
Pipeline d'entraînement à précision mixte utilisant des entiers de 16 bits sur un matériel universel ; précision SOTA pour les CNN de classe ImageNet ; meilleure précision rapportée pour la tâche de classification ImageNet-1K avec tout entraînement à précision réduite ;
Cet article montre qu'une mise en œuvre soignée du calcul dynamique à virgule fixe en précision mixte permet d'atteindre une précision de pointe en utilisant un modèle d'apprentissage profond à précision réduite avec une représentation entière de 16 bits.
Propose un schéma de "point fixe dynamique" qui partage la partie exposant pour un tenseur et développe des procédures pour faire du calcul NN avec ce format et en fait la démonstration pour une formation à précision limitée.
Un modèle acoustique ConvNet basé sur les lettres permet d'obtenir un pipeline de reconnaissance vocale simple et compétitif.
Cet article applique les réseaux de neurones convolutifs à grille à la reconnaissance de la parole, en utilisant le critère de formation ASG.
Un cadre GAN noval qui utilise des caractéristiques invariantes de transformation pour apprendre des représentations riches et des générateurs forts.
Propose un objectif GAN modifié composé d'un terme GAN classique et d'un terme d'encodage invariant.
Cet article présente le IVE-GAN, un modèle qui introduit l'encodeur dans le cadre du Generative Adversarial Network.
Nous proposons une méthode d'apprentissage de la structure de dépendance latente dans les autoencodeurs variationnels.
Utilise une matrice de variables aléatoires binaires pour capturer les dépendances entre les variables latentes dans un modèle génératif profond hiérarchique.
Cet article présente une approche VAE dans laquelle une structure de dépendance sur la variable latente est apprise pendant la formation.
Les auteurs proposent d'augmenter l'espace latent d'un VAE avec une structure auto-régressive, afin d'améliorer l'expressivité du réseau d'inférence et du préalable latent.
Nous présentons une architecture de réseau neuronal à échelle invariable pour la détection des points de changement dans les séries temporelles multivariées.
L'article exploite le concept de transformée en ondelettes dans une architecture profonde pour résoudre la détection des points de changement.
Cet article propose un réseau de neurones basé sur une pyramide et l'applique à des signaux 1D avec des processus sous-jacents se produisant à différentes échelles de temps où la tâche est la détection de points de changement.
RL trouve de meilleures heuristiques pour les algorithmes de raisonnement automatique.
L'objectif est d'apprendre une heuristique pour un algorithme de recherche par retour en arrière en utilisant l'apprentissage par renforcement et propose un modèle qui utilise des réseaux de neurones graphiques pour produire un encastrement des littérateurs et des clauses, et les utiliser pour prédire la qualité de chaque littérateur afin de décider de la probabilité de chaque action.
L'article propose une approche pour l'apprentissage automatique d'heuristiques de sélection de variables pour QBF en utilisant l'apprentissage profond.
Évaluez si votre GAN fait réellement autre chose que de mémoriser les données d'entraînement.
L'objectif est de fournir une mesure/test de qualité pour les GAN et propose d'évaluer l'approximation actuelle d'une distribution apprise par un GAN en utilisant la distance de Wasserstein entre deux distributions constituées d'une somme de Diracs comme performance de base. 
Cet article propose une procédure d'évaluation des performances des GANs en reconsidérant la clé d'observation, en utilisant la procédure pour tester et améliorer les GANs actuels.
Nous utilisons des techniques de recherche pour découvrir de nouvelles fonctions d'activation, et notre meilleure fonction d'activation découverte, f(x) = x * sigmoïde(beta * x), surpasse ReLU sur un certain nombre de tâches difficiles comme ImageNet.
Propose une approche basée sur l'apprentissage par renforcement pour trouver la non-linéarité en recherchant des combinaisons à partir d'un ensemble d'opérateurs unaires et binaires.
Cet article utilise l'apprentissage par renforcement pour rechercher la combinaison d'un ensemble de fonctions unaires et binaires résultant en une nouvelle fonction d'activation.
L'auteur utilise l'apprentissage par renforcement pour trouver de nouvelles fonctions d'activation potentielles à partir d'un riche ensemble de candidats possibles. 
Un algorithme ascendant qui étend les CNNs commençant avec une caractéristique par couche à des architectures avec une capacité de représentation suffisante.
Propose d'ajuster dynamiquement la profondeur de la carte de caractéristiques d'un réseau neuronal entièrement convolutif, en formulant une mesure d'auto-resemblance et en améliorant les performances.
Introduit une métrique simple basée sur la corrélation pour mesurer si les filtres des réseaux neuronaux sont utilisés efficacement, en tant qu'indicateur de la capacité effective.
Vise à résoudre le problème de recherche d'architecture d'apprentissage profond par l'ajout et la suppression incrémentiels de canaux dans les couches intermédiaires du réseau.
Nous formons un réseau feedforward sans backprop en utilisant un modèle basé sur l'énergie pour fournir des cibles locales.
Cet article vise à accélérer la procédure d'inférence itérative dans les modèles basés sur l'énergie entraînés avec la propagation de l'équilibre (EP), en proposant d'entraîner un réseau feedforward pour prédire un point fixe du "réseau équilibrant". 
Formation d'un réseau distinct pour initialiser les réseaux récurrents formés à l'aide de la propagation de l'équilibre 
Apprenez les représentations pour les images qui tiennent compte d'un seul attribut.
Cet article s'appuie sur les GAN VAE conditionnels pour permettre la manipulation des attributs dans le processus de synthèse.
Cet article propose un modèle génératif pour apprendre la représentation qui peut séparer l'identité d'un objet d'un attribut, et étend l'autoencodeur adverse en ajoutant un réseau auxiliaire.
Nous présentons un modèle pour la reconstruction 3D cohérente et la prédiction vidéo sautée, c'est-à-dire la production de trames d'image plusieurs pas de temps dans le futur sans générer de trames intermédiaires.
Cet article propose une méthode générale de modélisation des données indexées en codant les informations de l'index avec l'observation dans un réseau neuronal, puis en décodant la condition d'observation sur l'index cible.
Propose d'utiliser un VAE qui code la vidéo d'entrée d'une manière invariable par permutation pour prédire les images futures d'une vidéo.
Analyse de l'optimiseur populaire Adam
L'article tente d'améliorer Adam en se basant sur l'adaptation de la variance avec momentum en proposant deux algorithmes
Cet article analyse l'invariance d'échelle et la forme particulière du taux d'apprentissage utilisé dans Adam, en soutenant que la mise à jour d'Adam est une combinaison d'un sign-update et d'un taux d'apprentissage basé sur la variance.
L'article divise l'algorithme ADAM en deux composantes : la direction stochastique en signe de gradient et la direction adaptative par étapes avec variance relative, et deux algorithmes sont proposés pour tester chacune d'entre elles.
Nous proposons un nouveau cadre pour ajuster de manière adaptative les taux d'abandon pour le réseau neuronal profond basé sur une limite de complexité de Rademacher.
Les auteurs relient les paramètres d'abandon à une limite de la complexité de Rademacher du réseau.
Relie la complexité de l'apprenabilité des réseaux aux taux d'abandon dans la rétropropagation.
Une architecture d'apprentissage profond optimisée pour la fusion de capteurs est proposée.
Les auteurs améliorent plusieurs limitations de l'architecture négative de base en proposant une architecture de fusion à granularité plus grossière et une architecture de fusion à granularité en deux étapes.
Il propose deux architectures d'apprentissage profond à déclenchement pour la fusion de capteurs et, grâce aux caractéristiques groupées, il démontre une meilleure performance, en particulier en présence de bruit et de défaillances aléatoires des capteurs.
La normalisation des lots provoque l'explosion des gradients dans les réseaux feedforward de type vanille.
Développe une théorie du champ moyen pour la normalisation par lots (BN) dans les réseaux entièrement connectés avec des poids initialisés de façon aléatoire.
Fournit une perspective dynamique sur les réseaux neuronaux profonds en utilisant l'évolution de la matrice de covariance avec les couches.
Nous formons un réseau de graphes pour prédire la satisfiabilité booléenne et montrons qu'il apprend à chercher des solutions, et que les solutions qu'il trouve peuvent être décodées à partir de ses activations.
L'article décrit une architecture générale de réseau neuronal pour la prédiction de la satisfiabilité.
Cet article présente l'architecture NeuroSAT qui utilise un réseau neuronal profond à passage de messages pour prédire la satisfiabilité des instances CNF.
Un modèle de séquence neuronale qui apprend à prévoir sur un graphe dirigé.
L'article propose une architecture de réseau neuronal convolutif récurrent de diffusion pour le problème de la prévision spatio-temporelle du trafic.
Propose de construire un modèle de prévision du trafic en utilisant un processus de diffusion pour les réseaux de neurones récurrents convolutifs afin de traiter l'autocorrélation saptio-temporelle.
Nous entraînons les réseaux neuronaux à être incertains sur des entrées bruyantes afin d'éviter des prédictions trop sûres en dehors de la distribution d'entraînement.
Présente une approche permettant d'obtenir des estimations de l'incertitude pour les prédictions de réseaux neuronaux qui a de bonnes performances lors de la quantification de l'incertitude prédictive à des points qui sont en dehors de la distribution d'apprentissage.
L'article considère le problème de l'estimation de l'incertitude des réseaux neuronaux et propose d'utiliser l'approche bayésienne avec un antécédent contrastif nocif.
Cette étude met en évidence une différence essentielle entre la vision humaine et les CNN : alors que la reconnaissance des objets chez l'homme repose sur l'analyse des formes, les CNN n'ont pas ce biais de forme.
Cherche à établir, via une série d'expériences bien conçues, que les CNN formés pour la classification d'images n'encodent pas de biais de forme comme la vision humaine.
Cet article met en évidence le fait que les CNN n'apprendront pas nécessairement à reconnaître les objets sur la base de leur forme et montre qu'ils surdétermineront les caractéristiques basées sur le bruit.
Nous décrivons un nouveau modèle génératif multi-vues qui peut générer plusieurs vues du même objet, ou plusieurs objets dans la même vue sans avoir besoin d'étiqueter les vues.
Cet article présente une méthode de génération d'images basée sur un GAN qui tente de séparer les variables latentes décrivant le contenu de l'image de celles décrivant les propriétés de la vue.
Cet article propose une architecture GAN qui vise à décomposer la distribution sous-jacente d'une classe particulière en "contenu" et "vue".
Propose un nouveau modèle génératif basé sur le Generative Adversarial Network (GAN) qui démêle le contenu et la vue des objets sans supervision de la vue et étend le GMV en un modèle génératif conditionnel qui prend une image d'entrée et génère différentes vues de l'objet dans l'image d'entrée. 
Un algorithme de quantification de poids tenant compte de la perte est proposé, qui considère directement son effet sur la perte.
Propose une méthode de compression de réseau au moyen de la ternarisation des poids. 
L'article propose une nouvelle méthode pour former des DNN avec des poids quantifiés, en incluant la quantification comme une contrainte dans un algorithme proximal quasi-Newton, qui apprend simultanément une mise à l'échelle pour les valeurs quantifiées.
L'article étend le schéma de binarisation pondérée en fonction des pertes à la terarisation et à la quantification m-bit arbitraire et démontre ses performances prometteuses.
Nous développons une nouvelle méthode de gradient de politique pour l'apprentissage automatique de politiques avec options en utilisant une étape d'inférence différentiable.
L'article présente une nouvelle technique de gradient de politique pour l'apprentissage des options, où un seul échantillon peut être utilisé pour mettre à jour toutes les options.
Propose une méthode hors politique pour l'apprentissage des options dans les problèmes continus complexes.
Sélection non supervisée de caractéristiques par la capture de la structure linéaire locale des données
Propose une sélection de caractéristiques non supervisée et localement linéaire.
L'article propose la méthode LLUFS pour la sélection des caractéristiques.
À l'aide d'une tâche de navigation simple axée sur le langage, nous étudions les capacités de composition des réseaux récurrents seq2seq modernes.
Cet article se concentre sur les capacités de composition de l'apprentissage zéro coup des RNN modernes de séquence à séquence et expose les lacunes des architectures RNN seq2seq actuelles.
Le document analyse les capacités de composition des RNN, en particulier, la capacité de généralisation des RNN sur un sous-ensemble aléatoire de commandes SCAN, sur des commandes SCAN plus longues, et de composition sur des commandes primitives. 
Les auteurs présentent un nouveau jeu de données qui facilite l'analyse d'un cas d'apprentissage Seq2Seq.
Nous abordons le problème de l'apprentissage de la similarité pour les objets structurés avec des applications notamment en sécurité informatique, et proposons un nouveau modèle de réseaux de correspondance de graphes qui excelle dans cette tâche.
Les auteurs présentent un réseau d'appariement de graphes pour la récupération et l'appariement d'objets structurés en graphes.
Les auteurs s'attaquent au problème de l'appariement des graphes en proposant une extension des réseaux d'insertion de graphes.
Les auteurs présentent deux méthodes d'apprentissage d'un score de similarité entre des paires de graphes et montrent l'intérêt d'introduire des idées issues de l'appariement de graphes dans les réseaux neuronaux de graphes.
Nous avons proposé un modèle d'encodage-décodage RNN-CNN pour un apprentissage rapide et non supervisé de la représentation des phrases.
Modifications du cadre d'apprentissage de l'intégration des phrases par le saut de pensée.
Cet article présente une nouvelle conception hybride d'encodeur RNN et de décodeur CNN à utiliser en pré-entraînement, qui ne nécessite pas de décodeur autorégressif lors du pré-entraînement des encodeurs.
Les auteurs étendent Skip-thought en décodant une seule phrase cible à l'aide d'un décodeur CNN.
Une approche statistique pour calculer les vraisemblances d'échantillon dans les réseaux adversariaux génératifs.
Montrer que le WGAN avec régularisation entropique maximise une borne inférieure sur la vraisemblance de la distribution des données observées.
Les auteurs affirment qu'il est possible de tirer parti de la limite supérieure d'un transport optimal régularisé par l'entropie pour obtenir une mesure de la "probabilité de l'échantillon".
Nous présentons geomstats, un paquetage Python efficace pour la modélisation et l'optimisation riemannienne sur les manifolds, compatible avec numpy et tensorflow .
L'article présente le progiciel geomstats, qui permet d'utiliser simplement les manifolds et les métriques riemanniennes dans les modèles d'apprentissage automatique.
Propose un paquet Python pour l'optimisation et les applications sur les manifolds riemanniens et souligne les différences entre le paquet Geomstats et les autres paquets.
Présente une boîte à outils géométrique, Geomstats, pour l'apprentissage automatique sur les collecteurs riemanniens.
Nous construisons un graphe dynamique clairsemé par le biais d'une recherche de réduction de la dimension afin de réduire les coûts de calcul et de mémoire à la fois dans l'apprentissage et l'inférence du DNN.
Les auteurs proposent d'utiliser un graphe de calcul dynamique clairsemé pour réduire le coût en mémoire et en temps de calcul dans un réseau neuronal profond (DNN).
Cet article propose une méthode pour accélérer la formation et l'inférence des réseaux neuronaux profonds en utilisant l'élagage dynamique du graphe de calcul.
Nous développons une extension pratique de l'échantillonnage dirigé par l'information pour l'apprentissage par renforcement, qui tient compte de l'incertitude paramétrique et de l'hétéroscédasticité dans la distribution des retours pour l'exploration.
Les auteurs proposent une façon d'étendre l'échantillonnage dirigé par l'information à l'apprentissage par renforcement en combinant deux types d'incertitude pour obtenir une stratégie d'exploration simple basée sur l'IDS. 
Cet article étudie des approches d'exploration sophistiques pour l'apprentissage par renforcement basées sur l'échantillonnage direct de l'information et sur l'apprentissage par renforcement distributif.
l'utilisation d'un réseau de graphes neuronaux pour modéliser les informations structurelles des agents afin d'améliorer la politique et la transférabilité. 
Méthode de représentation et d'apprentissage d'une politique structurée pour des tâches de contrôle continu à l'aide de réseaux neuronaux graphiques.
La soumission propose d'incorporer une structure supplémentaire dans les problèmes d'apprentissage par renforcement, en particulier la structure de la morphologie de l'agent.
Proposer une application des réseaux de neurones graphiques à l'apprentissage de politiques de contrôle de robots "mille-pattes" de différentes longueurs.
Cet article présente un cadre d'apprentissage par renforcement hiérarchique basé sur des politiques d'option déterministes et la maximisation de l'information mutuelle. 
Propose un algorithme HRL qui tente d'apprendre les options qui maximisent leur information mutuelle avec la densité état-action sous la politique optimale.
Cet article propose un système HRL dans lequel l'information mutuelle de la variable latente et des paires état-action est approximativement maximisée.
Propose un critère qui vise à maximiser l'information mutuelle entre les options et les paires état-action et montre empiriquement que les options apprises décomposent l'espace état-action mais pas l'espace état. 
Nous introduisons et validons les interprétations locales hiérarchiques, la première technique pour rechercher et afficher automatiquement les interactions importantes pour les prédictions individuelles faites par les LSTM et les CNN.
Une nouvelle approche pour expliquer les prédictions des réseaux neuronaux par l'apprentissage de représentations hiérarchiques de groupes de caractéristiques d'entrée et leur contribution à la prédiction finale.
Étend une méthode existante d'interprétation des caractéristiques pour les LSTM à des DNN plus génériques et introduit un regroupement hiérarchique des caractéristiques d'entrée et les contributions de chaque regroupement à la prédiction finale.
Cet article propose une extension hiérarchique de la décomposition contextuelle.
Nous proposons une méthode facile à mettre en œuvre, mais efficace, pour la compression des réseaux neuronaux. La PFA exploite la corrélation intrinsèque entre les réponses des filtres au sein des couches du réseau pour recommander une empreinte de réseau plus petite.
Propose d'élaguer les réseaux convolutifs en analysant la corrélation observée entre les filtres d'une même couche, exprimée par le spectre des valeurs propres de leur matrice de covariance.
Cet article présente une approche de la compression des réseaux neuronaux en examinant la corrélation des réponses des filtres dans chaque couche via deux stratégies.
Ce document propose une méthode de compression basée sur l'analyse spectrale.
Des agents de reformulation de requêtes multiples et diversifiées formés par apprentissage par renforcement pour améliorer les moteurs de recherche.
Parelléisation de la méthode d'ensemble dans l'apprentissage par réinvestissement pour la reformulation de requêtes, accélérant l'apprentissage et améliorant la diversité des formulations apprises.
Les auteurs proposent de former plusieurs agents distincts, chacun sur un sous-ensemble différent de l'ensemble de formation.
Les auteurs proposent une approche d'ensemble pour la reformulation des requêtes.
Nous présentons une méthode d'intégration de réseau qui tient compte des informations préalables sur le réseau, ce qui permet d'obtenir des performances empiriques supérieures.
L'article propose d'utiliser une distribution préalable pour contraindre l'intégration du réseau. Pour la formulation, cet article a utilisé des distributions gaussiennes très restreintes.
Propose d'apprendre des encastrements de nœuds non supervisés en considérant les propriétés structurelles des réseaux.
Nous analysons la convergence des algorithmes de type Adam et fournissons des conditions suffisantes légères pour garantir leur convergence, nous montrons également que la violation des conditions peut faire diverger un algorithme.
Présente une analyse de convergence dans le cadre non convexe pour une famille d'algorithmes d'optimisation.
Cet article étudie la condition de convergence des optimiseurs de type Adam dans les problèmes d'optimisation non convexes sans contrainte.
Auto-encodeur à poids liés avec la fonction abs comme fonction d'activation, apprend à faire de la classification dans le sens direct et de la régression dans le sens inverse grâce à une fonction de coût spécialement définie.
L'article propose d'utiliser la fonction d'activation de la valeur absolue dans une architecture d'auto-codage avec un terme supplémentaire d'apprentissage supervisé dans la fonction objectif.
Cet article présente un réseau réversible dont la fonction d'activation est la valeur absolue.
Nous proposons un modèle d'extraction de relations basé sur un transformateur qui utilise des représentations linguistiques pré-entraînées au lieu de caractéristiques linguistiques explicites.
Présente un modèle d'extraction de relations basé sur un transformateur qui tire parti du pré-entraînement sur du texte non étiqueté avec un objectif de modélisation du langage.
Cet article décrit une nouvelle application des réseaux Transformer pour l'extraction de relations.
L'article présente une architecture basée sur un transformateur pour l'extraction de la relaxation, évaluée sur deux ensembles de données.
Nous proposons une méthode simple et efficace de recherche d'architecture pour les réseaux de neurones convolutifs.
Propose une méthode de recherche par architecture neuronale qui atteint une précision proche de l'état de l'art sur CIFAR10 et nécessite beaucoup moins de ressources informatiques.
Présente une méthode de recherche d'architectures de réseaux neuronaux en même temps que la formation, ce qui permet de réduire considérablement le temps de formation et le temps de recherche des architectures.
propose une variante de la recherche d'architecture neuronale utilisant des morphismes de réseau pour définir un espace de recherche utilisant des architectures CNN remplissant la tâche de classification d'images CIFAR
Utilisation des GAN pour générer des graphes par marche aléatoire.
Les auteurs ont proposé un modèle génératif de marches aléatoires sur des graphes qui permet un apprentissage agnostique, un ajustement contrôlable, la génération de graphes d'ensemble.
Propose une formulation WGAN pour générer des graphes basés sur des marches aléatoires en utilisant l'intégration des nœuds et une architecture LSTM pour la modélisation.
Nous proposons de résoudre un problème de classification et de détection de nouveauté simultanées dans le cadre du GAN.
Propose un GAN pour unifier la classification et la détection de la nouveauté.
L'article présente une méthode de détection de la nouveauté basée sur un GAN multi-classes qui est entraîné à produire des images générées à partir d'un mélange des distributions nominale et nouvelle.
L'article propose un GAN pour la détection de la nouveauté en utilisant un générateur de mélange avec une perte de correspondance des caractéristiques.
La performance de la vérification du locuteur peut être améliorée de manière significative en adaptant le modèle aux données du domaine à l'aide de réseaux adversariaux génératifs. En outre, l'adaptation peut être effectuée de manière non supervisée.
Proposer un certain nombre de variantes de GAN sur la tâche de reconnaissance du locuteur dans la condition de non correspondance de domaine.
Minimiser l'information mutuelle synergique entre les latents et les données pour la tâche de démêlage en utilisant le cadre VAE.
Propose une nouvelle fonction objectif pour l'apprentissage de représentations dientangulaires dans un cadre variationnel en minimisant la synergie des informations fournies.
Les auteurs visent à former un VAE qui a démêlé les représentations latentes de manière "synergique" maximale. 
Cet article propose une nouvelle approche pour imposer le désenchevêtrement dans les VAE en utilisant un terme qui pénalise l'information mutuelle synergique entre les variables latentes.
Accélérer le SGD en disposant les exemples différemment
L'article présente une méthode permettant d'améliorer le taux de convergence de la descente de gradient stochastique pour l'apprentissage des encastrements en regroupant les échantillons d'entraînement similaires.
Propose une stratégie d'échantillonnage non-uniforme pour construire des minibatchs dans SGD pour la tâche d'apprentissage des embeddings pour les associations d'objets.
Combinaison d'informations entre l'intégration de mots préconstruite et la représentation de mots spécifiques à la tâche pour résoudre le problème du hors-vocabulaire.
Cet article propose une approche visant à améliorer la prédiction de l'incorporation hors vocabulaire pour la tâche de modélisation des conversations de dialogue avec des gains considérables par rapport aux lignes de base.
Propose de combiner les incorporations de mots pré-entraînées externes et les incorporations de mots pré-entraînées sur les données de formation en les conservant comme deux vues.
Proposition d'une méthode pour étendre la couverture des encastrements de mots pré-entraînés afin de traiter le problème de l'OOV qui se pose lorsqu'on les applique à des ensembles de données conversationnelles et application de nouvelles variantes du modèle LSTM à la tâche de sélection des réponses dans la modélisation du dialogue.
Nous analysons les problèmes qui se posent lors de l'entraînement des optimiseurs appris, nous traitons ces problèmes par le biais de l'optimisation variationnelle en utilisant deux estimateurs de gradient complémentaires, et nous entraînons des optimiseurs qui sont 5 fois plus rapides en temps d'horloge murale que les optimiseurs de base (par exemple Adam).
Cet article utilise l'optimisation non roulée pour apprendre des réseaux neuronaux pour l'optimisation.
Cet article aborde le problème de l'apprentissage d'un optimiseur, en particulier les auteurs se concentrent sur l'obtention de gradients plus propres à partir de la procédure d'apprentissage non enroulée.
Présente une méthode pour "apprendre un optimiseur" en utilisant une optimisation variationnelle pour la perte de l'optimiseur "externe" et propose l'idée de combiner à la fois le gradient reparamétré et l'estimateur de la fonction de score pour l'objectif variationnel et les pondère en utilisant une formule de produit de Gauss pour la moyenne.
Une méthode pour une formation distribuée asynchrone efficace de modèles d'apprentissage profond ainsi que des limites de regret théoriques.
L'article propose un algorithme pour limiter la stagnation dans les SGD asynchrones et fournit une analyse théorique.
Propose un algorithme hybride pour éliminer le retard du gradient des méthodes asynchrones.
Nous avons conçu une nouvelle méthodologie de quantification pour optimiser conjointement l'efficacité et la robustesse des modèles d'apprentissage profond.
Propose un schéma de régularisation pour protéger les réseaux neuronaux quantifiés contre les attaques adverses en utilisant un filtrage constant de Lipschitz des entrées-sorties des couches internes.
Une modification pour les architectures RNN existantes qui leur permet de sauter les mises à jour d'état tout en préservant les performances des architectures originales.
Propose le modèle Skip RNN qui permet à un réseau récurrent d'ignorer sélectivement la mise à jour de son état caché pour certaines entrées, ce qui permet de réduire le calcul au moment du test.
Propose un nouveau modèle de RNN dans lequel l'entrée et la mise à jour de l'état des cellules récurrentes sont sautées de manière adaptative pour certains pas de temps.
Un solveur rapide du second ordre pour l'apprentissage profond qui fonctionne sur des problèmes à l'échelle d'ImageNet sans réglage des hyperparamètres.
Choix de la direction en utilisant une seule étape de descente du gradient "vers l'étape de Newton" à partir d'une estimation originale, puis en prenant cette direction au lieu du gradient original.
Une nouvelle méthode d'optimisation approximative du second ordre à faible coût de calcul qui remplace le calcul de la matrice hessienne par une seule étape de gradient et une stratégie de démarrage à chaud.
Modèle basé sur l'attention entraîné avec REINFORCE avec une ligne de base pour l'apprentissage d'heuristiques avec des résultats compétitifs sur TSP et d'autres problèmes de routage.
Présente une approche basée sur l'attention pour apprendre une politique de résolution de TSP et d'autres problèmes d'optimisation combinatoire de type routage.
Ce document tente d'apprendre des heuristiques pour résoudre des problèmes d'optimisation combinatoire.
Un algorithme pour optimiser les hyper-paramètres de régularisation pendant l'apprentissage.
L'article propose un moyen de réinitialiser y à chaque mise à jour de lambda et une procédure d'écrêtage de y pour maintenir la stabilité du système dynamique.
Propose un algorithme pour l'optimisation des hyperparamètres qui peut être vu comme une extension de Franceschi 2017 où certaines estimations sont recommencées à chaud pour augmenter la stabilité de la méthode.
Propose une extension à une méthode existante pour optimiser les hyperparamètres de régularisation.
Montrer que les LSTM sont aussi bons, voire meilleurs, que les innovations récentes pour LM et que l'évaluation des modèles est souvent peu fiable.
Cet article décrit une validation complète des modèles de langage de mots et de caractères basés sur les LSTM, ce qui a conduit à un résultat significatif dans la modélisation du langage et à une étape importante dans l'apprentissage profond.
Nous montrons comment l'utilisation de connexions sautées peut rendre les modèles d'amélioration de la parole plus interprétables, car ils utilisent des mécanismes similaires à ceux qui ont été explorés dans la littérature DSP.
Les auteurs proposent d'incorporer les blocs Residual, Highway et Masking dans un pipeline entièrement convolutif afin de comprendre comment l'inférence itérative de la sortie et du masquage est effectuée dans une tâche d'amélioration de la parole.
Les auteurs interprètent les connexions d'autoroute, résiduelles et de masquage. 
Les auteurs génèrent leur propre discours bruyant en ajoutant artificiellement du bruit provenant d'un ensemble de données de bruit bien établi à un ensemble de données de discours propre moins connu.
Méthode d'élimination de la variance du gradient et de réglage automatique des prieurs pour un entraînement efficace des réseaux neuronaux bayésiens
Propose une nouvelle approche pour effectuer l'inférence variationnelle déterministe pour les BNN à action directe avec des fonctions d'activation non linéaires spécifiques par l'approximation des moments de la couche.
L'article considère une approche purement déterministe de l'apprentissage des approximations variationnelles postérieures pour les réseaux neuronaux bayésiens.
Une approche par méthode formelle de la composition des compétences dans les tâches d'apprentissage par renforcement.
L'article combine RL et les contraintes exprimées par des formules logiques en mettant en place une automatisation à partir de formules scTLTL.
Propose une méthode qui aide à construire une politique à partir de sous-tâches apprises sur le thème de la combinaison de tâches RL avec des formules de logique temporelle linéaire.
Dérivation d'une formulation générale d'une VAE multimodale à partir de la log-vraisemblance marginale conjointe.
Propose un VAE multi-modal avec une limite variationnelle dérivée de la règle de la chaîne.
Cet article propose un objectif, M^2VAE, pour les VAE multimodaux, qui est censé apprendre une représentation de l'espace latent plus significative.
Nous nous basons sur le Monte Carlo séquentiel à encodage automatique, nous obtenons de nouvelles perspectives théoriques et nous développons une procédure de formation améliorée basée sur ces perspectives.
L'article propose une version de la formation de type IWAE qui utilise le SMC au lieu de l'échantillonnage par importance classique.
Ce travail propose un Monte Carlo séquentiel (SMC) à codage automatique, étendant le cadre VAE à un nouvel objectif de Monte Carto basé sur le SMC. 
Nous proposons une architecture pour l'apprentissage des fonctions de valeur qui permet l'utilisation de n'importe quel algorithme linéaire d'évaluation de politique en tandem avec l'apprentissage non linéaire de caractéristiques.
L'article propose un cadre à deux échelles temporelles pour l'apprentissage de la fonction de valeur et d'une représentation d'état en même temps que des approximateurs non linéaires.
Cet article propose des réseaux à deux échelles de temps (TTN) et prouve la convergence de cette méthode en utilisant des méthodes d'approximation stochastique à deux échelles de temps. 
Cet article présente un réseau à deux échelles de temps (TTN) qui permet d'utiliser des méthodes linéaires pour apprendre des valeurs. 
Nous proposons des algorithmes simples, mais efficaces, de factorisation matricielle (MF) à faible rang pour accélérer le temps d'exécution, économiser la mémoire et améliorer les performances des LSTM.
Propose d'accélérer les LSTM en utilisant la MF comme stratégie de compression post-traitement et mène des expériences approfondies pour montrer les performances.
Une métrique médico-légale pour déterminer si une image donnée est une copie (avec manipulation possible) d'une autre image d'un ensemble de données donné.
Présente le réseau siamois pour identifier les images dupliquées et copiées/modifiées, qui peut être utilisé pour améliorer la surveillance de la littérature publiée et en révision par les pairs.
L'article présente une application des réseaux convolutifs profonds pour la détection d'images en double.
Ce travail aborde le problème de la recherche d'images dupliquées ou presque dupliquées dans des publications biomédicales. Il propose un CNN standard et des fonctions de perte et l'applique à ce domaine.
Entraînement stable du GAN en haute dimension par l'utilisation d'un réseau de discriminateurs, chacun ayant une vue en basse dimension des échantillons générés.
L'article propose de stabiliser la formation du GAN en utilisant un ensemble de discriminateurs, chacun travaillant sur une projection aléatoire des données d'entrée, pour fournir le signal de formation du modèle générateur.
L'article propose une méthode de formation GAN pour améliorer la stabilité de la formation. 
L'article propose une nouvelle approche de la formation du GAN, qui fournit des gradients stables pour former le générateur.
nous montrons une méthode géométrique qui permet d'encoder parfaitement les informations de l'arbre des catégories dans des mots-embeddings pré-entraînés.
L'article propose l'intégration de N-balles pour les données taxonomiques où une N-balle est une paire d'un vecteur centroïde et du rayon du centre.
L'article présente une méthode permettant de modifier les incorporations vectorielles existantes d'objets catégoriels (tels que les mots), afin de les convertir en incorporations de balles qui suivent des hiérarchies.
Se concentre sur l'ajustement des encastrements de mots pré-entraînés afin qu'ils respectent la relation hypernymie/hyponymie par une encapsulation appropriée de la boule n.
Nous proposons les CRFs convolutifs comme une alternative rapide, puissante et entraînable aux CRFs entièrement connectés.
Les auteurs remplacent la grande étape de filtrage dans le réseau permutoédrique par un noyau convolutif variant dans l'espace et montrent que l'inférence est plus efficace et l'apprentissage plus facile. 
Propose d'effectuer le passage de messages sur un CRF à noyau gaussien tronqué en utilisant un noyau défini et un passage de messages parallélisé sur GPU.
Nous proposons une approche agnostique du modèle pour la validation de la robustesse des systèmes de questions-réponses et démontrons les résultats sur les modèles de questions-réponses les plus récents.
Aborde le problème de la robustesse aux informations contradictoires dans la réponse aux questions.
Amélioration de la robustesse de la compréhension automatique/réponse aux questions.
multi-générateur pour capturer les données Pdata, résoudre le problème de la concurrence et du "one-beat-all".
Propose des GANs parallèles pour éviter l'effondrement des modes dans les GANs par une combinaison de plusieurs générateurs faibles. 
Segmentation d'images faiblement supervisée utilisant la structure compositionnelle des images et des modèles génératifs.
Cet article crée une représentation en couches afin de mieux apprendre la segmentation à partir d'images non étiquetées.
Cet article propose un modèle génératif basé sur un GAN qui décompose les images en plusieurs couches, l'objectif du GAN étant de distinguer les images réelles des images formées par la combinaison des couches.
Cet article propose une architecture de réseau neuronal autour de l'idée de composition de scènes en couches.
Nous présentons un cadre géométrique pour prouver les garanties de robustesse et nous soulignons l'importance de la codimension dans les exemples contradictoires. 
Cet article présente une analyse théorique des exemples contradictoires, montrant qu'il existe un compromis entre la robustesse dans différentes normes, que la formation contradictoire est inefficace en termes d'échantillonnage et que le classificateur du plus proche voisin peut être robuste dans certaines conditions.
CharNMT est fragile
Cet article étudie l'impact du bruit au niveau des caractères sur quatre systèmes de traduction automatique neuronale différents.
Cet article étudie de manière empirique les performances des systèmes de T.N.M. au niveau des caractères face au bruit au niveau des caractères, qu'il soit synthétisé ou naturel.
Cet article étudie l'impact des données bruitées sur la traduction automatique et teste les moyens de rendre les modèles de traduction automatique plus robustes.
Nous apprenons des réseaux profonds d'unités à seuil dur en fixant les cibles des unités cachées par optimisation combinatoire et les poids par optimisation convexe, ce qui permet d'améliorer les performances sur ImageNet.
L'article explique et généralise les approches d'apprentissage des réseaux neuronaux à activation dure.
Cet article examine le problème de l'optimisation des réseaux profonds d'unités à seuil dur.
L'article aborde le problème de l'optimisation des réseaux neuronaux avec un seuil difficile et propose une nouvelle solution à ce problème avec une collection d'heuristiques/approximations.
À l'aide d'un nouveau défi contrôlé de relations visuelles, nous montrons que les tâches de type identique-différent mettent à rude épreuve la capacité des CNN ; nous soutenons que les relations visuelles peuvent être mieux résolues en utilisant des stratégies mnémoniques d'attention.
Démontre que les réseaux neuronaux convolutionnels et relationnels ne parviennent pas à résoudre les problèmes de relations visuelles en entraînant les réseaux sur des données de relations visuelles générées artificiellement. 
Cet article explore comment les CNN et les réseaux relationnels actuels ne parviennent pas à reconnaître les relations visuelles dans les images.
Nous proposons AD-VAT, où le traqueur et l'objet cible, considérés comme deux agents apprenants, sont des adversaires et peuvent s'améliorer mutuellement pendant l'apprentissage.
Ce travail vise à résoudre le problème du suivi visuel actif à l'aide d'un mécanisme d'entraînement dans lequel le suiveur et la cible servent d'adversaires mutuels.
Cet article présente une tâche simple de RL profond multi-agent où un tracker mobile essaie de suivre une cible mobile.
Propose une nouvelle fonction de récompense - "somme nulle partielle", qui encourage la concurrence entre le tracker et la cible uniquement lorsqu'ils sont proches et la pénalise lorsqu'ils sont trop éloignés.
Nous avons présenté une méthode permettant d'apprendre conjointement un emboîtement hiérarchique de mots (HWE) à l'aide d'un corpus et d'une taxonomie pour identifier les relations d'hypernymie entre les mots.
L'article présente une méthode d'apprentissage conjoint de l'intégration des mots à l'aide de statistiques de cooccurrence et de l'intégration d'informations hiérarchiques provenant de réseaux sémantiques.
Cet article propose une méthode d'apprentissage conjoint des hypernoms à partir de textes bruts et de données de taxonomie supervisées. 
Cet article propose d'ajouter une mesure de différence d'"inclusion distributionnelle" à l'objectif GloVE dans le but de représenter les relations hypernymiques.
intégration de l'auto-organisation et de l'apprentissage supervisé dans un réseau neuronal hiérarchique
L'article traite de l'apprentissage dans un réseau neuronal à trois couches, dont la couche intermédiaire est organisée de manière topographique, et étudie l'interaction entre l'apprentissage non supervisé et l'apprentissage supervisé hiérarchique dans un contexte biologique.
Une variante supervisée de la carte auto-organisée (SOM) de Kohonen, mais où la couche de sortie linéaire est remplacée par une couche softmax avec une erreur quadratique et une entropie croisée.
Propose un modèle utilisant des neurones cachés avec une fonction d'activation auto-organisatrice, dont les sorties alimentent un classificateur avec une fonction de sortie softmax. 
autoroute de précision ; concept généralisé de flux d'informations de haute précision pour la quantification sur 4 bits 
Étudie le problème de la quantification des réseaux neuronaux en utilisant une autoroute de précision de bout en bout pour réduire l'erreur de quantification accumulée et permettre une précision ultra-basse dans les réseaux neuronaux profonds. 
Cet article étudie les méthodes permettant d'améliorer les performances des réseaux neuronaux quantifiés.
Cet article propose de conserver un flux d'activation/gradient élevé dans deux types de structures de réseaux, ResNet et LSTM.
Un algorithme pour l'entraînement efficace de réseaux neuronaux sur des données temporellement redondantes.
L'article décrit un schéma de codage neuronal pour l'apprentissage basé sur les pointes dans les réseaux neuronaux profonds.
Cet article présente une méthode d'apprentissage basée sur les pointes qui vise à réduire le calcul nécessaire pendant l'apprentissage et le test lors de la classification de données temporelles redondantes.
Cet article applique une version de codage prédictif du schéma de codage Sigma-Delta pour réduire la charge de calcul d'un réseau d'apprentissage profond, en combinant les trois composants d'une manière inédite.
Le goulot d'étranglement de l'information se comporte de manière surprenante lorsque la sortie est une fonction déterministe de l'entrée.
Affirme que la plupart des problèmes de classification réels présentent une telle relation déterministe entre les étiquettes de classe et les entrées X et explore plusieurs problèmes qui résultent de telles pathologies.
Exploration des questions qui se posent lors de l'application des concepts d'information bottlenext aux modèles déterministes d'apprentissage supervisé.
Les auteurs clarifient plusieurs comportements contre-intuitifs de la méthode du goulot d'information pour l'apprentissage supervisé d'une règle déterministe.
Nous prouvons que les réseaux neuronaux bayésiens idéalisés ne peuvent pas avoir d'exemples contradictoires, et nous donnons des preuves empiriques avec des BNN du monde réel.
L'article étudie la robustesse des classificateurs bayésiens et énonce deux conditions qui, selon eux, sont suffisantes pour que des "modèles idéalisés" sur des "ensembles de données idéalisés" ne présentent pas d'exemples contradictoires.
L'article pose une classe de classificateurs bayésiens discriminatifs qui n'ont pas d'exemples contradictoires.
Nous présentons la première instance d'attaques adversariales qui reprogramment le modèle cible pour qu'il exécute une tâche choisie par l'attaquant - sans que ce dernier ait besoin de spécifier ou de calculer la sortie souhaitée pour chaque entrée du temps de test.
Les auteurs présentent un nouveau schéma d'attaque contradictoire dans lequel un réseau neuronal est réaffecté pour accomplir une tâche différente de celle pour laquelle il a été formé à l'origine.
Cet article propose une "reprogrammation contradictoire" de réseaux neuronaux fixes et bien entraînés et montre que la reprogrammation contradictoire est moins efficace sur les réseaux non entraînés.
L'article étend l'idée des "attaques adverses" dans l'apprentissage supervisé des réseaux neuronaux à une réaffectation complète de la solution d'un réseau entraîné.
Nous développons un nouveau schéma pour prédire l'écart de généralisation dans les réseaux profonds avec une grande précision.
Les auteurs suggèrent d'utiliser une marge géométrique et une distribution de la marge par couche pour prédire l'écart de généralisation.
Les résultats empiriques montrent un lien intéressant entre les statistiques de marge proposées et l'écart de généralisation, qui peut être utilisé pour fournir des indications normatives pour comprendre la généralisation dans les réseaux neuronaux profonds. 
Nous proposons un algorithme pour récupérer de manière prouvable les paramètres (poids de convolution et de sortie) d'un réseau convolutif avec des patchs qui se chevauchent.
Cet article étudie l'apprentissage théorique des réseaux de neurones convolutifs à une couche cachée. Il en résulte un algorithme d'apprentissage et des garanties prouvables à l'aide de cet algorithme.
Cet article présente un nouvel algorithme pour l'apprentissage d'un réseau neuronal à deux couches qui implique un seul filtre convolutif et un vecteur de poids pour différents emplacements.
Un nouveau terme de régularisation peut améliorer votre formation de gans wasserstein
Cet article propose un schéma de régularisation pour le GAN de Wasserstein basé sur la relaxation des contraintes sur la constante de Lipschitz de 1.
L'article traite de la régularisation/pénalisation dans l'ajustement des GANs, lorsqu'elle est basée sur une métrique de Wasserstein L_1.
Nous proposons la méthode proximale de Wasserstein pour l'entraînement des GANs. 
Propose une nouvelle procédure GAN qui prend en compte les points générés lors de l'itération précédente et met à jour le générateur à effectuer l fois.
Considère l'apprentissage par gradient naturel dans l'apprentissage GAN, où la structure riemannienne induite par la distance de Wasserstein-2 est employée.
L'article entend utiliser le gradient naturel induit par la distance de Wasserstein-2 pour entraîner le générateur dans le GAN et les auteurs proposent l'opérateur proximal de Wasserstein comme régularisation.
Cet article présente une fonction heuristique basée sur l'élimination pour la prise de décision séquentielle, adaptée pour guider les algorithmes de recherche ET/OU pour la résolution des diagrammes d'influence.
généralise l'heuristique d'inférence des minibuckets aux diagrammes d'influence.
Approximation de la moyenne et de la variance de la sortie du NN sur une entrée bruyante / un abandon / des paramètres incertains. Approximations analytiques pour les couches argmax, softmax et max.
Les auteurs se concentrent sur le problème de la propagation de l'incertitude DNN
Cet article revisite la propagation feed-forward de la moyenne et de la variance dans les neurones, en abordant le problème de la propagation de l'incertitude à travers des couches de max-pooling et softmax.
Un discriminateur qui n'est pas facilement trompé par un exemple contradictoire rend la formation du GAN plus robuste et conduit à un objectif plus lisse.
Cet article propose une nouvelle façon de stabiliser le processus de formation du GAN en régularisant le discriminateur pour qu'il soit robuste aux exemples adverses.
L'article propose une méthode systématique d'entraînement des GANs avec des termes de régularisation de la robustesse, permettant un entraînement plus lisse des GANs. 
Présente l'idée selon laquelle, en rendant un discriminateur robuste aux perturbations adverses, l'objectif du GAN peut être rendu lisse, ce qui donne de meilleurs résultats à la fois visuellement et en termes de FID.
Nous modélisons la fonction d'activation de chaque neurone comme un processus gaussien et l'apprenons en même temps que le poids avec l'inférence variationnelle.
Proposer de placer des prieurs de processus gaussiens sur la forme fonctionnelle de chaque fonction d'activation dans le réseau neuronal pour apprendre la forme des fonctions d'activation.
Nous fournissons une étude théorique des propriétés des réseaux ReLU circulants-diagonaux profonds et démontrons qu'ils sont des approximateurs universels de largeur bornée.
L'article propose d'utiliser des matrices circulantes et diagonales pour accélérer les calculs et réduire les besoins en mémoire dans les réseaux neuronaux.
Cet article prouve que les réseaux ReLU diagonaux-circulants à largeur limitée (DC-ReLU) sont des approximateurs universels.
StarHopper est une nouvelle interface à écran tactile pour la navigation efficace et flexible des drones à caméra centrée sur l'objet.
Les auteurs décrivent une nouvelle interface de contrôle de drone, StarHopper, qu'ils ont développée. Elle combine le pilotage automatisé et manuel en une nouvelle interface de navigation hybride et se débarrasse de l'hypothèse selon laquelle l'objet cible se trouve déjà dans le champ de vision du drone en utilisant une caméra aérienne supplémentaire.
Cet article présente StarHopper, un système de navigation semi-automatique pour drones dans le contexte de l'inspection à distance.
Présente StarHopper, une application qui utilise des techniques de vision par ordinateur avec une entrée tactile pour soutenir le pilotage de drones avec une approche centrée sur l'objet.
Un réseau d'auto-attention pour le codage de séquences sans RNN/CNN avec une faible consommation de mémoire, un calcul hautement parallélisable et des performances de pointe pour plusieurs tâches de TAL.
Propose d'appliquer l'auto-attention à deux niveaux pour limiter le besoin de mémoire dans les modèles basés sur l'attention avec un impact négligeable sur la vitesse.
Cet article présente le modèle d'auto-attention par blocs bidirectionnels comme un codeur polyvalent pour diverses tâches de modélisation de séquences en langage naturel.
Un nouveau modèle de pointe pour la réponse aux questions à preuves multiples utilisant l'attention hiérarchique à gros grain et à grain fin.
Propose une méthode d'AQ multi-sauts basée sur deux modules distincts (modules à grain grossier et à grain fin).
Cet article propose une architecture intéressante de réseau de coattention à gros grain et à grain fin pour répondre à des questions à preuves multiples.
Se concentre sur l'AQ multi-choix et propose un cadre de notation grossier à fin.
Nous proposons une nouvelle méthodologie pour le transport optimal déséquilibré en utilisant des réseaux adversariaux génératifs.
Les auteurs considèrent le problème du transport optimal non équilibré entre deux mesures de masse totale différente en utilisant un algorithme stochastique min-max et une mise à l'échelle locale.
Les auteurs proposent une approche pour estimer le transport optimal non équilibré entre les mesures échantillonnées qui s'échelonne bien dans la dimension et dans le nombre d'échantillons.
L'article introduit une formulation statique pour le transport optimal déséquilibré en apprenant simultanément une carte de transport T et un facteur d'échelle xi.
Nous proposons une nouvelle méthode d'extraction de cartes de saillance qui permet d'extraire des cartes de meilleure qualité.
Propose une méthode agnostique de classification pour l'extraction de cartes de saillance.
Cet article présente un nouvel extracteur de carte de saillance qui semble améliorer les résultats de l'état de l'art.
Les auteurs soutiennent que lorsqu'une carte de saillance extraite dépend directement d'un modèle, elle peut ne pas être utile pour un classificateur différent, et proposent un schéma pour approcher la solution.
Nous proposons une nouvelle méthode pour incorporer l'ensemble des attributs d'instance pour la traduction d'image à image.
Cet article propose une méthode -- InstaGAN -- qui s'appuie sur CycleGAN en prenant en compte des informations sur les instances sous la forme de masques de segmentation par instance, avec des résultats qui se comparent favorablement à CycleGAN et à d'autres lignes de base.
 Propose d'ajouter des masques de segmentation tenant compte des instances pour le problème de la traduction d'image à image non appariée.
La carte paramètre-fonction des réseaux profonds est fortement biaisée ; cela peut expliquer pourquoi ils se généralisent. Nous utilisons PAC-Bayes et les processus gaussiens pour obtenir des limites non-vacuantes.
L'article étudie les capacités de généralisation des réseaux neuronaux profonds, à l'aide de la théorie de l'apprentissage PAC-Bayes et d'intuitions étayées empiriquement.
Cet article propose une explication des comportements de généralisation des grands réseaux neuronaux sur-paramétrés en affirmant que la carte paramètre-fonction des réseaux neuronaux est biaisée vers des fonctions "simples" et que le comportement de généralisation sera bon si le concept cible est également "simple".
Nous présentons l'EM évolutionnaire comme un nouvel algorithme pour la formation non supervisée de modèles génératifs avec des variables latentes binaires, qui relie intimement l'EM variationnel à l'optimisation évolutionnaire.
L'article présente une combinaison de calcul évolutionnaire et de EM variationnelle pour les modèles avec des variables latentes binaires représentées par une approximation basée sur des particules.
L'article tente d'intégrer étroitement les algorithmes d'apprentissage par maximisation des attentes aux algorithmes évolutionnaires.
Nous proposons un modèle de réseau récurrent efficace pour la prédiction prospective sur des distributions variant dans le temps.
Cet article propose une méthode de création de réseaux neuronaux qui fait correspondre des distributions historiques à des distributions et applique la méthode à plusieurs tâches de prédiction de distribution.
Propose un réseau de régression de distribution récurrent qui utilise une architecture récurrente sur un modèle précédent de réseau de régression de distribution.
Cet article traite de la régression sur les distributions de probabilité en étudiant les distributions variant dans le temps dans le cadre d'un réseau neuronal récurrent.
Une nouvelle approche du traitement des données structurées en graphes par des réseaux neuronaux, en tirant parti de l'attention portée au voisinage d'un nœud. Obtient des résultats de pointe sur des tâches de réseaux de citations transductives et une tâche d'interaction protéine-protéine inductive.
Cet article propose une nouvelle méthode de classification des nœuds d'un graphe, qui peut être utilisée dans des scénarios semi-supervisés et sur un graphe complètement nouveau. 
L'article présente une architecture de réseau neuronal permettant d'opérer sur des données structurées en graphes, appelée Graph Attention Networks.
Fournit une discussion juste et presque complète de l'état de l'art des approches pour apprendre des représentations vectorielles pour les nœuds d'un graphe.
Une nouvelle approche basée sur l'apprentissage par renforcement pour comprimer les réseaux neuronaux profonds avec distillation des connaissances.
Cet article propose d'utiliser l'apprentissage par renforcement au lieu d'heuristiques prédéfinies pour déterminer la structure du modèle compressé dans le processus de distillation des connaissances.
Introduit une méthode de compression de réseau à réseau fondée sur des principes, qui utilise les gradients de politique pour optimiser deux politiques qui compriment un enseignant fort en un modèle d'étudiant fort mais plus petit.
Nous prouvons que l'effondrement des modes dans les GANs conditionnels est largement attribué à une inadéquation entre la perte de reconstruction et la perte de GAN et nous introduisons un ensemble de nouvelles fonctions de perte comme alternatives à la perte de reconstruction.
L'article propose une modification de l'objectif traditionnel du GAN conditionnel afin de promouvoir la génération diversifiée et multimodale d'images. 
Cet article propose une alternative aux erreurs L1/L2 qui sont utilisées pour augmenter les pertes adverses lors de l'entraînement des GANs conditionnels.
Nous utilisons l'inférence causale pour caractériser l'architecture des modèles génératifs.
Cet article examine la nature des filtres convolutifs dans l'encodeur et le décodeur d'un VAE, et dans un générateur et un discriminateur d'un GAN.
Ce travail exploite le principe de causalité pour quantifier comment les poids des couches successives s'adaptent les uns aux autres.
Une approche pour apprendre un espace d'intégration partagé entre des jeux visuellement distincts.
Une nouvelle approche pour l'apprentissage de la structure sous-jacente de jeux visuellement distincts combinant des couches convolutionnelles pour le traitement des images d'entrée, la critique asynchrone de l'acteur pour l'apprentissage par renforcement profond et une approche contradictoire pour forcer la représentation d'intégration à être indépendante de la représentation visuelle des jeux.
Présente une méthode pour apprendre une politique sur des jeux visuellement distincts en adaptant l'apprentissage par renforcement profond.
Cet article présente une architecture d'agent qui utilise une représentation partagée pour entraîner plusieurs tâches avec des statistiques visuelles différentes au niveau des sprites.
Nous étudions l'équation d'état d'un réseau neuronal récurrent. Nous montrons que le SGD peut apprendre efficacement la dynamique inconnue à partir de quelques observations d'entrée/sortie sous des hypothèses appropriées.
L'article étudie les systèmes dynamiques à temps discret avec une équation d'état non linéaire, prouvant que l'exécution de SGD sur une trajectoire de longueur fixe donne une convergence logarithmique.
Ce travail considère le problème de l'apprentissage d'un système dynamique non linéaire dans lequel la sortie est égale à l'état. 
Cet article étudie la capacité de SGD à apprendre la dynamique d'un système linéaire et l'activation non linéaire.
Une nouvelle méthode de distillation des connaissances pour l'apprentissage par transfert
Ce travail introduit une méthode de distillation des connaissances utilisant le concept de collecteur de neurones proposé. 
Propose une méthode de distillation des connaissances dans laquelle le collecteur neuronal est considéré comme la connaissance transférée.
Nous présentons une méthode simple et générale pour former un seul réseau neuronal exécutable à différentes largeurs (nombre de canaux dans une couche), permettant des compromis instantanés et adaptatifs entre précision et efficacité au moment de l'exécution.
L'article propose de combiner des modèles de taille différente en un seul réseau partagé, ce qui améliore considérablement les performances de détection.
Cet article entraîne un seul exécutable réseau à différentes largeurs.
Réseau de similarité pour apprendre une estimation non métrique de la similarité visuelle entre une paire d'images
Les auteurs proposent une mesure de similarité d'apprentissage pour la similarité visuelle et obtiennent par ce biais une amélioration des ensembles de données très connus d'Oxford et de Paris pour la récupération d'images.
L'article soutient qu'il est plus approprié d'utiliser des distances non métriques plutôt que des distances métriques.
Un nouvel apprentissage contradictoire cyclique augmenté d'un modèle de tâche auxiliaire qui améliore les performances d'adaptation au domaine dans des situations supervisées et non supervisées à faibles ressources. 
Propose une extension des méthodes d'adaptation adversatives cohérentes avec le cycle afin de s'attaquer à l'adaptation au domaine lorsque des données cibles supervisées limitées sont disponibles.
Cet article introduit une approche d'adaptation au domaine basée sur l'idée du GAN cyclique et propose deux algorithmes différents.
Nous développons une méthode d'apprentissage des signatures structurelles dans les réseaux basée sur la diffusion des ondelettes spectrales de graphes.
Utilisation des modèles de diffusion spectrale par ondelettes du voisinage local d'un nœud pour intégrer le nœud dans un espace de faible dimension.
L'article présente une méthode de comparaison des nœuds d'un graphe basée sur l'analyse en ondelettes du laplacien du graphe. 
Un simulateur de conduite en réalité mixte utilisant des caméras stéréo et la RV par transparence a été évalué dans le cadre d'une étude auprès de 24 participants.
Propose un système complexe pour la simulation de conduite.
Cet article présente un simulateur de conduite à réalité mixte permettant d'améliorer la sensation de présence.
Propose un simulateur de conduite en réalité mixte qui intègre la génération de trafic et revendique une "présence" accrue grâce à un système de RM.
Règles de quadrature pour l'approximation par noyau.
L'article propose d'améliorer l'approximation du noyau des caractéristiques aléatoires en utilisant des règles de quadrature comme les règles sphériques-radiales stochastiques.
Les auteurs proposent une nouvelle version de l'approche des cartes de caractéristiques aléatoires pour résoudre de manière approximative les problèmes de noyaux à grande échelle.
Cet article montre que les techniques dues à Genz & Monahan (1998) peuvent être utilisées pour obtenir une faible erreur d'approximation des noyaux dans le cadre de la caractéristique de fourier aléatoire, une nouvelle façon d'appliquer les règles de quadrature pour améliorer l'approximation des noyaux.
Comment construire des haut-parleurs/auditeurs neuronaux qui apprennent les caractéristiques fines des objets 3D, à partir du langage référentiel.
Les auteurs présentent une étude sur l'apprentissage de la référence aux objets 3D, en collectant un ensemble de données d'expressions référentielles et en formant plusieurs modèles en expérimentant un certain nombre de choix architecturaux.
Nous présentons un cadre pour l'apprentissage de représentations centrées sur l'objet, adaptées à la planification dans des tâches qui nécessitent une compréhension de la physique.
L'article présente une plateforme permettant de prédire les images d'objets interagissant entre eux sous l'effet des forces gravitationnelles.
L'article présente une méthode qui apprend à reproduire des "tours de blocs" à partir d'une image donnée.
Propose une méthode qui apprend à raisonner sur l'interaction physique de différents objets sans supervision des propriétés des objets.
Nous fournissons des conditions nécessaires et suffisantes vérifiables efficacement pour l'optimalité globale des réseaux neuronaux linéaires profonds, avec quelques extensions initiales à des paramètres non linéaires.
L'article donne des conditions pour l'optimalité globale de la fonction de perte des réseaux neuronaux linéaires profonds.
L'article donne des résultats théoriques concernant l'existence de minima locaux dans la fonction objectif des réseaux neuronaux profonds.
Étudie certaines propriétés théoriques des réseaux linéaires profonds.
Utilisation d'un modèle auto-encodeur récurrent pour extraire des caractéristiques multidimensionnelles de séries temporelles
Ce document décrit une application de l'autoencodeur récurrent pour analyser des séries temporelles multidimensionnelles.
L'article décrit un modèle d'auto-encodeur séquence à séquence qui est utilisé pour apprendre des représentations de séquences, montrant que pour leur application, de meilleures performances sont obtenues lorsque le réseau est seulement entraîné à reconstruire un sous-ensemble des mesures de données. 
Propose une stratégie inspirée du modèle d'auto-encodeur récurrent de sorte que le regroupement de données multidimensionnelles de séries temporelles puisse être effectué sur la base de vecteurs de contexte.
Nous présentons un cadre d'encodage-décodage de graphe à graphe pour l'apprentissage de diverses traductions de graphes.
Propose un modèle de traduction de graphe en graphe pour l'optimisation des molécules, inspiré de l'analyse des paires moléculaires appariées.
Extension de la JT-VAE au scénario de traduction de graphe à graphe en ajoutant la variable latente pour capturer la multi-modalité et une régularisation adversariale dans l'espace latent.
Propose un système assez complexe, impliquant de nombreux choix et composants différents, pour obtenir des compendiums chimiques aux propriétés améliorées à partir de corpus donnés.
Nous apprenons un solveur neuronal rapide pour les EDP qui a des garanties de convergence.
Développe une méthode pour accélérer la méthode des différences finies dans la résolution des EDP et propose un cadre révisé pour l'itération du point fixe après discrétisation.
Les auteurs proposent une méthode linéaire pour accélérer les solveurs d'EDP.
Nous réalisons une inférence variationnelle fonctionnelle sur les processus stochastiques définis par les réseaux neuronaux bayésiens.
Ajustement d'approximations variationnelles de réseaux neuronaux bayésiens sous forme fonctionnelle et en considérant l'appariement à un antécédent de processus stochastique implicitement via des échantillons.
Présente un nouvel objectif ELBO pour l'entraînement des BNNs qui permet d'encoder des prieurs plus significatifs dans le modèle plutôt que les prieurs de poids moins informatifs caractéristiques de la littérature.
Présente un nouvel algorithme d'inférence variationnelle pour les modèles de réseaux neuronaux bayésiens où la priorité est spécifiée de manière fonctionnelle plutôt que par une priorité sur les poids. 
Nous intégrons les mots dans l'espace hyperbolique et faisons le lien avec les intégrations gaussiennes de mots.
Cet article adapte l'encastrement de mots Glove à un espace hyperbolique donné par le modèle du demi-plan de Poincaré.
Cet article propose une approche pour mettre en œuvre un modèle d'incorporation de mots hyperbolique basé sur GLOVE, qui est optimisé via les méthodes d'optimisation riemannienne.
Les réseaux de mémoire n'apprennent pas le raisonnement multi-sauts, sauf si nous les supervisons.
Le raisonnement multi-sauts n'est pas facile à apprendre directement et nécessite une supervision directe. Le fait de bien réussir sur WikiHop ne signifie pas nécessairement que le modèle apprend réellement à sauter.
L'article propose d'étudier le problème bien connu de l'apprentissage des réseaux de mémoire et plus précisément la difficulté de la supervision de l'apprentissage de l'attention avec de tels modèles.
Cet article soutient que le réseau de mémoire ne parvient pas à apprendre un raisonnement multi-sauts raisonnable.
Nous introduisons des réseaux génératifs qui n'ont pas besoin d'être appris avec un discriminateur ou un encodeur ; ils sont obtenus en inversant un opérateur d'encastrement spécial défini par une transformée de diffusion en ondelettes.
Présente les transformées de diffusion en tant que modèles génératifs d'images dans le contexte des réseaux adversariens génératifs et suggère pourquoi elles pourraient être considérées comme des transformées de gaussianisation avec une perte d'information et une inversion contrôlées. 
L'article propose un modèle génératif pour les images qui ne nécessite pas l'apprentissage d'un discriminateur (comme dans les GAN) ou d'un encastrement appris.
Nous proposons un nouveau modèle de lecture rapide neuronale qui utilise la structure de ponctuation inhérente d'un texte pour définir un comportement efficace de saut et d'évitement.
L'article propose un modèle Structural-Jump-LSTM pour accélérer la lecture automatique avec deux agents au lieu d'un.
Propose un nouveau modèle de lecture rapide neuronale dans lequel le nouveau lecteur a la capacité de sauter un mot ou une séquence de mots.
L'article propose une méthode de lecture rapide utilisant des actions de saut et d'évitement, montrant que la méthode proposée est aussi précise que le LSTM mais utilise beaucoup moins de calcul.
Nous proposons l'architecture IRN pour augmenter la récompense d'achat éparse et retardée pour la recommandation basée sur la session.
Cet article propose d'améliorer les performances des systèmes de recommandation par l'apprentissage par renforcement en utilisant un réseau de reconstruction de l'imagination.
L'article présente une approche de recommandation basée sur la session en se concentrant sur les achats des utilisateurs plutôt que sur les clics. 
Explication théorique et empirique de la généralisation des algorithmes stochastiques d'apprentissage profond par la robustesse d'ensemble
Cet article présente une adaptation de la robustesse algorithmique de Xu&Mannor'12 et présente des limites d'apprentissage et une expérience montrant la corrélation entre la robustesse empirique de l'ensemble et l'erreur de généralisation. 
Propose une étude de la capacité de généralisation des algorithmes d'apprentissage profond en utilisant une extension de la notion de stabilité appelée robustesse d'ensemble et donne des limites sur l'erreur de généralisation d'un algorithme aléatoire en termes de paramètre de stabilité et fournit une étude empirique tentant de connecter la théorie à la pratique.
L'article a étudié la capacité de généralisation des algorithmes d'apprentissage du point de vue de la robustesse dans un contexte d'apprentissage profond.
Apprentissage en quelques clics PixelCNN
L'article propose d'utiliser l'estimation de la densité lorsque la disponibilité des données de formation est faible, en utilisant un modèle de méta-apprentissage.
Cet article examine le problème de l'estimation de la densité d'un ou de plusieurs tirages, en utilisant des techniques de méta-apprentissage qui ont été appliquées à l'apprentissage supervisé d'un ou de plusieurs tirages.
L'article se concentre sur l'apprentissage en quelques coups avec l'estimation de densité autorégressive et améliore PixelCNN avec l'attention neuronale et les techniques de méta-apprentissage.
Nous montrons que SGD apprend des réseaux neuronaux surparamétrés à deux couches avec des activations Leaky ReLU qui se généralisent de manière prouvable sur des données linéairement séparables.
L'article étudie des modèles surparamétrés capables d'apprendre des solutions bien généralisées en utilisant un réseau à une couche cachée avec une couche de sortie fixe.
Cet article montre que, sur des données linéairement séparées, la SGD sur un réseau surparamétré peut toujours conduire à un classificateur qui se généralise de manière prouvable.
L'entraînement des agents avec des goulots d'étranglement en matière d'information sur les politiques d'objectifs favorise le transfert et permet d'obtenir une prime d'exploration puissante.
Propose de régulariser les pertes RL standard avec l'information mutuelle conditionnelle négative pour la recherche de politique dans un cadre RL multi-objectifs.
Cet article propose le concept d'état de décision et propose une régularisation de divergence KL pour apprendre la structure des tâches et utiliser cette information pour encourager la politique à visiter les états de décision.
L'article propose une méthode de régularisation des politiques conditionnées par un objectif avec un terme d'information mutuelle. 
Nous proposons une méthode d'optimisation pour les cas où seuls des gradients biaisés sont disponibles - nous définissons un nouvel estimateur de gradient pour ce scénario, nous dérivons le biais et la variance de cet estimateur, et nous l'appliquons à des exemples de problèmes.
Les auteurs proposent une approche qui combine la recherche aléatoire avec l'information sur le gradient de substitution et présentent une discussion sur le compromis variance-bias ainsi qu'une discussion sur l'optimisation des hyperparamètres.
 L'article propose une méthode pour améliorer la recherche aléatoire en construisant un sous-espace des k gradients de substitution précédents.
Cet article tente d'accélérer l'évolution du type OpenAI en introduisant une distribution non isotrophe avec une matrice de covariance de la forme I + UU^t et des informations externes telles qu'un gradient de substitution pour déterminer U
Un GAN utilisant des opérations de convolution de graphes avec des graphes calculés dynamiquement à partir de caractéristiques cachées.
L'article propose une version des GANs spécifiquement conçue pour générer des nuages de points, l'opération de suréchantillonnage constituant la principale contribution de ce travail.
Cet article propose des GAN à convolution graphique pour les nuages de points 3D irréguliers qui apprennent le domaine et les caractéristiques en même temps.
Nous présentons un algorithme rapide et facile à mettre en œuvre, qui est robuste au bruit des données.
L'article vise à éliminer les exemples potentiels avec le bruit de l'étiquette en écartant ceux qui présentent de grandes pertes dans la procédure de formation.
Les attaques basées sur le gradient contre les réseaux neuronaux binarisés ne sont pas efficaces en raison de la non-différentiabilité de ces réseaux. Notre algorithme IPROP résout ce problème en utilisant l'optimisation des nombres entiers.
Propose un nouvel algorithme de style de propagation de cible pour générer des attaques adverses fortes sur les réseaux neuronaux binarisés.
Cet article propose un nouvel algorithme d'attaque basé sur MILP sur les réseaux neuronaux binaires.
Cet article présente un algorithme permettant de trouver des attaques adverses contre des réseaux neuronaux binaires. Cet algorithme trouve itérativement les représentations souhaitées couche par couche, du sommet à l'entrée, et est plus efficace que la résolution complète de la programmation linéaire mixte en nombres entiers (MILP).
Le décodage du dernier token du contexte à l'aide de la distribution prédite du token suivant agit comme un régularisateur et améliore la modélisation du langage.
Les auteurs introduisent l'idée d'un décodage passé à des fins de régularisation pour améliorer la perplexité sur Penn Treebank.
Propose un terme de perte supplémentaire à utiliser lors de la formation d'un LSTM LM et montre qu'en ajoutant ce terme de perte, ils peuvent atteindre la perplexité SOTA sur un certain nombre de repères LM.
Suggère une nouvelle technique de régularisation qui peut être ajoutée à celles utilisées dans le modèle AWD-LSTM de Merity et al. (2017) avec une faible surcharge.
Proposer d'observer les ordres implicites dans les ensembles de données dans une optique de modèle génératif.
Les auteurs traitent du problème de l'ordre implicite dans un ensemble de données et de la difficulté de le récupérer. Ils proposent d'apprendre un modèle sans métrique de distance qui suppose une chaîne de Markov comme mécanisme génératif des données. 
L'article propose des réseaux de Markov génératifs - une approche basée sur l'apprentissage profond pour modéliser les séquences et découvrir l'ordre dans les ensembles de données.
Propose d'apprendre l'ordre d'un échantillon de données non ordonné par l'apprentissage d'une chaîne de Markov.
Synthèse de programmes à partir d'une description en langage naturel et d'exemples d'entrée/sortie via la recherche d'arbres sur le modèle Seq2Tree.
Présente un modèle seq2Tree pour traduire un énoncé de problème en langage naturel en programme fonctionnel correspondant en DSL, qui a montré une amélioration par rapport à l'approche de base seq2seq.
Cet article aborde le problème de la synthèse de programmes lorsqu'on dispose d'une description du problème et d'un petit nombre d'exemples d'entrée-sortie.
L'article présente une technique de synthèse de programmes impliquant une grammaire restreinte de problèmes qui est analysée par faisceau à l'aide d'un réseau encodeur-décodeur attentionnel.
Cet article étudie les propriétés de discrimination et de généralisation des GAN lorsque l'ensemble des discriminateurs est une classe de fonctions restreinte comme les réseaux neuronaux.
Équilibre les capacités des classes de générateurs et de discriminateurs dans les GAN en garantissant que les IPM induits sont des métriques et non des pseudo-métriques.
Cet article fournit une analyse mathématique du rôle de la taille de l'ensemble adversaire/discriminateur dans les GAN.
Tout ce dont vous avez besoin pour former des réseaux résiduels profonds est une bonne initialisation ; les couches de normalisation ne sont pas nécessaires.
Une méthode est présentée pour l'initialisation et la normalisation des réseaux résiduels profonds. Cette méthode est basée sur des observations de l'explosion vers l'avant et vers l'arrière dans de tels réseaux. Les performances de la méthode sont comparables aux meilleurs résultats obtenus par d'autres réseaux avec une normalisation plus explicite.
Les auteurs proposent une nouvelle façon d'initialiser les réseaux résiduels, qui est motivée par la nécessité d'éviter les gradients explosifs/évanouissants.
Propose une nouvelle méthode d'initialisation utilisée pour entraîner des RedNets très profonds sans utiliser de norme de lot.
Cet article vise à apprendre une meilleure métrique pour l'apprentissage non supervisé, tel que la génération de texte, et montre une amélioration significative par rapport à SeqGAN.
Décrit une approche pour générer des séquences temporelles en apprenant des valeurs état-action, où l'état est la séquence générée jusqu'à présent, et l'action est le choix de la valeur suivante. 
Cet article examine le problème de l'amélioration de la génération de séquences par l'apprentissage de meilleures métriques, en particulier le problème du biais d'exposition.
Décomposer la tâche d'apprentissage d'un modèle génératif en apprenant des facteurs latents démêlés pour des sous-ensembles de données, puis en apprenant le joint sur ces facteurs latents.  
Facteurs localement désenchevêtrés pour le modèle génératif hiérarchique à variables latentes, qui peut être considéré comme une variante hiérarchique de l'inférence adversariale apprise.
L'article étudie le potentiel des modèles de variables latentes hiérarchiques pour générer des images et des séquences d'images et propose d'entraîner plusieurs modèles ALI empilés les uns sur les autres pour créer une représentation hiérarchique des données.
L'article vise à apprendre les hiérarchies pour l'entraînement du GAN dans un programme d'optimisation hiérarchique directement au lieu d'être conçu par un humain.
Nous proposons un modèle conjoint pour incorporer les connaissances visuelles dans les représentations de phrases.
L'article propose une méthode permettant d'utiliser des vidéos accompagnées de légendes pour améliorer l'intégration des phrases.
Cette soumission propose un modèle pour l'apprentissage de phrases dont les représentations sont fondées, sur la base de données vidéo associées.
Propose une méthode pour améliorer les incorporations de phrases basées sur le texte grâce à un cadre multimodal conjoint.
