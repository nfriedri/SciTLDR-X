Nous concevons un échelonnement adaptatif des pertes pour améliorer l'apprentissage de la précision mixte qui surpasse les résultats de l'état de l'art.
Proposition d'une méthode de mise à l'échelle adaptative des pertes pendant la rétropropagation pour l'apprentissage de précision mixte où le taux d'échelle est décidé automatiquement pour réduire le sous-débit.
Les auteurs proposent une méthode d'entraînement des modèles en précision FP16 qui adopte une méthode plus élaborée pour minimiser l'underflow dans chaque couche simultanément et automatiquement.
Nous présentons une nouvelle approche pour apprendre à prédire des ensembles dont la permutation et la cardinalité sont inconnues, en utilisant des réseaux neuronaux profonds à action directe.
Une formulation pour apprendre la distribution sur les variables de permutation inobservables basée sur des réseaux profonds pour le problème de prédiction d'ensemble.
Nous comparons les performances de reconnaissance d'objets sur des images qui sont sous-échantillonnées uniformément et avec trois schémas de fovéation différents.
Nous développons des méthodes pour former des modèles neuronaux profonds qui sont à la fois robustes aux perturbations adverses et dont la robustesse est beaucoup plus facile à vérifier.
L'article présente plusieurs façons de régulariser les réseaux ReLU simples pour optimiser la robustesse contre les adversaires, la robustesse contre les adversaires prouvable et la vitesse de vérification.
Cet article propose des méthodes pour former des réseaux neuronaux robustes qui peuvent être vérifiés plus rapidement, en utilisant des méthodes d'élagage pour encourager la sparsité des poids et la régularisation pour encourager la stabilité ReLU.
Étude de la façon dont BatchNorm provoque une vulnérabilité adverse et comment l'éviter. 
Cet article traite de la vulnérabilité aux perturbations adverses dans BatchNorm, et propose une alternative appelée RobustNorm, qui utilise le redimensionnement min-max au lieu de la normalisation.
Cet article étudie la raison de la vulnérabilité de BatchNorm et propose la normalisation robuste, une méthode de normalisation qui permet d'obtenir de bien meilleurs résultats face à diverses méthodes d'attaque.
Notre réseau d'imputation variationnel-récurrent (V-RIN) tient compte des caractéristiques corrélées, de la dynamique temporelle et utilise en outre l'incertitude pour atténuer le risque d'estimations biaisées des valeurs manquantes.
Un réseau d'imputation des données manquantes pour incorporer la corrélation, les relations temporelles et l'incertitude des données pour le problème de la rareté des données dans les DSE, qui donne une meilleure AUC sur les tâches de classification du taux de mortalité.
L'article présente une méthode qui combine VAE et GRU conscient de l'incertitude pour l'imputation séquentielle des données manquantes et la prédiction des résultats.
Une méthode adaptative pour la quantification à point fixe des réseaux neuronaux basée sur une analyse théorique plutôt que sur des heuristiques. 
Propose une méthode de quantification des réseaux neuronaux qui permet de quantifier les poids avec une précision différente selon leur importance, en tenant compte de la perte.
L'article propose une technique pour quantifier les poids d'un réseau neuronal avec une profondeur de bit/précision variant sur une base par paramètre.
Sur la base de la théorie des ensembles flous, nous proposons un modèle qui, compte tenu uniquement des tailles des différences symétriques entre les paires de multisets, apprend des représentations de ces multisets et de leurs éléments.
Cet article propose une nouvelle tâche d'apprentissage des ensembles, la prédiction de la taille de la différence symétrique entre les multisets, et donne une méthode pour résoudre la tâche basée sur la théorie des ensembles flous.
Cet article propose une méthode assistée par l'apprentissage profond pour obtenir des échantillons crédibles d'agents intéressés. 
Les auteurs proposent un cadre d'élicitation d'échantillons pour le problème de l'élicitation d'échantillons crédibles auprès des agents pour des distributions complexes, suggèrent que les cadres neuronaux profonds peuvent être appliqués dans ce cadre, et relient l'élicitation d'échantillons et le f-GAN.
Cet article étudie le problème de l'élicitation d'échantillons, en proposant une approche d'apprentissage profond qui repose sur l'expression duale de la f-divergence qui s'écrit comme un maximum sur un ensemble de fonctions t.
Apprentissage de graphiques et de séquences avec des réseaux de neurones basés sur l'attention
Une architecture graph2seq qui combine un codeur de graphes mélangeant des composants GGNN et GCN avec un codeur de séquences attentionnelles, et qui montre des améliorations par rapport aux lignes de base.
Ce travail propose un modèle de bout en bout d'encodeur de graphe à décodeur de séquence avec un mécanisme d'attention entre les deux.
Un cadre de segmentation de type "zero-shot" pour la segmentation de parties d'objets 3D. Modélisation de la segmentation comme un processus de décision et résolution comme un problème de bandit contextuel.
Une méthode de segmentation des nuages de points 3D d'objets en parties constitutives, axée sur la généralisation des groupements de parties à de nouvelles catégories d'objets non vues pendant la formation, qui présente de bonnes performances par rapport aux lignes de base.
Cet article propose une méthode de segmentation des parties dans les nuages de points d'objets.
Une nouvelle perspective sur la manière de collecter la corrélation entre les nœuds sur la base des propriétés de diffusion.
Une nouvelle opération de diffusion pour les réseaux neuronaux à graphes qui ne nécessite pas le calcul des valeurs propres et peut se propager exponentiellement plus rapidement par rapport aux réseaux neuronaux à graphes traditionnels.
L'article propose de résoudre le problème de la vitesse de diffusion en introduisant la marche balistique.
Nous proposons un pipeline de formation à faible supervision basé sur le cadre de programmation de données pour les tâches de classement, dans lequel nous formons un modèle de classement basé sur BERT et établissons le nouveau SOTA.
Les auteurs proposent une combinaison de BERT et du cadre de supervision faible pour aborder le problème du classement des passages, obtenant des résultats meilleurs que l'état de l'art entièrement supervisé.
Dans les problèmes réels, nous avons constaté que les DNN adaptent souvent les fonctions cibles de basses à hautes fréquences pendant le processus de formation.
Cet article analyse la perte des réseaux neuronaux dans le domaine de Fourier et constate que les DNN ont tendance à apprendre les composantes à basse fréquence avant celles à haute fréquence.
L'article étudie le processus de formation des NNs par le biais de l'analyse de Fourier, et conclut que les NNs apprennent les composantes de basse fréquence avant les composantes de haute fréquence.
Nous proposons un codeur-décodeur multirésolution, à couplage hiérarchique, pour la traduction de graphe en graphe.
Un modèle hiérarchique de traduction de graphe en graphe pour générer des graphes moléculaires en utilisant des sous-structures chimiques comme blocs de construction. Ce modèle est entièrement autorégressif et apprend des représentations multi-résolution cohérentes, ce qui surpasse les modèles précédents.
Les auteurs présentent une méthode de traduction hiérarchique de graphe en graphe pour générer de nouvelles molécules organiques.
Nous utilisons l'attention pour restreindre les réseaux neuronaux équivariants à l'ensemble des transformations cooccurrentes dans les données. 
Cet article combine l'attention avec l'équivariance de groupe, en examinant spécifiquement le groupe p4m des rotations, translations et retournements, et dérive une forme d'auto-attention qui ne détruit pas la propriété d'équivariance.
Les auteurs proposent un mécanisme d'auto-attention pour les réseaux neuronaux équivariants de rotation qui améliore les performances de classification par rapport aux réseaux équivariants de rotation ordinaires.
Nous entraînons un GAN pour générer et récupérer des squelettes de protéines à atomes entiers, et nous montrons que dans certains cas, nous pouvons récupérer les protéines générées après la conception de la séquence et le repliement ab initio.
Un modèle génératif pour le squelette des protéines qui utilise un GAN, un réseau de type autoencodeur et un processus de raffinement, et un ensemble d'évaluations qualitatives suggérant des résultats positifs.
Cet article présente une approche de bout en bout pour générer des squelettes de protéines à l'aide de réseaux adversatifs génératifs.
Meta Learning pour Few L'apprentissage par tirs suppose que les tâches de formation et les tâches de test sont tirées de la même distribution. Que faites-vous si ce n'est pas le cas ? Le méta-apprentissage avec adaptation du domaine au niveau de la tâche !
Cet article propose un modèle combinant l'adaptation de domaine adversariale non supervisée avec des réseaux prototypiques qui donne de meilleurs résultats que les bases de l'apprentissage en quelques coups sur des tâches d'apprentissage en quelques coups avec changement de domaine.
Les auteurs ont proposé une adaptation au méta-domaine pour traiter le scénario de changement de domaine dans une configuration de méta-apprentissage, démontrant des améliorations de performance dans plusieurs expériences.
Diviser, Conquérir et Combiner est un nouveau schéma d'inférence qui peut être exécuté sur les programmes probabilistes à support stochastique, c'est-à-dire que l'existence même des variables est stochastique.
Un algorithme d'incorporation de nœuds préservant les communautés qui permet une détection plus efficace des communautés avec un regroupement sur l'espace incorporé.
Un modèle d'apprentissage profond autorégressif pour générer divers nuages de points.
Une approche pour générer des formes 3D sous forme de nuages de points qui prend en compte l'ordre lexicographique des points selon les coordonnées et entraîne un modèle pour prédire les points dans l'ordre.
L'article présente un modèle génératif pour les nuages de points utilisant un modèle auto-régressif de type RNN de pixels et un modèle d'attention pour gérer les interactions à plus longue portée.
Décrit une série de techniques d'explicabilité appliquées à un simple contrôleur de réseau neuronal utilisé pour la navigation.
Cet article fournit des idées et des explications sur le problème de la fourniture d'explications pour un perceptron multicouche utilisé comme contrôleur inverse pour le mouvement du rover, et des idées sur la façon d'expliquer un modèle de boîte noire.
Nous proposons un agent d'autocontrôle pour la tâche de navigation par vision et langage.
Une méthode de navigation vision+langage qui permet de suivre la progression de l'instruction à l'aide d'un moniteur de progression et d'un module de co-référencement visuel-textuel, et qui obtient de bons résultats aux tests de référence standard.
Cet article décrit un modèle de navigation par vision et langage avec une attention visuelle panoramique et une perte auxiliaire de suivi de la progression, en donnant des résultats à la pointe de la technologie.
découverte d'événements pour représenter l'historique de l'agent dans RL
Les auteurs étudient le problème de RL sous des paramètres partiellement observés, et proposent une solution qui utilise un FFNN mais qui fournit une représentation de l'historique, surpassant le PPO.
Cet article propose une nouvelle façon de représenter l'histoire passée comme entrée d'un agent RL, qui s'avère plus performante que le PPO et une variante RNN du PPO.
Nous montrons que les modèles autorégressifs peuvent générer des images de haute fidélité. 
Une architecture utilisant des composants de décodeur, de décodeur de mise à l'échelle de la taille et de décodeur de mise à l'échelle de la profondeur pour résoudre le problème de l'apprentissage des dépendances à longue portée dans les images afin d'obtenir des images de haute fidélité.
Cet article aborde le problème de la génération d'images de haute fidélité, en montrant avec succès des échantillons Imagenet convaincants avec une résolution de 128x128 pour un modèle de densité de vraisemblance.
Un modèle d'espace d'état hiérarchique profond dans lequel les transitions d'état d'objets corrélés sont coordonnées par des réseaux de neurones à graphes.
Un modèle hiérarchique à variables latentes de processus dynamiques séquentiels de plusieurs objets lorsque chaque objet présente une stochasticité significative.
L'article présente un modèle relationnel d'espace d'état qui simule les transitions d'état conjointes d'objets corrélés qui sont coordonnés hiérarchiquement dans une structure de graphe.
Nous introduisons un nouveau biais inductif qui intègre les structures arborescentes dans les réseaux neuronaux récurrents.
Cet article propose ON-LSTM, une nouvelle unité RNN qui intègre la structure d'arbre latent dans les modèles récurrents et qui donne de bons résultats sur la modélisation du langage, l'analyse syntaxique non supervisée, l'évaluation syntaxique ciblée et l'inférence logique.
Les manifolds dégénérés découlant de la non-identifiabilité du modèle ralentissent l'apprentissage dans les réseaux profonds ; les connexions sautées aident en brisant les dégénérescences.
Les auteurs montrent que les singularités d'élimination et les singularités de chevauchement entravent l'apprentissage dans les réseaux neuronaux profonds, et démontrent que les connexions par saut peuvent réduire la prévalence de ces singularités, accélérant ainsi l'apprentissage.
L'article examine l'utilisation de connexions sautées dans les réseaux profonds comme moyen d'atténuer les singularités de la matrice hessienne pendant la formation.
Apprendre des représentations d'état qui capturent les facteurs nécessaires au contrôle
Une approche de l'apprentissage par représentation dans le contexte de l'apprentissage par renforcement qui distingue fonctionnellement deux étapes en termes d'actions nécessaires pour les atteindre.
L'article présente une méthode pour apprendre des représentations où la proximité en distance euclidienne représente des états qui sont atteints par des politiques similaires.
Nous étudions le comportement d'un CNN lorsqu'il maîtrise de nouvelles tâches tout en conservant la maîtrise des tâches apprises précédemment.
Morty réadapte les incorporations de mots prétraitées pour soit : (a) améliorer la performance globale de l'intégration (pour les paramètres multi-tâches) ou améliorer la performance d'une seule tâche, tout en ne demandant qu'un effort minimal.
L'augmentation sélective des points difficiles à classer permet une formation efficace.
Les auteurs étudient le problème de l'identification des stratégies de sous-échantillonnage pour l'augmentation des données et proposent des stratégies basées sur l'influence et la perte des modèles ainsi qu'une évaluation comparative empirique des méthodes proposées.
Les auteurs proposent d'utiliser des méthodes basées sur l'influence ou la perte pour sélectionner un sous-ensemble de points à utiliser pour augmenter les ensembles de données pour les modèles d'entraînement où la perte est additive sur les points de données.
Un modèle génératif profond pour les molécules organiques qui génère d'abord des blocs de construction réactifs avant de les combiner à l'aide d'un prédicteur de réaction.
Un modèle génératif moléculaire qui génère des molécules via un processus en deux étapes et qui fournit les voies de synthèse des molécules générées, permettant aux utilisateurs d'examiner l'accessibilité synthétique des composés générés.
Améliorer la robustesse et l'efficacité énergétique d'un réseau neuronal profond en utilisant les représentations cachées.
Cet article vise à réduire les erreurs de classification des réseaux neuronaux profonds d'une manière économe en énergie en ajoutant des cellules auxiliaires pertinentes basées sur des caractéristiques après une ou plusieurs couches cachées pour décider si la classification doit être interrompue prématurément.
Comprendre la structure de la représentation des graphes de connaissances à l'aide de l'intégration des mots.
Cet article tente de comprendre la structure latente qui sous-tend les méthodes d'intégration des graphes de connaissances, et démontre que la capacité d'un modèle à représenter un type de relation dépend des limites de l'architecture du modèle en ce qui concerne les conditions de relation.
Cet article propose une étude détaillée sur l'explicabilité des modèles de prédiction de liens (LP) en utilisant une interprétation récente des encastrements de mots pour fournir une meilleure compréhension de la performance des modèles LP.
Un nouveau mécanisme d'auto-attention pour l'imputation de séries chronologiques multivariées et géolocalisées.
Cet article propose le problème de l'application du réseau de transformation aux données spatio-temporelles d'une manière efficace sur le plan computationnel, et étudie les moyens de mettre en œuvre l'attention 3D.
Cet article étudie empiriquement l'efficacité des modèles de transformation pour l'imputation de données de séries temporelles à travers les dimensions de l'entrée.
Nous avons conçu et testé un REDNET (ResNet Encoder-Decoder) avec 8 connexions de saut pour supprimer le bruit des documents, y compris le flou et les filigranes, ce qui donne un réseau profond très performant pour le nettoyage des images de documents. 
Nous identifions une famille de techniques de défense et montrons que la compression déterministe avec perte et les perturbations aléatoires de l'entrée conduisent à des gains similaires de robustesse.
Cet article examine les moyens de déstabiliser une attaque contradictoire donnée, ce qui rend les images contradictoires non robustes, et s'il est possible pour les attaquants d'utiliser un modèle universel de perturbations pour rendre leurs exemples contradictoires robustes contre ces perturbations.
L'article étudie la robustesse des attaques adversariales aux transformations de leur entrée.
Nous proposons une méthode d'évaluation des optimiseurs qui tient compte du processus de réglage des hyperparamètres.
Introduction d'une nouvelle métrique pour capturer l'accordabilité d'un optimiseur, et une comparaison empirique complète des optimiseurs d'apprentissage profond sous différentes quantités d'accordage d'hyper-paramètres. 
Cet article présente une mesure simple de l'accordabilité qui permet de comparer les optimiseurs sous des contraintes de ressources, et montre que l'accord du taux d'apprentissage des optimiseurs d'Adam est le plus facile pour trouver des configurations d'hyperparamètres performantes.
Nous introduisons un réseau de neurones profonds semi-supervisé pour approximer la solution du problème de phase en microscopie électronique.
Word2net est une nouvelle méthode d'apprentissage de représentations de mots par réseaux de neurones qui peut utiliser des informations syntaxiques pour apprendre de meilleures caractéristiques sémantiques.
Cet article étend le SGNS avec un changement d'architecture, passant d'un modèle de type sac de mots à un modèle de type feedforward, et apporte une nouvelle forme de régularisation en liant un sous-ensemble de couches entre différents réseaux associés.
Une méthode pour utiliser la combinaison non linéaire des vecteurs de contexte pour apprendre la représentation vectorielle des mots, où l'idée principale est de remplacer chaque encastrement de mot par un réseau neuronal.
En utilisant une fenêtre de 10s de signaux IRMf, notre modèle GCN a identifié 21 conditions de tâches différentes à partir de l'ensemble de données HCP avec une précision de test de 89%.
Induction efficace de réseaux neuronaux profonds à faible rang via l'apprentissage SVD avec des valeurs singulières éparses et des vecteurs singuliers orthogonaux.
Cet article présente une approche de la compression de réseau en encourageant la matrice de poids de chaque couche à avoir un rang faible et en factorisant explicitement les matrices de poids dans une factorisation de type SVD pour les traiter comme de nouveaux paramètres.
Proposition de paramétrer chaque couche d'un réseau de neurones profonds, avant l'entraînement, avec une décomposition matricielle de bas rang, de remplacer en conséquence les convolutions par deux convolutions consécutives, puis d'entraîner la méthode décomposée.
Nous proposons un modèle d'apprentissage en quelques coups qui est conçu spécifiquement pour les tâches de régression.
Cet article propose une nouvelle méthode d'apprentissage par injection pour les problèmes de régression sur petits échantillons.
Une méthode qui apprend un modèle de régression avec quelques échantillons et qui surpasse les autres méthodes.
Nous présentons une nouvelle approche pour détecter les pixels hors distribution dans la segmentation sémantique.
Cet article traite de la détection de la non-répartition pour aider le processus de segmentation, et propose une approche consistant à former un classificateur binaire qui distingue les taches d'image d'un ensemble connu de classes de celles d'un ensemble inconnu.
Cet article vise à détecter les pixels hors distribution pour la segmentation sémantique, et ce travail utilise des données provenant d'autres domaines pour détecter les classes indéterminées afin de mieux modéliser l'incertitude.
Quantification précise, rapide et automatisée d'un réseau neuronal à noyau avec une précision mixte à l'aide de l'apprentissage par renforcement profond hiérarchique.
Procédé de quantification des poids et des activations d'un réseau neuronal qui utilise l'apprentissage par renforcement profond pour sélectionner la largeur de bit des noyaux individuels dans une couche et qui obtient de meilleures performances, ou une meilleure latence, que les approches précédentes.
Cet article propose de rechercher automatiquement des schémas de quantification pour chaque noyau du réseau neuronal, en utilisant le RL hiérarchique pour guider la recherche. 
Gaggle, un système interactif d'analyse visuelle pour aider les utilisateurs à naviguer de manière interactive dans l'espace des modèles pour les tâches de classification et de classement.
Un nouveau système d'analyse visuelle qui vise à permettre aux utilisateurs non experts de naviguer de manière interactive dans un espace de modèles en utilisant une approche basée sur la démonstration.
Un système d'analyse visuelle qui aide les analystes novices à naviguer dans l'espace des modèles pour effectuer des tâches de classification et de classement.
Nous proposons un nouveau réseau d'attention avec l'encodeur hybird pour résoudre le problème de la représentation du texte dans la classification des textes chinois, en particulier les phénomènes linguistiques liés aux prononciations comme le polyphone et l'homophone.
Cet article propose un modèle basé sur l'attention composé de l'encodeur de mots et de l'encodeur Pinyin pour la tâche de classification de textes chinois, et étend l'architecture pour l'encodeur de caractères Pinyin.
Proposition d'un réseau d'attention où le mot et le pinyin sont tous deux pris en compte pour la représentation du chinois, avec des résultats améliorés montrés dans plusieurs ensembles de données pour la classification de textes.
apprentissage d'imitation multimodale à partir de démonstrations non structurées à l'aide d'une intention de modélisation par réseau de neurones stochastiques. 
Une nouvelle approche basée sur l'échantillonnage pour l'inférence dans les modèles à variables latentes qui s'applique à l'apprentissage de l'imitation multimodale et fonctionne mieux que les réseaux de neurones déterministes et les réseaux de neurones stochastiques pour une tâche réelle de robotique visuelle.
Cet article montre comment apprendre plusieurs modalités par imitation à partir de données visuelles en utilisant des réseaux de neurones stochastiques, ainsi qu'une méthode d'apprentissage à partir de démonstrations où plusieurs modalités de la même tâche sont données.
Une nouvelle approche pour construire des explications hiérarchiques pour la classification de textes en détectant les interactions entre les caractéristiques.
Une nouvelle méthode pour fournir des explications aux prédictions faites par les classificateurs de texte qui surpasse les résultats de base sur les scores d'importance au niveau des mots, et une nouvelle métrique, la perte de cohésion, pour évaluer l'importance au niveau de l'empan.
Une méthode d'interprétation basée sur les interactions entre les caractéristiques et le score d'importance des caractéristiques par rapport aux contributions indépendantes des caractéristiques.
Nous accélérons l'exécution des couches convolutionnelles en renforçant et en supprimant dynamiquement les canaux dans le calcul des caractéristiques.
Une méthode d'amplification et de suppression des caractéristiques pour l'élagage dynamique des canaux qui prédit l'importance de chaque canal et utilise ensuite une fonction affine pour amplifier/supprimer l'importance des canaux.
Proposition d'une méthode d'élagage des canaux pour la sélection dynamique des canaux pendant les tests.
Les réseaux neuronaux peuvent être prédéfinis pour présenter une connectivité éparse sans dégradation des performances.
Cet article examine les modèles de connexions éparses dans les couches supérieures des réseaux de classification d'images convolutives, et introduit des heuristiques pour distribuer les connexions entre les fenêtres/groupes et une mesure appelée dispersion pour construire des masques de connectivité.
Proposition de réduire le nombre de paramètres appris par un réseau profond en mettant en place des poids de connexion épars dans les couches de classification, et introduction d'un concept de "dispersion".
Nous fournissons un point de référence complet, rigoureux et cohérent pour évaluer la robustesse adversariale des modèles d'apprentissage profond.
Cet article présente une évaluation de différents types de modèles de classification sous diverses méthodes d'attaque adversariale.
Une étude empirique à grande échelle comparant différentes techniques d'attaque et de défense adversariales, et l'utilisation de courbes de précision par rapport au budget de perturbation et de précision par rapport à la force d'attaque pour évaluer les attaques et les défenses.
Nous proposons une modification des réseaux neuronaux artificiels traditionnels motivée par la biologie des neurones pour permettre à la forme de la fonction d'activation de dépendre du contexte.
Une méthode pour mettre à l'échelle les activations d'une couche de neurones dans un ANN en fonction des entrées à cette couche qui rapporte des améliorations au-dessus des lignes de base.
Introduction d'un changement d'architecture pour les neurones de base dans un réseau neuronal, et l'idée de multiplier la sortie de la combinaison linéaire des neurones par un modulateur avant de l'introduire dans la fonction d'activation.
Nous identifions le problème de l'oubli dans le réglage fin des modèles NLG pré-entraînés, et proposons la stratégie de révision des mélanges pour le résoudre.
Cet article analyse le problème de l'oubli dans le cadre de la préformation et du réglage fin du point de vue de la sensibilité au contexte et du transfert de connaissances, et propose une stratégie de réglage fin qui surpasse la méthode de décroissance du poids.
Étude du problème de l'oubli dans le cadre de la méthode "pretrain-finetune", en particulier dans les tâches de génération de réponses de dialogue, et proposition d'une stratégie de révision mixte pour atténuer le problème de l'oubli.
L'amélioration de la modélisation des systèmes complexes fait appel à la composition de modèles hybrides neuronaux/domaines, à de nouvelles fonctions de perte de décorrélation et à des ensembles de tests extrapolatifs. 
Cet article mène des expériences pour comparer les prédictions extrapolatives de divers modèles hybrides composés de modèles physiques, de réseaux neuronaux et de modèles stochastiques, et s'attaque au problème de la dynamique non modélisée qui constitue un goulot d'étranglement.
Cet article présente des approches permettant de combiner les réseaux neuronaux avec des modèles non-NN pour prédire le comportement de systèmes physiques complexes.
Nous apprenons des scores denses et un modèle dynamique en tant que prieurs à partir des données d'exploration et les utilisons pour induire une bonne politique dans les nouvelles tâches en condition de zéro coup.
Cet article traite de la généralisation du tir zéro dans de nouveaux environnements, et propose une approche avec des résultats sur Grid-World, Super Mario Bros, et 3D Robotics.
Une méthode visant à apprendre des prieurs agnostiques pour la généralisation de zéro coup, avec l'idée d'employer une approche de modélisation au-dessus du cadre RL basé sur le modèle.
Analyser les mécanismes sous-jacents de l'effondrement de la variance du SVGD en haute dimension.
Repenser la généralisation nécessite de revoir de vieilles idées : approches de la mécanique statistique et comportement d'apprentissage complexe
Les auteurs suggèrent que les idées de la mécanique statistique aideront à comprendre les propriétés de généralisation des réseaux neuronaux profonds, et donnent une approche qui fournit des descriptions qualitatives solides des résultats empiriques concernant les réseaux neuronaux profonds et les algorithmes d'apprentissage.
Un ensemble d'idées liées à la compréhension théorique des propriétés de généralisation des réseaux neuronaux multicouches, et une analogie qualitative entre les comportements de l'apprentissage profond et les résultats de l'analyse quantitative de la physique statistique des réseaux neuronaux à une et deux couches.
Nous présentons un softmax doublement clairsemé, le mélange clairsemé d'experts clairsemés, pour améliorer l'efficacité de l'inférence softmax en exploitant la hiérarchie de chevauchement à deux niveaux. 
Cet article propose une approximation rapide du calcul de la softmax lorsque le nombre de classes est très grand.
Cet article propose un mélange d'experts épars qui apprend une hiérarchie de classe à deux niveaux pour une inférence efficace de softmax.
Nous explorons l'utilisation de données oculométriques collectées passivement pour réduire la quantité de données étiquetées nécessaires pendant la formation.
Une méthode permettant d'utiliser les informations relatives au regard pour réduire la complexité de l'échantillon d'un modèle et l'effort d'étiquetage nécessaire pour obtenir une performance cible, avec de meilleurs résultats pour les échantillons de taille moyenne et les tâches plus difficiles.
Une méthode pour incorporer les signaux du regard dans les CNN standards pour la classification d'images, en ajoutant un terme de fonction de perte basé sur la différence entre la carte d'activation de classe du modèle et la carte construite à partir des informations de suivi des yeux.
Nous appliquons la correction des pertes aux réseaux neuronaux graphiques pour former un modèle plus robuste au bruit.
Cet article présente la correction des pertes pour les réseaux neuronaux graphiques afin de traiter le bruit symétrique des étiquettes des graphes, en se concentrant sur une tâche de classification des graphes.
Cet article propose l'utilisation d'une perte de correction du bruit dans le contexte des réseaux de neurones à graphes pour traiter les étiquettes bruyantes.
Nous utilisons la co-attention des graphes dans un système de formation de graphes appariés pour la classification et la régression des graphes.
Cet article injecte un mécanisme de co-attention à têtes multiples dans le GCN qui permet à un médicament d'assister à un autre médicament pendant la prédiction des effets secondaires du médicament.
Une méthode pour étendre l'apprentissage basé sur les graphes avec une couche co-attentionnelle, qui surpasse les autres précédentes sur une tâche de classification de graphes par paires.
Le sous-titrage d'images en tant que formation conditionnelle de GAN avec de nouvelles architectures, étudie également deux méthodes discrètes de formation de GAN. 
Un modèle GAN amélioré pour le sous-titrage d'images qui propose un sous-titreur LSTM sensible au contexte, introduit un discriminateur co-attentif plus fort avec de meilleures performances, et utilise le SCST pour l'entraînement du GAN.
Exploiter la courbure pour que les méthodes MCMC convergent plus rapidement que l'état de l'art.
Keras pour les réseaux neuronaux infinis.
Nous proposons une nouvelle fonction de perte qui permet d'obtenir des résultats de pointe dans la détection de la non-distribution avec l'exposition des aberrations, à la fois pour les tâches de classification d'images et de textes.
Cet article aborde les problèmes de la détection de la non-répartition et du calibrage du modèle en adaptant la fonction de perte de la technique d'exposition des aberrations. Les résultats démontrent une performance accrue par rapport à l'exposition des aberrations sur des repères de vision et de texte et un meilleur calibrage du modèle.
Proposition d'une nouvelle fonction de perte pour entraîner le réseau avec l'exposition aux aberrations qui conduit à une meilleure détection des aberrations par rapport aux fonctions de perte simples utilisant la divergence KL.
Un pré-entraînement agnostique peut façonner le paysage d'attracteurs du RNN et former divers biais inductifs pour différentes tâches de navigation.   
Cet article étudie les représentations internes des réseaux de neurones récurrents entraînés à des tâches de navigation et constate que les RNN pré entraînés à utiliser l'intégration des chemins contiennent des attracteurs continus 2D tandis que les RNN pré entraînés à la mémoire des points de repère contiennent des attracteurs discrets.
Cet article explore comment le pré-entraînement des réseaux récurrents sur différents objectifs de navigation confère différents avantages pour la résolution des tâches en aval, et montre comment ce pré-entraînement se manifeste par différentes structures dynamiques dans les réseaux après le pré-entraînement.
Vérification par réseau neuronal des propriétés temporelles et des modèles de génération de séquences
Cet article étend la propagation des limites d'intervalle au calcul récurrent et aux modèles auto-régressifs, introduit et étend la logique temporelle des signaux pour spécifier les contraintes temporelles, et fournit la preuve que la STL avec la propagation des limites peut garantir la conformité des modèles neuronaux à la spécification temporelle.
Une façon d'entraîner les régresseurs de séries temporelles de manière vérifiable par rapport à un ensemble de règles définies par la logique temporelle du signal, et un travail de dérivation des règles de propagation des bornes pour le langage STL.
Nous proposons une solution universelle de réseaux neuronaux pour dériver automatiquement des architectures de réseaux neuronaux efficaces pour les données tabulaires.
Une nouvelle procédure de formation de réseaux neuronaux, conçue pour les données tabulaires, qui cherche à exploiter les grappes de caractéristiques extraites des GBDT.
Proposition d'un algorithme hybride d'apprentissage automatique utilisant des arbres de décision à relance par gradient et des réseaux neuronaux profonds, avec une orientation de recherche prévue sur des données tabulaires.
Système d'apprentissage de règles probabilistes utilisant l'inférence levée
Un modèle d'apprentissage de règles probabilistes pour automatiser le remplissage de bases de données probabilistes qui utilise AMIE+ et l'inférence levée pour améliorer l'efficacité du calcul.
Nous proposons le premier modèle neuronal non autorégressif pour le suivi de l'état du dialogue (DST), atteignant la précision SOTA (49,04%) sur le benchmark MultiWOZ2.1, et réduisant la latence d'inférence d'un ordre de grandeur.
Un nouveau modèle pour la tâche DST qui réduit la complexité du temps d'inférence avec un décodeur non autorégressif, obtient une précision DST compétitive et montre des améliorations par rapport aux autres lignes de base.
Proposition d'un modèle capable de suivre les états du dialogue de manière non récursive.
Une nouvelle architecture de réseau pour effectuer des zooms ou des gros plans en 3D profonde.
Une méthode pour créer une "image zoomée" pour une image d'entrée donnée, et une nouvelle perte de reconstruction par rétroprojection qui permet au réseau d'apprendre la structure 3D sous-jacente et de maintenir une apparence naturelle.
Un algorithme pour synthétiser le comportement du zoom 3D lorsque la caméra se déplace vers l'avant, une structure de réseau intégrant l'estimation de la disparité dans un cadre GAN pour synthétiser de nouvelles vues, et une nouvelle tâche de vision par ordinateur proposée.
Un raffinement quantitatif du théorème d'approximation universelle via une approche algébrique.
Les auteurs dérivent les preuves de la propriété d'approximation universelle de manière algébrique et affirment que les résultats sont généraux à d'autres types de réseaux neuronaux et d'apprenants similaires.
Une nouvelle preuve de la version de Leshno de la propriété d'approximation universelle pour les réseaux neuronaux, et de nouveaux aperçus de la propriété d'approximation universelle.
Cadre modulaire pour la classification des documents et technique d'agrégation des données pour rendre le cadre robuste à diverses distorsions et au bruit et se concentrer uniquement sur les mots importants. 
Les auteurs envisagent l'entraînement d'une classification de texte basée sur un RNN lorsqu'il y a une restriction de ressources sur la prédiction au moment du test, et proposent une approche utilisant un mécanisme de masquage pour réduire les mots/phrases/séquences utilisés dans la prédiction, suivi d'un classificateur pour traiter ces composants.
Permettre la connexion partielle de canaux dans les super-réseaux pour régulariser et accélérer la recherche d'architectures différentiables
Une extension de la méthode de recherche d'architecture neuronale DARTS qui remédie à son immense coût en mémoire en utilisant un sous-ensemble aléatoire de canaux et une méthode pour normaliser les bords.
Cet article propose d'améliorer le DARTS en termes d'efficacité de la formation, à partir d'importants frais de mémoire et de calcul, et propose un DARTS à connexion partielle avec connexion partielle des canaux et normalisation des bords.
Les agents interagissent (parlent, agissent) et peuvent atteindre des objectifs dans un monde riche au langage varié, comblant ainsi le fossé entre le bavardage et le dialogue axé sur les objectifs.
Cet article étudie une tâche de dialogue multi-agent dans laquelle l'agent d'apprentissage vise à générer des actions en langage naturel qui suscitent une action particulière de la part de l'autre agent, et montre que les agents RL peuvent atteindre des niveaux d'achèvement de tâche plus élevés que les bases d'apprentissage par imitation.
Cet article explore la mise en place d'un dialogue orienté vers un objectif avec l'apprentissage par renforcement dans un jeu d'aventure textuel fantastique et observe que les approches RL surpassent les modèles d'apprentissage supervisé.
Une nouvelle méthode partiellement agnostique de politique pour l'évaluation de politique hors politique à horizon infini avec de multiples politiques de comportement connues ou inconnues.
Une politique de mélange estimé qui reprend les idées des estimateurs à horizon infini de l'évaluation de la politique hors politique et de l'échantillonnage d'importance de régression pour le poids d'importance, et les étend à de nombreuses politiques et à des politiques inconnues.
Un algorithme pour résoudre l'évaluation de la politique hors horizon infini avec des politiques à comportements multiples en estimant une politique mixte sous régression, et une preuve théorique qu'un ratio de politique estimé peut réduire la variance.
Nous introduisons une architecture neuronale plus efficace pour l'inférence amortie, qui combine des flux de normalisation continus et conditionnels en utilisant un choix de principe de la structure de sparsité.
Nous montrons que ENAS avec l'optimisation ES dans RL est hautement évolutive, et nous l'utilisons pour compacter les politiques des réseaux neuronaux par le partage des poids.
Les auteurs construisent des politiques d'apprentissage par renforcement avec très peu de paramètres en comprimant un réseau neuronal à action directe, en le forçant à partager des poids et en utilisant une méthode d'apprentissage par renforcement pour apprendre la correspondance des poids partagés.
Cet article combine les idées des méthodes ENAS et ES pour l'optimisation, et introduit l'architecture de réseau chromatique, qui partitionne les poids du réseau RL en sous-groupes liés.
Nous présentons Deep SAD, une méthode profonde pour la détection semi-supervisée générale des anomalies qui tire particulièrement parti des anomalies étiquetées.
Une nouvelle méthode pour trouver des données anormales, lorsque certaines anomalies étiquetées sont données, qui applique la perte dérivée de la théorie de l'information basée sur les données normales ayant généralement une entropie plus faible que les données anormales.
Proposition d'un cadre de détection des anomalies dans des situations où des données non étiquetées, des données positives étiquetées et des données négatives étiquetées sont disponibles, et proposition d'approche de la DA semi-supervisée d'un point de vue théorique de l'information.
Cet article analyse la dynamique de formation et les points critiques de la formation du réseau ReLU profond via SGD dans le cadre d'une relation enseignant-étudiant. 
Étude de la sur-paramétrisation dans les réseaux ReLU multicouches élève-enseignant, une partie théorique sur les points critiques de SGD pour le cadre élève-enseignant, et une partie heuristique et empirique sur la dynamique de l'algorithme SDG en fonction des réseaux d'enseignants.
Sous certaines conditions sur les transformations linéaires d'entrée et de sortie, GD et SGD peuvent atteindre une convergence globale pour l'apprentissage de ResNets linéaires profonds.
Les auteurs étudient la convergence de la descente de gradient dans la formation de réseaux résiduels linéaires profonds, et établissent une convergence globale de GD/SGD et des taux de convergence linéaire de SG/SGD.
Etude des propriétés de convergence de GD et SGD sur les résnets linéaires profonds, et preuve que sous certaines conditions sur les transformations d'entrée et de sortie et avec une initialisation nulle, GD et SGD convergent vers des minima globaux.
Nous analysons le processus de formation des réseaux profonds et montrons qu'ils commencent par apprendre rapidement des exemples classifiables peu profonds et généralisent lentement à des points de données plus difficiles.
Apprentissage de MRFs profonds à variables latentes avec un objectif à point selle dérivé de l'approximation de la fonction de partition de Bethe.
Une méthode d'apprentissage de MRF latent-variable profond avec un objectif d'optimisation qui utilise l'énergie libre de Bethe, qui résout également les contraintes sous-jacentes des optimisations de l'énergie libre de Bethe.
Un objectif pour l'apprentissage de MRF à variables latentes basé sur l'énergie libre de Bethe et l'inférence amortie, différent de l'optimisation de l'ELBO standard.
Un cadre général pour la génération d'explications à l'aide de la logique.
Cet article étudie la génération d'explications d'un point de vue KR et mène des expériences mesurant la taille des explications et le temps d'exécution sur des formules aléatoires et des formules d'une instance de Blocksworld.
Cet article offre une perspective sur les explications entre deux bases de connaissances, et est parallèle aux travaux sur la réconciliation des modèles dans la littérature de planification.
Les réseaux neuronaux profonds et étroits convergeront vers des états moyens ou médians erronés de la fonction cible en fonction de la perte avec une forte probabilité.
Cet article étudie les modes de défaillance des réseaux profonds et étroits, en se concentrant sur des modèles aussi petits que possible pour lesquels le comportement indésirable se produit.
Cet article montre que la formation de réseaux neuronaux ReLU profonds convergera vers un classificateur constant avec une probabilité élevée par rapport à une initialisation aléatoire si les largeurs des couches cachées sont trop petites.
Nous proposons une formation MMA pour maximiser directement la marge de l'espace d'entrée afin d'améliorer la robustesse des adversaires, principalement en supprimant la nécessité de spécifier une limite de distorsion fixe.
Une approche de formation contradictoire basée sur la marge adaptative pour former des DNN robustes, en maximisant la marge la plus courte des entrées par rapport à la limite de décision, qui rend possible la formation contradictoire avec de grandes perturbations.
Une méthode d'apprentissage robuste contre les attaques adverses où la marge de l'espace d'entrée est directement maximisée et une variante softmax de la marge max est introduite.
Nous proposons une méthode de détection des anomalies avec les GAN en recherchant dans l'espace latent du générateur de bonnes représentations d'échantillons.
Les auteurs proposent d'utiliser le GAN pour la détection des anomalies, une méthode basée sur le gradient-descent pour mettre à jour de manière itérative les représentations latentes, et une nouvelle mise à jour des paramètres des générateurs.
Une approche basée sur le GAN pour faire de la détection d'anomalie pour les données d'image où l'espace latent du générateur est exploré pour trouver une représentation pour une image de test.
Le comportement transitoire des algorithmes MCMC et d'inférence variationnelle basés sur le gradient est plus similaire qu'on pourrait le croire, ce qui remet en question l'affirmation selon laquelle l'inférence variationnelle est plus rapide que le MCMC.
Un cadre de convolution graphique basé sur la composition pour les graphes multi-relationnels.
Les auteurs développent le GCN sur des graphes multi-relationnels et proposent le CompGCN, qui exploite les connaissances issues de l'intégration des graphes de connaissances et apprend les représentations des nœuds et des relations pour atténuer le problème de la sur-paramétrisation.
Cet article présente un cadre GCN pour les graphes multi-relationnels et généralise plusieurs approches existantes de l'intégration des graphes de connaissances en un seul cadre.
Nous quantifions entièrement le transformateur à 8 bits et améliorons la qualité de la traduction par rapport au modèle de précision totale.
Une méthode de quantification à 8 bits pour quantifier le modèle de traduction automatique Transformer, proposant d'utiliser une quantification uniforme min-max pendant l'inférence et des pondérations de type bucketing avant la quantification pour réduire l'erreur de quantification.
Une méthode de réduction de l'espace mémoire requis par une technique de quantification, axée sur la réduction de celui-ci pour l'architecture Transformer.
Latent Embedding Optimization (LEO) est un méta-apprentissage novateur basé sur le gradient, dont les performances sont à la pointe du progrès dans les tâches difficiles de classification de miniImageNet et de tieredImageNet à 5 voies et à 1 coup.
Un nouveau cadre de méta-apprentissage qui apprend l'espace latent dépendant des données, effectue une adaptation rapide dans l'espace latent, est efficace pour l'apprentissage en quelques coups, a une initialisation de l'adaptation dépendant de la tâche et fonctionne bien pour la distribution multimodale des tâches.
Cet article propose une méthode d'optimisation de l'intégration latente pour le méta-apprentissage, et affirme que la contribution est de découpler les techniques de méta-apprentissage basées sur l'optimisation de l'espace hautement dimensionnel des paramètres du modèle.
Les biais inductifs relationnels améliorent les capacités de généralisation hors distribution dans les agents d'apprentissage par renforcement sans modèle.
Une architecture de réseau relationnel partagé pour paramétrer le réseau d'acteurs et de critiques, axée sur les algorithmes distribués d'avantages acteurs-critiques, qui améliore les techniques de renforcement profond sans modèle avec des connaissances relationnelles sur l'environnement afin que les agents puissent apprendre des représentations d'état interprétables.
Une analyse et une évaluation quantitative et qualitative du mécanisme d'auto-attention combiné à un réseau de relations dans le contexte du RL sans modèle.
Nous proposons un modèle génératif simple pour la traduction non supervisée d'images et la détection de la saillance.
Nous définissons un concept de réseaux neuronaux profonds à modèles parallèles par couches, pour lesquels les couches fonctionnent en parallèle, et nous fournissons une boîte à outils pour concevoir, former, évaluer et interagir en ligne avec ces réseaux.
Une boîte à outils accélérée par GPU pour la mise à jour parallèle des neurones, écrite en Theano, qui prend en charge différents ordres de mise à jour dans les réseaux récurrents et les réseaux dont les connexions sautent des couches. 
Une nouvelle boîte à outils pour l'apprentissage et l'évaluation des réseaux neuronaux profonds, et une proposition pour un changement de paradigme des réseaux séquentiels en couches aux réseaux parallèles en couches.
Une méthode de défense contre les adversaires qui allie la robustesse des réseaux neuronaux profonds à la stabilité de Lyapunov.
Les auteurs formulent l'entraînement des NNs comme la recherche d'un contrôleur optimal pour un système dynamique discret, ce qui leur permet d'utiliser la méthode des approximations successives pour entraîner un NN de manière à être plus robuste aux attaques adverses.
Cet article utilise la vision théorique d'un réseau neuronal comme une EDO discrétisée pour développer une théorie de contrôle robuste visant à entraîner le réseau tout en renforçant la robustesse.
Nous proposons un schéma de repondération simple mais efficace pour les GCN, soutenu théoriquement par la théorie du champ moyen.
Une méthode, connue sous le nom de DrGCN, pour repondérer les différentes dimensions des représentations de noeuds dans les réseaux convolutifs de graphes en réduisant la variance entre les dimensions.
Notre approche est la première tentative de tirer parti d'un modèle séquentiel à variables latentes pour la sélection des connaissances dans le dialogue multi-tour fondé sur les connaissances. Elle atteint la nouvelle performance de pointe sur le benchmark Wizard of Wikipedia.
Un modèle séquentiel à variables latentes pour la sélection des connaissances dans la génération de dialogues qui étend le modèle d'attention postérieure au problème de la sélection des connaissances latentes et obtient des performances supérieures à celles des modèles précédents de pointe.
Une nouvelle architecture pour la sélection de dialogues multi-tours fondés sur la connaissance, qui permet d'atteindre l'état de l'art sur des ensembles de données de référence pertinents et d'obtenir de meilleurs résultats dans les évaluations humaines.
Nous proposons une méthode de méta-apprentissage qui amortit efficacement l'inférence variationnelle hiérarchique sur les épisodes de formation.
Une adaptation aux modèles de type MAML qui tient compte de l'incertitude postérieure dans les variables latentes spécifiques à la tâche en utilisant l'inférence variationnelle pour les paramètres spécifiques à la tâche dans une vue bayésienne hiérarchique de MAML.
Les auteurs considèrent le méta-apprentissage pour apprendre un a priori sur les poids des réseaux neuronaux, via une inférence variationnelle amortie.
Représentation/distillation des connaissances en maximisant l'information mutuelle entre l'enseignant et l'étudiant.
Cet article combine un objectif contrastif mesurant l'information mutuelle entre les représentations apprises par les réseaux de l'enseignant et de l'étudiant pour la distillation de modèles, et propose un modèle avec une amélioration par rapport aux alternatives existantes sur les tâches de distillation.
Les réseaux qui apprennent avec des connexions de rétroaction et des règles de plasticité locale peuvent être optimisés en utilisant le méta-apprentissage.
Les CNN avec des connexions latérales d'inspiration biologique apprises de manière non supervisée sont plus robustes aux entrées bruyantes. 
Cet article vise à développer une approche de représentation de produits tensoriels pour les applications de traitement du langage naturel basées sur l'apprentissage profond.
Nous étudions la robustesse certifiée pour les prédictions top-k via un lissage aléatoire sous un bruit gaussien et nous dérivons une limite de robustesse serrée dans la norme L_2.
Cet article étend le travail sur la déduction d'un rayon certifié en utilisant le lissage aléatoire, et montre le rayon auquel un classificateur lissé sous perturbations gaussiennes est certifié pour les k meilleures prédictions.
Cet article s'appuie sur la technique de lissage aléatoire pour les prédictions top-1, et vise à fournir une certification sur les prédictions top-k.
Nous présentons des prieurs structurés pour l'apprentissage non supervisé des représentations désenchevêtrées dans les VAE qui atténuent considérablement le compromis entre le désenchevêtrement et la perte de reconstruction.
Un cadre général permettant d'utiliser la famille de distributions L^p-nested comme antériorité pour le vecteur de code de VAE, démontrant un MIG plus élevé.
Les auteurs soulignent les problèmes des approches VAE actuelles et offrent une nouvelle perspective sur le compromis entre la reconstruction et l'orthogonalisation pour le VAE, le beta-VAE et le beta-TCVAE.
Nous généralisons les blocs résiduels aux blocs tandem, qui utilisent des cartes linéaires arbitraires au lieu de raccourcis, et améliorons les performances par rapport aux ResNets.
Cet article effectue une analyse des raccourcis de connexion dans les architectures de type ResNet, et propose de remplacer les raccourcis d'identité par un autre bloc convolutif appelé bloc tandem.
Cet article étudie l'effet du remplacement des connexions de saut d'identité par des connexions de saut convolutives entraînables dans ResNet et constate une amélioration des performances.
Nous présentons un nouveau cadre pour adapter les méthodes de type Adam, à savoir AdamT, afin d'inclure les informations de tendance lors de la mise à jour des paramètres avec la taille de pas et les gradients adaptatifs.
Un nouveau type de variante d'Adam qui utilise la méthode linéaire de Holt pour calculer le momentum lissé de premier ordre et de second ordre au lieu d'utiliser la moyenne pondérée exponentielle.
Une méthode pour expliquer un classificateur, en générant une perturbation visuelle d'une image en exagérant ou en diminuant les caractéristiques sémantiques que le classificateur associe à une étiquette cible.
Un modèle qui, lorsqu'il reçoit une requête en entrée d'une boîte noire, vise à expliquer le résultat en fournissant des variations plausibles et progressives de la requête qui peuvent entraîner une modification de la sortie.
Une méthode pour expliquer la sortie de la classification d'images en boîte noire, qui génère une perturbation graduelle des sorties en réponse à des requêtes d'entrée graduellement perturbées.
Nous présentons une approche orientée vers l'influence pour construire des explications du comportement des réseaux convolutifs profonds, et nous montrons comment elle peut être utilisée pour répondre à un large ensemble de questions qui n'ont pas pu être traitées par des travaux antérieurs.
Une façon de mesurer l'influence qui satisfait à certains axiomes, et une notion d'influence qui peut être utilisée pour identifier quelle partie de l'entrée est la plus influente pour la sortie d'un neurone dans un réseau neuronal profond.
Cet article propose de mesurer l'influence de neurones uniques par rapport à une quantité d'intérêt représentée par un autre neurone.
Nous mettons en évidence une technique par laquelle les systèmes de traitement du langage naturel peuvent apprendre un nouveau mot à partir du contexte, ce qui leur permet d'être beaucoup plus flexibles.
Une technique d'exploitation des connaissances antérieures pour apprendre des représentations d'enchâssement pour les nouveaux mots avec un minimum de données.
Une architecture de mémoire qui supporte le raisonnement déductif.
Cet article propose des changements à l'architecture du réseau de mémoire End2End, introduit une nouvelle tâche d'inférence associative appariée que la plupart des modèles existants peinent à résoudre, et montre que l'architecture proposée résout mieux la tâche.
Une nouvelle tâche (inférence d'association par paires) tirée de la psychologie cognitive, et proposition d'une nouvelle architecture de mémoire dont les caractéristiques permettent une meilleure performance sur la tâche d'association par paires.
Les convolutions séparables en profondeur améliorent la traduction automatique neuronale : plus elles sont séparables, mieux c'est.
Cet article propose d'utiliser des couches de convolution séparables en profondeur dans un modèle de traduction automatique neuronal entièrement convolutif, et introduit une nouvelle couche de convolution super-séparable qui réduit davantage le coût de calcul.
La formation GAN non saturante minimise efficacement une f-divergence inverse de type KL.
Cet article propose une expression utile de la classe des f-divergences, étudie les propriétés théoriques des f-divergences populaires à partir d'outils nouvellement développés, et étudie les GAN avec le schéma d'apprentissage non saturant.
Nous présentons une nouvelle méthode de représentation du texte qui permet d'appliquer des classificateurs d'images à des problèmes de classification de texte, et nous appliquons cette méthode à la désambiguïsation des noms d'inventeurs.
Une méthode pour convertir une paire d'informations textuelles en une image RVB 2D qui peut être utilisée par des réseaux neuronaux convolutionnels 2D (classificateurs d'images).
Les auteurs se penchent sur le problème de la désambiguïsation des noms des inventeurs de brevets et proposent de construire une représentation en image des deux chaînes de noms à comparer et d'appliquer un classificateur d'images.
Nous avons proposé le modèle "Difference-Seeking Generative Adversarial Network" (DSGAN) pour apprendre la distribution cible qui est difficile à collecter des données d'entraînement.
Cet article présente DS-GAN, qui vise à apprendre la différence entre deux distributions quelconques dont les échantillons sont difficiles ou impossibles à collecter, et montre son efficacité sur des tâches d'apprentissage semi-supervisé et d'apprentissage adversarial.
Cet article examine le problème de l'apprentissage d'un GAN pour capturer une distribution cible avec seulement très peu d'échantillons d'entraînement de cette distribution disponibles.
Une méthode générale qui améliore les performances de traduction d'images du cadre GAN en utilisant un discriminateur intégré à l'attention.
Un mécanisme de rétroaction dans le cadre du GAN qui améliore la qualité des images générées lors de la traduction d'image à image, et dont le discriminateur produit une carte indiquant où le générateur devrait se concentrer pour rendre ses résultats plus convaincants.
Proposition d'un GAN avec un discriminateur basé sur l'attention pour la traduction I2I qui fournit la probabilité de vrai/faux et une carte d'attention qui reflète la saillance pour la génération d'images.
Nous proposons un nouveau jeu de données pour étudier le problème de l'implication dans le cadre de tableaux semi-structurés comme prémisses.
Cet article propose un nouveau jeu de données pour la vérification des faits à partir de tableaux et présente des méthodes pour cette tâche.
Les auteurs proposent le problème de la vérification des faits avec des sources de données semi-structurées telles que des tableaux, créent un nouvel ensemble de données et évaluent des modèles de base avec des variations.
Nous développons une architecture de correspondance profonde de graphes qui affine les correspondances initiales afin d'atteindre un consensus de voisinage.
Un cadre pour répondre à des questions d'appariement de graphes consistant en des incorporations locales de nœuds avec une étape de raffinement par passage de messages.
Une architecture à base de GNN en deux étapes pour établir des correspondances entre deux graphes qui donne de bons résultats dans des tâches réelles de correspondance d'images et d'alignement d'entités de graphes de connaissances.
Cet article étend la preuve de la densité des réseaux neuronaux dans l'espace des fonctions continues (ou même mesurables) sur les espaces euclidiens aux fonctions sur les ensembles compacts de mesures de probabilité. 
Cet article étudie les propriétés d'approximation d'une famille de réseaux neuronaux conçus pour traiter les problèmes d'apprentissage multi-instances, et montre que les résultats obtenus pour les architectures standard à une couche s'appliquent à ces modèles.
Cet article généralise le théorème d'approximation universelle aux fonctions réelles sur l'espace des mesures.
Un nouveau cadre pour l'explication des prédictions en fonction du contexte et sans contexte
Les auteurs étendent la méthode d'attribution locale linéaire LIME pour l'interprétation des modèles de boîte noire, et proposent une méthode permettant de discerner les interactions dépendantes et non dépendantes du contexte.
Une méthode qui peut fournir des explications hiérarchiques pour un modèle, y compris des explications dépendant du contexte et sans contexte par un algorithme d'interprétation locale.
Le réglage fin après la quantification correspond ou dépasse les réseaux de pointe de pleine précision à la fois à la quantification de 8 et 4 bits.
Cet article propose d'améliorer les performances des modèles de faible précision en effectuant une quantification sur des modèles pré-entraînés, en utilisant des lots de grande taille et en utilisant un recuit de taux d'apprentissage approprié avec un temps d'apprentissage plus long.
Une méthode de quantification à faible nombre de bits pour permettre l'inférence sur du matériel efficace qui atteint une précision totale sur ResNet50 avec des poids et des activations de 4 bits, basée sur l'observation que l'ajustement fin à faible précision introduit du bruit dans le gradient.
Deux méthodes basées sur l'analyse de similarité représentationnelle (RSA) et les noyaux d'arbre (TK) qui quantifient directement le degré de correspondance entre les informations encodées dans les schémas d'activation neuronale et les informations représentées par des structures symboliques.
Cet article présente un cadre pour l'apprentissage de représentation efficace en termes de données par échantillonnage adaptatif dans l'espace latent.
Procédé de sélection séquentielle et adaptative d'exemples d'apprentissage à présenter à l'algorithme d'apprentissage, où la sélection se produit dans l'espace latent sur la base du choix d'échantillons dans la direction du gradient de la perte.
Une méthode pour sélectionner efficacement des échantillons durs pendant l'entraînement de réseaux neuronaux, réalisée par le biais d'un auto-encodeur variationnel qui code les échantillons dans un espace latent.
Une méthode basée sur l'apprentissage contradictoire pour démêler deux ensembles complémentaires de variations dans un ensemble de données où seul l'un d'entre eux est étiqueté, testé sur le style et le contenu des illustrations d'anime.
Une méthode de génération d'images combinant des GANs conditionnels et des VAEs conditionnels qui génère des images d'anime haute fidélité avec différents styles de différents artistes. 
Proposition d'une méthode d'apprentissage des représentations dissociées du style (artiste) et du contenu dans les anime.
Nous introduisons une régularisation de lissage pour les noyaux convolutifs de CNN qui peut aider à améliorer la robustesse des adversaires et conduire à des gradients alignés sur le plan perceptif.
Cet article propose un nouveau schéma de régularisation qui encourage les noyaux convolutifs à être plus lisses, en faisant valoir que la réduction de la dépendance des réseaux neuronaux à l'égard des composantes à haute fréquence améliore la robustesse contre les exemples adverses. 
Les auteurs proposent une méthode d'apprentissage de noyaux convolutifs plus lisses, plus précisément, un régularisateur pénalisant les changements importants entre les pixels consécutifs du noyau avec l'intuition de pénaliser l'utilisation de composantes d'entrée à haute fréquence.
Nous étudions les comportements à grand échantillon des estimations de la valeur Q et proposons une stratégie d'exploration efficace qui repose sur l'estimation des écarts relatifs entre les estimations Q. 
Nous formons les encastrements de mots sur la base de l'implication plutôt que de la similarité, ce qui permet de prédire avec succès l'implication lexicale.
L'article présente un algorithme d'intégration de mots pour l'implication lexicale qui suit le travail de Henderson et Popa (ACL, 2016).
Apprentissage non supervisé pour l'apprentissage par renforcement à l'aide d'un programme automatique d'autodidaxie
Une nouvelle formulation pour explorer l'environnement d'une manière non supervisée pour aider une tâche spécifique plus tard, où un agent propose des tâches de plus en plus difficiles et l'agent apprenant essaie de les accomplir.
Un modèle d'auto-jeu où un agent apprend à proposer des tâches qui sont faciles pour lui mais difficiles pour un adversaire, créant ainsi une cible mobile d'objectifs d'auto-jeu et un programme d'apprentissage. 
Exploitation de détails structurels riches dans des données structurées par des graphes via des "empreintes structurelles" adaptatives.
Une méthodologie basée sur la structure de graphe pour augmenter le mécanisme d'attention des réseaux neuronaux de graphe, avec l'idée principale d'explorer les interactions entre différents types de nœuds du voisinage local d'un nœud racine.
Cet article étend l'idée d'auto-attention dans les réseaux de graphes, qui est généralement basée sur la similarité des caractéristiques entre les nœuds, pour inclure la similarité structurelle.
Nous proposons un algorithme évolutif d'apprentissage par renforcement bayésien qui apprend une correction bayésienne sur un ensemble d'experts clairvoyants pour résoudre des problèmes avec des récompenses latentes et des dynamiques complexes.
Cet article considère le problème de l'apprentissage par renforcement bayésien sur des processus de décision de Markov (MDP) latents en prenant des décisions avec des experts.
Dans cet article, les auteurs motivent et proposent un algorithme d'apprentissage, appelé Bayesian Residual Policy Optimization (BRPO), pour les problèmes d'apprentissage par renforcement bayésien.
Nous prouvons que la descente de gradient permet d'atteindre une perte d'apprentissage nulle avec un taux linéaire sur les réseaux neuronaux sur-paramétrés.
Ce travail considère l'optimisation d'un réseau ReLU sur-paramétré à deux couches avec la perte au carré et étant donné un ensemble de données avec des étiquettes arbitraires.
Cet article étudie les réseaux neuronaux à une couche cachée avec perte carrée, où ils montrent que dans un cadre sur-paramétré, l'initialisation aléatoire et la descente du gradient permettent d'obtenir une perte nulle.
Analyser les problèmes inverses à l'aide de réseaux neuronaux inversables.
L'auteur propose d'utiliser des réseaux inversibles pour résoudre les problèmes inverses ambigus et suggère de former non seulement le modèle avant, mais aussi le modèle inverse avec une critique MMD.
Le document de recherche propose un réseau inversible avec des observations pour la probabilité postérieure de distributions d'entrée complexes avec un schéma d'apprentissage bidirectionnel théoriquement valide.
Les mesures de performance sont des spécifications incomplètes ; la fin ne justifie pas toujours les moyens.
Les auteurs montrent comment le méta-apprentissage révèle les incitations cachées au changement de distribution et proposent une approche basée sur l'échange d'apprenants entre les environnements afin de réduire le changement de distribution auto-induit.
L'article généralise l'incitation inhérente à l'apprenant de gagner en rendant la tâche plus facile dans le méta-apprentissage à une plus grande classe de problèmes.
Nous proposons une approche de détection d'anomalies qui combine la modélisation de la classe de premier plan par le biais de multiples densités locales avec un apprentissage contradictoire.
L'article propose une technique pour rendre les modèles génératifs plus robustes en les rendant cohérents avec la densité locale.
Nous proposons une variante de GAN qui apprend à générer des nuages de points. Différentes études ont été explorées, notamment une estimation plus précise de la distance de Wasserstein, la génération conditionnelle, la généralisation à des nuages de points non vus et l'image à un nuage de points.
Cet article propose d'utiliser le GAN pour générer un nuage de points 3D et introduit un objectif de prise en sandwich, en faisant la moyenne des limites supérieure et inférieure de la distance de Wasserstein entre les distributions.
Cet article propose un nouveau modèle génératif pour les données non ordonnées, avec une application particulière aux nuages de points, qui comprend une méthode d'inférence et une nouvelle fonction objective. 
L'article présente une nouvelle approche des mécanismes attentionnels qui peut profiter à une série de tâches telles que la traduction automatique et le sous-titrage d'images.
Cet article étend les modèles d'attention actuels du niveau du mot à la combinaison de mots adjacents, en appliquant les modèles aux éléments constitués de mots adjacents fusionnés.
Nous identifions un phénomène, le lavage de cerveau neuronal, et introduisons une perte de plasticité pondérale statistiquement justifiée pour y remédier.
Cet article aborde le phénomène du "lavage de cerveau neuronal", qui fait référence au fait que la performance d'un modèle est affectée par un autre modèle partageant les paramètres du modèle.
Cet article présente Morpho-MNIST, une collection de métriques de forme et de perturbations, dans une démarche d'évaluation quantitative de l'apprentissage des représentations.
Cet article aborde le problème de l'évaluation et du diagnostic des représentations apprises à l'aide d'un modèle génératif.
Les auteurs présentent un ensemble de critères pour catégoriser les digues MNIST et un ensemble de perturbations intéressantes pour modifier le jeu de données MNIST.
exploration structurée dans l'apprentissage par renforcement profond via la découverte et le contrôle non supervisés de l'abstraction visuelle
L'article présente des abstractions visuelles qui sont utilisées pour l'apprentissage par renforcement, où un algorithme apprend à "contrôler" chaque abstraction ainsi qu'à sélectionner les options pour réaliser la tâche globale.
Un nouvel algorithme de gradient de politique conçu pour aborder les problèmes d'optimisation combinatoire de type boîte noire. L'algorithme ne repose que sur des évaluations de fonctions, et renvoie des solutions localement optimales avec une forte probabilité.
L'article propose une approche pour construire des objectifs de substitution pour l'application des méthodes de gradient de politique à l'optimisation combinatoire dans le but de réduire le besoin de réglage des hyper-paramètres.
L'article propose de remplacer le terme de récompense dans l'algorithme de gradient de politique par sa distribution cumulative empirique centrée. 
Estimation rapide et calibrée de l'incertitude pour les réseaux neuronaux sans échantillonnage
Cet article propose une nouvelle approche pour estimer la confiance des prédictions dans un contexte de régression, ouvrant la porte à des applications en ligne avec des estimations d'incertitude entièrement intégrées.
Cet article propose une régression évidentielle profonde, une méthode d'entraînement des réseaux neuronaux permettant non seulement d'estimer la sortie, mais aussi les preuves associées à l'appui de cette sortie.
Nous proposons un nouvel algorithme qui permet de trouver rapidement des tickets gagnants dans les réseaux neuronaux.
Cet article propose une nouvelle fonction objective qui peut être utilisée pour optimiser conjointement un objectif de classification tout en encourageant la sparsification dans un réseau qui fonctionne avec une grande précision.
Ce travail propose une nouvelle méthode d'élagage itérative appelée Sparsification continue, qui élague continuellement le poids actuel jusqu'à ce qu'il atteigne le ratio cible.
Introduire un cadre formel pour la formation budgétisée et proposer un programme de taux d'apprentissage linéaire tenant compte du budget.
Ce travail présente une technique permettant de régler le taux d'apprentissage pour la formation de réseaux neuronaux lorsque le nombre d'époques est fixe.
Cet article a analysé quel programme de taux d'apprentissage devrait être utilisé lorsque le nombre d'itérations est limité en utilisant un concept introduit de BAS (Budget-Aware Schedule).
Nous effectuons l'exploration en utilisant des récompenses intrinsèques qui sont basées sur une distance pondérée des plus proches voisins dans l'espace de représentation.
Cet article propose une méthode d'exploration efficace dans les MDP tabulaires ainsi que dans un environnement de contrôle simple, en utilisant des codeurs déterministes pour apprendre une représentation de faible dimension de la dynamique de l'environnement.
Cet article propose une méthode d'exploration efficace par échantillonnage pour un agent RL en utilisant une combinaison d'approches basées sur le modèle et sans modèle avec une métrique de nouveauté.
Les performances en matière de robustesse des modèles formés par PGD sont sensibles à la transformation des ensembles de données d'image préservant la sémantique, ce qui implique que l'évaluation des algorithmes d'apprentissage robuste est délicate en pratique.
Nous proposons un gradient de politique de classement qui apprend le rang optimal des actions pour maximiser le rendement. Nous proposons un cadre général d'apprentissage hors politique avec les propriétés de préservation de l'optimalité, de réduction de la variance et d'efficacité de l'échantillon.
Cet article propose de reparamétrer la politique en utilisant une forme de classement pour convertir le problème RL en un problème d'apprentissage supervisé.
Cet article présente un nouveau point de vue sur les méthodes de gradient politique du point de vue du classement. 
En combinant la classification et la recherche d'images dans une architecture de réseau de neurones, nous obtenons une amélioration pour les deux tâches.
Cet article propose une intégration unifiée pour la classification d'images et la recherche d'instances afin d'améliorer les performances de ces deux tâches.
L'article propose d'entraîner conjointement un réseau neuronal profond pour la classification d'images, la reconnaissance d'instances et de copies.
Nous étudions la mise en correspondance de la relation d'hyponymie du réseau de mots avec des vecteurs de caractéristiques.
Cet article étudie comment l'hyponymie entre les mots peut être mise en correspondance avec des représentations de caractéristiques.
Cet article explore la notion d'hyponymie dans les représentations vectorielles de mots et décrit une méthode d'organisation des relations de WordNet en une structure arborescente pour définir l'hyponymie.
Nous construisons un générateur de langage naturel plus puissant en formant de manière discriminante des fonctions de notation qui classent les générations candidates en fonction de diverses qualités de bonne écriture.
Cet article propose de réunir plusieurs biais inductifs qui espèrent corriger les incohérences du décodage des séquences et propose d'optimiser les paramètres d'une combinaison prédéfinie de divers sous-objectifs. 
Cet article combine un modèle de langage RNN avec plusieurs modèles formés de manière discriminatoire pour améliorer la génération de langage.
Cet article propose d'améliorer la génération de modèles de langage RNN en utilisant des objectifs augmentés inspirés des maximes de communication de Grice.
Solution d'équilibrage de charge évolutive et à faible communication pour les systèmes multi-dispatcheurs à serveurs hétérogènes, avec de solides garanties théoriques et des résultats empiriques prometteurs. 
Une mesure quantitative pour prédire les performances des modèles de réseaux neuronaux profonds.
L'article propose une nouvelle quantité qui compte le nombre de chemins dans le réseau neuronal et qui permet de prédire la performance des réseaux neuronaux avec le même nombre de paramètres.
L'article présente une méthode de comptage des chemins dans les réseaux neuronaux profonds qui peut sans doute être utilisée pour mesurer la performance du réseau.
Cet article présente une étude rigoureuse des raisons pour lesquelles les schémas de taux d'apprentissage utilisés en pratique (pour un budget de calcul donné) offrent des avantages significatifs, même si ces schémas ne sont pas préconisés par la théorie classique de l'approximation stochastique.
Cet article présente une étude théorique de différents programmes de taux d'apprentissage qui a abouti à des limites inférieures statistiques minimax pour les schémas polynomiaux et à coupure constante.
L'article étudie l'effet des choix de taux d'apprentissage pour l'optimisation stochastique, en se concentrant sur les moindres carrés avec des pas décroissants.
Nous présentons des planificateurs basés sur des convnets qui sont efficaces en termes d'échantillonnage et qui se généralisent à des instances plus importantes de problèmes de navigation et de localisation.
Propose des méthodes, qui peuvent être considérées comme des modifications des Value Iteration Networks (VIN), avec quelques améliorations visant à améliorer l'efficacité de l'échantillonnage et la généralisation à des environnements de grande taille.
L'article présente une extension des réseaux d'itération de valeur originaux (VIN) en considérant une fonction de transition dépendant de l'état.
apprendre de meilleures incorporations de domaines par l'apprentissage tout au long de la vie et le méta-apprentissage
Présente une méthode d'apprentissage tout au long de la vie pour apprendre les incorporations de mots.
Cet article propose une approche pour l'apprentissage d'enchâssements dans de nouveaux domaines et surpasse de manière significative la ligne de base dans une tâche d'extraction d'aspects. 
 nous proposons une nouvelle méthode d'élagage basée sur la régularisation (appelée IncReg) pour affecter de manière incrémentielle différents facteurs de régularisation à différents groupes de poids en fonction de leur importance relative.
Cet article propose une méthode d'élagage basée sur la régularisation pour affecter de manière incrémentielle différents facteurs de régularisation à différents groupes de poids en fonction de leur importance relative.
Les schémas de momentum/accélération existants, tels que la méthode de la boule lourde et l'accélération de Nesterov, employés avec des gradients stochastiques, ne sont pas meilleurs que la descente de gradient stochastique vanille, en particulier lorsqu'ils sont employés avec des lots de petite taille.
Nous montrons que les tâches de planification de sursouscription peuvent être résolues en utilisant A* et nous introduisons de nouvelles heuristiques sensibles aux limites pour les tâches de planification de sursouscription.
Présente une approche pour résoudre de manière optimale les tâches de planification de sursouscription (OSP) en utilisant une traduction de la planification classique avec des fonctions de coût multiples.
L'article propose des modifications aux heuristiques admissibles pour les rendre mieux informées dans un cadre multicritères où.
Nous développons des méthodes de méta-apprentissage pour l'apprentissage à quelques coups robuste contre l'adversité.
Cet article présente une méthode qui améliore la robustesse de l'apprentissage en quelques coups en introduisant une attaque adverse des données d'interrogation dans la phase de réglage fin des tâches internes d'un méta-algorithme d'apprentissage.
Les auteurs de cet article proposent une nouvelle approche pour la formation d'un modèle robuste à quelques coups. 
Nous constatons que le pooling seul ne détermine pas la stabilité de la déformation dans les CNN et que la fluidité du filtre joue un rôle important dans la détermination de la stabilité. 
Nous proposons un cadre d'auto-ensemble pour former des modèles d'apprentissage profond plus robustes dans des ensembles de données étiquetées bruyantes.
Cet article propose un "filtrage d'étiquettes auto-ensemble" pour l'apprentissage avec des étiquettes bruyantes où le bruit des étiquettes est indépendant de l'instance, ce qui permet une identification plus précise des prédictions incohérentes. 
Cet article propose un algorithme d'apprentissage à partir de données avec des étiquettes bruyantes qui alterne entre la mise à jour du modèle et la suppression des échantillons qui semblent avoir des étiquettes bruyantes.
Nous étudions l'élagage des DNN avant la formation et apportons une réponse à la question de savoir quelle topologie doit être utilisée pour la formation de réseaux a priori épars.
Les auteurs proposent de remplacer les couches denses par des couches linéaires à connexion éparse et une approche pour trouver la meilleure topologie en mesurant la façon dont les couches éparses se rapprochent des poids aléatoires de leurs homologues denses.
L'article propose une architecture en cascade éparse qui est une multiplication de plusieurs matrices éparses et un modèle de connectivité spécifique qui surpasse les autres considérations fournies.
Nous présentons Multitask Neural Model Search, un méta-apprenant qui peut concevoir des modèles pour plusieurs tâches simultanément et transférer l'apprentissage à des tâches non vues.
Cet article étend la recherche d'architecture neuronale au problème de l'apprentissage multitâche où un contrôleur de recherche de modèle conditionné par la tâche est appris pour traiter plusieurs tâches simultanément.
Dans cet article, les auteurs résument leur travail sur la construction d'un cadre, appelé contrôleur de recherche de modèles neuronaux multitâches, pour la construction automatisée de réseaux neuronaux sur plusieurs tâches simultanément.
Nous modélisons les processus visuels non linéaires en tant que bruit autorégressif via l'apprentissage profond génératif.
Propose une nouvelle méthode qui modélise un processus visuel non linéaire avec une version profonde d'un processus linéaire (processus de Markov).
Cet article propose un nouveau modèle génératif profond pour les séquences, en particulier les séquences d'images et les vidéos, qui utilise une structure linéaire dans une partie du modèle.
Cet article propose un nouveau réseau feed-forward, appelé PDE-Net, pour apprendre les EDP à partir de données. 
L'article présente l'utilisation de machines d'apprentissage profond dans le but d'identifier des systèmes dynamiques spécifiés par des EDP.
Cet article propose un algorithme basé sur un réseau de neurones pour l'apprentissage à partir de données provenant de systèmes dynamiques dont les équations directrices peuvent être écrites sous forme d'équations différentielles partielles.
Cet article traite de la modélisation de systèmes dynamiques complexes par le biais d'équations différentielles partielles non paramétriques en utilisant des architectures neuronales. L'idée la plus importante du papier (PDE-net) est d'apprendre à la fois les opérateurs différentiels et la fonction qui régit l'EDP.
Nous donnons une procédure d'échantillonnage rapide de type flux normalisateur pour les modèles de variables latentes discrètes.
Cet article utilise une approximation variationnelle de filtrage autorégressif pour l'estimation des paramètres des systèmes dynamiques discrets en utilisant des itérations à point fixe.
Les auteurs posent une famille postérieure autorégressive générale pour les variables discrètes ou leurs relaxations continues. 
Cet article a deux contributions principales : il étend les flux de normalisation aux paramètres discrets et présente une règle de mise à jour approximative à point fixe pour les séries temporelles autorégressives qui peut exploiter le parallélisme des GPU. 
Nous proposons un cadre qui apprend à coder les connaissances de manière symbolique et à générer des programmes pour raisonner sur les connaissances codées.
Les auteurs proposent la machine N-Gram pour répondre à des questions sur des documents longs.
Cet article présente la machine à n-grammes, un modèle qui encode les phrases en représentations symboliques simples qui peuvent être interrogées efficacement.
Cet article propose un objectif de méta-apprentissage basé sur la vitesse d'adaptation aux distributions de transfert pour découvrir une décomposition modulaire et des variables causales.
L'article montre qu'un modèle dont la structure sous-jacente est correcte s'adaptera plus rapidement à une intervention causale qu'un modèle dont la structure est incorrecte.
Dans ce travail, les auteurs ont proposé un cadre général et systématique de l'objectif de méta-transfert incorporant l'apprentissage de la structure causale sous des interventions inconnues.
Une autre perspective sur l'oubli catastrophique
Cet article présente un cadre de lutte contre l'oubli catastrophique basé sur la modification du terme de perte pour minimiser les changements dans la vraisemblance du classificateur, obtenue par une approximation de la série de Taylor.
Cet article tente de résoudre le problème de l'apprentissage continu en se concentrant sur les approches de régularisation, et il propose une stratégie L_1 pour atténuer le problème.
Nous proposons une approche pour construire des modèles faciaux morphables 3D réalistes (3DMM) qui permet un flux de travail intuitif d'édition des attributs faciaux en sélectionnant les meilleurs ensembles de vecteurs propres et de mesures anthropométriques.
Propose un modèle morphable par morceaux pour les maillages de visages humains et propose également une correspondance entre les mesures anthropométriques du visage et les paramètres du modèle afin de synthétiser et d'éditer des visages avec les attributs souhaités. 
Cet article décrit une méthode de modèle facial morphable basé sur des parties permettant un contrôle localisé par l'utilisateur.
Deux algorithmes ont obtenu de meilleurs résultats que huit autres lors d'une expérience de BCI basée sur l'EEG.
Nous apprenons aux agents à négocier en utilisant uniquement l'apprentissage par renforcement ; les agents égoïstes peuvent le faire, mais uniquement en utilisant un canal de communication fiable, et les agents prosociaux peuvent négocier en utilisant des paroles faciles.
Les auteurs décrivent une variante du jeu de la négation avec la prise en compte d'un canal de communication secondaire pour les propos mesquins, et constatent que le canal secondaire améliore les résultats de la négation.
Cet article explore la manière dont les agents peuvent apprendre à communiquer pour résoudre une tâche de négociation et constate que les agents prosociaux sont capables d'apprendre à fonder des symboles à l'aide de la RL, mais pas les agents intéressés.
Examine les problèmes de la façon dont les agents peuvent utiliser la communication pour maximiser leurs récompenses dans un jeu de négociation simple.
Nous proposons un nouveau cadre de méta-apprentissage pour l'inférence transductive qui classifie l'ensemble des tests en une seule fois afin d'atténuer le problème du manque de données.
Cet article propose d'aborder l'apprentissage de few-shot d'une manière transductive en apprenant un modèle de propagation d'étiquettes de bout en bout. C'est le premier à apprendre la propagation d'étiquettes pour l'apprentissage transductif de few-shot et il a produit des résultats empiriques efficaces. 
Cet article propose un cadre de méta-apprentissage qui exploite les données non étiquetées en apprenant la propogation d'étiquettes basée sur les graphes de bout en bout.
Étudie l'apprentissage de quelques hôtes dans un cadre transductif : utilisation du méta-apprentissage pour apprendre à propager les étiquettes des échantillons de formation aux échantillons de test. 
Nous décrivons l'utilisation d'un système d'ordonnancement automatisé pour la conception de la politique d'observation et pour l'ordonnancement des opérations de la mission ECOSTRESS de la NASA.
Cet article présente une adaptation d'un système d'ordonnancement automatisé, CLASP, pour cibler une expérience d'OT (ECOSTRESS) sur l'ISS. 
Le stockage et la représentation hybrides des connaissances acquises peuvent être à l'origine d'exemples contradictoires.
Nouvelles expériences et théorie pour le Q-Learning basé sur Adam
Cet article fournit un résultat de convergence pour l'apprentissage traditionnel Q avec approximation de fonction linéaire lorsqu'on utilise une mise à jour de type Adam. 
Cet article décrit une méthode pour améliorer l'algorithme AltQ en utilisant une combinaison d'un optimiseur Adam et d'un redémarrage régulier des paramètres internes de l'optimiseur Adam.
Un nouveau réseau de capsules qui converge plus rapidement sur nos expériences de référence en matière de santé.
Présente une variante des réseaux de capsules qui, au lieu d'utiliser le routage EM, utilise un sous-espace linéaire couvert par le vecteur propre dominant de la matrice des votes pondérés de la capsule précédente.
L'article propose une méthode de routage améliorée, qui utilise les outils de décomposition de l'aiguille pour trouver l'activation et la pose de la capsule.
Nous proposons une méthode de réglage fin distribué des modèles de langue sur les appareils des utilisateurs sans collecte de données privées.
Cet article traite de l'amélioration des modèles de langage sur les équipements mobiles à partir d'une petite partie du texte que l'utilisateur a saisi en utilisant des objectifs linéairement interpolés entre le texte spécifique de l'utilisateur et l'anglais général. 
Cet article utilise l'analyse de la perte de Lipschitz sur un espace d'hypothèse borné pour dériver de nouveaux algorithmes de type ERM avec de fortes garanties de performance qui peuvent être appliqués au modèle GP clairsemé non conjugué.
Nous proposons une méthode de régularisation pour les réseaux neuronaux et une méthode d'analyse du bruit.
Cet article propose une nouvelle méthode de régularisation pour atténuer le problème de suradaptation des réseaux neuronaux profonds en faisant tourner les caractéristiques avec une matrice de rotation aléatoire pour réduire la co-adaptation.
Cet article propose une nouvelle méthode de régularisation pour la formation des réseaux neuronaux, qui ajoute des neurones de bruit de manière interdépendante.
Un cadre probabiliste pour l'apprentissage par renforcement multi-agent
Cet article propose un nouvel algorithme nommé Multi-Agent Soft Actor-Critic (MA-SAC) basé sur l'algorithme de critique d'acteur à entropie maximale hors politique Soft Actor-Critic (SAC).
Nous fournissons une relaxation continue à l'opérateur de tri, permettant une optimisation stochastique de bout en bout, basée sur le gradient.
L'article examine comment trier un certain nombre d'éléments sans nécessairement connaître explicitement leur signification ou leur valeur réelle et propose une méthode pour effectuer l'optimisation via une relaxation continue.
Ce travail s'appuie sur une identité sum(top k) pour dériver un échantillonneur différentiable par chemin de matrices "stochastiques à rangées unimodales".
Introduit une relaxation continue de l'opérateur de tri afin de construire une optimisation de bout en bout basée sur le gradient et présente une extension stochastique de sa méthode en utilisant les distributions de Placket-Luce et Monte Carlo.
Nous réalisons un apprentissage de transfert efficace et flexible dans le cadre de l'optimisation bayésienne par le biais de fonctions d'acquisition neuronales méta-apprises.
Les auteurs présentent MetaBO qui utilise l'apprentissage par renforcement pour méta-apprendre la fonction d'acquisition pour l'optimisation bayésienne, montrant une efficacité d'échantillonnage croissante sur de nouvelles tâches.
Les auteurs proposent une alternative basée sur le méta-apprentissage aux fonctions d'acquisition standard (AF), par laquelle un réseau neuronal pré-entraîné produit des valeurs d'acquisition en fonction de caractéristiques choisies manuellement.
Les réseaux neuronaux profonds déterministes ne rejettent pas d'informations, mais ils regroupent leurs entrées.
Cet article fournit une manière raisonnée d'examiner la phrase de compression dans les réseaux neuronaux profonds en fournissant un estimateur d'entropie de sondage théorique pour estimer l'information mutuelle. 
Nous proposons des objectifs de régularisation pour les algorithmes RL multi-agents qui favorisent la coordination sur les tâches coopératives.
Cet article propose deux méthodes pour orienter les agents vers l'apprentissage de comportements coordonnés et les évalue rigoureusement dans des domaines multi-agents d'une complexité appropriée.
Cet article propose deux méthodes basées sur MADDPG pour encourager la collaboration entre les agents MARL décentralisés.
Nous présentons un modèle qui apprend des représentations conjointes robustes en effectuant des traductions cycliques hiérarchiques entre plusieurs modalités.
Cet article présente le réseau de traduction cyclique multimodal (MCTN) et l'évalue pour l'analyse multimodale des sentiments.
Comprendre les valeurs propres du réseau neuronal Hessian sous la distribution génératrice de données.
Cet article analyse le spectre de la matrice hessienne des grands réseaux neuronaux, avec une analyse des valeurs propres max/min et une visualisation des spectres à l'aide d'une approche de quadrature de Lanczos.
Cet article utilise la théorie des matrices aléatoires pour étudier la distribution du spectre du Hessien empirique et du Hessien réel pour l'apprentissage profond, et propose une méthode efficace de visualisation du spectre.
Une astuce simple pour améliorer les modèles de séquence : Les combiner avec un modèle de graphe
Cet article présente un modèle de résumé structurel avec un encodeur à base de graphe étendu à partir du RNN.
Ce travail combine les réseaux neuronaux graphiques avec une approche séquentielle du résumé abstrait, efficace sur tous les ensembles de données par rapport aux bases de référence externes.
Un classificateur clairsemé basé sur un modèle de mélange gaussien discriminant, qui peut également être intégré dans un réseau neuronal.
L'article présente un modèle de mélange gaussien entraîné via des arguments de descente de gradient qui permet d'induire la sparsité et de réduire les paramètres des couches du modèle entraînables.
Cet article propose un classificateur, appelé SDGM, basé sur un mélange gaussien discriminant et sur l'estimation de ses paramètres épars.
Initialisation des poids à l'aide de livres de codes Grassmanniens disponibles sur le marché, formation plus rapide et meilleure précision.
Une approche d'adaptation de domaine non supervisée qui s'adapte à la fois au niveau des pixels et des caractéristiques.
Cet article propose une approche d'adaptation au domaine en étendant le CycleGAN avec des fonctions de perte spécifiques à la tâche et des pertes imposées à la fois sur les pixels et les caractéristiques. 
Cet article propose l'utilisation de CycleGANs pour l'adaptation au domaine.
Cet article apporte une nouvelle extension aux travaux précédents sur le CycleGAN en le couplant avec des approches d'adaptation adversariale, en incluant une nouvelle fonctionnalité et une perte sémantique dans l'objectif global du CycleGAN, avec des avantages évidents.
Amharic Light Stemmer est conçu pour améliorer les performances de la classification des sentiments en amharique.
Cet article étudie le déracinement pour les langues riches en morphologie avec un déracineur léger qui ne supprime que les affixes dans la mesure où l'information sémantique originale du mot est conservée.
Cet article propose une technique d'épuration de la lumière en amharique utilisant une cascade de transformations qui normalisent la forme, suppriment les suffixes, les préfixes et les infixes.
Nous avons étudié si les réseaux profonds simples possèdent des neurones artificiels semblables à des cellules de grille pendant la récupération de la mémoire dans l'espace conceptuel appris.
Avantages et inconvénients de la vision par ordinateur basée sur la saccade dans une perspective de codage prédictif
Présente un cadre de calcul pour le problème de la vision active et explique comment la politique de contrôle peut être apprise pour réduire l'entropie de la croyance postérieure.
Nous étudions théoriquement la cohérence du spectre du Laplacien et l'utilisons pour l'intégration de graphes entiers.
Cet article se concentre sur le spectre laplacien d'un graphe comme moyen de générer une représentation qui sera utilisée pour comparer les graphes et les classer.
Ce travail propose d'utiliser le spectre laplacien des graphes pour apprendre la représentation des graphes.
L'entraînement contradictoire basé sur FGSM, avec randomisation, fonctionne tout aussi bien que l'entraînement contradictoire basé sur PGD : nous pouvons l'utiliser pour entraîner un classificateur robuste en 6 minutes sur CIFAR10, et en 12 heures sur ImageNet, sur une seule machine.
Cet article revisite la méthode Random+FGSM pour former des modèles robustes contre les fortes attaques d'évasion de PGD plus rapidement que les méthodes précédentes.
La principale affirmation de cet article est qu'une stratégie simple de randomisation et de formation contradictoire par la méthode du signe du gradient rapide (FGSM) produit des réseaux neuronaux robustes.
Nous proposons des régularisateurs presque partout différentiables et invariants d'échelle pour l'élagage des DNN, qui peuvent conduire à une sparsité supremum par le biais d'une formation SGD standard.
L'article propose un régularisateur à échelle invariante (DeepHoyer) inspiré de la mesure de Hoyer pour renforcer la sparsité dans les réseaux neuronaux. 
Nous montrons que des données supplémentaires non étiquetées ne sont pas nécessaires pour que les tâches auxiliaires auto-supervisées soient utiles pour la classification des séries temporelles, et nous présentons des tâches auxiliaires nouvelles et efficaces.
Cet article propose une méthode d'apprentissage auto-supervisée à partir de données de séries temporelles dans le domaine de la santé, en concevant des tâches auxiliaires basées sur la structure interne des données afin de créer des tâches d'entraînement auxiliaires plus étiquetées.
Cet article propose une approche pour l'apprentissage auto-supervisé des séries temporelles.
Les valeurs propres du conjugué (alias NNGP) et du noyau tangent neuronal peuvent être calculées sous une forme fermée sur le cube booléen et révèlent les effets des hyperparamètres sur le biais inductif, la formation et la généralisation des réseaux neuronaux.
Cet article présente une analyse spectrale du noyau conjugué des réseaux neuronaux et du noyau tangent des réseaux neuronaux sur un cube booléen afin d'expliquer pourquoi les réseaux profonds ont tendance à privilégier les fonctions simples.
Toutes les parcellations fonctionnelles du cerveau sont fausses, mais certaines sont utiles
Imitation de pixels, avec peu ou pas de récompense, à l'aide de RL hors politique et d'une minuscule fonction de récompense apprise de manière contradictoire.
L'article propose d'utiliser un "adversaire minimal" dans l'apprentissage génératif d'imitation adversariale dans des espaces visuels à haute dimension.
Cet article vise à résoudre le problème de l'estimation des récompenses éparses dans un cadre d'entrée à haute dimension.
Nous montrons des stratégies permettant d'identifier facilement les faux échantillons générés à l'aide du cadre Generative Adversarial Network.
Montrer que les faux échantillons créés avec des implémentations courantes de réseaux adversariens génératifs (GAN) sont facilement identifiés à l'aide de diverses techniques statistiques. 
L'article propose des statistiques pour identifier les fausses données générées à l'aide de GANs en se basant sur de simples statistiques marginales ou des spécifications formelles générées automatiquement à partir de données réelles.
Nous présentons un cadre analytique pour déterminer les exigences en matière de largeur de bit d'accumulation dans les trois GEMM de formation à l'apprentissage profond et nous vérifions la validité et la rigueur de notre méthode par le biais d'expériences d'étalonnage.
Les auteurs proposent une méthode analytique pour prédire le nombre de bits de mantisse nécessaires aux sommations partielles pour les couches convolutionnelles et entièrement connectées.
Les auteurs effectuent une analyse approfondie de la précision numérique requise pour les opérations d'accumulation dans la formation des réseaux neuronaux et montrent l'impact théorique de la réduction du nombre de bits dans l'accumulateur à virgule flottante.
Une nouvelle théorie d'adaptation non supervisée du domaine pour l'apprentissage métrique à distance et son application à la reconnaissance des visages à travers diverses variations ethniques.
Propose un nouveau réseau de transfert de caractéristiques qui optimise la perte d'adversité de domaine et la perte de séparation de domaine.
Nous proposons un algorithme de descente de gradient stochastique de type proximal convergent pour les problèmes d'optimisation non convexes contraints et non lisses.
Cet article propose Prox-SGD, un cadre théorique pour les algorithmes d'optimisation stochastique qui convergent asymptotiquement vers la stationnarité pour une perte lisse non convexe + contrainte/régularisateur convexe.
L'article propose un nouvel algorithme d'optimisation stochastique basé sur le gradient avec moyenne du gradient en adaptant la théorie des algorithmes proximaux au cadre non convexe.
Nous donnons une limite pour les NNs sur l'erreur de sortie dans le cas de défaillances de poids aléatoires en utilisant une expansion de Taylor dans la limite continue où les neurones proches sont similaires.
Cet article examine le problème de l'abandon de neurones d'un réseau neuronal, en montrant que si l'objectif est de devenir robuste aux neurones abandonnés de manière aléatoire pendant l'évaluation, il suffit de s'entraîner avec abandon.
Cette contribution étudie l'impact des suppressions de neurones aléatoires sur la précision de prédiction de l'architecture entraînée, avec l'application à l'analyse des défaillances et le contexte spécifique du matériel neuromorphique.
Nous explorons et étudions les synergies entre le son et l'action.
Cet article explore les liens entre l'action et le son en construisant un ensemble de données son-action-vision avec un robot basculant.
Cet article étudie le rôle de l'audio dans la perception des objets et des actions, ainsi que la manière dont les informations auditives peuvent aider à l'apprentissage des modèles de dynamique directe et inverse.
Nous proposons l'entraînement hiérarchique par objectifs complémentaires, un nouveau paradigme d'entraînement pour exploiter efficacement la hiérarchie des catégories dans l'espace d'étiquetage pour la classification d'images et la segmentation sémantique.
Une méthode qui régularise l'entropie de la distribution postérieure sur les classes, ce qui peut être utile pour les tâches de classification et de segmentation des images.
Notre article identifie le problème de l'approche existante de partage de poids dans la recherche d'architecture neuronale et propose une méthode pratique, qui donne de bons résultats.
L'auteur identifie un problème avec le NAS appelé évanouissement postérieur et introduit le NAS convergent postérieur pour atténuer cet effet.
Nous proposons une nouvelle approche de formation en deux phases basée sur l'"arrêt précoce" pour une formation robuste sur des étiquettes bruyantes.
L'article propose d'étudier comment l'arrêt précoce dans l'optimisation permet de trouver des exemples fiables.
Cet article propose une méthode de formation en deux phases pour l'apprentissage avec le bruit des étiquettes.
Nous présentons IC3Net, un réseau unique qui peut être utilisé pour former des agents dans des scénarios coopératifs, compétitifs et mixtes. Nous montrons également que les agents peuvent apprendre quand communiquer à l'aide de notre modèle.
L'auteur propose une nouvelle architecture pour l'apprentissage par renforcement multi-agent qui utilise plusieurs contrôleurs LSTM avec des poids liés qui se transmettent un vecteur continu.
Les auteurs proposent un schéma intéressant permettant aux agents de communiquer dans un contexte de RL multi-agents. 
Nous présentons le premier modèle neuronal de résumé abstrait capable de personnaliser les résumés générés.
Nous proposons un cadre logiciel basé sur les idées de l'algorithme d'apprentissage-compression, qui permet de compresser n'importe quel réseau neuronal par différents mécanismes de compression (élagage, quantification, bas rang, etc.).
Cet article présente la conception d'une bibliothèque logicielle qui permet à l'utilisateur de compresser plus facilement ses réseaux en masquant les détails des méthodes de compression.
Cet article propose une méthode de génération multimodale de bout en bout du visage humain à partir de la parole, basée sur un cadre d'apprentissage auto-supervisé.
Cet article présente un cadre d'apprentissage multimodal qui relie l'étape d'inférence et l'étape de génération pour rechercher la possibilité de générer le visage humain à partir de la voix uniquement.
Ce travail vise à construire un cadre de génération d'image de visage conditionnelle à partir du signal audio. 
Une approche descendante permettant de représenter récursivement des formules propositionnelles par des réseaux de neurones est présentée.
Cet article propose un nouveau modèle de réseau neuronal pour les formules logiques qui rassemble des informations sur une formule donnée en parcourant son arbre d'analyse de haut en bas.
L'article poursuit le chemin d'un réseau arborescent isomorphe à l'arbre d'analyse d'une formule du calcul propositionnel, mais en faisant passer l'information de haut en bas plutôt que de bas en haut.
Ape-X DQfD = Distributed (many actors + one learner + prioritized replay) DQN avec des démonstrations optimisant le rendement non escompté de 0,999 sur Atari.
L'article propose trois extensions (mise à jour de Bellman, perte de cohérence temporelle et démonstration par un expert) au DQN pour améliorer les performances d'apprentissage sur les jeux Atari, en obtenant des résultats supérieurs à ceux de l'état de l'art pour les jeux Atari. 
Cet article propose un opérateur de Bellman transformé qui vise à résoudre la sensibilité à la récompense non écrêtée, la robustesse à la valeur du facteur d'escompte et le problème d'exploration.
Méthode d'apprentissage permettant d'appliquer des contraintes strictes sur les encastrements appris pendant l'apprentissage supervisé. Appliqué à la réponse aux questions visuelles.
Les auteurs proposent un cadre pour incorporer des connaissances sémantiques préalables supplémentaires dans la formation traditionnelle des modèles d'apprentissage profond afin de régulariser l'espace d'intégration au lieu de l'espace des paramètres.
L'article plaide pour l'encodage de connaissances externes dans la couche d'intégration linguistique d'un réseau neuronal multimodal, sous la forme d'un ensemble de contraintes dures.
Résoudre les problèmes inverses en utilisant des approximations lisses des algorithmes directs pour entraîner les modèles inverses.
Une méthode d'apprentissage profond pour la localisation ponctuelle faiblement supervisée qui apprend en utilisant uniquement les étiquettes au niveau de l'image. Elle s'appuie sur l'entropie conditionnelle pour localiser les régions pertinentes et non pertinentes dans le but de minimiser les régions faussement positives.
Ce travail explore le problème du WSL en utilisant une nouvelle conception des termes de régularisation et un algorithme d'effacement récursif.
Cet article présente une nouvelle approche faiblement supervisée pour apprendre la segmentation d'objets avec des étiquettes de classe au niveau de l'image.
Acquérir des états de la région haute fréquence pour le contrôle de la recherche dans Dyna.
Les auteurs proposent d'effectuer l'échantillonnage dans le domaine des hautes fréquences pour augmenter l'efficacité de l'échantillonnage.
Cet article propose une nouvelle façon de sélectionner les états à partir desquels les transitions do do dans l'algorithme dyna.
Nous présentons un cadre de codage de source distribué basé sur un autoencodeur récurrent distribué pour la compression d'images évolutive (DRASIC).
L'article propose un auto-encodeur récurrent distribué pour la compression d'images qui utilise un ConvLSTM pour apprendre des codes binaires qui sont construits progressivement à partir des résidus d'informations précédemment codées.
Les auteurs proposent une méthode d'entraînement des modèles de compression d'images sur des sources multiples, avec un codeur distinct sur chaque source et un décodeur partagé. 
Les portes font tout le travail dans les LSTM en calculant les sommes pondérées par éléments, et la suppression du RNN simple interne ne dégrade pas les performances du modèle.
Cet article propose une variante simplifiée du LSTM en supprimant la non-linéarité de l'élément de contenu et de la porte de sortie.
Cet article présente une analyse des LSTM, montrant qu'ils ont une forme où le contenu de la cellule de mémoire à chaque étape est une combinaison pondérée des valeurs de " mise à jour du contenu " calculées à chaque étape temporelle et offre une simplification des LSTM qui calculent la valeur par laquelle la cellule de mémoire à chaque étape temporelle en termes d'une fonction déterministe de l'entrée plutôt qu'une fonction de l'entrée et du contexte actuel.
L'article propose une nouvelle vision du LSTM dans laquelle le noyau est une somme pondérée par éléments et soutient que le LSTM est redondant en ne conservant que les portes d'entrée et d'oubli pour calculer les poids.
En analysant plus de 300 articles présentés lors de conférences récentes sur l'apprentissage automatique, nous avons constaté que les applications de l'apprentissage automatique pour la santé (ML4H) sont à la traîne par rapport aux autres domaines de l'apprentissage automatique en termes de mesures de reproductibilité.
Cet article effectue un examen quantitatif et qualitatif de l'état de la reproductibilité pour les applications ML dans le domaine de la santé et propose des recommandations pour rendre la recherche plus reproductible.
Nous formons de nombreux petits réseaux, chacun pour une opération spécifique, qui sont ensuite combinés pour effectuer des opérations complexes.
Cet article propose d'utiliser les réseaux neuronaux pour évaluer les expressions mathématiques en concevant 8 petits blocs de construction pour 8 opérations fondamentales, par exemple l'addition, la soustraction, etc. et en concevant ensuite la multiplication et la division à plusieurs chiffres à l'aide de ces petits blocs.
Ce document propose une méthode de conception d'un moteur d'évaluation d'expressions mathématiques basé sur un NN.
Améliorer la qualité et la stabilité des GAN à l'aide d'un discriminateur relativiste ; les GAN IPM (tels que WGAN-GP) constituent un cas particulier.
L'article propose un "discriminateur relativiste", qui est utile dans certains cas, mais qui est un peu sensible aux hyperparamètres, aux architectures et aux ensembles de données.
Dans ce travail, les auteurs considèrent une variation du GAN en diminuant simultanément la probabilité que les données réelles soient réelles pour le générateur.
Une version de MPO basée sur la fonction état-valeur qui obtient de bons résultats dans un large éventail de tâches de contrôle discret et continu.
Cet article présente un algorithme pour l'apprentissage par renforcement on-policy qui peut gérer à la fois le contrôle continu/discret, l'apprentissage mono/multi-tâche et utiliser à la fois des états et des pixels de faible dimension.
L'article propose une variante en ligne de MPO, V-MPO, qui apprend la fonction V et met à jour la distribution non-paramétrique vers les avantages.
Nous proposons des moteurs d'exécution neuronaux (NEE), qui s'appuient sur un masque appris et des traces d'exécution supervisées pour imiter la fonctionnalité des sous-programmes et démontrer une forte généralisation.
Cet article étudie le problème de la construction d'un moteur d'exécution de programme avec des réseaux neuronaux et propose un modèle basé sur les transformateurs pour apprendre les sous-routines de base et les applique dans plusieurs algorithmes standard.
Cet article traite du problème de la conception d'architectures de réseaux neuronaux capables d'apprendre et d'implémenter des programmes généraux.
La détection bayésienne des points de changement permet le méta-apprentissage directement à partir de données de séries chronologiques.
L'article considère le méta-apprentissage dans le cadre de tâches non segmentées et applique la détection bayésienne en ligne des points de changement avec le méta-apprentissage.
Cet article pousse le méta-apprentissage vers des paramètres de tâches non segmentées, où le cadre MOCA adopte un schéma d'estimation bayésien des points de changement pour la détection des changements de tâches.
Une approche basée sur l'apprentissage profond pour la détection des phonèmes fricatifs à retard nul.
Cet article applique des méthodes d'apprentissage profond supervisé pour détecter la durée exacte d'un phonème fricatif afin d'améliorer l'algorithme pratique de réduction de la fréquence.
Un mécanisme d'attention en ligne et en temps linéaire qui effectue une attention douce sur des morceaux de la séquence d'entrée localisés de manière adaptative.
Cet article propose une petite modification de l'attention monotonique dans [1] en ajoutant une attention douce au segment prédit par l'attention monotonique.
L'article propose une extension d'un modèle d'attention monotone antérieur (Raffel et al 2017) pour assister à une fenêtre de taille fixe jusqu'à la position d'alignement.
Développer de nouvelles techniques qui s'appuient sur la réorganisation des patchs pour permettre une analyse détaillée de la relation entre les ensembles de données et les performances de formation et de généralisation.
Nous produisons des agents d'apprentissage par renforcement qui se généralisent bien à un large éventail d'environnements en utilisant une nouvelle technique de régularisation.
L'article présente le défi des politiques de variance élevée dans la randomisation de domaines pour l'apprentissage par renforcement et se concentre principalement sur le problème de la randomisation visuelle, où les différents domaines randomisés ne diffèrent que dans l'espace d'état et où les récompenses et la dynamique sous-jacentes sont les mêmes.
Afin d'améliorer la capacité de généralisation des agents RL profonds à travers les tâches avec différents modèles visuels, cet article propose une technique de régularisation simple pour la randomisation du domaine.
Nous explorons l'intersection des neurosciences des réseaux et de l'apprentissage profond. 
Cet article présente un système de construction de base de connaissances non supervisé et de haute précision utilisant un programme probabiliste pour définir un processus de conversion des faits de la base de connaissances en texte non structuré.
Aperçu de la base de connaissances existante construite à l'aide d'un modèle probabiliste. L'approche de construction de la base de connaissances est évaluée par rapport à d'autres approches de bases de connaissances : YAGO2, NELL, Knowledge Vault et DeepDive.
Cet article utilise un programme probabiliste décrivant le processus par lequel des faits décrivant des entités peuvent être réalisés dans du texte et un grand nombre de pages web, pour apprendre à effectuer l'extraction de faits sur des personnes en utilisant un seul fait de base.
Nouvelle méthode d'extraction de signaux dans le domaine de Fourier
Améliorer l'évolutivité des réseaux de neurones à graphes pour l'apprentissage par imitation et la prédiction des mouvements d'essaims.
L'article propose un nouveau modèle de série temporelle pour l'apprentissage d'une séquence de graphes.
Ce travail considère les problèmes de prédiction de séquence dans un système multi-agent.
Nous proposons un cadre de quantification par produit différentiable qui peut réduire la taille de la couche d'intégration dans une formation de bout en bout sans coût de performance.
Cet article traite des méthodes de compression des couches d'intégration pour l'inférence à faible mémoire, dans lesquelles les couches d'intégration comprimées sont apprises avec les modèles spécifiques à la tâche d'une manière différentiable de bout en bout.
Nous présentons un algorithme de régression modale simple et novateur, facile à adapter à de gros problèmes. 
L'article propose une approche de fonction implicite pour apprendre les modes de régression multimodale.
Le présent travail propose une approche paramétrique pour estimer le mode conditionnel en utilisant le théorème de la fonction implicite pour les distributions multimodales. 
Méta-RL efficace sur le plan de l'échantillonnage en combinant l'inférence variationnelle des variables de tâches probabilistes avec le RL hors politique. 
Cet article propose d'utiliser la RL hors politique pendant le temps de méta-formation pour améliorer considérablement l'efficacité de l'échantillonnage des méthodes de méta-RL.
Cet article se concentre sur l'identification de sources web de haute qualité pour le pipeline d'augmentation des bases de connaissances industrielles.
Nous étudions les mérites de l'utilisation de réseaux neuronaux dans le problème de prédiction de correspondance où l'on cherche à estimer la probabilité qu'un groupe de M articles soit préféré à un autre, sur la base de données partielles de comparaison de groupes.
Cet article propose une solution de réseau neuronal profond au problème de classement d'ensembles et conçoit une architecture pour cette tâche inspirée d'algorithmes antérieurs conçus manuellement.
Cet article propose une technique pour résoudre le problème de la prédiction des matchs en utilisant une architecture d'apprentissage profond.
Une nouvelle approche pour maintenir des matrices de poids récurrentes orthogonales dans un RNN.
Présente un schéma d'apprentissage de la matrice de paramètres récurrents dans un réseau neuronal qui utilise la transformée de Cayley et une matrice de poids d'échelle. 
Cet article propose une reparamétrisation RNN des poids récurrents avec une matrice asymétrique en utilisant la transformation de Cayley pour garder la matrice des poids récurrents orthogonale.
Une nouvelle paramétrisation des RNN permet de représenter relativement facilement les matrices de poids orthogonales.
Nous utilisons un seul et même modèle pour résoudre une grande variété de tâches d'analyse du langage naturel en les formulant dans un format unifié d'empan-relation.
Cet article généralise un large éventail de tâches de traitement du langage naturel dans un cadre unique basé sur l'empan et propose une architecture générale pour résoudre tous ces problèmes.
Ce travail présente une formulation unifiée de diverses tâches NLP au niveau de la phrase et du jeton.
Nous présentons une limite inférieure variationnelle pour les modèles GP qui peut être optimisée sans calculer des opérations matricielles coûteuses comme les inverses, tout en fournissant les mêmes garanties que les approximations variationnelles existantes.
Les auto-codeurs variationnels avec des espaces latents modélisés comme des produits de collecteurs riemanniens à courbure constante améliorent la reconstruction d'images par rapport aux variantes à collecteur unique.
Cet article introduit une formulation générale de la notion de VAE avec un espace latent composé d'un collecteur courbe.
Cet article porte sur le développement des VAE dans les espaces non euclidiens.
Nous introduisons un algorithme de boîte noire pour l'optimisation répétée des composés en utilisant un cadre de traduction.
Les auteurs présentent l'optimisation des molécules comme un problème de séquence à séquence, et étendent les méthodes existantes pour améliorer les molécules, montrant qu'il est avantageux d'optimiser le logP mais pas le QED.
L'article s'appuie sur des modèles de traduction existants développés pour l'optimisation moléculaire, en utilisant de manière itérative des modèles de traduction de séquence à séquence ou de graphe à graphe.
Proposition du premier cadre de filigrane pour l'intégration et l'extraction de signatures multi-bits en utilisant les sorties du DNN. 
Propose une méthode de filtrage multibit des réseaux neuronaux dans un cadre de boîte noire et démontre que les prédictions des modèles existants peuvent porter une chaîne multibit qui peut être utilisée ultérieurement pour vérifier la propriété.
L'article propose une approche du filigrane de modèle où le filigrane est une chaîne de bits incorporée dans le modèle dans le cadre d'une procédure de réglage fin.
Vous ne savez pas comment optimiser ? Alors apprenez à optimiser !
Cet article propose un moyen d'entraîner les modèles de classification d'images pour qu'ils soient résistants aux attaques par perturbation de L-infinité.
Cet article propose d'utiliser le cadre de l'apprentissage par l'apprentissage pour apprendre un attaquant.
Une méthode simple et facile à entraîner pour la prédiction multimodale dans les séries temporelles. 
Cet article présente un modèle de prédiction de séries temporelles qui apprend un mappage déterministe et entraîne un autre réseau pour prédire les images futures à partir de l'entrée et de l'erreur résiduelle du premier réseau.
L'article propose un modèle de prédiction en cas d'incertitude où l'on distingue la prédiction déterministe des composants et la prédiction incertaine des composants.
Cet article présente et motive simple_rl, une nouvelle bibliothèque open source permettant de réaliser des expériences d'apprentissage par renforcement dans Python 2 et 3 en mettant l'accent sur la simplicité.
Cet article traite de la stabilité de l'optimisation simple du gradient de pénalité $\mu$-WGAN en introduisant un concept de différentiation à valeur mesurée.
On étudie le WGAN avec un terme de pénalité de gradient centré sur zéro au carré par rapport à une mesure générale.
Caractérise la convergence du GAN de Wasserstein pénalisé par le gradient.
Méthode d'apprentissage de pointe pour les réseaux de poids binaires et ternaires basée sur l'optimisation alternée de partitions de poids relaxées de façon aléatoire
Cet article propose un nouveau schéma de formation pour l'optimisation d'un réseau neuronal ternaire.
Les auteurs proposent la RPR, un moyen de partitionner et de quantifier les poids de manière aléatoire et d'entraîner les paramètres restants, suivi de la relaxation en cycles alternés pour entraîner les modèles quantifiés.
Nous remplaçons certains chemins de gradients dans les RNN hiérarchiques par une perte auxiliaire. Nous montrons que cela permet de réduire le coût mémoire tout en préservant les performances.
L'article présente une architecture RNN hiérarchique qui pourrait être entraînée de manière plus efficace en termes de mémoire.
L'article proposé suggère de découpler les différentes couches de la hiérarchie dans le RNN en utilisant des pertes auxiliaires.
Les réseaux neuronaux qui font un bon travail de classification projettent les points dans des formes plus sphériques avant de les comprimer en moins de dimensions.
Nous proposons une nouvelle méthode d'apprentissage pour la reconnaissance profonde des sons, appelée apprentissage BC.
Les auteurs ont défini une nouvelle tâche d'apprentissage qui demande à un DNN de prédire le rapport de mélange entre les sons de deux classes différentes afin d'augmenter le pouvoir discriminant du réseau final appris.
Propose une méthode pour améliorer la performance d'une méthode d'apprentissage générique en générant des échantillons d'entraînement "entre les classes" et présente l'intuition de base et la nécessité de la technique proposée.
Nous proposons une méthode qui permet de déduire le niveau de qualité des données variant dans le temps pour les prévisions spatio-temporelles sans avoir recours à des étiquettes explicites.
Introduit une nouvelle définition de la qualité des données qui s'appuie sur la notion de variation locale définie dans (Zhou et Scholkopf) et l'étend à de multiples sources de données hétérogènes.
Ce travail a proposé une nouvelle façon d'évaluer la qualité de différentes sources de données avec le modèle de graphe variable dans le temps, le niveau de qualité étant utilisé comme un terme de régularisation dans la fonction objectif.
Nous proposons des programmes de forme 3D, une représentation structurée et compositionnelle des formes. Notre modèle apprend à déduire et à exécuter des programmes de forme pour expliquer les formes 3D.
Une approche pour déduire des programmes de forme à partir de modèles 3D, avec une architecture consistant en un réseau récurrent qui encode une forme 3D et émet des instructions, et un second module qui rend le programme en 3D.
Cet article introduit une description sémantique de haut niveau pour les formes 3D, donnée par le ShapeProgram.
Nous montrons que les méthodes de régularisation classiques (par exemple, $L_2$, dropout), qui ont été largement ignorées dans les méthodes RL, peuvent être très efficaces dans l'optimisation des politiques.
Les auteurs étudient un ensemble de méthodes d'optimisation directe des politiques existantes dans le domaine de l'apprentissage par renforcement et fournissent une enquête détaillée sur l'effet des réglementations sur la performance et le comportement des agents qui suivent ces méthodes.
Cet article présente une étude sur l'effet de la régularisation sur les performances dans des environnements de formation aux méthodes d'optimisation des politiques dans des tâches de contrôle continu multiples.
Nous présentons un ensemble de données de questions-réponses, FigureQA, comme une première étape vers le développement de modèles capables de reconnaître intuitivement des modèles à partir de représentations visuelles de données.
Cet article présente un ensemble de données de réponses à des questions modélisées sur des figures, impliquant un raisonnement sur les éléments de la figure.
L'article présente un nouvel ensemble de données de raisonnement visuel appelé Figure-QA, composé de 140 000 images de figures et de 1,55 million de paires d'AQ, qui peut aider à développer des modèles capables d'extraire des informations utiles des représentations visuelles des données.
Cette prise de position analyse les différents types d'auto-explication qui peuvent apparaître dans la planification et les systèmes connexes. 
Discute des différents aspects des explications, notamment dans le contexte de la prise de décision séquentielle. 
La première approche d'apprentissage profond de la MFSR pour résoudre l'enregistrement, la fusion et le suréchantillonnage de bout en bout.
Cet article propose un algorithme de super-résolution multi-image de bout en bout, qui repose sur des co-registres par paire et des blocs de fusion (blocs résiduels convolutifs), intégrés dans un réseau codeur-décodeur 'HighRes-net' qui estime l'image de super-résolution.
Cet article propose un cadre incluant la fusion récursive à la perte de co-registration pour résoudre le problème des résultats de super-résolution et des étiquettes de haute résolution qui ne sont pas alignés en termes de pixels.
Pour la formation distribuée sur des réseaux à forte latence, utilisez le calcul approximatif de la moyenne distribuée basé sur les commérages au lieu du calcul exact de la moyenne distribuée comme AllReduce.
Les auteurs proposent d'utiliser les algorithmes de commérage comme méthode générale de calcul d'une moyenne approximative sur un ensemble de travailleurs.
L'article prouve la convergence de la SGP pour les fonctions lisses non convexes et montre que la SGP peut atteindre une accélération significative dans un environnement à faible latence sans sacrifier trop de performances prédictives. 
Cet article développe un cadre d'apprentissage contradictoire pour les modèles neuronaux de conversation avec persona.
Cet article propose une extension de hredGAN pour apprendre simultanément un ensemble d'incorporations d'attributs qui représentent la personne de chaque locuteur et générer des réponses basées sur la personne.
Les réseaux neuronaux artificiels bio-inspirés, composés de neurones positionnés dans un espace bidimensionnel, sont capables de former des groupes indépendants pour effectuer différentes tâches.
Transformateur discret qui utilise l'attention dure pour s'assurer que chaque étape ne dépend que d'un contexte fixe.
Cet article présente les modifications apportées à l'architecture standard du transformateur dans le but d'améliorer l'interprétabilité tout en conservant les performances dans les tâches NLP.
Cet article propose trois transformateurs discrets : un module d'attention discret et stochastique basé sur Gumbel-softmax, un transformateur syntaxique et sémantique à deux flux et une régularisation de la sparsité.
Nous montrons des preuves empiriques que les modèles de codage prédictif produisent des représentations plus corrélées aux données du cerveau que les modèles de reconnaissance d'images supervisés.
Un cadre générique pour gérer le transfert et l'apprentissage multi-tâches en utilisant des paires d'auto-codeurs avec des poids spécifiques à la tâche et partagés.
Propose un cadre générique pour l'apprentissage par transfert de bout en bout / l'adaptation au domaine avec des réseaux neuronaux profonds. 
Cet article propose un modèle permettant aux architectures de réseaux neuronaux profonds de partager des paramètres entre différents ensembles de données, et l'applique à l'apprentissage par transfert.
L'article se concentre sur l'apprentissage de caractéristiques communes à partir de données provenant de domaines multiples et aboutit à une architecture générale pour l'apprentissage multitâche, semi-supervisé et par transfert.
Nous proposons un cadre pour combiner les arbres de décision et les réseaux de neurones, et montrons sur des tâches de classification d'images qu'il bénéficie des avantages complémentaires des deux approches, tout en abordant les limites des travaux antérieurs.
Les auteurs ont proposé un nouveau modèle, les arbres neuronaux adaptatifs, en combinant l'apprentissage de la représentation et l'optimisation du gradient des réseaux neuronaux avec l'apprentissage de l'architecture des arbres de décision.
Cet article propose l'approche des arbres neuronaux adaptatifs pour combiner les deux paradigmes d'apprentissage que sont les réseaux neuronaux profonds et les arbres de décision.
La traduction de certaines parties de l'entrée pendant la formation peut améliorer les performances interlinguistiques.
Cet article propose une méthode d'augmentation des données multilingues pour améliorer les tâches d'inférence linguistique et de réponse aux questions.
Cet article propose d'augmenter les données interlinguales avec des échanges heuristiques en utilisant des traductions alignées, comme le font les humains bilingues dans les échanges de codes.
Nous proposons un cadre d'autoencodeur variationnel conditionnel qui atténue l'effondrement postérieur dans les scénarios où le signal de conditionnement est suffisamment fort pour qu'un décodeur expressif puisse générer une sortie plausible à partir de celui-ci.
Cet article considère les modèles génératifs fortement conditionnés, et propose une fonction objective et une paramétrisation de la distribution variationnelle telle que les variables latentes dépendent explicitement des conditions d'entrée.
Cet article soutient que lorsque le décodeur est conditionné par la concaténation de variables latentes et d'informations auxiliaires, alors l'effondrement postérieur est plus probable que dans le VAE vanille.
Nous proposons une étude de la stabilité de plusieurs algorithmes d'apprentissage en quelques coups soumis à des variations des hyper-paramètres et des schémas d'optimisation tout en contrôlant la graine aléatoire.
Cet article étudie la reproductibilité de l'apprentissage en quelques coups.
Nous traduisons une limite sur la sous-optimalité des représentations en un objectif de formation pratique dans le contexte de l'apprentissage par renforcement hiérarchique.
Les auteurs proposent une nouvelle approche de l'apprentissage d'une représentation pour les LRH et établissent un lien intéressant entre l'apprentissage de la représentation et la limitation de la sous-optimalité, ce qui donne lieu à un algorithme basé sur le gradient.
Cet article propose une façon de traiter la sous-optimalité dans le contexte des représentations d'apprentissage qui font référence à la sous-optimalité de la polarité hiérarchique par rapport à la récompense de la tâche.
Le métaraisonnement dans un planificateur temporel situé
Cet article aborde le problème de la planification temporelle située, en proposant une nouvelle simplification des stratégies gourmandes précédemment proposées par Shperberg.
Les performances en matière de robustesse des modèles formés par PGD sont sensibles à la transformation des ensembles de données d'image préservant la sémantique, ce qui implique que l'évaluation des algorithmes d'apprentissage robuste est délicate en pratique.
L'article clarifie la différence entre la précision propre et la précision robuste et montre que le changement de la distribution marginale des données d'entrée P(x) tout en préservant leur sémantique P(y|x) affecte la robustesse du modèle.
Cet article étudie l'origine du manque de robustesse des classifieurs aux perturbations des entrées adverses sous l-inf perturbations bornées.
Appariement de phrases par apprentissage des structures d'arbres de constituants latents avec une variante de l'algorithme inside-outside intégré comme une couche de réseau neuronal.
Cet article présente un mécanisme d'attention structurée pour calculer les scores d'alignement parmi tous les espaces possibles dans deux phrases données.
Cet article propose un modèle d'alignements structurés entre les phrases comme moyen de comparer les phrases en faisant correspondre leurs structures latentes.
Apprendre à démêler la représentation de manière non supervisée.
Les auteurs présentent un cadre dans lequel un codeur automatique (E, D) est régularisé de telle sorte que sa représentation latente partage l'information mutuelle avec une représentation d'espace latent générée.
Les GAN conditionnels entraînés à générer des échantillons de données augmentés de leurs entrées conditionnelles sont utilisés pour améliorer la classification de vanille et les systèmes d'apprentissage ponctuel tels que les réseaux de correspondance et la distance entre pixels.
Les auteurs proposent une méthode d'augmentation des données dans laquelle les transformations interclasses sont mises en correspondance avec un espace latent de faible dimension à l'aide d'un GAN conditionnel.
Nous développons une méthode simple de sélection de caractéristiques diagnostique de modèle basée sur la régression pour interpréter les processus de génération de données avec un contrôle de la FDR, et nous surpassons plusieurs lignes de base populaires sur plusieurs ensembles de données simulées, médicales et d'images.
Cet article propose une amélioration pratique du test de randomisation conditionnelle et une nouvelle statistique de test, prouve que la divergence f est un choix possible, et montre que la divergence KL annule certaines distributions conditionnelles.
Cet article aborde le problème de la recherche de caractéristiques utiles dans une entrée qui dépendent d'une variable de réponse même lorsqu'elle est conditionnée par toutes les autres variables d'entrée.
Une méthode agnostique de modèle pour fournir une interprétation de l'influence des caractéristiques d'entrée sur la réponse d'un modèle au niveau de la machine jusqu'au niveau de l'instance, et des statistiques de test appropriées pour la sélection de caractéristiques agnostiques de modèle.
Une nouvelle approche pour apprendre un modèle à partir d'annotations bruyantes provenant du public.
Cet article propose une méthode d'apprentissage à partir d'étiquettes bruitées, en se concentrant sur le cas où les données ne sont pas étiquetées de manière redondante, avec une validation théorique et expérimentale.
Cet article se concentre sur le problème de l'apprentissage à partir de foules, où la mise à jour conjointe des poids des classificateurs et des matrices de confusion des travailleurs peut aider au problème de l'estimation avec des étiquettes rares provenant de foules.
Il propose un algorithme d'apprentissage supervisé pour modéliser la qualité des étiquettes et des travailleurs et utilise l'algorithme pour étudier le niveau de redondance requis dans le crowdsourcing et si une faible redondance avec des exemples de bruit abondants conduit à de meilleures étiquettes.
Nouvelle façon d'expliquer pourquoi un réseau neuronal a mal classé une image
Cet article propose une méthode pour expliquer les erreurs de classification des réseaux neuronaux. 
Vise à mieux comprendre la classification des réseaux neuronaux et explore l'espace latent d'un auto-codeur variationnel et considère les perturbations de l'espace latent afin d'obtenir la bonne classification.
Une méthode pour la construction automatique de réseaux multi-tâches ramifiés avec une forte évaluation expérimentale sur divers ensembles de données multi-tâches.
Cet article propose un nouveau cadre d'apprentissage multi-tâches à partage de paramètres doux basé sur une structure arborescente.
Cet article présente une méthode pour déduire l'architecture des réseaux multi-tâches afin de déterminer quelle partie du réseau doit être partagée entre les différentes tâches.
Il est possible de substituer la matrice de poids dans une couche convolutionnelle pour l'entraîner comme une couche efficace structurée, aussi performante que la décomposition de rangs bas.
Ce travail applique les couches linéaires efficaces structurées précédentes aux couches convolutionnelles et propose des couches convolutionnelles efficaces structurées pour remplacer les couches convolutionnelles originales.
Nous présentons SVDocNet, un réseau de neurones récurrents spatiaux (RNN) basé sur un U-Net entraînable de bout en bout pour le débourrage aveugle de documents.
Nous étendons le codage clairsemé bilinéaire et tirons parti des séquences vidéo pour apprendre des filtres dynamiques.
Nous proposons un nouveau détecteur d'OOD qui utilise des images floues comme exemples adverses. Notre modèle atteint des performances significatives de détection des OOD dans différents domaines.
Cet article présente l'idée d'utiliser des images floues comme exemples de régularisation pour améliorer la performance de la détection de la non-répartition basée sur la distillation de réseau aléatoire.
Cet article s'attaque à la distribution hors des données en tirant parti de la RND appliquée aux augmentations de données en formant un modèle pour faire correspondre les sorties d'un réseau aléatoire avec une augmentation en entrée.
Un optimiseur rapide pour les applications générales et l'apprentissage par lots.
Dans cet article, les auteurs ont réalisé une étude sur l'entraînement à grande échelle pour le BERT, et ont réussi à entraîner un modèle de BERT en 76 minutes.
Cet article développe une stratégie d'adaptation par couche qui permet d'entraîner les modèles BERT avec de grands mini-batchs de 32k contre 512 de base.
Nous avons analysé le rôle de deux taux d'apprentissage dans le méta-apprentissage diagnostique des modèles en convergence.
Les auteurs ont abordé le problème de l'instabilité de l'optimisation dans MAML en étudiant les deux taux d'apprentissage.
Cet article étudie une méthode permettant de régler les deux taux d'apprentissage utilisés dans l'algorithme de formation MAML.
Modèle neuronal indépendant de la tâche pour l'apprentissage d'associations entre groupes de mots interdépendants.
L'article propose une méthode d'apprentissage de vecteurs de mots spécifiques à une fonction, dans laquelle chaque mot est représenté par trois vecteurs appartenant chacun à une catégorie différente (Sujet-Verbe-Objet).
Cet article propose un réseau neuronal pour apprendre des représentations de travail spécifiques à une fonction et démontre son avantage par rapport aux alternatives.
Utilisation de la méthode d'apprentissage profond pour effectuer des mesures automatiques d'images SEM dans l'industrie des semi-conducteurs
Cet article décrit et analyse trois méthodes de planification des activités à durée non fixe en présence de ressources consommables.
L'article présente trois approches pour l'ordonnancement à bord des activités d'un rover planétaire sous des contraintes de ressources réservoirs.
Description de la soumission au NeurIPS2019 Disentanglement Challenge basé sur des autoencodeurs variationnels hypersphériques
Une détection d'anomalie qui : utilise la classification par transformation aléatoire pour la généralisation aux données non-image.
Cet article propose une méthode profonde pour la détection des anomalies qui unifie les approches récentes de classification profonde à une classe et de classification basée sur la transformation.
Cet article propose une approche de la détection des anomalies basée sur la classification pour les données générales en utilisant la transformation affine y = Wx+b.
Nous réduisons les biais de sentiment en nous basant sur une évaluation contrefactuelle de la génération de texte à l'aide de modèles de langage.
Cet article mesure le biais de sentiment dans les modèles de langage tel qu'il est reflété par le texte généré par les modèles, et ajoute d'autres termes objectifs à l'objectif habituel de modélisation du langage pour réduire le biais.
Cet article propose d'évaluer le biais dans les modèles de langage pré-entraînés en utilisant un système de sentiment fixe et teste plusieurs modèles de préfixe différents.
Une méthode basée sur la similarité sémantique et une méthode basée sur la similarité des sentiments pour débiaiser les modèles neuronaux de langage formés à partir de grands ensembles de données.
Un modèle thématique bayésien non paramétrique avec des encodeurs automatiques variationnels qui atteint l'état de l'art sur des repères publics en termes de perplexité, de cohérence thématique et de tâches de recherche.
Cet article construit un modèle thématique infini avec des encodeurs automatiques variationnels en combinant l'encodeur automatique variationnel de Nalisnick & Smith avec l'allocation latente de Dirichlet et plusieurs techniques d'inférence utilisées dans Miao.
Nous présentons un nouveau cadre de distillation des connaissances qui utilise des échantillons de pairs comme enseignants.
Propose une méthode pour améliorer l'efficacité de la distillation des connaissances en adoucissant les étiquettes utilisées et en employant un ensemble de données au lieu d'un seul échantillon.
Cet article propose d'aborder le coût de calcul supplémentaire de la formation avec la distillation des connaissances, en s'appuyant sur la technique de distillation des instantanés récemment proposée.
apprendre des sous-politiques hiérarchiques par un apprentissage de bout en bout sur une distribution de tâches
Les auteurs considèrent le problème de l'apprentissage d'un ensemble utile de â€˜sub policiesâ€™ qui peuvent être partagées entre les tâches afin de démarrer l'apprentissage sur de nouvelles tâches tirées de la distribution des tâches. 
Cet article propose une nouvelle méthode pour induire une structure hiérarchique temporelle dans un cadre multi-tâches spécialisé.
Modèle de réseau neuronal convolutif pour l'intégration non supervisée de documents.
Présente un nouveau modèle pour la tâche générale d'induction de représentations de documents (embeddings) qui utilise une architecture CNN pour améliorer l'efficacité de calcul.
Cet article propose d'utiliser des CNN avec un objectif de type "skip-gram" comme moyen rapide de produire des incorporations de documents.
Nous prouvons des limites de généralisation pour les réseaux neuronaux convolutifs qui tiennent compte de la liaison des poids.
Étudie le pouvoir de généralisation des CNN et améliore les limites supérieures des erreurs de généralisation, en montrant la corrélation entre l'erreur de généralisation des CNN appris et le terme dominant de la limite supérieure.
Cet article présente une limite de généralisation pour les réseaux neuronaux convolutifs basée sur le nombre de paramètres, la constante de Lipschitz et la distance des poids finaux par rapport à l'initialisation.
Économie de deux fois la taille du modèle et réduction de 28 % de l'énergie pour les MobileNets sur ImageNet, sans perte de précision, en utilisant des couches hybrides composées de filtres classiques de pleine précision et de filtres ternaires.
Se concentre sur la quantification de l'architecture MobileNets en valeurs ternaires, ce qui réduit l'espace et le calcul nécessaires afin de rendre les réseaux neuronaux plus efficaces sur le plan énergétique.
L'article propose un banc de filtres hybride en couches qui quantifie uniquement une fraction des filtres convolutifs en valeurs ternaires pour l'architecture MobileNets.
Nous établissons une référence de bruit réel contrôlé et révélons plusieurs résultats intéressants sur les données bruyantes du monde réel.
Cet article compare 6 méthodes d'apprentissage d'étiquettes bruyantes existantes dans deux contextes de formation : à partir de zéro et avec un réglage fin.
Les auteurs établissent un grand ensemble de données et un repère de bruit contrôlé dans le monde réel pour réaliser des expériences contrôlées sur des données bruitées dans l'apprentissage profond.
Nous apprenons à résoudre le problème de la conception d'ARN avec l'apprentissage par renforcement en utilisant des approches de méta apprentissage et d'autoML.
Utilisation de l'optimisation par gradient de politique pour générer des séquences d'ARN qui se replient dans une structure secondaire cible, ce qui a permis d'améliorer nettement la précision et le temps d'exécution. 
La formation de petits réseaux est plus efficace que l'élagage, mais l'élagage permet de former de bons petits réseaux qui sont faciles à copier.
Nous étudions le problème de l'apprentissage de la prédiction de la diversité sous-jacente des croyances présentes dans les domaines d'apprentissage supervisé.
Nous avons introduit une stratégie qui permet de réaliser des modèles d'inpainting sur des ensembles de données de différentes tailles.
Aide à l'inpeinture d'images à l'aide de GANs en utilisant un filtre d'augmentation comparative et en ajoutant un bruit aléatoire à chaque pixel.
Nous trouvons des preuves que la minimisation de la divergence peut ne pas être une caractérisation précise de l'entraînement des GAN.
La soumission vise à présenter des preuves empiriques que la théorie de la minimisation de la divergence est plus un outil pour comprendre le résultat de l'entraînement des GANs qu'une condition nécessaire à appliquer pendant l'entraînement lui-même.
Cet article étudie les GANs non saturants et l'effet de deux approches de gradient pénalisé, en considérant plusieurs expériences de pensée pour démontrer les observations et les valider sur des expériences de données réelles.
Un nouveau test statistique pratique de dépendance utilisant des réseaux neuronaux, évalué sur des ensembles de données IRMf synthétiques et réels.
Propose une estimation de l'information mutuelle basée sur un réseau neuronal qui peut fonctionner de manière fiable avec de petits ensembles de données, réduisant la complexité de l'échantillon en découplant le problème de l'apprentissage du réseau et le problème de l'estimation.
Sous-titrage d'images à l'aide d'une intégration bidimensionnelle des mots.
LEAP combine la force de l'échantillonnage adaptatif avec celle de l'apprentissage en ligne par mini-lots et de l'apprentissage par représentation adaptative pour formuler une stratégie d'auto-apprentissage représentative dans un protocole de formation DNN de bout en bout. 
Présente une méthode pour créer des mini-lots pour un réseau d'étudiants en utilisant un deuxième espace de représentation appris pour sélectionner dynamiquement des exemples selon leur "facilité et leur vraie diversité".
Expérimente la précision de classification sur les ensembles de données MNIST, FashionMNIST et CIFAR-10 pour apprendre une représentation avec une sélection de minibatchs de style d'apprentissage par curriculum dans un cadre de bout en bout.
Nous proposons de construire les macro-actions par un algorithme génétique, ce qui élimine la dépendance de la procédure de dérivation des macro-actions par rapport aux politiques passées de l'agent.
Cet article propose un algorithme générique pour construire des macro-actions pour l'apprentissage par renforcement profond en ajoutant une macro-action à l'espace des actions primitives.
Nous proposons une extension de LFADS capable d'inférer les trains de pointes pour reconstruire les traces de fluorescence calcique en utilisant des VAE hiérarchiques.
Nous présentons la première méthode réussie pour entraîner la traduction automatique neuronale de manière non supervisée, en utilisant uniquement des corpus monolingues.
Les auteurs présentent un modèle de NMT non supervisé qui ne nécessite aucun corpus parallèle entre les deux langues concernées. 
Il s'agit d'un article sur la TA non supervisée qui entraîne une architecture standard utilisant des encastrements de mots dans un espace d'encastrement partagé uniquement avec des papiers de mots bilingues et un encodeur-décodeur entraîné en utilisant des données monolingues.
Nous entraînons les réseaux adversariens génératifs de manière progressive, ce qui nous permet de générer des images haute résolution de grande qualité.
Introduction de la croissance progressive et d'une fonction simple de statistique sommaire sans paramètre pour l'utilisation dans l'entraînement du GAN afin de permettre la synthèse d'images à haute résolution.
Un CNN sphérique basé sur un graphe qui présente un équilibre intéressant de compromis pour une grande variété d'applications.
Combine les cadres CNN existants basés sur la discrétisation d'une sphère comme un graphe pour montrer un résultat de convergence qui est lié à l'équivalence de rotation sur une sphère.
Les auteurs utilisent la formulation existante du CNN graphique et une stratégie de mise en commun qui exploite les pixellisations hiérarchiques de la sphère pour apprendre de la sphère discrétisée.
Nous prouvons les relations fluctuation-dissipation pour SGD, qui peuvent être utilisées pour (i) fixer de manière adaptative les taux d'apprentissage et (ii) sonder les surfaces de perte.
Les concepts de Paper fonctionnent dans le formalisme en temps discret, utilisent l'équation maîtresse et ne dépendent pas d'une approximation localement quadratique de la fonction de perte ou d'une hypothèse gaussienne du bruit SGD. 
Les auteurs dérivent les relations stationnaires fluctuation-dissipation qui relient les quantités mesurables et les hyperparamètres dans la SGD et utilisent ces relations pour définir le programme d'entraînement de manière adaptative et analyser le paysage de la fonction de perte.
Nous proposons un mécanisme de débruitage de l'état interne d'un RNN pour améliorer les performances de généralisation.
Pour les environnements dictés partiellement par des processus d'entrée externes, nous dérivons une ligne de base dépendante de l'entrée qui réduit de manière prouvée la variance des méthodes de gradient de politique et améliore la performance de la politique dans une large gamme de tâches RL.
Les auteurs considèrent le problème de l'apprentissage dans des environnements dirigés par les entrées, montrent comment le théorème PG s'applique toujours pour un critique conscient des entrées, et montrent que les lignes de base dépendantes des entrées sont les meilleures à utiliser dans la conjecture avec ce critique.
Cet article introduit la notion de ligne de base dépendante de l'entrée dans les méthodes de gradient de politique en RL, et propose différentes méthodes pour entraîner la fonction de ligne de base dépendante de l'entrée afin d'aider à éliminer la variance de la perturbation des facteurs externes.
L'ajout d'une mémoire de style à la couche supérieure d'un réseau de classification lui permet d'être génératif.
Cet article propose d'entraîner un réseau neuronal classificateur non seulement pour classer, mais aussi pour reconstruire une représentation de son entrée, afin de factoriser l'information de classe à partir de l'apparence.
L'article propose de former un auto-codeur de telle sorte que la représentation de la couche intermédiaire consiste en l'étiquette de classe de l'entrée et en une représentation vectorielle cachée.
Les modèles de routage par exemple bénéficient de la diversité architecturale, mais ont toujours du mal à s'adapter à un grand nombre de décisions de routage.
Ajoute une diversité au type d'unité architecturale disponible pour le routeur à chaque décision et une mise à l'échelle vers des réseaux plus profonds, en atteignant des performances de pointe sur Omniglot. 
Ce travail permet d'étendre les réseaux de routage afin d'utiliser diverses architectures à travers les modules routés.
Nous présentons des RNN pour l'entraînement de modèles de substitution d'EDP, dans lesquels les contraintes de cohérence garantissent que les solutions sont physiquement significatives, même lorsque l'entraînement utilise des domaines beaucoup plus petits que ceux auxquels le modèle entraîné est appliqué.
Nous proposons l'utilisation d'un décentrage miroir optimiste pour résoudre les problèmes de cyclage dans l'apprentissage des GANs. Nous présentons également l'algorithme Optimistic Adam
Cet article propose l'utilisation de la descente de miroir optimiste pour entraîner les WGAN.
L'article propose d'utiliser la descente de gradient optimiste pour la formation de GAN, ce qui permet d'éviter le comportement cyclique observé avec la SGD et ses variantes et donne des résultats prometteurs dans la formation de GAN.
Cet article propose une modification simple de la descente de gradient standard, prétendant améliorer la convergence des GANs et autres problèmes d'optimisation minimax.
Une simple extension de la factorisation matricielle généralisée permet de surpasser les approches de recommandation les plus récentes.
Ce travail présente un cadre de factorisation matricielle pour renforcer l'effet des données historiques lors de l'apprentissage des préférences des utilisateurs dans des contextes de filtrage collaboratif.
Une méthode qui construit des représentations de données séquentielles et de leur dynamique par le biais de modèles génératifs avec un processus actif.
Combine des réseaux neuronaux et des distributions gaussiennes pour créer une architecture et un modèle génératif pour les images et les vidéos qui minimise l'erreur entre les images générées et fournies.
L'article propose un modèle de réseau bayésien, réalisé sous forme de réseau neuronal, qui apprend différentes données sous la forme d'un système dynamique linéaire.
Nous proposons des polynômes comme fonctions d'activation.
Les auteurs introduisent des fonctions d'activation apprenables qui sont paramétrées par des fonctions polynomiales et montrent des résultats légèrement meilleurs que ReLU.
Une méthode simple de motivation intrinsèque utilisant un modèle de dynamique avancée erreur dans l'espace des caractéristiques de la politique.
Nous montrons que les VAE démêlés sont plus robustes que les VAE classiques face aux attaques adverses qui visent à les tromper en décodant l'entrée adverse vers une cible choisie. Nous développons ensuite un VAE démêlé hiérarchique encore plus robuste, le Seatbelt-VAE.
Les auteurs proposent un nouveau modèle VAE appelé seatbelt-VAE, qui s'avère plus robuste pour les attaques latentes que les benchmarks.
Nous démontrons que les changements de fonction dans la rétropropagation sont équivalents à un taux d'apprentissage implicite.
Une approche d'apprentissage par renforcement pour le transfert de style de texte
Présente une méthode basée sur le RL qui exploite un modèle de langage pré-entraîné pour transférer le style d'un texte, sans objectif de désenchevêtrement, tout en utilisant les générations de transfert de style d'un autre modèle.
Les auteurs proposent une récompense combinée composée de la fluidité, du contenu et du style pour le transfert de style de texte.
Nous montrons qu'une hiérarchie sémantique hautement structurée émerge dans les représentations génératives profondes comme résultat de la synthèse des scènes.
L'article étudie les aspects encodés par les variables latentes entrées dans les différentes couches de StyleGAN.
L'article présente une interprétation guidée visuellement des activations des couches de convolution dans le générateur de StyleGAN sur la disposition, la catégorie de scène, les attributs de la scène et la couleur.
Nous mettons en commun les messages entre plusieurs chaînes SMILES de la même molécule afin de transmettre les informations le long de tous les chemins du graphe moléculaire, produisant ainsi des représentations latentes qui dépassent largement l'état de l'art dans une variété de tâches.
La méthode utilise des entrées multiples de chaînes SMILES, la fusion de caractéristiques par caractère à travers ces chaînes, et l'entraînement du réseau par le biais de cibles de sortie multiples de chaînes SMILES, créant une représentation latente robuste de longueur fixe indépendante de la variation des SMILES.  
Les auteurs décrivent une nouvelle méthode de type auto-codeur variationnel pour les molécules qui codent les molécules sous forme de chaînes de caractères afin de réduire les opérations nécessaires au partage d'informations entre les atomes de la molécule.
Nous proposons une approche simple et générale qui permet d'éviter le problème d'effondrement des modes dans divers GAN conditionnels.
L'article propose un terme de régularisation pour l'objectif conditionnel du GAN afin de promouvoir une génération multimodale diversifiée et d'empêcher l'effondrement des modes.
L'article propose une méthode permettant de générer diverses sorties pour divers cadres GAN conditionnels, notamment la traduction d'image à image, la peinture d'image et la prédiction vidéo, qui peut être appliquée à divers cadres de synthèse conditionnelle pour diverses tâches. 
Le fait de doter le modèle de transformation de raccourcis vers la couche d'intégration libère la capacité du modèle pour l'apprentissage de nouvelles informations.
Nous examinons la relation entre les valeurs de densité de probabilité et le contenu des images dans les GAN non inversables.
Les auteurs tentent d'estimer la distribution de probabilité de l'image à l'aide du GAN et développent une approximation correcte des PDF dans l'espace latent.
Nous proposons une convolution à brassage spatial qui permet à la convolution régulière d'incorporer les informations provenant de l'extérieur de son champ réceptif.
Propose une convulation SS qui utilise des informations en dehors de sa RF, montrant des résultats améliorés lorsqu'elle est testée sur plusieurs modèles CNN.
Les auteurs ont proposé une stratégie de brassage des couches de convolution dans les réseaux neuronaux convolutifs.
Une méthode pour modéliser la distribution générative de séquences provenant d'entités connectées par un graphe.
Les auteurs proposent une méthode pour modéliser des données séquentielles provenant de plusieurs sources interconnectées en utilisant un mélange de pool commun de HMM.
Notre travail applique le méta-apprentissage à l'apprentissage par renforcement multi-agent pour aider notre agent à s'adapter efficacement aux nouveaux adversaires.
Cet article se concentre sur l'adaptation rapide aux nouveaux comportements des autres agents de l'environnement en utilisant une méthode basée sur MAML.
L'article présente une approche de l'apprentissage multi-agent basée sur le cadre du méta-apprentissage agnostique pour la tâche de modélisation des opposants pour le RL multi-agent.
Nous caractérisons les valeurs singulières de la transformation linéaire associée à une couche convolutive multicanaux 2D standard, ce qui permet leur calcul efficace. 
L'article est consacré au calcul des valeurs singulières des couches convolutionnelles.
Déduit des formules exactes pour calculer les valeurs singulières des couches de convolution des réseaux neuronaux profonds et montre que le calcul des valeurs singulières peut être effectué beaucoup plus rapidement que le calcul de la SVD complète de la matrice de convolution en faisant appel à des transformations FFT rapides.
VariBAD ouvre la voie à une exploration Bayes-optimale approximative traçable pour la RL profonde en utilisant les idées du méta-apprentissage, de la RL bayésienne et de l'inférence variationnelle approximative.
Cet article présente une nouvelle méthode d'apprentissage par renforcement profond qui permet de concilier efficacement l'exploration et l'exploitation et qui combine le méta-apprentissage, l'inférence variationnelle et l'apprentissage rationnel bayésien.
Nous montrons que l'apprentissage métrique peut aider à réduire les oublis catastrophiques.
Cet article applique l'apprentissage métrique pour réduire les oublis catastrophiques sur les réseaux neuronaux en améliorant l'expressivité de la couche finale, ce qui conduit à de meilleurs résultats en apprentissage continu.
Nous présentons NormCo, un modèle de cohérence profonde qui prend en compte la sémantique d'une mention d'entité, ainsi que la cohérence topique des mentions au sein d'un même document pour effectuer la normalisation des entités de maladie.
Utilise un autoencodeur GRU pour représenter le "contexte" (entités liées à une maladie donnée dans le cadre d'une phrase), résolvant la tâche BioNLP avec des améliorations significatives par rapport aux méthodes les plus connues.
Nous explorons le rôle de l'interaction multiplicative en tant que cadre unificateur pour décrire une série de motifs architecturaux de réseaux neuronaux classiques et modernes, tels que le gating, les couches d'attention, les hyperréseaux et les convolutions dynamiques, entre autres.
Présente l'interaction multiplicative comme une caractérisation unifiée pour représenter les composants de conception d'architecture de modèle couramment utilisés, en montrant la preuve empirique d'une performance supérieure sur des tâches comme RL et la modélisation de séquence.
L'article explore différents types d'interactions multiplicatives et trouve des modèles MI capables d'atteindre une performance de pointe sur des problèmes de modélisation du langage et d'apprentissage par renforcement.
Un cadre GAN efficace de conditionnement de texte pour générer des vidéos à partir de texte
Cet article présente une méthode basée sur les GAN pour la génération de vidéos conditionnées par une description textuelle, avec une nouvelle méthode de conditionnement qui génère des filtres de convolution à partir du texte codé, et les utilise pour une convolution dans le discriminateur.
Cet article propose des modèles GAN conditionnels pour la synthèse texte-vidéo : développement de filtres CNN conditionnés par les caractéristiques du texte et construction d'un ensemble de données de formes mobiles avec des performances améliorées pour la génération de vidéos et d'images.
SplitLBI est appliqué à l'apprentissage profond pour explorer l'éparpillement structurel du modèle, ce qui permet d'obtenir des performances de pointe dans ImageNet-2012 et de dévoiler une architecture de sous-réseau efficace.
Propose un algorithme basé sur l'optimisation pour trouver les structures éparses importantes des réseaux neuronaux à grande échelle en couplant l'apprentissage de la matrice de poids et les contraintes d'éparpillement, offrant une convergence garantie sur les problèmes d'optimisation non convexes.
Nous proposons des mécanismes de déclenchement pour améliorer l'ISTA appris pour le codage clairsemé, avec des garanties théoriques sur la supériorité de la méthode. 
Propose des extensions de LISTA qui traitent la sous-estimation en introduisant des "portes de gain" et en incluant le momentum avec des "portes de dépassement", montrant des taux de convergence améliorés.
Cet article se concentre sur la résolution de problèmes de codage clairsemé en utilisant des réseaux de type LISTA en proposant une "fonction de déclenchement de gain" pour atténuer la faiblesse de l'hypothèse "pas de faux positifs".
Nous présentons un cadre efficace et adaptatif pour comparer les classificateurs d'images afin de maximiser les écarts entre les classificateurs, au lieu de les comparer sur des ensembles de test fixes.
Mécanisme de détection des erreurs qui compare les classificateurs d'images en échantillonnant leur ensemble de test "le plus en désaccord", en mesurant le désaccord par une distance sémantique dérivée de l'ontologie WordNet.
Nous proposons une technique qui modifie les structures des CNN afin d'améliorer la robustesse tout en conservant une grande précision des tests, et nous remettons en question la pertinence de la définition actuelle des exemples contradictoires en générant des exemples contradictoires capables de tromper les humains.
Cet article propose une technique simple pour améliorer la robustesse des réseaux neuronaux contre les attaques de type "boîte noire".
Les auteurs proposent une méthode simple pour augmenter la robustesse des réseaux neuronaux convolutifs contre les exemples adverses, avec des résultats étonnamment bons.
Nous proposons de comparer l'apprentissage semi-supervisé et l'apprentissage robuste pour les étiquettes bruyantes dans un cadre commun.
Les auteurs proposent une stratégie basée sur la mixité pour la formation d'un modèle dans un cadre formel qui inclut les tâches d'apprentissage semi-supervisé et robuste comme cas particuliers.
Cet article démontre de manière expérimentale l'effet bénéfique des connexions descendantes dans l'algorithme de codage spartiate hiérarchique.
Cet article présente une étude qui compare les techniques de codage hiérarchique spartiate, montrant que le terme descendant est bénéfique pour réduire l'erreur de prédiction et peut apprendre plus rapidement.
Une approche de boîte noire pour expliquer les prédictions d'un modèle de similarité d'images.
Présente une méthode d'explication du modèle de similarité des images qui identifie les attributs qui contribuent positivement au score de similarité et les associe à une carte de saillance générée.
L'article propose un mécanisme d'explication qui associe les régions typiques de la carte de saillance à des attributs pour les réseaux neuronaux profonds de mise en correspondance par similarité.
Comment évaluer les attaques adverses contre seq2seq ?
Les auteurs étudient les moyens de générer des exemples contradictoires et montrent que l'entraînement contradictoire avec l'attaque la plus cohérente avec les critères de préservation du sens introduits permet d'améliorer la robustesse à ce type d'attaque sans dégradation dans le cadre non contradictoire.
L'article porte sur les perturbations adversariales préservant le sens dans le contexte des modèles Seq2Seq.
Une technique de normalisation alternative à la normalisation par lots
Introduit une technique de normalisation, qui normalise les poids des couches convolutionnelles. 
Ce manuscrit introduit une nouvelle transformation par couche, EquiNorm, pour améliorer la normalisation par lot qui ne modifie pas les entrées des couches mais plutôt les poids des couches.
Représenter chaque entité comme une distribution de probabilité sur des contextes intégrés dans un espace de base.
Propose de construire des encastrements de mots à partir d'un histogramme sur les mots du contexte, au lieu de vecteurs ponctuels, ce qui permet de mesurer les distances entre deux mots en termes de transport optimal entre les histogrammes grâce à une méthode qui augmente la représentation d'une entité d'un "point dans un espace vectoriel" standard à un histogramme avec des bacs situés à certains points de cet espace vectoriel. 
Il faut s'attendre à de petites perturbations adverses étant donné les taux d'erreur observés des modèles en dehors de la distribution naturelle des données.
Cet article propose une vision alternative pour les exemples adverses dans les espaces de haute dimension en considérant le "taux d'erreur" dans une distribution gaussienne centrée sur chaque point de test.
Étudie comment l'apprentissage auto-supervisé et la distillation des connaissances interagissent dans le contexte de la construction de modèles compacts.
Étudie l'entraînement de modèles de langage compacts pré-entraînés par distillation et montre que l'utilisation d'un enseignant pour distiller un modèle compact d'étudiant donne de meilleurs résultats que le pré-entraînement direct du modèle.
Cette présentation montre que le pré-entraînement d'un étudiant directement sur la modélisation du langage masqué est meilleur que la distillation, et le mieux est de combiner les deux et de distiller à partir de ce modèle étudiant pré-entraîné.
Nous présentons le schéma universel de compression des réseaux neuronaux profonds, qui s'applique universellement à la compression de n'importe quel modèle et dont les performances sont quasi optimales quelle que soit la distribution des poids.
Présente un pipeline pour la compression de réseau qui est similaire à la compression profonde et utilise une quantification aléatoire en treillis au lieu de la quantification vectorielle classique, et utilise le codage de source universel (bzip2) au lieu du codage de Huffman.
Cet article tente d'aborder de manière préliminaire le désenchevêtrement d'un point de vue théorique dans une situation idéaliste et d'un point de vue pratique par la modélisation du bruit dans un cas réaliste.
Étudie l'importance de la modélisation du bruit dans le VAE gaussien et propose d'entraîner le bruit en utilisant une méthode similaire à celle de l'Empirical-Bayes.
Modification de la façon dont les facteurs de bruit sont traités lors de l'élaboration de modèles VAE
Nous étudions la régularisation par décroissance de poids pour différents optimiseurs et identifions trois mécanismes distincts par lesquels la décroissance de poids améliore la généralisation.
Discute de l'effet de la décroissance des poids sur l'apprentissage des modèles de réseaux profonds avec et sans normalisation des lots et lors de l'utilisation de méthodes d'optimisation du premier ou du second ordre et émet l'hypothèse qu'un taux d'apprentissage plus élevé a un effet de régularisation.
Le tout premier ensemble de données d'adaptation au domaine librement disponible pour la détection d'événements sonores.
Estimateur d'information mutuelle basé sur une mécanique statistique non extensive
Cet article tente d'établir de nouvelles limites inférieures variationnelles pour l'information mutuelle en introduisant le paramètre q et en définissant l'algèbre q. Il montre que les limites inférieures ont une variance plus faible et atteignent des valeurs élevées.
Nous montrons que l'ascension par gradient stochastique converge vers un optimum global pour le WGAN avec un réseau générateur à une couche.
Tente de prouver que la descente de gradient stochastique peut converger vers une solution globale pour le problème min-max de WGAN.
Nous montrons empiriquement que l'entraînement contradictoire est efficace pour supprimer les perturbations universelles, rend les exemples contradictoires moins robustes aux transformations de l'image, et les laisse détectables pour une approche de détection.
Analyse l'entraînement contradictoire et son effet sur les exemples contradictoires universels ainsi que sur les exemples contradictoires standard (itération de base) et comment l'entraînement contradictoire affecte la détection. 
Les auteurs montrent que l'entraînement contradictoire est efficace pour se protéger contre les perturbations contradictoires "partagées", en particulier contre les perturbations universelles, mais moins efficace pour se protéger contre les perturbations singulières.
Nous présentons des techniques permettant d'entraîner un réseau unique qui s'adapte à de nombreuses plateformes matérielles.
La méthode permet d'obtenir un réseau à partir duquel on peut extraire des sous-réseaux pour diverses contraintes de ressources (latence, mémoire) qui donnent de bons résultats sans nécessiter de réapprentissage.
Cet article tente d'aborder le problème de la recherche des meilleures architectures pour les scénarios de déploiement de ressources spécialisées avec une méthode NAS basée sur la prédiction.
Proposer une approche pour booster les modèles génératifs par des modèles à variables cachées en cascade.
Cet article propose une nouvelle approche de boosting en cascade pour booster les modèles génératifs qui permet à chaque méta-modèle d'être entraîné séparément et avec avidité.
Nous étudions la structure des phrases dans ELMo et dans des modèles d'intégration contextuelle connexes. Nous constatons que les modèles existants encodent efficacement la syntaxe et montrent des preuves de dépendances à longue portée, mais n'offrent que de faibles améliorations pour les tâches sémantiques.
Propose la méthode "edge probing" et se concentre sur la relation entre les travées plutôt que sur les mots individuels, ce qui permet aux auteurs d'examiner les constituants syntaxiques, les dépendances, les étiquettes d'entités et les étiquettes de rôles sémantiques.
Fournit de nouveaux aperçus sur ce qui est capturé des encastrements de mots contextualisés en compilant un ensemble de tâches de "sondage de bord". 
Nous présentons le DPFRL, un cadre pour l'apprentissage par renforcement dans le cadre d'observations partielles et complexes avec un filtre à particules discriminant entièrement différentiable.
Présente des idées pour former des agents DLR avec des variables d'état latentes, modélisées comme une distribution de croyances, afin qu'ils puissent gérer des environnements partiellement observés.
Cet article présente une méthode de principe pour le POMDP RL : l'apprentissage par renforcement avec filtre de particules discriminatif qui permet de raisonner avec des observations partielles sur de multiples étapes temporelles, atteignant l'état de l'art sur des benchmarks.
Les objectifs de Monte Carlo sont analysés à l'aide de l'inférence variationnelle des variables auxiliaires, ce qui donne lieu à une nouvelle analyse du CPC et du NCE ainsi qu'à un nouveau modèle génératif.
Propose un point de vue différent sur l'amélioration des limites variationnelles avec des modèles auxiliaires de variables latentes et explore l'utilisation de ces modèles dans le modèle génératif.
Nous améliorons le fonctionnement de tous les algorithmes de descente de gradient existants.
Les auteurs proposent d'échantillonner les gradients stochastiques à partir d'une fonction monotone proportionnelle à l'ampleur des gradients en utilisant LSH. 
Considère le SGD sur un objectif de la forme d'une somme sur les exemples d'une perte quadratique.
Les limites de l'IA actuelle sont généralement reconnues, mais moins de gens savent que nous comprenons suffisamment le cerveau pour proposer immédiatement de nouvelles formules d'IA.
Nous utilisons la réponse à des questions pour évaluer la quantité de connaissances sur l'environnement que les agents peuvent apprendre par prédiction auto-supervisée.
Propose l'AQ comme outil pour étudier ce que les agents apprennent dans le monde, en faisant valoir qu'il s'agit d'une méthode intuitive pour les humains qui permet une complexité arbitraire.
Les auteurs proposent un cadre pour évaluer les représentations construites par les modèles prédictifs qui contiennent suffisamment d'informations pour répondre aux questions sur l'environnement sur lequel ils sont entraînés. Ils ont montré que celles de SimCore contenaient suffisamment d'informations pour que le LSTM réponde aux questions avec précision.
Nous développons une nouvelle méthode de classification déséquilibrée en utilisant des exemples contradictoires.
Propose un nouvel objectif d'optimisation qui génère des échantillons synthétiques en sur-échantillonnant les classes majoritaires au lieu des classes minoritaires, ce qui résout le problème du sur-ajustement des classes minoritaires.
Les auteurs proposent de s'attaquer à la classification des déséquilibres en utilisant des méthodes de rééchantillonnage, en montrant que des exemples contradictoires dans la classe minoritaire aideraient à former un nouveau modèle qui généraliserait mieux.
Une application intéressante du CNN dans les expériences de physique de la matière condensée molle.
Les auteurs démontrent qu'une approche d'apprentissage profond permet d'améliorer à la fois la précision d'identification et le taux d'identification des défauts des cristaux liquides nématiques.
Appliquer un modèle neuronal bien connu (YOLO) pour détecter les boîtes de délimitation des objets dans les images.
Une analyse des effets de la compositionnalité et de la localité sur l'apprentissage de la représentation pour l'apprentissage à zéro.
Propose un cadre d'évaluation pour ZSL où le modèle ne peut pas être pré-entraîné et où les paramètres du modèle sont initialisés de manière aléatoire afin de mieux comprendre ce qui se passe dans ZSL.
L'erreur de l'adversaire a une forme de loi de puissance similaire pour tous les ensembles de données et modèles étudiés, et l'architecture a son importance.
Nous présentons une formulation de la curiosité comme un problème d'apprentissage de représentation visuelle et montrons qu'elle permet d'obtenir de bonnes représentations visuelles dans les agents.
Cet article formule la formation RL basée sur la curiosité comme l'apprentissage d'un modèle de représentation visuelle, en soutenant que se concentrer sur une meilleure LR et maximiser la perte de modèle pour les scènes nouvelles permettra d'obtenir de meilleures performances globales.
À partir d'un balayage RVB-D incomplet d'une scène, nous cherchons à détecter les instances d'objets individuels qui composent la scène et à déduire leur géométrie complète.
Propose une structure CNN 3D de bout en bout qui combine des caractéristiques de couleur et des caractéristiques 3D pour prédire la structure 3D manquante d'une scène à partir de scans RVB-D.
Les auteurs proposent un nouveau réseau convolutif 3D de bout en bout qui prédit l'achèvement des instances sémantiques 3D comme les boîtes de délimitation des objets, les étiquettes de classe et la géométrie complète des objets.
XGAN est un modèle non supervisé pour la traduction d'image à image au niveau des caractéristiques, appliqué à des problèmes de transfert de style sémantique tels que la tâche " visage à dessin animé ", pour laquelle nous introduisons un nouveau jeu de données.
Cet article propose un nouveau modèle basé sur le GAN pour la traduction image à image non appariée, similaire au DTN.
Les travailleurs envoient des signes de gradient au serveur, et la mise à jour est décidée par un vote majoritaire. Nous montrons que cet algorithme est convergent, efficace en termes de communication et tolérant aux pannes, tant en théorie qu'en pratique.
Présente une implémentation distribuée de signSGD avec le vote majoritaire comme agrégation.
Nous corrigeons la variation de la nuisance pour les incorporations d'images dans différents domaines, en ne préservant que les informations pertinentes.
Discute d'une méthode pour ajuster l'intégration d'images afin de séparer la variation technique du signal biologique.
Les auteurs présentent une méthode permettant de supprimer les informations spécifiques au domaine tout en préservant les informations biologiques pertinentes en formant un réseau qui minimise la distance de Wasserstein entre les distrbutions.
Un estimateur d'information mutuelle évolutif en taille d'échantillon et en dimensions.
La nouvelle combinaison de l'apprentissage par renforcement et de l'apprentissage supervisé, qui réduit considérablement le nombre d'échantillons requis pour l'apprentissage sur vidéo.
Cet article propose d'exploiter les données contrôlées étiquetées pour accélérer l'apprentissage par renforcement d'une politique de contrôle.
Apprentissage rapide via la mémoire épisodique vérifié par un cadre biologiquement plausible pour le circuit cortex préfrontal-ganglion basal-hippocampe (PFC-BG)
Dans ce travail, nous mettons en évidence une nouvelle connexion entre l'expressivité des DNN et le théorème de Sharkovsky des systèmes dynamiques, qui nous permet de caractériser les compromis profondeur-largeur des réseaux ReLU. 
Montre comment le pouvoir expressif d'un réseau national dépend de sa profondeur et de sa largeur, ce qui permet de mieux comprendre l'avantage des réseaux profonds pour représenter certaines classes de fonctions.
Les auteurs dérivent des conditions de compromis profondeur-largeur pour savoir quand les réseaux relu sont capables de représenter des fonctions périodiques en utilisant l'analyse des systèmes dynamiques.
Nous étudions l'apprentissage tenant compte de la quantification dans les repéreurs de mots-clés quantifiés à très faible nombre de bits afin de réduire le coût du repérage des mots-clés sur le dispositif.
Cette soumission propose une combinaison de la décomposition de rangs bas et de l'approche de quantification pour compresser les modèles DNN pour le repérage de mots-clés.
Un nouveau cadre de traitement du signal par graphe pour quantifier les effets des perturbations expérimentales dans les données biomédicales d'une seule cellule.
Cet article présente plusieurs méthodes de traitement des résultats expérimentaux sur les cellules biologiques et propose un algorithme MELD mettant en correspondance les affectations de groupes dures avec les affectations douces, permettant de regrouper les groupes de cellules pertinents.
Nous proposons une classe de modèles d'utilisateurs basée sur l'utilisation de processus gaussiens appliqués à un espace transformé défini par des règles de décision.
Nous proposons un algorithme d'optimisation bayésienne optimale pour le réglage des hyperparamètres en exploitant des approximations bon marché.
Étudie l'optimisation des hyperparamètres par l'optimisation bayésienne, en utilisant le cadre du gradient de connaissance et en permettant à l'optimiseur bayésien de régler la fidélité en fonction du coût.
Nous vérifions efficacement la robustesse des modèles neuronaux profonds avec plus de 100 000 ReLU, certifiant plus d'échantillons que l'état de l'art et trouvant plus d'exemples adverses qu'une forte attaque de premier ordre.
Effectue une étude approfondie des approches de programmation linéaire en nombres entiers mixtes pour vérifier la robustesse des réseaux neuronaux aux perturbations adverses et propose trois améliorations aux formulations MILP de la vérification des réseaux neuronaux.
Un ensemble de méthodes permettant d'obtenir une estimation de l'incertitude d'un modèle donné sans avoir à le reconcevoir, à le réentraîner ou à l'affiner.
Décrit plusieurs approches pour mesurer l'incertitude dans des réseaux neuronaux arbitraires lorsqu'il n'y a pas de distorsion pendant la formation.
Proposition d'une opération d'ordre supérieur pour l'apprentissage contextuel
Propose un nouveau bloc convolutif 3D qui convolue l'entrée vidéo avec son contexte, basé sur l'hypothèse que le contexte pertinent est présent autour de l'objet de l'image.
Les modèles basés sur la cohérence pour l'apprentissage semi-supervisé ne convergent pas vers un point unique mais continuent à explorer un ensemble diversifié de solutions plausibles sur le périmètre d'une région plate. La moyenne des poids permet d'améliorer les performances de généralisation.
L'article propose d'appliquer la méthode de la moyenne stochastique des poids au contexte de l'apprentissage semi-supervisé, en soutenant que les modèles semi-supervisés MT/Pi se prêtent particulièrement bien à la méthode SWA et en proposant une méthode SWA rapide pour accélérer la formation.
Nous avons réussi à convertir un détecteur populaire RPN en un tracker performant du point de vue de la fonction de perte.
Une architecture neuronale pour noter et classer les candidats à la réparation de programmes afin d'effectuer la réparation sémantique de programmes de manière statique sans accès aux tests unitaires.
Présente une architecture de réseau neuronal composée des parties partage, spécialisation et compétition pour réparer le code dans quatre cas.
Est-il possible de co-concevoir la précision, la robustesse et l'efficacité des modèles pour atteindre leur triple objectif ? Oui !
Exploite les sorties précoces multiples adaptées à l'entrée pour le domaine de l'attaque et de la défense adverses, réduisant la complexité moyenne de l'inférence sans contredire l'hypothèse d'une plus grande capacité.
Nous montrons que les unités individuelles dans les représentations CNN apprises dans les tâches NLP sont sélectivement sensibles à des concepts spécifiques du langage naturel.
Utilise les unités grammaticales du langage naturel qui préservent les significations pour montrer que les unités des CNN profonds appris dans des tâches de PNL pourraient agir comme un détecteur de concepts en langage naturel.
Il s'agit d'un article essentiellement théorique qui décrit les difficultés à démêler les facteurs de variation, en utilisant des autoencodeurs et des GAN.
Cet article examine le démêlage des facteurs de variation dans les images, montre qu'en général, sans autres hypothèses, on ne peut pas distinguer deux facteurs de variation différents, et propose une nouvelle architecture AE+GAN pour essayer de démêler les facteurs de variation.
Cet article étudie les difficultés de démêler les facteurs de variation indépendants dans le cadre de données faiblement étiquetées et introduit le terme d'ambiguïté de référence pour la mise en correspondance des points de données.
apprendre à classer avec plusieurs encastrements et attentions
Propose d'utiliser l'attention pour combiner des représentations d'entrée multiples pour la requête et les résultats de la recherche dans la tâche d'apprentissage du classement.
Nous avons développé un algorithme qui prend en entrée des enregistrements de l'activité neuronale et renvoie des groupes de neurones par type de cellule et des modèles d'activité neuronale contraints par ces groupes.
Nous supervisons les réseaux neuronaux de graphes pour imiter les sorties intermédiaires et par étapes des algorithmes de graphes classiques, en obtenant des résultats très favorables.
Suggère d'entraîner les réseaux neuronaux à imiter les algorithmes de graphes en apprenant les primitives et les sous-programmes plutôt que le résultat final.
Nous décrivons une architecture permettant de générer diverses hypothèses de buts intermédiaires lors de tâches de manipulation robotique.
Évalue la qualité d'un modèle prédictif génératif proposé pour générer des plans d'exécution de robots.
Cet article propose une méthode d'apprentissage d'une fonction de transition de haut niveau qui est utile pour la planification des tâches.
Cet article propose une nouvelle analyse des algorithmes de gradient adaptatif pour la résolution de problèmes non convexes non concaves de type min-max en tant que GAN, et explique la raison pour laquelle les méthodes de gradient adaptatif sont plus performantes que leurs homologues non adaptatifs par des études empiriques.
Développe des algorithmes pour la résolution d'inégalités variationnelles dans un cadre stochastique, en proposant une variation de la méthode de l'extragradient.
Nous apprenons les trajectoires sohpistiquées d'un objet uniquement à partir de pixels dans un jeu de données de vidéos de jouets en utilisant une structure VAE avec un processus gaussien préalable.
Nous étudions la base neuronale du rappel des rêves à l'aide d'un réseau neuronal convolutif et de techniques de visualisation des caractéristiques, comme le tSNE et la rétropropagation guidée.
Cet article propose une nouvelle formulation et un nouveau protocole de communication pour les problèmes de contrôle multi-agents en réseau.
Concerne les N-MARL où les agents mettent à jour leur politique en se basant uniquement sur les messages des nœuds voisins, montrant que l'introduction d'un facteur d'escompte spatial stabilise l'apprentissage.
Le champ moyen VB utilise deux fois plus de paramètres ; nous lions les paramètres de variance dans le champ moyen VB sans aucune perte dans ELBO, en gagnant de la vitesse et des gradients de variance plus faibles.
Nous utilisons efficacement quelques mots-clés comme supervision faible pour former des réseaux neuronaux pour l'extraction d'aspects.
Discute d'une variante de la distillation des connaissances qui utilise un "enseignant" basé sur un classificateur de type "bag-of-words" avec des mots de départ et un "étudiant" qui est un réseau neuronal basé sur l'intégration.
Les connexions de rétroaction horizontales et descendantes sont responsables des stratégies complémentaires de regroupement perceptif dans les systèmes de vision biologiques et récurrents.
En utilisant les réseaux neuronaux comme modèle computationnel du cerveau, elle examine l'efficacité de différentes stratégies pour résoudre deux défis visuels.
Nous présentons GAN-TTS, un réseau adversarial génératif pour la conversion texte-parole, qui obtient un score d'opinion moyen (MOS) de 4,2.
Résout le problème des GAN dans la synthèse des formes d'onde brutes et commence à combler l'écart de performance existant entre les modèles autorégressifs et les GAN pour les sons bruts.
nous proposons un algorithme d'apprentissage pour élaguer le réseau en appliquant des pénalités de sparsité de structure
Cet article présente une approche de l'élagage lors de la formation d'un réseau en utilisant les pénalités lasso et split LBI.
Nous présentons l'apprentissage continu non supervisé (UCL) et une architecture neuro-inspirée qui résout le problème de l'UCL.
Propose d'utiliser des hiérarchies de modules STAM pour résoudre le problème UCL, en fournissant la preuve que les représentations que les modules apprennent sont bien adaptées à la classification en quelques coups.
Nouvelle méthode d'extraction de signaux dans le domaine de Fourier
Contribue à une version convolutionnelle à valeur complexe de la modulation linéaire à valeur caractéristique qui permet l'optimisation des paramètres et conçoit une perte qui tient compte de la magnitude et de la phase.
Nous présentons un nouveau cadre pour apprendre la représentation démêlée du contenu et du style d'une manière totalement non supervisée. 
Proposer un modèle basé sur un cadre d'autoencodeur pour démêler la représentation d'un objet, les résultats montrent que le modèle peut produire des représentations capturant le contenu et le style.
Nous développons une stratégie d'estimation CATE qui tire parti de certaines des propriétés intrigantes des réseaux neuronaux. 
Montre les améliorations apportées à X-learner en modélisant la fonction de réponse au traitement, la fonction de réponse du contrôle, et le mappage de l'effet de traitement imputé à l'effet de traitement moyen conditionnel, comme des réseaux neuronaux.
Les auteurs proposent le Y-learner pour estimer l'effet de traitement moyen conditionnel (CATE), qui met à jour simultanément les paramètres des fonctions de résultats et l'estimateur CATE.
Exécution du micrologiciel en fonction du dispositif
Une architecture pour les données tabulaires, qui émule les branches des arbres de décision et utilise une connectivité résiduelle dense. 
Cet article propose la forêt neuronale profonde, un algorithme qui cible les données tabulaires et intègre les points forts de l'amélioration du gradient des arbres de décision.
Une nouvelle architecture de réseau neuronal imitant le fonctionnement des forêts de décision pour résoudre le problème général de la formation de modèles profonds pour les données tabulaires et montrer une efficacité comparable à celle du GBDT.
YellowFin est un optimiseur basé sur SGD avec une adaptabilité à la fois du momentum et du taux d'apprentissage.
Propose une méthode pour régler automatiquement le paramètre de momentum dans les méthodes SGD de momentum, qui permet d'obtenir de meilleurs résultats et une vitesse de convergence plus rapide que l'algorithme Adam de pointe.
Attaques adversaires sur l'espace latent des auto-codeurs variationnels pour modifier le sens sémantique des entrées.
Cet article concerne la sécurité et l'apprentissage automatique et propose une attaque de type "man-in-middle" qui modifie le codage VAE des données d'entrée de sorte que la sortie décodée soit mal classée.
Une étude empirique qui examine l'efficacité de différentes combinaisons encodeur-décodeur pour l'analyse syntaxique des dépendances.
Analyse empirique de divers codeurs, décodeurs et de leurs dépendances pour l'analyse syntaxique des dépendances basée sur les graphes.
Un enseignant qui forme des méta-apprenants comme des humains
Nous introduisons une approche d'espace d'intégration pour contraindre la distribution de probabilité de sortie des réseaux neuronaux.
Cet article présente une méthode permettant d'effectuer un apprentissage semi-supervisé avec des réseaux neuronaux profonds. Le modèle atteint une précision relativement élevée, compte tenu d'une petite taille d'apprentissage.
Cet article intègre la distribution des étiquettes dans l'apprentissage des modèles lorsqu'un nombre limité d'instances d'apprentissage est disponible, et propose deux techniques pour traiter le problème de la distribution des étiquettes de sortie qui est faussement biaisée.
Nous introduisons un nouveau type de représentation contextuelle profonde des mots qui améliore de manière significative l'état de l'art pour une série de tâches NLP difficiles.
Ce travail introduit une nouvelle fonction de perte pour l'entraînement robuste de DNN de localisation temporelle en présence d'étiquettes mal alignées.
Une nouvelle perte pour les modèles d'entraînement qui prédisent où les événements se produisent dans une séquence d'entraînement avec des étiquettes bruitées en comparant l'étiquette lissée et la séquence de prédiction.
Nous introduisons la notion de décompositions tensorielles mixtes et l'utilisons pour prouver que l'interconnexion des réseaux convolutifs dilatés augmente leur pouvoir d'expression.
Cet article valide théoriquement que l'interconnexion de réseaux avec des dilatations différentes peut conduire à une efficacité expressive en utilisant la décomposition tensorielle mixte.
Les auteurs étudient les réseaux convolutifs dilatés et montrent que l'entrelacement de deux réseaux convolutifs dilatés A et B à différents stades est plus efficace sur le plan expressif que le fait de ne pas les entrelacer.
Montre que l'hypothèse structurelle du WaveNet, à savoir un seul arbre binaire parfait, nuit à ses performances et que les architectures de type WaveNet présentant des structures arborescentes mixtes plus complexes sont plus performantes.
l'apprentissage multitâche fonctionne 
Cet article présente un réseau neuronal multi-tâches pour la classification sur des ensembles de données de type MNIST.
Nous proposons un nouvel examen de la notion d'exemples adverses, fondé sur des principes et sur l'optimisation, et nous développons des méthodes qui produisent des modèles robustes face à un large éventail d'adversaires.
Étudie une formulation minimax de l'apprentissage des réseaux profonds afin d'accroître leur robustesse, en utilisant la descente de gradient projetée comme principal adversaire. 
Cet article propose d'étudier comment rendre les réseaux neuronaux résistants aux pertes adverses dans le cadre des problèmes de points selle. 
De nombreux ensembles de données de classification de graphes comportent des doublons, ce qui soulève des questions sur les capacités de généralisation et la comparaison équitable des modèles. 
Les auteurs discutent du biais d'isomorphisme dans les ensembles de données de graphes, l'effet de surajustement dans les réseaux d'apprentissage lorsque des caractéristiques d'isomorphisme de graphe sont incorporées dans le modèle, théoriquement analogue aux effets de fuite de données.
Nous introduisons une notion de fonctions de valeur extrapolée de manière conservatrice, qui mène de manière prouvée à des politiques qui peuvent s'autocorriger pour rester proches des états de démonstration, et nous les apprenons avec une nouvelle technique d'échantillonnage négatif.
Un algorithme appelé itération de valeur avec échantillonnage négatif pour résoudre le problème du décalage des covariables dans l'apprentissage par imitation.
Les modèles de monde structuré entraînés de manière contrastive (C-SWM) apprennent des représentations d'état orientées objet et un modèle relationnel d'un environnement à partir d'une entrée de pixels bruts.
Les auteurs surmontent le problème de l'utilisation de pertes basées sur les pixels dans la construction et l'apprentissage de modèles de monde structuré en utilisant un espace latent contrastif.
Méthodes non supervisées pour la recherche, l'analyse et le contrôle des neurones importants en NMT
Ce travail propose de trouver des neurones "significatifs" dans les modèles de traduction automatique neuronale par un classement basé sur la corrélation entre des paires de modèles, différentes époques ou différents ensembles de données, et propose un mécanisme de contrôle pour les modèles.
Nous présentons un softmax doublement clairsemé, le mélange clairsemé d'experts clairsemés, pour améliorer l'efficacité de l'inférence softmax en exploitant la hiérarchie de chevauchement à deux niveaux. 
L'article propose une nouvelle mise en œuvre de l'algorithme Softmax avec deux niveaux hiérarchiques de sparsité qui accélère les opérations de modélisation du langage.
Cet article présente des preuves empiriques soutenant la découverte d'un indicateur de généralisation : l'évolution au cours de la formation de la distance cosinus entre le vecteur de poids de chaque couche et son initialisation.
Les modèles de code source qui combinent des caractéristiques globales et structurelles apprennent des représentations plus puissantes des programmes.
Une nouvelle méthode pour modéliser le code source pour la tâche de réparation des bogues en utilisant un modèle sandwich comme [RNN GNN RNN] qui améliore considérablement la localisation et la précision de la réparation.
Les RNN incrémentiels résolvent le problème de l'explantation et du gradient évanouissant en mettant à jour les vecteurs d'état sur la base de la différence entre l'état précédent et celui prédit par une ODE.
Les auteurs abordent le problème de la propagation du signal dans les réseaux neuronaux récurrents en construisant un système attracteur pour la transition du signal et en vérifiant s'il converge vers un équilibre. 
Nous fournissons des preuves contre les affirmations classiques concernant le compromis biais-variance et proposons une nouvelle décomposition de la variance.
Nous avons proposé un nouveau cadre de classification d'images par apprentissage profond qui peut à la fois classer les images avec précision et protéger la vie privée des utilisateurs.
Cet article propose un cadre qui préserve les informations privées dans l'image et ne compromet pas la facilité d'utilisation de l'image.
Ces travaux actuels proposent d'utiliser des réseaux adversaires pour obscurcir les images et permettre ainsi de les collecter sans souci de confidentialité afin de les utiliser pour l'entraînement de modèles d'apprentissage automatique.
un modèle 2vec pour les graphes de transactions de crypto-monnaies
L'article propose d'utiliser un autoencodeur, networkX et node2Vec pour prédire si une adresse Bitcoin deviendra vide après un an, mais les résultats sont moins bons que ceux d'une base de référence existante.
Preuve de convergence de la méthode des sous-gradients stochastiques et variations sur des problèmes minimax convexes-concaves
Une analyse du sous-gradient stochastique simultané, du gradient simultané avec optimisme et du gradient simultané avec ancrage dans le contexte des jeux concaves convexes minmax.
Cet article analyse la dynamique de la descente de gradient stochastique lorsqu'elle est appliquée aux jeux convexes-concaves, ainsi que la GD avec optimisme et un nouvel algorithme de GD ancré qui converge sous des hypothèses plus faibles que la SGD ou la SGD avec optimisme.
Nous proposons un cadre algorithmique pour programmer des constellations de petits engins spatiaux avec des capacités de réorientation 3-DOF, mis en réseau avec des liaisons inter-satellites.
Cet article propose un module de communication pour optimiser le calendrier de communication pour le problème des constellations d'engins spatiaux, et compare l'algorithme dans des configurations distribuées et centralisées.
Nous avons proposé un nouvel algorithme d'échantillonnage par importance compressé à noyau.
Nous étudions la structure de la régression ridge dans un cadre asymptotique à haute dimension, et nous obtenons des informations sur la validation croisée et l'esquisse.
Une étude théorique de la régression ridge en exploitant une nouvelle caractérisation asymptotique de l'estimateur de la régression ridge.
Nous analysons le paysage des pertes des réseaux neuronaux avec attention et expliquons pourquoi l'attention est utile à la formation des réseaux neuronaux pour obtenir de bonnes performances.
Cet article prouve, d'un point de vue théorique, que les réseaux d'attention peuvent généraliser mieux que les bases de non-attention pour l'attention fixe (monocouche et multicouche) et l'auto-attention dans le cadre monocouche.
Modèle de mélange pour le désenchevêtrement neuronal
Nous avons développé des estimations robustes de l'information mutuelle pour les DNN et les avons utilisées pour observer la compression dans les réseaux avec des fonctions d'activation non saturantes.
Cet article étudie la croyance populaire selon laquelle les réseaux neuronaux profonds font de la compression d'information pour les tâches supervisées.
Cet article propose une méthode d'estimation de l'information mutuelle pour les réseaux avec des fonctions d'activation non bornées et l'utilisation de la régularisation L2 pour induire plus de compression.
Nous présentons le TimbreTron, un pipeline permettant de réaliser un transfert de timbre de haute qualité sur des formes d'onde musicales en utilisant le transfert de style du domaine CQT.
Une méthode pour convertir des enregistrements d'un instrument de musique spécifique en un autre en appliquant CycleGAN, développé pour le transfert de style d'image, au transfert de spectrogrammes.
Les auteurs utilisent plusieurs techniques/outils pour permettre le transfert neuronal du timbre (conversion de la musique d'un instrument à un autre) sans exemples d'entraînement appariés. 
Décrit un modèle de transfert de timbre musical. Les résultats indiquent que le système proposé est efficace pour le transfert de la hauteur et du tempo, ainsi que pour l'adaptation du timbre.
Cet article présente le recâblage profond, un algorithme qui peut être utilisé pour former des réseaux neuronaux profonds lorsque la connectivité du réseau est fortement limitée pendant la formation.
Une approche pour mettre en œuvre l'apprentissage profond directement sur des graphes peu connectés, permettant aux réseaux d'être formés efficacement en ligne et pour un apprentissage rapide et flexible.
Les auteurs proposent un algorithme simple capable de s'entraîner avec une mémoire limitée.
Les méthodes d'élagage existantes échouent lorsqu'elles sont appliquées à des GAN s'attaquant à des tâches complexes. Nous présentons donc une méthode simple et robuste d'élagage des générateurs qui fonctionne bien pour une grande variété de réseaux et de tâches.
Les auteurs proposent une modification de la méthode classique de distillation pour la tâche de compression d'un réseau afin de remédier à l'échec des solutions précédentes lorsqu'elles sont appliquées aux réseaux adversariaux génératifs.
nous constatons que 99,9 % de l'échange de gradient dans le SGD distribué est redondant ; nous réduisons la bande passante de communication de deux ordres de grandeur sans perdre en précision. 
Cet article propose une amélioration supplémentaire par rapport à l'élimination du gradient pour améliorer l'efficacité de la communication.
Nous proposons le réseau Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) qui conditionne le processus de traduction à une image exemplaire dans le domaine cible.
Discute d'un défaut fondamental et du besoin de modèles de traduction I2I.
L'article explore l'idée qu'une image a deux composantes et applique un modèle d'attention où les masques de caractéristiques qui dirigent le processus de traduction ne nécessitent pas d'étiquettes sémantiques.
Imposer une structure de graphe aux couches de réseaux neuronaux pour améliorer l'interprétabilité visuelle.
Un nouveau régularisateur pour imposer une structure graphique aux couches cachées d'un réseau neuronal afin d'améliorer l'interprétabilité des représentations cachées.
Met en évidence la contribution du régularisateur spectral de graphe à l'interprétabilité des réseaux neuronaux.
Nous montrons que les modèles basés sur l'énergie, lorsqu'ils sont entraînés sur le résidu d'un modèle de langage auto-régressif, peuvent être utilisés de manière efficace et efficiente pour générer du texte. 
Une proposition de modèle basé sur l'énergie résiduelle (EBM) pour la génération de texte qui opère au niveau de la phrase, et peut donc tirer parti de BERT, et atteint une perplexité plus faible et est préféré par l'évaluation humaine.
étude systématique des modèles de reconnaissance d'images à grande échelle basés sur le cache, en se concentrant particulièrement sur leurs propriétés de robustesse
Cet article propose d'utiliser une mémoire cache pour améliorer la robustesse face à des exemples d'images adverses, et conclut que l'utilisation d'une grande mémoire cache continue n'est pas supérieure à l'attention soutenue.
L'article décrit un cadre flexible pour construire des CNN qui sont équivoques pour une grande classe de groupes de transformations.
Un cadre pour la construction de CNN de groupe avec un groupe de Lie arbitraire G, qui montre une supériorité sur un CNN dans la classification des tumeurs et la localisation des points de repère. 
Une analyse comparative de neuf systèmes de mise en commun mondiaux représentatifs révèle des résultats intéressants.
Pour les tâches de classification à grain fin, cet article a validé que le maxpooling encouragerait des cartes de caractéristiques plus éparses que le avgpooling et le surpasserait. 
L'autosupervision permet d'améliorer la reconnaissance de quelques clichés sur des ensembles de données de petite taille et difficiles sans recourir à des données supplémentaires. Les données supplémentaires ne sont utiles que si elles proviennent du même domaine ou d'un domaine similaire.
Une étude empirique de différentes méthodes d'apprentissage auto-supervisé (SSL), montrant que le SSL est plus utile lorsque l'ensemble de données est plus difficile, que le domaine est important pour la formation, et une méthode pour choisir des échantillons à partir d'un ensemble de données non étiquetées. 
Nous créons des modèles abstraits d'environnements à partir de notre expérience et les utilisons pour apprendre de nouvelles tâches plus rapidement.
Une méthodologie qui utilise l'idée des homomorphismes MDP pour transformer un MDP complexe avec un espace d'état continu en un MDP plus simple.
Nous étendons la dissection de réseau pour inclure l'interprétation des actions et examiner les chemins des caractéristiques interprétables pour comprendre la hiérarchie conceptuelle utilisée pour classer une action.
Nous proposons un nouveau modèle pour représenter les notes et leurs propriétés, qui peut améliorer la génération automatique de mélodies.
Cet article propose un modèle génératif de mélodie symbolique (MIDI) dans la musique populaire occidentale qui encode conjointement les symboles de notes avec des informations de temps et de durée pour former des "mots" musicaux.
L'article propose de faciliter la génération de mélodies en représentant les notes comme des "mots", représentant toutes les propriétés de la note et permettant ainsi la génération de "phrases" musicales.
Une méthode qui fait croître automatiquement les couches des réseaux neuronaux pour découvrir la profondeur optimale.
Un cadre permettant d'entrelacer la formation d'un réseau moins profond et l'ajout de nouvelles couches, qui donne un aperçu du paradigme des "réseaux en croissance".
Exploration de l'apprentissage de la représentation dans le domaine pour les ensembles de données de télédétection.
Cet article a fourni plusieurs ensembles de données de télédétection standardisés et a montré que la représentation dans le domaine pouvait produire de meilleurs résultats de base pour la télédétection par rapport à un réglage fin sur ImageNet ou à un apprentissage à partir de zéro.
Évitez de générer des réponses un mot à la fois en utilisant une supervision faible pour entraîner un classificateur à choisir une réponse complète.
Une façon de générer des réponses pour un dialogue médical en utilisant un classificateur pour choisir parmi des réponses élaborées par des experts en fonction du contexte de la conversation.
CNNs entraînés SGD à largeur finie contre CNNs entièrement bayésiens à largeur infinie. Qui gagne ?
L'article établit un lien entre le réseau neuronal convolutif bayésien à canaux infinis et les processus gaussiens.
Nous transposons l'inférence bayésienne à la classification ImageNet et obtenons des résultats compétitifs en termes de précision et de calibrage de l'incertitude.
Un algorithme MCMC à bruit adaptatif pour la classification des images qui ajuste dynamiquement le momentum et le bruit appliqués à chaque mise à jour des paramètres, et qui est robuste à l'ajustement excessif et fournit une mesure d'incertitude avec les prédictions. 
Une étude empirique sur les fausses images révèle que la texture est un indice important qui différencie les fausses images des vraies. Notre modèle amélioré capturant les statistiques globales de texture montre une meilleure performance de détection d'images fausses entre les GAN.
L'article propose un moyen d'améliorer les performances du modèle de détection des faux visages dans les images générées par un GAN afin qu'il soit plus généralisable en fonction des informations sur la texture.
La distance de Wasserstein est difficile à minimiser avec la descente de gradient stochastique, alors que la distance de Cramer peut être optimisée facilement et fonctionne tout aussi bien.
Le manuscrit propose d'utiliser la distance de Cramer pour agir comme une perte lors de l'optimisation d'une fonction objective en utilisant la descente de gradient stochastique car elle a des gradients d'échantillon non biaisés.
La contribution de l'article est liée aux critères de performance, en particulier à la métrique de Wasserstein/Mallows.
Nous apprenons la flèche du temps pour les MDP et l'utilisons pour mesurer l'accessibilité, détecter les effets secondaires et obtenir un signal de récompense de la curiosité. 
Ce travail propose le potentiel h comme solution à un objectif qui mesure l'asymétrie état-transition dans un MDP.
Nous avons formulé la SGD comme un problème de filtrage bayésien, et nous montrons que cela donne lieu à RMSprop, Adam, AdamW, NAG et d'autres caractéristiques des méthodes adaptatives de pointe.
L'article analyse la descente de gradient stochastique par le biais du filtrage bayésien comme cadre d'analyse des méthodes adaptatives.
Les auteurs tentent d'unifier les méthodes de gradient adaptatif existantes dans le cadre du filtrage bayésien avec le préalable dynamique.
Nous introduisons l'idée de l'apprentissage contradictoire dans l'augmentation automatique des données pour améliorer la généralisation d'un réseau de targe.
Une technique appelée Adversarial AutoAugment qui apprend dynamiquement de bonnes politiques d'augmentation des données pendant la formation en utilisant une approche contradictoire.
L'étude introduit deux approches pour améliorer la généralisation du méta-apprentissage de premier ordre et présente une évaluation empirique sur la classification d'images de quelques images.
L'article présente une étude empirique de l'algorithme Reptile de méta-apprentissage du premier ordre, en examinant une technique de régularisation proposée et des réseaux plus profonds.
Cet article propose d'utiliser la factorisation matricielle au moment de la formation pour la traduction automatique neuronale, ce qui permet de réduire la taille du modèle et de diminuer le temps de formation sans nuire aux performances.
Cet article propose de compresser les modèles en utilisant la factorisation matricielle pendant la formation des réseaux neuronaux profonds de traduction automatique.
Différentes méthodes d'analyse des ORET suggèrent des conclusions différentes (mais compatibles) dans une étude de cas sur les NPI.
Dans ce travail, nous présentons V1Net -- un nouveau réseau neuronal récurrent modélisant les connexions horizontales corticales qui donnent lieu à des représentations visuelles robustes par regroupement perceptuel.
Les auteurs proposent de modifier une variante convolutive de LSTM pour inclure des connexions horizontales inspirées des interactions connues dans le cortex visuel.
Nous proposons un lien entre l'équivariance des permutations et la généralisation compositionnelle, et fournissons des modèles de langage équivariants.
Ce travail se concentre sur l'apprentissage de représentations et de fonctions localement équivariantes sur les mots d'entrée/sortie pour les besoins de la tâche SCAN.
L'article propose un algorithme pour augmenter la flexibilité du postérieur variationnel dans les réseaux neuronaux bayésiens par le biais d'une optimisation itérative.
Procédé d'apprentissage de distributions postérieures variationnelles flexibles, appliqué aux réseaux neuronaux bayésiens pour effectuer une inférence de variation (VI) sur les poids.
Nouveau cadre de pointe pour la restauration d'images
L'article propose une architecture de réseau neuronal convolutif qui comprend des blocs pour les mécanismes d'attention locale et non locale, qui sont censés être responsables de l'obtention d'excellents résultats dans quatre applications de restauration d'images.
Cet article propose un réseau d'attention non local résiduel pour la restauration d'images.
Approche hybride de l'acquisition de modèles qui compense le manque de données disponibles par des connaissances spécifiques au domaine fournies par des experts.
Une approche d'acquisition de domaine qui envisage d'utiliser une représentation différente pour le modèle de domaine partiel en utilisant des relations mutex schématiques à la place des conditions pré/post.
Nous publions un ensemble de données construit à partir des données ECG à une seule dérivation de 11 000 patients à qui l'on a prescrit l'utilisation du dispositif {DEVICENAME}(TM).
Cet article décrit un ensemble de données ECG à grande échelle que les auteurs ont l'intention de publier et propose une analyse et une visualisation non supervisées de cet ensemble de données.
Une nouvelle convolution contextuelle qui incorpore des informations contextuelles globales dans les CNN en modulant explicitement les noyaux de convolution, et capture ainsi des modèles locaux plus représentatifs et extrait des caractéristiques discriminantes.
Cet article utilise le contexte global pour moduler les poids des couches convolutives et aider les CNN à capturer des caractéristiques plus discriminantes avec des performances élevées et moins de paramètres que la modulation des cartes de caractéristiques.
Nous analysons le compromis entre le bruit de quantification et la distorsion d'écrêtage dans les réseaux de faible précision, et montrons des améliorations notables par rapport aux schémas de quantification standard qui évitent normalement l'écrêtage.
Déduit une formule pour trouver les valeurs d'écrêtage minimales et maximales pour une quantification uniforme qui minimisent l'erreur quadratique résultant de la quantification, pour une distribution de Laplace ou gaussienne sur la valeur préquantifiée.
Nous proposons une nouvelle méthode de normalisation pour traiter les cas de petites tailles de lots.
Une méthode pour traiter le problème de la petite taille des lots de BN qui applique l'opération de moyenne mobile sans trop de surcharge et réduit le nombre de statistiques de BN pour une meilleure stabilité.
Preuve de séparation de profondeur ReLU MLP avec des arguments géométriques
Une preuve que les réseaux plus profonds nécessitent moins d'unités que les réseaux moins profonds pour une famille de problèmes. 
Un nouvel algorithme d'apprentissage en quelques coups basé sur le GAN par la synthèse d'éléments divers et discriminants.
Une méthode de méta-apprentissage qui apprend un modèle génératif qui peut augmenter l'ensemble de soutien d'un apprenant à quelques coups qui optimise une combinaison de pertes.
Nous démontrons comment la structure des ensembles de données a un impact sur les réseaux neuronaux et nous introduisons un modèle génératif pour les ensembles de données synthétiques qui reproduit cet impact.
L'article étudie comment différents paramètres de la structure des données affectent l'apprentissage des réseaux neuronaux et comment imiter le comportement sur des ensembles de données réels lors de l'apprentissage sur un ensemble synthétique.
Nous formons des réseaux neuronaux profonds basés sur des matrices diagonales et circulantes, et montrons que ce type de réseaux est à la fois compact et précis dans les applications du monde réel.
Les auteurs fournissent une analyse théorique du pouvoir expressif des réseaux de neurones circulants diagonaux (DCNN) et proposent un schéma d'initialisation pour les DCNN profonds.
Nous proposons de tirer parti de la distillation de modèles pour apprendre des explications additives globales sous la forme de formes de caractéristiques (qui sont plus expressives que les attributions de caractéristiques) pour des modèles tels que les réseaux neuronaux formés sur des données tabulaires.
Cet article intègre des modèles additifs généralisés (GAM) avec la distillation de modèles pour fournir des explications globales des réseaux neuronaux.
Un cadre d'apprentissage multi-tâches à grande échelle avec divers objectifs de formation pour apprendre des représentations de phrases de longueur fixe.
Cet article traite de l'apprentissage d'enchâssements de phrases en combinant plusieurs signaux d'entraînement : saut de pensée, prédiction de la traduction, classification des relations d'implication et prédiction de l'analyse syntaxique des constituants.
Nous proposons un annotateur de biais neuronal pour évaluer la robustesse des modèles face à des ensembles de données textuelles biaisées.
Une méthode pour générer des ensembles de données biaisées pour le langage naturel, en s'appuyant sur un auto-codeur conditionnel régularisé de façon adversariale (CARA).
Nous proposons de superviser les modèles de sujets de type VAE en ajustant intelligemment l'antériorité sur une base par document. Nous trouvons qu'un postérieur logit-normal fournit la meilleure performance.
Une méthode flexible de supervision faible d'un modèle thématique pour obtenir un meilleur alignement avec l'intuition de l'utilisateur.
Première analyse complète du plan d'information des réseaux neuronaux profonds à grande échelle utilisant l'entropie basée sur la matrice et les noyaux tenseurs.
Les auteurs proposent un estimateur basé sur le noyau tensoriel pour l'estimation de l'information mutuelle entre les couches à haute dimension d'un réseau neuronal.
Nous proposons un cadre modulaire capable d'accomplir des tâches spécifiées par des programmes et de réaliser une généralisation à des tâches plus complexes.
Cet article étudie la formation d'agents RL avec des instructions et des décompositions de tâches formalisées sous forme de programmes, en proposant un modèle d'agent guidé par programme qui interprète un programme et propose des sous-buts à un module d'action.
Nous prouvons que la descente de gradient initialisée aléatoirement (stochastique) apprend un filtre convolutif en temps polynomial.
Etudie le problème de l'apprentissage d'un seul filtre convolutif en utilisant SGD et montre que sous certaines conditions, SGD apprend un seul filtre convolutif.
Cet article étend l'hypothèse de la distribution gaussienne à une hypothèse plus générale de lissage angulaire, qui couvre une plus grande famille de distributions d'entrée.
La première méthode d'augmentation des données spécialement conçue pour améliorer la robustesse générale des DNN sans aucune hypothèse sur les algorithmes d'attaque.
Propose une méthode d'apprentissage par augmentation des données afin d'accroître la robustesse du modèle contre les perturbations adverses, en augmentant les échantillons uniformément aléatoires d'une sphère à rayon fixe centrée sur les données d'apprentissage. 
Utilisation des GAN de Wasserstein pour générer une activité neuronale réaliste et pour détecter les caractéristiques les plus pertinentes présentes dans les modèles de population neuronale.
Une méthode pour simuler des trains de pointes provenant de populations de neurones qui correspondent à des données empiriques en utilisant un GAN semi-convolutionnel.
Le document propose d'utiliser les GAN pour synthétiser des modèles d'activité neuronale réalistes.
Les estimateurs de gradient doublement reparamétrés permettent une réduction non biaisée de la variance qui conduit à une amélioration des performances.
L'auteur a constaté expérimentalement que l'estimateur du travail existant (STL) est biaisé et propose de réduire le biais pour améliorer l'estimateur du gradient de l'ELBO.
Gradientless Descent est un algorithme sans gradient dont l'efficacité est prouvée, qui est monotone-invariant et rapide pour l'optimisation d'ordre zéro en haute dimension.
Cet article propose des algorithmes stables de descente sans gradient (GLD) qui ne reposent pas sur l'estimation du gradient.
Nous proposons une nouvelle classe de modèles génératifs visuels : les prédicteurs conditionnés par le but. Nous montrons expérimentalement que le conditionnement sur le but permet de réduire l'incertitude et de produire des prédictions sur des horizons beaucoup plus longs.
Cet article reformule le problème de la prédiction vidéo en interpolation au lieu d'extrapolation en conditionnant la prédiction sur l'image de début et de fin (but), ce qui permet d'obtenir des prédictions de meilleure qualité.
Nous proposons un cadre profond d'apprentissage multi-instance basé sur des réseaux neuronaux récurrents qui utilise des fonctions de mise en commun et des mécanismes d'attention pour les tâches d'annotation de concepts.
L'article traite de la classification des séries chronologiques de données médicales et propose de modéliser la relation temporelle entre les instances de chaque série à l'aide d'une architecture de réseau neuronal récurrent. 
Propose une nouvelle formulation d'apprentissage à instances multiples (MIL) appelée Relation MIL (RMIL), et discute d'un certain nombre de ses variantes avec LSTM, Bi-LSTM, S2S, etc. et explore l'intégration de RMIL avec divers mécanismes d'attention, et démontre son utilisation sur la prédiction de concepts médicaux à partir de données de séries temporelles. 
Les couches d'intégration sont factorisées à l'aide de la décomposition Tensor Train afin de réduire leur empreinte mémoire.
Cet article propose un modèle de décomposition tensorielle à faible rang pour paramétrer la matrice d'intégration dans le traitement du langage naturel (NLP), ce qui permet de compresser le réseau et parfois d'augmenter la précision des tests.
Régularisation de la décroissance du poids dans les méthodes de gradient adaptatif telles que Adam
Propose une idée pour découpler la décroissance du poids du nombre d'étapes du processus d'optimisation.
Le document présente une autre façon de mettre en œuvre la décroissance du poids dans Adam avec des résultats empiriques montrés
Examine les problèmes de décroissance de poids rencontrés dans les variantes de SGD et propose une méthode de découplage entre la décroissance de poids et la mise à jour basée sur le gradient.
Apprentissage distributionnel permanent grâce à une architecture élève-enseignant couplée à un régularisateur postérieur à modèle croisé.
Autoencodeurs profonds pour apprendre une bonne représentation des données géométriques des nuages de points en 3D ; modèles génératifs pour les nuages de points.
Approches d'apprentissage de modèles génératifs de type GAN utilisant l'architecture PointNet et le GAN à espace latent.
Nous proposons une nouvelle méthode pour supprimer la vulnérabilité de l'espace des caractéristiques latentes afin d'obtenir des réseaux robustes et compacts.
Cet article propose une méthode d'"élagage neuronal contradictoire" consistant à former un masque d'élagage et une nouvelle perte de suppression de vulnérabilité pour améliorer la précision et la robustesse contradictoire.
Nous avons proposé deux modifications de VAE qui tiennent compte des exemples de données négatives, et nous les avons utilisées pour la détection semi-supervisée des anomalies.
Les articles proposent deux méthodes de type VAE pour la détection semi-supervisée de la nouveauté, MML-VAE et DP-VAE.
Une nouvelle compréhension de la dynamique de l'apprentissage et des mesures de la difficulté de mémorisation permettent un apprentissage efficace et prouvable des programmes.
Cet article formule le DIH comme un problème d'apprentissage de programme qui peut utiliser plus efficacement les données pour entraîner les DNN, et dérive la théorie sur la limite d'approximation.
Historique des développements parallèles des lois de mise à jour et des concepts entre le contrôle adaptatif et l'optimisation dans l'apprentissage machine.
Convolution récurrente pour la compression de modèles et une astuce pour l'entraîner, c'est-à-dire apprendre des couches BN indépendantes par étapes.
L'auteur modifie le réseau neuronal à convolution récurrent (RCNN) avec une normalisation indépendante des lots. Les résultats expérimentaux du RCNN sont compatibles avec l'architecture du réseau neuronal ResNet lorsque celui-ci contient le même nombre de couches.
Les champs réceptifs dynamiques à structure spatiale gaussienne sont précis et efficaces.
Cet article propose un opérateur de convolution structuré pour modéliser les déformations des régions locales d'une image, ce qui réduit considérablement le nombre de paramètres.
Une astuce sur les échantillons adversaires pour que les étiquettes mal classées soient imperceptibles dans l'espace des étiquettes pour les observateurs humains.
Procédé pour construire des attaques adverses qui sont moins détectables par les humains sans coût dans l'espace image en changeant la classe cible pour qu'elle soit similaire à la classe originale de l'image.
Cet article présente la classification par type/position de divers bruits d'impact générés dans un bâtiment, ce qui constitue un grave problème de conflit dans les complexes d'appartements.
Ce travail décrit l'utilisation de réseaux neuronaux convolutifs dans un nouveau domaine d'application de la classification du type et de la position du bruit des bâtiments. 
Les réseaux neuronaux récurrents apprennent à augmenter et à réduire la dimensionnalité de leur représentation interne d'une manière adaptée à la tâche, en fonction de la dynamique du réseau initial.
Au lieu des alignements stricts de distribution dans les objectifs traditionnels d'adaptation aux domaines profonds, qui échouent lorsque la distribution des étiquettes de la cible change, nous proposons d'optimiser un objectif relaxé avec une nouvelle analyse, de nouveaux algorithmes et une validation expérimentale.
Cet article propose des métriques relaxées pour l'adaptation au domaine qui donnent de nouvelles limites théoriques sur l'erreur cible.
nous explorons la tâche de génération de résumés d'articles et proposons un schéma de génération hiérarchique ainsi qu'un cadre d'apprentissage par renforcement conjoint de bout en bout pour entraîner le modèle hiérarchique.
Pour résoudre le problème de la dégénérescence dans la génération de résumés d'articles, cet article propose une approche de génération hiérarchique qui génère d'abord une ébauche intermédiaire de l'article, puis l'article complet.
Nous proposons une régularisation contrefactuelle pour se prémunir contre les changements de domaine adverses résultant de changements dans la distribution des "caractéristiques de style" latentes des images.
L'article examine les moyens de se prémunir contre les changements de domaine adverses avec la régularisation contrefactuelle en apprenant un classificateur qui est invariant aux changements superficiels (ou caractéristiques de "style") dans les images.
Cet article vise une classification robuste des images contre les changements de domaine adverses et l'objectif est atteint en évitant d'utiliser les caractéristiques de style changeant.
Nous proposons un méta-apprentissage qui permet de s'adapter rapidement à plusieurs tâches, même en une seule étape, dans un contexte de "few-shot".
Cet article propose une méthode de méta-apprentissage d'un module de correction du gradient dans lequel le préconditionnement est paramétré par un réseau neuronal, et intègre un processus de mise à jour du gradient en deux étapes pendant l'adaptation. 
Les modèles de réponse aux questions qui modélisent la distribution conjointe des questions et des réponses peuvent apprendre davantage que les modèles discriminants.
Cet article propose une approche générative de l'AQ textuelle et visuelle, où une distribution conjointe sur l'espace des questions et des réponses compte tenu du contexte est apprise, ce qui permet de saisir des relations plus complexes.
Cet article présente un modèle génératif pour la réponse aux questions et propose de modéliser p(q,a|c), factorisé comme p(a|c) * p(q|a,c). 
Les auteurs proposent un modèle d'AQ génératif, qui optimise conjointement la distribution des questions et des réponses dans un document/contexte donné. 
Une nouvelle fonction d'activation appelée unité linéaire de redressement déplacée est proposée. Elle permet d'améliorer les performances de formation et d'inférence des réseaux neuronaux convolutifs normalisés par lots.
Le document compare et suggère de ne pas utiliser la normalisation par lot après avoir utilisé des unités linéaires de redressement.
Cet article propose une fonction d'activation, appelée ReLU déplacée, pour améliorer les performances des CNN qui utilisent la normalisation par lots.
Nous construisons des réseaux neuronaux convolutifs équivariants à l'échelle dans la forme la plus générale avec une efficacité de calcul et une robustesse aux déformations prouvée.
Les auteurs proposent une architecture CNN qui est théoriquement équivariante aux mises à l'échelle et translations isotropes en ajoutant une dimension d'échelle supplémentaire aux tenseurs d'activation.
Nous diagnostiquons des réseaux neuronaux profonds pour le traitement des nuages de points 3D afin d'explorer l'utilité de différentes architectures de réseau. 
L'article étudie différentes architectures de réseaux neuronaux pour le traitement des nuages de points en 3D et propose des mesures de la robustesse aux adversaires, de la robustesse à la rotation et de la cohérence du voisinage.
L'utilisation de la structure des distributions améliore l'inférence variationnelle semi-implicite
Apprentissage par auto-imitation de diverses trajectoires avec politique conditionnée par la trajectoire
Cet article aborde les tâches d'exploration difficiles en appliquant l'auto-imitation à une sélection variée de trajectoires issues de l'expérience passée, afin de conduire une exploration plus efficace dans les problèmes à récompense éparse, en obtenant des résultats SOTA.
Une méthode qui entraîne des réseaux neuronaux de grande capacité avec une précision nettement améliorée et un coût de calcul dynamique plus faible
Une méthode d'entraînement d'un réseau à grande capacité, dont seules certaines parties sont utilisées au moment de l'inférence en fonction de l'entrée, en utilisant une sélection conditionnelle à grain fin et une nouvelle méthode de régularisation, le "batch shaping".
Nous présentons une architecture différenciable de bout en bout qui apprend à faire correspondre des pixels à des prédicats, et nous l'évaluons sur une série de tâches simples de raisonnement relationnel.
Une architecture de réseau basée sur le module d'auto-attention à têtes multiples pour apprendre une nouvelle forme de représentations relationnelles, qui améliore l'efficacité des données et la capacité de généralisation de l'apprentissage des programmes.
Nous utilisons les réseaux neuronaux pour projeter les informations superficielles vers l'extérieur pour l'inférence en langage naturel en définissant et en identifiant les informations superficielles du point de vue de la logique du premier ordre.
Cet article tente de réduire les informations superficielles dans l'inférence en langage naturel afin d'éviter les surajustements, et introduit un réseau neuronal graphique pour modéliser la relation entre les prémisses et les hypothèses. 
Une approche pour traiter l'inférence en langage naturel à l'aide de la logique du premier ordre et pour infuser les modèles NLI avec des informations logiques afin d'être plus robuste à l'inférence.
Algorithme pour l'apprentissage d'un classificateur équitable individuel utilisant la robustesse adversariale
Cet article propose une nouvelle définition de l'équité algorithmique et un algorithme permettant de trouver de manière prouvée un modèle ML qui satisfait à la contrainte d'équité.
L'ensemencement et l'augmentation sont-ils tout ce dont vous avez besoin pour classifier les chiffres dans n'importe quelle langue ?
Cet article présente de nouveaux ensembles de données pour cinq langues et propose un nouveau cadre (SAT) pour la génération d'ensembles de données d'images de polices pour la classification universelle des chiffres.
Le succès de MAML repose sur la réutilisation des caractéristiques de la méta-initialisation, qui permet également une simplification naturelle de l'algorithme, avec la suppression de la boucle interne pour le corps du réseau, ainsi que d'autres informations sur la tête et le corps.
L'article constate que la réutilisation des caractéristiques est le facteur dominant du succès de MAML, et propose de nouveaux algorithmes qui dépensent beaucoup moins de calcul que MAML.
Nous proposons un algorithme indépendant de la méthode pour décider quand il faut procéder à un entraînement incrémentiel plutôt qu'à un entraînement complet. Cet algorithme permet d'accélérer considérablement l'entraînement complet et d'éviter les oublis catastrophiques.
Cet article propose une approche permettant de décider s'il est préférable de recycler un modèle de manière incrémentielle ou complète dans le cadre du développement itératif d'un modèle pour des tâches de remplissage de créneaux.
Nous développons un cadre théorique pour caractériser les tâches de raisonnement qu'un réseau neuronal peut bien apprendre.
L'article propose une mesure des classes d'alignement algorithmique qui permet de déterminer dans quelle mesure les réseaux neuronaux sont "proches" des algorithmes connus, prouvant ainsi le lien entre plusieurs classes d'algorithmes connus et les architectures de réseaux neuronaux.
Nous explorons les interactions cellule-cellule à travers les contextes de l'environnement tumoral observés dans des images hautement multiplexées, par synthèse d'image en utilisant une nouvelle architecture GAN d'attention.
Une nouvelle méthode pour modéliser les données générées par l'imagerie multiplexée par faisceau d'ions en temps de vol (MIBI-TOF) en apprenant la correspondance multiple entre les types de cellules et les niveaux d'expression des marqueurs de protéines.
Une approche en deux étapes, consistant en une sélection de phrases suivie d'une sélection d'empan, peut être rendue plus robuste aux attaques adverses par rapport à un modèle à une étape entraîné sur le contexte complet.
Cet article examine un modèle existant et constate qu'une méthode d'AQ formée en deux étapes n'est pas plus robuste aux attaques adverses que les autres méthodes.
Vérification d'un modèle de conducteur humain basé sur une architecture cognitive et synthèse d'un ADAS correct-par-construction à partir de celui-ci.
Une nouvelle approche hybride d'apprentissage profond fournit la meilleure solution à un problème de données limitées (qui est important pour la conservation de la langue hawaïenne).
Nous étudions quantitativement la détection de la non-distribution dans un cadre de peu de tirs, établissons des résultats de base avec ProtoNet, MAML, ABML, et les améliorons.
L'article propose deux nouveaux scores de confiance qui conviennent mieux à la détection de la non-distribution dans la classification de quelques clichés et montre qu'une approche basée sur la métrique de la distance améliore les performances.
Cet article présente la distillation progressive des connaissances pour l'apprentissage de modèles génératifs orientés vers les tâches de reconnaissance.
Cet article fait la démonstration d'un apprentissage de programme facile à difficile pour entraîner un modèle génératif afin d'améliorer la classification de quelques clichés.
Nous proposons une nouvelle méthode pour améliorer la transférabilité des exemples contradictoires en utilisant le gradient réduit par le bruit.
Cet article postule qu'une perturbation adverse est constituée d'une composante spécifique au modèle et d'une composante spécifique aux données, et que l'amplification de cette dernière est la mieux adaptée aux attaques adverses.
Cet article se concentre sur l'amélioration de la transférabilité des exemples contradictoires d'un modèle à un autre.
Nous présentons le flux de décomposition CP itératif à deux passages pour accélérer efficacement les réseaux neuronaux convolutifs (CNN) existants.
L'article propose un nouveau flux de travail pour l'accélération et la compression des CNN et propose également un moyen de déterminer le rang cible de chaque couche compte tenu de l'accélération globale cible. 
Cet article aborde le problème de l'apprentissage d'une opération de filtrage tensoriel à faible rang pour les couches de filtrage dans les réseaux neuronaux profonds (DNN). 
Limites supérieures de la constante de Lipschitz des réseaux neuronaux basées sur la méthode LP
Les auteurs étudient le problème de l'estimation de la constante de Lipschitz d'un réseau neuronal profond avec fonction d'activation ELO, en le formulant comme un problème d'optimisation polynomiale.
Nous abordons la classification multi-domaines en construisant plusieurs modèles pour représenter cette distribution complexe de tâches de manière collective et en simplifiant l'adaptation spécifique à la tâche comme un problème de sélection parmi ces modèles pré-entraînés.
Cet article s'attaque à la classification en quelques coups avec de nombreux domaines différents en construisant un ensemble de modèles d'intégration pour capturer les caractéristiques invariantes et spécifiques au domaine sans augmentation significative du nombre de paramètres.
Suppression neuronale des artefacts d'encre des documents (soulignements, taches, etc.) sans données d'entraînement annotées manuellement.
Nous proposons une attaque boîte noire efficace en termes de requêtes qui utilise l'optimisation bayésienne en combinaison avec la sélection de modèles bayésiens pour optimiser la perturbation adverse et le degré optimal de réduction de la dimension de l'espace de recherche. 
Les auteurs proposent d'utiliser l'optimisation bayésienne avec un substitut de GP pour la génération d'images adverses, en exploitant la structure additive et en utilisant la sélection de modèles bayésiens pour déterminer une réduction optimale de la dimensionnalité.
Nous proposons un modèle pour apprendre des représentations multimodales factorisées qui sont discriminantes, génératives et interprétables.
Cet article présente un " modèle de factorisation multimodale " qui factorise les représentations en facteurs discriminants multimodaux partagés et en facteurs génératifs spécifiques à la modalité. 
Nous développons un algorithme hiérarchique et critique des acteurs pour le transfert de composition par le partage des composants de la politique et nous démontrons la spécialisation des composants et les avantages directs connexes dans les domaines multitâches ainsi que son adaptation pour les tâches uniques.
Une combinaison de différentes techniques d'apprentissage pour l'acquisition de la structure et l'apprentissage avec des données asymétriques, utilisées pour entraîner une politique de LRH.
Les auteurs présentent une structure de politique hiérarchique à utiliser dans l'apprentissage par renforcement à la fois pour une seule tâche et pour plusieurs tâches, et évaluent l'utilité de cette structure pour des tâches robotiques complexes.
Nous comptons empiriquement le nombre de régions linéaires des réseaux de redresseurs et affinons les limites supérieures et inférieures.
Cet article présente des limites améliorées pour le comptage du nombre de régions linéaires dans les réseaux ReLU.
Nous analysons les propriétés de mémorisation par un convnet de l'ensemble d'entraînement et proposons plusieurs cas d'utilisation où nous pouvons extraire certaines informations sur l'ensemble d'entraînement. 
Illumine les propriétés de généralisation/mémorisation des ConvNet larges et profonds et tente de développer des procédures liées à l'identification de l'entrée d'un ConvNet entraîné qui a effectivement été utilisée pour entraîner le réseau.
Les GAN peuvent en principe apprendre des distributions de manière efficace par échantillonnage, si la classe de discriminateurs est compacte et possède un fort pouvoir de distinction par rapport à la classe de générateurs particulière.
Propose la notion d'approximabilité restreinte, et fournit une limite de complexité d'échantillon, polynomiale dans la dimension, qui est utile pour étudier le manque de diversité dans les GANs.
Analyse que la métrique de probabilité intégrale peut être une bonne approximation de la distance de Wasserstein sous certaines hypothèses légères.
Dans la première phase de formation des réseaux neuronaux profonds, il existe un "point d'équilibre" qui détermine les propriétés de l'ensemble de la trajectoire d'optimisation.
Ce travail analyse l'optimisation des réseaux neuronaux profonds en considérant comment les hyper-paramètres de taille de lot et de pas modifient les trajectoires d'apprentissage.
Nous proposons le HWGCN pour mélanger les informations de voisinage pertinentes à différents ordres afin de mieux apprendre les représentations des nœuds.
Les auteurs proposent une variante du GCN, HWGCN, pour prendre en compte la convolution au-delà des voisins à une étape, ce qui est comparable aux méthodes de pointe.
Nous introduisons une nouvelle mesure de l'aplatissement aux minima locaux de la surface de perte des réseaux neuronaux profonds qui est invariante par rapport aux reparamétrages par couche et nous relions l'aplatissement à la robustesse et à la généralisation des caractéristiques.
Les auteurs proposent une notion de robustesse des caractéristiques qui est invariante par rapport au changement d'échelle du poids et discutent de la relation de cette notion avec la généralisation.
Cet article définit une notion de robustesse des caractéristiques et la combine avec la représentativité epsilon d'une fonction pour décrire un lien entre la planéité des minima et la généralisation dans les réseaux neuronaux profonds.
Nous proposons de sparifier les préactivations des portes et le flux d'information dans les LSTM pour les rendre constants et augmenter le niveau de sparsité des neurones.
Cet article propose une méthode de sparsification pour les réseaux neuronaux récurrents en éliminant les neurones avec zéro préactivation pour obtenir des réseaux compacts.
Nous introduisons une approche de réseau neuronal pour assister les solveurs d'équations aux dérivées partielles.
Les auteurs visent à améliorer la précision des solveurs numériques en entraînant un réseau neuronal sur des données de référence simulées qui corrige le solveur numérique.
une méthode d'apprentissage confédéré qui entraîne un modèle à partir de données médicales séparées horizontalement et verticalement 
Une méthode d'apprentissage automatique "confédérée" qui apprend à travers les divisions des données médicales séparées à la fois horizontalement et verticalement.
Cet article propose l'activation quantifiée stochastique qui résout les problèmes de surajustement dans la formation adverse FGSM et atteint rapidement une robustesse comparable à la formation en plusieurs étapes.
L'article propose un modèle pour améliorer l'apprentissage contradictoire en introduisant des perturbations aléatoires dans les activations de l'une des couches cachées.
Nous étudions la structure du bruit dans le cerveau et constatons qu'il peut aider à la généralisation en déplaçant les représentations le long des variations de stimulus dans la classe.
Nous présentons une nouvelle conception, à savoir l'auto-assemblage avec des groupes agnostiques, pour l'adaptation à des domaines fermés et ouverts.
Une nouvelle approche de l'adaptation de domaine en jeu ouvert, où les catégories du domaine source sont contenues dans les catégories du domaine cible afin de filtrer les catégories aberrantes et de permettre l'adaptation dans les classes partagées.
Nous montrons comment apprendre les décompositions spectrales des opérateurs linéaires avec l'apprentissage profond, et nous l'utilisons pour l'apprentissage non supervisé sans modèle génératif.
Les auteurs proposent d'utiliser un cadre d'apprentissage profond pour résoudre le calcul des plus grands vecteurs propres.
Cet article présente un cadre pour apprendre les fonctions propres via un processus stochastique et propose de relever le défi du calcul des fonctions propres dans un contexte à grande échelle en les approchant à l'aide d'un processus d'optimisation stochastique à deux phases.
Application de l'algorithme Riemannian SGD (RSGD) pour l'entraînement des RNN Tensor-Train afin de réduire davantage les paramètres du modèle.
L'article propose d'utiliser l'algorithme du gradient stochastique riemannien pour l'apprentissage de trains tensoriels à faible rang dans les réseaux profonds.
Propose un algorithme d'optimisation des réseaux neuronaux paramétrés par la décomposition du train tensoriel basé sur l'optimisation riemannienne et l'adaptation du rang, et conçoit une architecture LSTM TT bidirectionnelle.
Nous proposons un nouvel algorithme qui apprend des politiques satisfaisant les contraintes, et nous fournissons une analyse théorique et une démonstration empirique dans le contexte de l'apprentissage par renforcement avec contraintes.
Cet article présente un algorithme d'optimisation des politiques sous contrainte utilisant un processus d'optimisation en deux étapes, où les politiques qui ne satisfont pas la contrainte peuvent être projetées à nouveau dans l'ensemble des contraintes.
Nous proposons une représentation basée sur le gradient pour caractériser les informations que les réseaux profonds n'ont pas apprises.
Les auteurs présentent la création de représentations basées sur les gradients par rapport aux poids pour compléter les informations manquantes dans l'ensemble de données d'entraînement pour les réseaux profonds.
Nous présentons un cadre de réduction des artéfacts dans les images médicales, qui tire parti de la puissance de l'apprentissage profond, mais sans utiliser de réseaux généraux pré-entraînés ou de référence d'image propre. 
Nous appliquons le concept de goulot d'étranglement informationnel à l'attribution.
L'article propose une nouvelle méthode basée sur les perturbations pour calculer les cartes d'attribution/de salinité pour les classificateurs d'images basés sur des réseaux neuronaux profonds, en injectant des bruits artificiels dans une couche précoce du réseau.
Nous montrons que les RNN peuvent être élagués pour induire la sparsité des blocs, ce qui améliore la vitesse des opérations à faible densité sur le matériel existant.
Les auteurs proposent une approche d'élagage de sparsité par bloc pour compresser les RNN, en utilisant le groupe LASSO pour promouvoir la sparsité et élaguer, mais avec un programme très spécialisé quant à l'élagage et au poids d'élagage.
Nous proposons une amélioration des réseaux d'itération de valeur, avec des applications à la planification de la trajectoire des rovers planétaires.
Ce document apprend une fonction de récompense basée sur les trajectoires des experts en utilisant un module d'itération de valeur pour rendre l'étape de planification différentiable.
Une nouvelle couche d'attention qui combine les sous-couches d'auto-attention et de feed-forward des réseaux Transformer.
Cet article propose une modification du modèle Transformer en incorporant l'attention sur les vecteurs de mémoire "persistants" dans la couche d'auto-attention, ce qui permet d'obtenir des performances comparables à celles des modèles existants tout en utilisant moins de paramètres.
Nous trouvons efficacement un sous-ensemble d'images qui ont des activations plus élevées que prévu pour un certain sous-ensemble de nœuds.  Ces images semblent plus anormales et plus faciles à détecter lorsqu'elles sont considérées comme un groupe. 
L'article propose un schéma de détection de la présence d'entrées anormales basé sur une approche de "balayage de sous-ensembles" pour détecter les activations anormales dans le réseau d'apprentissage profond.
Les modèles récurrents stables peuvent être approximés par des réseaux de type "feed-forward" et donnent des résultats empiriques aussi bons que les modèles instables dans des tâches de référence.
Etudie la stabilité des RNN et l'étude de la normalisation spectrale pour les prédictions séquentielles.
Étude du rôle du partage des poids dans les réseaux neuronaux à l'aide de fonctions de hachage, et constatation qu'une fonction de hachage équilibrée et déterministe améliore les performances du réseau.
Proposer des ArbNets pour étudier le partage de poids de manière plus systématique en définissant la fonction de partage de poids comme une fonction de hachage.
 Nous présentons un système d'apprentissage relationnel statistique qui emprunte des idées à la logique de Markov mais qui apprend une représentation implicite des règles sous forme de réseau neuronal.
L'article propose une extension des réseaux logiques de Markov en supprimant leur dépendance à l'égard des règles logiques de premier ordre prédéfinies afin de modéliser davantage de domaines dans les tâches de complétion de bases de connaissances.
Une méthode évolutive pour l'apprentissage d'un antécédent expressif sur les réseaux neuronaux pour de multiples tâches.
L'article présente une méthode d'apprentissage d'un modèle probabiliste pour l'apprentissage par transfert multitâches en introduisant une variable latente par tâche pour capturer les points communs entre les instances de la tâche.
Ce travail propose une approche variationnelle du méta-apprentissage qui utilise des variables latentes correspondant à des ensembles de données spécifiques à une tâche.
Vise à apprendre une priorité sur les réseaux neuronaux pour des tâches multiples. 
MODÈLES D'ESPACE D'ÉTAT DÉSENCHEVÊTRÉS
L'article présente un modèle génératif d'espace d'état utilisant une variable latente globale E pour capturer les informations spécifiques à l'environnement.
Apprentissage par divergence de Bregman pour l'apprentissage à quelques coups. 
Nous introduisons un cadre de réseau qui peut modifier sa structure pendant l'apprentissage et montrons qu'il peut converger vers divers archétypes de réseaux ML tels que les MLP et les LCN. 
L'augmentation des données guidée par le domaine fournit une méthode robuste et stable de généralisation du domaine.
Cet article propose une approche de généralisation du domaine par l'augmentation des données en fonction du domaine.
Les auteurs présentent la méthode CrossGrad, qui entraîne à la fois une tâche de classification d'étiquettes et une tâche de classification de domaines.
Nous étudions des alternatives aux approches traditionnelles de modélisation des images en pixels, et proposons un modèle génératif pour les images vectorielles.
Cet article présente une architecture de réseau neuronal pour la génération de croquis inspirée de l'autoencodeur variationnel.
Nous fournissons une étude essayant de voir comment la récente adaptation du taux d'apprentissage en ligne prolonge la conclusion faite par Wilson et al. 2018 sur les méthodes de gradient adaptatif, ainsi qu'une comparaison et une analyse de sensibilité.
Rapporte les résultats de l'essai de plusieurs méthodes liées à l'ajustement de la taille de pas, y compris la SGD de vanille, la SGD avec le momentum de Neserov et ADAM, et compare ces méthodes avec l'hypergradient et sans. 
Nous étudions les comportements à grand échantillon des estimations de la valeur Q et nous proposons une stratégie d'exploration efficace qui repose sur l'estimation des écarts relatifs entre les estimations Q. 
Cet article présente un algorithme d'exploration pure pour l'apprentissage par renforcement basé sur une analyse asymptotique des valeurs Q et leur convergence vers la distribution centrale limite, surpassant les algorithmes d'exploration de référence.
Nous formons un réseau de traduction d'image à image qui prend en entrée l'image source et un échantillon d'une distribution antérieure pour générer un échantillon de la distribution cible.
Cet article formalise le problème de la traduction non supervisée et propose un cadre GAN augmenté qui utilise l'information mutuelle pour éviter le cas dégénéré.
Cet article formule le problème de la traduction non supervisée d'une image vers plusieurs autres et aborde le problème en minimisant l'information mutuelle. 
Apprendre à extraire des points clés distincts à partir d'une tâche proxy, rejet des valeurs aberrantes.
Cet article est consacré à l'apprentissage auto-supervisé des caractéristiques locales en utilisant la méthode RANSAC guidée par les neurones comme fournisseur de pertes auxiliaires supplémentaires pour améliorer l'interpolation des descripteurs.
Nous proposons une formulation de la motivation intrinsèque qui convient comme biais d'exploration dans les tâches synergiques multi-agents à récompense éparse, en encourageant les agents à affecter le monde d'une manière qui ne serait pas atteinte s'ils agissaient individuellement.
L'article se concentre sur l'utilisation de la motivation intrinsèque pour améliorer le processus d'exploration des agents d'apprentissage par renforcement dans les tâches qui requièrent l'intervention de plusieurs agents.
Un algorithme d'inférence probabiliste piloté par un réseau neuronal pour les modèles à structure de graphe
Cet article présente le passage de messages par politique, un réseau neuronal à graphes doté d'un mécanisme d'inférence qui attribue des messages aux arêtes de manière récurrente, indiquant des performances compétitives sur des tâches de raisonnement visuel.
Nous montrons comment vous pouvez améliorer les performances d'un réseau multitâche en réglant une fonction de perte adaptative multitâche qui est apprise en équilibrant directement les gradients du réseau.
Ce travail propose un schéma de mise à jour dynamique des poids qui met à jour les poids pour les différentes pertes de tâches pendant le temps de formation en utilisant les ratios de perte des différentes tâches.
Les DNN pour la segmentation d'images peuvent mettre en œuvre des solutions pour le problème de l'intériorité, mais seuls certains réseaux récurrents peuvent les apprendre avec un type de supervision spécifique.
Cet article introduit l'insidieux pour étudier la segmentation sémantique à l'ère de l'apprentissage profond, et les résultats peuvent aider les modèles à mieux se généraliser.
Étant donné un modèle pré-entraîné, nous avons exploré les gradients par échantillon des paramètres du modèle par rapport à une perte spécifique à la tâche, et construit un modèle linéaire qui combine les gradients des paramètres du modèle et l'activation du modèle.
Cet article propose d'utiliser les gradients de couches spécifiques de réseaux convolutifs comme caractéristiques dans un modèle linéarisé pour l'apprentissage par transfert et l'adaptation rapide.
Nous entraînons notre modèle de reconstruction de visage avec perte adversariale de manière semi-supervisée sur des lots hybrides d'images de visage étiquetées et non étiquetées afin d'exploiter la valeur de grandes quantités d'images de visage non étiquetées provenant de collections de photos sans contrainte.
Cet article propose un processus d'apprentissage semi-supervisé et contradictoire pour obtenir des représentations non linéaires démêlées exactes à partir d'une image de visage avec des fonctions de perte, ce qui permet d'atteindre des performances de pointe dans la reconstruction de visage.
Cet article présente ConceptFlow qui modélise explicitement le flux de conversation dans un graphe de connaissances de sens commun pour une meilleure génération de conversations.
L'article propose un système pour générer une réponse à tour de rôle à une énonciation postée dans un contexte de dialogue à domaine ouvert en utilisant la diffiusion dans les voisins des concepts fondés.
Nous examinons l'hypothèse selon laquelle l'entropie des espaces de solution pour les contraintes sur les poids synaptiques (la "flexibilité" de la contrainte) pourrait servir de fonction de coût pour le développement de circuits neuronaux.
Les ensembles infinis de réseaux neuronaux de largeur infinie constituent une famille de modèles intéressante du point de vue de la théorie de l'information.
Nous menons une étude comparative de l'alignement interlinguistique et des méthodes d'apprentissage conjointes et nous unifions ces deux paradigmes auparavant exclusifs dans un nouveau cadre. 
Cet article compare les approches de l'induction de lexiques bilingues et montre quelle méthode est la plus performante sur le lexique, l'induction et les tâches de NER et de MT.
Combinaison de techniques de compression de modèles orthogonaux pour obtenir une réduction significative de la taille du modèle et du nombre de flops requis pendant l'inférence.
Cet article propose de combiner la décomposition de Tucker avec l'élagage des filtres.
Présente JAUNE : une méthodologie pour remplacer les scores BLEU et ROUGE par des évaluateurs multidimensionnels, basés sur des modèles, pour évaluer les résumés.
Cet article propose une nouvelle métrique JAUNE pour l'évaluation des systèmes de traduction automatique et de résumé de texte, montrant que leur modèle correspond mieux aux étiquettes de similarité de la vérité du terrain que le BLEU.
nouveau formalisme GNN + expériences approfondies ; les différences entre GGNN/GCN/GAT sont moins importantes qu'on ne le pensait.
L'article propose une nouvelle architecture de réseau neuronal graphique qui utilise la modulation linéaire en fonction des caractéristiques pour conditionner le passage des messages du nœud source au nœud cible en fonction de la représentation du nœud cible.
Cet article propose un nouveau cadre de décomposition matricielle pour l'intégration et le regroupement simultanés de données de réseau attribuées.
Cet article propose un algorithme permettant d'effectuer conjointement l'intégration et le regroupement de réseaux d'attributs.
Nous proposons une technique de rendu guidée par l'image apprise qui combine les avantages du rendu basé sur l'image et de la synthèse d'image basée sur le GAN tout en tenant compte des effets dépendants de la vue.
Cette soumission propose une méthode pour traiter les effets dépendants de la vue dans le rendu neuronal, ce qui améliore la robustesse des méthodes de rendu neuronal existantes.
Les GAN sont évalués sur des ensembles de données synthétiques.
Nous proposons une méthode d'adaptation de la taille de pas efficace et efficiente pour les méthodes de gradient.
Une nouvelle adaptation de la taille du pas dans les méthodes de gradient du premier ordre qui établit un nouveau problème d'optimisation avec l'expansion du premier ordre de la fonction de perte et la régularisation, où la taille du pas est traitée comme une variable.
Nous avons tourné qu'une large classe de manifolds peut être générée par les réseaux ReLU et sigmoïde avec une précision arbitraire.
Cet article fournit certaines garanties de base sur le moment où les collecteurs peuvent être écrits comme l'image d'une carte approximée par un réseau neuronal, et relie les théorèmes de la géométrie des collecteurs et les résultats d'approximation universelle standard.
Cet article montre théoriquement que les modèles génératifs basés sur les réseaux neuronaux peuvent approximer les collecteurs de données et prouve que, sous des hypothèses légères, les réseaux neuronaux peuvent faire correspondre un espace latent à un ensemble proche du collecteur de données donné avec une petite distance de Hausdorff.
Nous concevons des algorithmes d'apprentissage par renforcement basés sur des modèles avec des garanties théoriques et obtenons des résultats de pointe sur les tâches de référence de Mujuco lorsqu'un million d'échantillons ou moins sont autorisés.
L'article propose un cadre pour la conception d'algorithmes de RL basés sur des modèles et fondés sur l'OFU, qui permet d'obtenir des performances SOTA pour les tâches MuJoCo.
Nous présentons des techniques supplémentaires pour utiliser la distillation des connaissances afin de compresser U-net par plus de 1000x.
Les auteurs ont introduit une stratégie de distillation modifiée pour compresser une architecture U-net par plus de 1000x tout en conservant une précision proche du U-net original.
Cet article propose une approche pour traiter l'oubli catastrophique par le biais d'estimations de courbure sans hessian.
Cet article propose une méthode approximative de Laplace pour la formation des réseaux neuronaux dans le cadre de l'apprentissage continu avec une faible complexité spatiale.
Méthode permettant d'améliorer les performances des modèles simples à partir d'un modèle complexe (précis).
L'article propose un moyen d'améliorer les prédictions d'un modèle à faible capacité qui présente des avantages par rapport aux approches existantes.
Un algorithme simple et pratique pour l'apprentissage d'un noyau invariant en translation ou sphériquement symétrique maximisant la marge à partir de données d'entraînement, en utilisant des outils de l'analyse de Fourier et de la minimisation du regret.
L'article propose d'apprendre un noyau invariant de translation ou de rotation personnalisé dans la représentation de Fourier afin de maximiser la marge du SVM.
Les auteurs proposent un algorithme intéressant pour apprendre le l1-SVM et le noyau représenté par Fourier ensemble.
Les auteurs envisagent l'apprentissage de représentations de Fourier directes de noyaux invariants de décalage/translation pour des applications d'apprentissage automatique, l'alignement du noyau sur les données étant la fonction objective à optimiser.
Une programmation probabiliste qui supporte nativement l'inférence causale et contrefactuelle
Inférence d'un modèle de comportement d'une grande population par un jeu de champ moyen (MFG) via une synthèse de MFG et de processus de décision de Markov.
Les auteurs traitent de l'inférence dans les modèles de comportement collectif en utilisant l'apprentissage par renforcement inverse pour apprendre les fonctions de récompense des agents dans le modèle.
Nous combinons des modèles génératifs profonds avec une supervision programmatique faible pour générer des trajectoires multi-agents coordonnées d'une qualité nettement supérieure à celle des bases précédentes.
Propose des modèles génératifs séquentiels multi-agents.
L'article propose de former des modèles génératifs qui produisent des trajectoires multi-agents en utilisant des fonctions heuristiques qui étiquettent des variables qui seraient autrement latentes dans les données de formation.
Apprendre à classer les courbes d'apprentissage afin d'arrêter précocement les travaux de formation non prometteurs. Nouveauté : utilisation de la perte de classement par paire pour modéliser directement la probabilité d'améliorer et de transférer l'apprentissage sur des ensembles de données afin de réduire les données d'entraînement requises.
L'article propose une méthode de classement des courbes d'apprentissage des réseaux neuronaux qui peut modéliser les courbes d'apprentissage sur différents ensembles de données, ce qui permet d'atteindre des vitesses plus élevées sur les tâches de classification d'images.
Nous montrons que, dans des contextes d'apprentissage continu, l'oubli catastrophique peut être évité en appliquant la RL hors politique à un mélange d'expérience nouvelle et de relecture, avec une perte de clonage comportemental.
Propose une variante particulière du rejeu d'expérience avec clonage de comportement comme méthode d'apprentissage continu.
Nous présentons une méthode qui apprend à intégrer des informations temporelles et des informations visuelles ambiguës dans le contexte d'agents en interaction.
Les auteurs proposent un VRNN graphique qui modélise l'interaction de plusieurs agents en déployant un VRNN pour chaque agent.
Cet article présente une architecture basée sur un réseau neuronal graphique qui est entraînée à localiser et à modéliser les interactions des agents dans un environnement directement à partir de pixels et montre l'avantage du modèle pour le suivi des tâches et la prévision des emplacements des agents.
Nous considérons un modèle simplifié de réseau neuronal convolutif profond. Nous montrons que toutes les couches de ce réseau peuvent être apprises approximativement avec une application correcte de la décomposition tensorielle.
Fournit des garanties théoriques pour l'apprentissage de réseaux neuronaux convolutifs profonds en utilisant la décomposition tensorielle rank-one.
Cet article propose une méthode d'apprentissage pour un cas restreint de réseaux convolutifs profonds, où les couches sont limitées au cas où elles ne se chevauchent pas et n'ont qu'un seul canal de sortie par couche.
Analyse le problème de l'apprentissage d'une classe très spéciale de CNNs : chaque couche consiste en un seul filtre, appliqué à des patchs non chevauchants de l'entrée.
Les réseaux neuronaux à action directe dont les poids peuvent être élagués après la formation pourraient avoir eu les mêmes poids élagués avant la formation.
Montre qu'il existe des sous-réseaux épars qui peuvent être formés à partir de zéro avec de bonnes performances de généralisation et propose un NNs non élagué, initialisé de manière aléatoire, contenant des sous-réseaux qui peuvent être formés à partir de zéro avec une précision de généralisation similaire.
Le document examine l'hypothèse selon laquelle les réseaux neuronaux initialisés de manière aléatoire contiennent des sous-réseaux qui convergent aussi rapidement, voire plus rapidement, et peuvent atteindre une précision de classification identique ou supérieure.
Dans cet article, nous soulignons la difficulté de former des réseaux neuronaux épars en réalisant des expériences d'interpolation dans le paysage énergétique. 
La symétrie de l'espace des poids dans les paysages de réseaux neuronaux donne lieu à un grand nombre de selles et de sous-espaces plats à haute dimension.
Cet article présente une méthode d'étude de la fonction de perte par rapport aux paramètres dans un réseau neuronal du point de vue de la symétrie poids-espace.
théorie de la propagation du signal appliquée aux substituts continus des réseaux binaires ; initialisation contre intuitive ; l'astuce de reparamétrage n'est pas utile
Les auteurs étudient la dynamique de formation des réseaux neuronaux binaires lorsqu'ils utilisent des substituts continus, étudient les propriétés que les réseaux doivent avoir à l'initialisation pour une formation optimale et fournissent des conseils concrets sur les poids stochastiques à l'initialisation.
Une exploration approfondie des réseaux binaires stochastiques, des substituts continus et de leur dynamique de formation, avec des indications sur la façon d'initialiser les poids pour obtenir les meilleures performances.
Nous proposons une approche de l'apprentissage semi-supervisé des analyseurs syntaxiques de dépendances sémantiques basée sur le cadre de l'autoencodeur CRF.
Cet article se concentre sur l'analyse sémantique semi-supervisée des dépendances en utilisant l'auto-codeur CRF pour former le modèle dans un style semi-supervisé, indiquant l'efficacité sur des tâches de données étiquetées à faibles ressources.
DeFINE utilise un réseau profond, hiérarchique et clairsemé avec de nouvelles connexions par saut pour apprendre efficacement de meilleures incorporations de mots. 
Cet article décrit une nouvelle méthode pour apprendre efficacement des représentations profondes au niveau des mots en utilisant une structure hiérarchique avec des connexions par saut pour l'utilisation de couches d'entrée et de sortie de faible dimension.
Nous reproduisons avec succès et donnons des remarques sur la comparaison avec les lignes de base d'une approche de méta-apprentissage pour la classification de quelques images qui fonctionne par rétropropagation à travers la solution d'un solveur à forme fermée.
La réaffectation dynamique des paramètres permet de réussir l'apprentissage direct de réseaux compacts et épars, et joue un rôle indispensable même lorsque nous connaissons a priori le réseau épars optimal.
3 axes servant de tremplin à l'apprentissage expérimental du module de vision par les robots
Étudie les performances des classificateurs d'images et des détecteurs d'objets existants. 
Toutes les couches de nos modèles acoustiques basés sur les CNN, à l'exception des deux premières, ont montré un certain degré de spécificité linguistique, mais l'entraînement au gel a permis un transfert réussi entre les langues.
L'article mesure la transférabilité des caractéristiques de chaque couche des modèles acoustiques basés sur les CNN d'une langue à l'autre, et conclut que les AM formés avec la technique de " l'apprentissage par le gel " sont plus performants que les autres modèles transférés.
Liaison entre les gradients de politique entropique de la région de Wasserstein-trust et l'équation de la chaleur.
L'article explore les liens entre l'apprentissage par renforcement et la théorie du transport optimal quadratique.
Les auteurs ont étudié le gradient de politique avec changement de politique limité par une région de confiance de distance de Wasserstein dans le cadre du bandit à bras multiples, montrant que dans la limite des petits pas, la dynamique de la politique est régie par l'équation de la chaleur (équation de Fokker-Planck).
La capacité discriminative de softmax pour l'apprentissage des vecteurs de caractéristiques des objets est efficacement améliorée par la virture de la normalisation isotrope sur la distribution globale des points de données.
Nous adaptons l'apprentissage Q avec bonus d'exploration UCB aux MDP à horizon infini avec récompenses actualisées sans accéder à un modèle génératif, et améliorons le meilleur résultat connu précédemment.
Cet article considère un algorithme d'apprentissage Q avec une politique d'exploration UCB pour un MDP à horizon infini.
Les perturbations peuvent être utilisées pour former les poids de rétroaction afin d'apprendre dans les réseaux neuronaux entièrement connectés et convolutifs.
Cet article propose une méthode qui traite le problème du "transport de poids" en estimant les poids pour le passage en arrière en utilisant un estimateur basé sur le bruit. 
Nous identifions certains modèles universels (c.-à-d. qui sont valables pour toutes les architectures) dans le comportement de différentes pertes de substitution (CE, MSE, perte 0-1) lors de la formation de réseaux neuronaux et nous présentons des preuves empiriques à l'appui.
