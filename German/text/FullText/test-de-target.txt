FearNet ist ein speichereffizientes neuronales Netz, das von der Gedächtnisbildung im Säugetiergehirn inspiriert ist und inkrementelles Klassenlernen ohne katastrophales Vergessen ermöglicht.
In diesem Beitrag wird eine neuartige Lösung für ein inkrementelles Klassifizierungsproblem auf der Grundlage eines dualen Speichersystems vorgestellt. 
Lernen mit mehreren Ansichten verbessert das unüberwachte Lernen von Satzrepräsentationen
Der Ansatz verwendet verschiedene, komplementäre Kodierer des Eingabesatzes und eine Konsensmaximierung.
Der Artikel präsentiert einen Multi-View-Rahmen zur Verbesserung der Satzrepräsentation in NLP-Aufgaben unter Verwendung generativer und diskriminativer Zielarchitekturen.
Diese Arbeit zeigt, dass Multi-View Frameworks beim Lernen von Satzrepräsentationen effektiver sind als die Verwendung einzelner Encoder.
Wir zeigen, wie diskrete Objekte auf unüberwachte Weise aus Pixeln gelernt werden können und wie man mit dieser Objektrepräsentation Reinforcement Learning durchführen kann.
Eine Methode zum Lernen von Objektrepräsentationen aus Pixeln für die Durchführung von Reinforcement Learning. 
Die Arbeit schlägt eine neuronale Architektur vor, um Videoströme auf eine diskrete Sammlung von Objekten abzubilden, ohne menschliche Annotationen, unter Verwendung eines unbeaufsichtigten Pixel-Rekonstruktionsverlustes. 
Ein umfangreicher Datensatz zum Training von Aufmerksamkeitsmodellen für die Objekterkennung führt zu einer genaueren, interpretierbaren und menschenähnlichen Objekterkennung.
Die jüngsten Fortschritte in der visuellen Erkennung sind auf die Verwendung visueller Aufmerksamkeitsmechanismen in tiefen Convolutional Networks zurückzuführen, die durch eine schwache Form der Überwachung auf der Grundlage von Bildklassenbezeichnungen lernen, worauf sie sich konzentrieren sollen.
Es wird ein neuer Ansatz zum Thema Aufmerksamkeit vorgestellt, bei dem ein großer Aufmerksamkeitsdatensatz gesammelt und verwendet wird, um ein NN auf überwachte Weise zu trainieren, um die selbstberichtete menschliche Aufmerksamkeit zu nutzen.
Diese Arbeit schlägt einen neuen Ansatz vor, um informativere Signale zu verwenden, insbesondere Regionen, die Menschen auf Bildern als wichtig erachten, um tiefe Convolutional Neural Networks zu verbessern.
Wir haben eine zeiteffiziente Verteidigungsmethode gegen einstufige und iterative adversarial Angriffe vorgeschlagen.
Vorschlag einer neuartigen, rechnerisch effizienten Methode mit dem Namen e2SAD, die für jede saubere Trainingsprobe einen Satz von zwei adversarial Trainingsbeispielen erzeugt.
Der Beitrag stellt eine zweistufige adversarische Verteidigungsmethode vor, die zwei adversarische Beispiele pro sauberer Probe erzeugt und sie in die eigentliche Trainingsschleife einbezieht, um Robustheit zu erreichen, und behauptet, dass sie teurere iterative Methoden übertreffen kann.
Die Arbeit stellt einen 2-Schritt-Ansatz vor, um starke Gegenbeispiele zu generieren, und zwar zu weitaus geringeren Kosten als bei aktuellen iterativen mehrstufigen Gegenangriffen.
Ein Vergleich von fünf tiefen neuronalen Netzwerkarchitekturen zur Erkennung bösartiger Domänennamen zeigt erstaunlich wenig Unterschiede.
Die Autoren schlagen die Verwendung von fünf tiefgreifenden Architekturen für die Cybersicherheitsaufgabe der Erkennung von Algorithmen der Domänenerzeugung vor.
Wendet verschiedene NN-Architekturen zur Klassifizierung von URLs an, die mit Gutartig und Malware in Verbindung stehen.
In diesem Beitrag wird vorgeschlagen, Domänennamen automatisch als bösartig oder gutartig zu erkennen, indem tiefe Netzwerke trainiert werden, um die Zeichenfolge direkt als solche zu klassifizieren.
Adversarial-Learning-Methoden ermutigen NLI-Modelle dazu, datensatzspezifische Verzerrungen zu ignorieren und helfen bei der Übertragung von Modellen auf andere Datensätze.
Die Arbeit schlägt einen adversarial Aufbau vor, um Annotationsartefakte in natürlichsprachlichen Inferenzdaten zu entschärfen
In diesem Beitrag wird eine Methode zur Beseitigung von Verzerrungen eines textuellen Entailment-Modells durch ein adversarial Trainingsziel vorgestellt. 
Ein neuer hochmoderner Ansatz für die Einbettung von Wissensgraphen.
Es wird eine neuronale Bewertungsfunktion für Verknüpfungsvorhersagen vorgestellt, die Symmetrie, Antisymmetrie, Inversion und Kompositionsmuster von Beziehungen in einer Wissensbasis ableiten kann.
In diesem Beitrag wird ein Ansatz zur Einbettung von Wissensgraphen vorgeschlagen, bei dem Beziehungen als Rotationen im komplexen Vektorraum modelliert werden.
Vorschlagen einer Methode zur Einbettung von Graphen, die für die Vorhersage von Links verwendet werden kann.
Wir haben das CNN mit Hilfe von HyperNetworks modifiziert und eine bessere Robustheit gegenüber ungünstigen Beispielen festgestellt.
Verbesserung der Robustheit und Zuverlässigkeit von tiefen neuronalen Convolutional Neural Networks durch Verwendung datenabhängiger Convolution Kernerls.
Wir schlagen einen Meta-Learning-Ansatz für die Few-Shot Klassifizierung vor, der durch Backpropagation über die Lösung von schnellen Solvern wie Ridge-Regression oder logistischer Regression eine hohe Leistung bei hoher Geschwindigkeit erzielt.
In dem Papier wird ein Algorithmus für das Meta-Lernen vorgeschlagen, der darauf hinausläuft, die Merkmale festzulegen (d. h. alle verborgenen Schichten eines tiefen NN) und jede Aufgabe als eigene Endschicht zu behandeln, die eine Ridge-Regression oder eine logistische Regression sein könnte.
In diesem Papier wird ein Meta-Learning-Ansatz für das Problem der Few-Shot Klassifizierung vorgeschlagen. Die Methode basiert auf der Parametrisierung des Lerners für jede Aufgabe durch einen geschlossene Form Löser.
Wir bieten eine neue Perspektive auf das Training eines maschinellen Lernmodells von Grund auf in einer hierarchischen Umgebung, d.h. wir betrachten es als eine wechselseitige Kommunikation zwischen Mensch und Algorithmus und untersuchen, wie wir die Effizienz messen und verbessern können. 
Es wird eine neue Einstellung für aktives Lernen eingeführt, bei der das Orakel eine partielle oder schwache Kennzeichnung anbietet, anstatt nach der Kennzeichnung eines bestimmten Beispiels zu fragen, was zu einem einfacheren Abruf von Informationen führt.
In dieser Arbeit wird eine Methode des aktiven Lernens mit partiellem Feedback vorgeschlagen, die bei einem begrenzten Budget besser abschneidet als die existierenden Basissysteme.
In dem Beitrag wird ein Mehrklassen-Klassifizierungsproblem betrachtet, bei dem Etiketten in eine bestimmte Anzahl M von Teilmengen gruppiert werden, die alle einzelnen Etiketten als Singletons enthalten.
Wir zeigen, dass Wasserstein-Räume gute Ziele für die Einbettung von Daten mit komplexer semantischer Struktur sind.
Lernt Einbettungen in einem diskreten Raum von Wahrscheinlichkeitsverteilungen unter Verwendung einer minimierten, regularisierten Version von Wasserstein-Distanzen.
Die Arbeit beschreibt eine neue Einbettungsmethode, die Daten in den Raum der Wahrscheinlichkeitsmaße einbettet, die mit der Wasserstein-Distanz ausgestattet sind. 
In dem Beitrag wird vorgeschlagen, die Daten in niedrigdimensionale Wasserstein-Räume einzubetten, die die zugrunde liegende Struktur der Daten genauer erfassen können.
Ein Clustering-Algorithmus, der eine gemeinsame nichtlineare Dimensionalitätsreduktion und ein Clustering durch Optimierung eines globalen kontinuierlichen Ziels durchführt.
Stellt einen Clustering-Algorithmus vor, der tiefe Autoencoder und Clustering als ein globales, kontinuierliches Ziel gemeinsam löst und bessere Ergebnisse als moderne Clustering-Schemata zeigt.
Deep Continuous Clustering ist eine Clustering-Methode, die das Ziel des Autoencoders mit dem Ziel des Clustering verbindet und dann mit SGD trainiert.
Um die Berechnung von Convolutional Neural Networks zu beschleunigen, schlagen wir eine neue zweistufige Pruning Technik vor, die eine höhere Sparsamkeit der Winograd-Domäne erreicht, ohne die Netzstruktur zu verändern.
Schlägt ein räumliches Winograd Pruning Framework vor, das es ermöglicht, Pruned Gewichte aus dem räumlichen Bereich im Winograd-Bereich zu behalten und die Spärlichkeit des Winograd-Bereichs zu verbessern.
Schlägt zwei Techniken für das Pruning von Convolutional Layers vor, die den Winograd-Algorithmus verwenden.
Wir schlagen ein nichtparametrisches Bayes'sches Modell für föderiertes Lernen mit neuronalen Netzen vor.
Verwendet das Beta-Verfahren für den föderalen neuronalen Abgleich.
Die Arbeit befasst sich mit dem föderativen Lernen neuronaler Netze, bei dem die Daten auf mehrere Rechner verteilt sind und die Verteilung der Datenpunkte potenziell inhomogen und unausgewogen ist.
Allgemeine Methode zum Trainieren ausdrucksstarker MCMC-Kerne, die mit tiefen neuronalen Netzen parametrisiert sind. Bei einer Zielverteilung p bietet unsere Methode einen schnell mischenden Sampler, der den Zustandsraum effizient erkunden kann.
Er schlägt eine verallgemeinerte HMC vor, indem er den Leapfrog-Integrator mit Hilfe neuronaler Netze modifiziert, um den Sampler schnell konvergieren und mischen zu lassen. 
Wir zeigen, dass seltene, aber katastrophale Fehler bei Zufallstests völlig übersehen werden können, was Probleme für einen sicheren Einsatz mit sich bringt. Unser vorgeschlagener Ansatz für adversarial Tests behebt dieses Problem.
Es wird eine Methode vorgeschlagen, mit der eine Vorhersage der Ausfallwahrscheinlichkeit für einen gelernten Agenten erlernt werden kann, was zu Vorhersagen darüber führt, welche Anfangszustände zum Ausfall eines Systems führen.
In dieser Arbeit wird ein Ansatz für die Auswahl von Fehlerfällen für RL-Algorithmen vorgeschlagen, der auf einer Funktion basiert, die über ein neuronales Netz für Fehler gelernt wird, die während des Agententrainings auftreten.
In dieser Arbeit wird ein kontradiktorischer Ansatz zur Identifizierung von katastrophalen Fehlern beim Reinforcement Learning vorgeschlagen.
Um den posterioren Kollaps in VAEs zu bekämpfen, schlagen wir ein neuartiges, aber einfaches Trainingsverfahren vor, das das Inferenznetzwerk mit mehr Updates aggressiv optimiert. Dieses neue Trainingsverfahren mildert den posterioren Kollaps und führt zu einem besseren VAE-Modell. 
Untersucht das Phänomen des posterioren Kollaps und zeigt, dass ein verstärktes Training des Inferenznetzes das Problem verringern und zu besseren Optima führen kann.
Die Autoren schlagen vor, das Trainingsverfahren der VAEs nur als Lösung für den posterioren Kollaps zu ändern und das Modell und das Ziel unangetastet zu lassen.
Generative Entdeckung bedeutungsvoller, neuartiger Entitätspaare mit einer bestimmten medizinischen Beziehung durch reines Lernen aus den vorhandenen bedeutungsvollen Entitätspaaren, ohne dass ein zusätzlicher Textkorpus für die diskriminative Extraktion erforderlich ist.
Präsentiert einen variationalen Autoencoder zur Generierung von Entity-Paaren anhand einer Beziehung in einem medizinischen Umfeld.
Im medizinischen Kontext wird in diesem Beitrag das klassische Problem der "Vervollständigung der Wissensbasis" aus ausschließlich strukturierten Daten beschrieben.
Wir analysieren die VAE-Zielfunktion genau und ziehen neue Schlussfolgerungen, die zu einfachen Verbesserungen führen.
schlägt eine zweistufige VAE-Methode vor, um qualitativ hochwertige Beispiele zu erzeugen und Unschärfen zu vermeiden.
In diesem Beitrag werden die Gaußschen VAEs analysiert.
Die Arbeit liefert eine Reihe von theoretischen Ergebnissen über "vanilla" Gaussian Variational Auto-Encoders, die dann verwendet werden, um einen neuen Algorithmus namens "2 stage VAEs" zu bauen.
Ein Algorithmus zur Abstimmung der Hyperparameter unter Verwendung der diskreten Fourier-Analyse und des komprimierten Sensings.
Untersucht das Problem der Optimierung von Hyperparametern unter der Annahme, dass die unbekannte Funktion approximiert werden kann, und zeigt, dass die approximative Minimierung über den booleschen Hyperwürfel durchgeführt werden kann.
Die Arbeit untersucht die Optimierung von Hyperparametern durch die Annahme einer Struktur in der unbekannten Funktion, die Hyperparameter auf die Klassifizierungsgenauigkeit abbildet.
Eine neue Methode für die Gradientenabstammung von Permutationen, mit Anwendungen für die Inferenz von latenten Übereinstimmungen und das überwachte Lernen von Permutationen mit neuronalen Netzen.
Die Arbeit verwendet eine endliche Annäherung des Sinkhorn-Operators, um zu beschreiben, wie man ein neuronales Netz zum Lernen aus permutationsbewerteten Trainingsdaten konstruieren kann. 
Die Arbeit schlägt eine neue Methode vor, die das diskrete Max-Gewicht für das Lernen latenter Permutationen annähert.
Ein neuartiger Suchrahmen für differenzierbare neuronale Architekturen zur gemischten Quantisierung von ConvNets.
Die Autoren stellen eine neue Methode für die Suche nach einer neuronalen Architektur vor, die die Präzisionsquantisierung der Gewichte in jeder Schicht des neuronalen Netzes auswählt, und verwenden sie im Zusammenhang mit der Netzkompression.
In diesem Beitrag wird ein neuer Ansatz für die Quantisierung von Netzwerken vorgestellt, bei dem verschiedene Schichten mit unterschiedlichen Bitbreiten quantisiert werden, und es wird ein neuer Suchrahmen für differenzierbare neuronale Architekturen eingeführt.
Glättende Verlustfunktion für Top-k-Fehlerminimierung.
Schlägt vor, den Top-k-Verlust mit tiefen Modellen zu verwenden, um das Problem der Klassenverwechslung mit ähnlichen Klassen zu lösen, die im Trainingsdatensatz sowohl vorhanden als auch nicht vorhanden sind.
Glättet die Top-k-Verluste.
In dieser Arbeit wird eine glatte Surrogate-Verlustfunktion für die Top-k-SVM eingeführt, um die SVM mit den tiefen neuronalen Netzen zu verbinden.
Wir stellen Mol-CycleGAN vor - ein neues generatives Modell zur Optimierung von Molekülen, um das Design von Medikamenten zu verbessern.
Die Arbeit stellt einen Ansatz zur Optimierung molekularer Eigenschaften vor, der auf der Anwendung von CycleGANs auf variationalen Autoencodern für Moleküle basiert und eine domänenspezifische VAE namens Junction Tree VAE (JT-VAE) verwendet.
In diesem Beitrag wird ein variationaler Autoencoder verwendet, um eine Übersetzungsfunktion zu erlernen, die von der Menge der Moleküle ohne die gewünschte Eigenschaft zur Menge der Moleküle mit der Eigenschaft führt. 
Wir nehmen die Gesichtserkennung als Bruchstelle und schlagen eine Modelldestillation mit Wissenstransfer von der Gesichtsklassifizierung zum Abgleich und zur Verifizierung vor.
In diesem Beitrag wird vorgeschlagen, den Klassifikator aus dem Modell für die Gesichtsklassifizierung auf die Aufgabe der Ausrichtung und Überprüfung zu übertragen.
Das Manuskript stellt Experimente vor, bei denen Wissen aus einem Gesichtsklassifizierungsmodell in Studentenmodelle für die Ausrichtung und Verifizierung von Gesichtern destilliert wurde.
Verbesserung der sitzungsbasierten Empfehlungen mit RNNs (GRU4Rec) um 35% durch neu entwickelte Verlustfunktionen und Beispiele.
Diese Arbeit analysiert bestehende Verlustfunktionen für sitzungsbasierte Empfehlungen und schlägt zwei neuartige Verlustfunktionen vor, die eine Gewichtung zu bestehenden rangbasierten Verlustfunktionen hinzufügen.
Vorstellen von Änderungen an früheren Arbeiten für sitzungsbasierte Empfehlungen unter Verwendung von RNN, indem negative Beispiele nach ihrer "Relevanz" gewichtet werden.
In diesem Beitrag werden die Probleme bei der Optimierung der Verlustfunktionen in GRU4Rec erörtert, Tricks zur Optimierung vorgeschlagen und eine verbesserte Version vorgeschlagen.
Wir entwickeln einen auf der PAC-Bayes-Theorie basierenden Ansatz des lifelong Learnings für das Transferlernen, bei dem die Prioritäten angepasst werden, wenn neue Aufgaben auftreten, wodurch das Lernen neuer Aufgaben erleichtert wird.
Eine neuartige PAC-Bayes'sche Risikogrenze, die als Zielfunktion für maschinelles Lernen mit mehreren Aufgaben dient, und ein Algorithmus zur Minimierung einer vereinfachten Version dieser Zielfunktion.
Erweitert die bestehenden PAC-Bayes-Grenzen auf Multi-Task-Lernen, damit der Prior für verschiedene Aufgaben angepasst werden kann.
Eine maßgeschneiderte Version von Adam für das Training von DNNs, die die Generalisierungslücke zwischen Adam und SGD schließt.
Vorschlagen einer Variante des ADAM-Optimierungsalgorithmus, bei der die Gewichte jeder versteckten Einheit durch Batch-Normalisierung normalisiert werden.
Erweiterung des Adam-Optimierungsalgorithmus, um die Aktualisierungsrichtung zu erhalten, indem die Lernrate für die eingehenden Gewichte einer versteckten Einheit gemeinsam unter Verwendung der L2-Norm des Gradientenvektors angepasst wird.
Wir zeigen, wie wir die Nachfolgerepräsentation verwenden können, um Eigenoptionen in stochastischen Domänen aus rohen Pixeln zu entdecken. Eigenoptionen sind Optionen, die gelernt werden, um in den latenten Dimensionen einer gelernten Darstellung zu navigieren.
Erweitert die Idee der Eigenoptionen auf Bereiche mit stochastischen Übergängen und in denen Zustandsmerkmale gelernt werden.
Zeigt die Äquivalenz zwischen Proto-Wertfunktionen und Nachfolgedarstellungen und leitet die Idee der Eigenoptionen als Mechanismus der Optionsfindung ab.
Das Papier knüpft an frühere Arbeiten von Machado et al. (2017) an, die zeigen, wie Proto-Wert-Funktionen verwendet werden können, um Optionen, sogenannte "Eigenoptionen", zu definieren.
Wir bieten verbesserte obere Schranken für die Anzahl der linearen Regionen, die in der Netzwerkexpressivität verwendet werden, und einen hocheffizienten Algorithmus (mit exakter Zählung), um probabilistische untere Schranken für die tatsächliche Anzahl der linearen Regionen zu erhalten.
Beitrag zur Untersuchung der Anzahl linearer Regionen in neuronalen Netzen von RELU durch Verwendung eines approximativen probabilistischen Zählalgorithmus und einer Analyse.
Aufbauen auf früheren Arbeiten, die die Zählung linearer Regionen in tiefen neuronalen Netzen untersuchten, und verbesserern der zuvor vorgeschlagene Obergrenze durch Änderung der Dimensionalitätsbeschränkung.
Die Arbeit befasst sich mit der Ausdruckskraft eines stückweise linearen neuronalen Netzes, das durch die Anzahl der linearen Regionen der modellierten Funktion gekennzeichnet ist, und nutzt probabilistische Algorithmen, um die Grenzen schneller zu berechnen und engere Grenzen zu beweisen.
Ein biologisch inspirierter Arbeitsspeicher, das in rekurrente visuelle Aufmerksamkeitsmodelle integriert werden kann, um den neuesten Stand der Technik zu erreichen.
Einführung einer neuen Netzarchitektur, die sich am visuell-aufmerksamen Arbeitsspeicher orientiert, und Anwendung auf Klassifizierungsaufgaben findest, sowie Verwendung als generatives Modell.
Die Arbeit erweitert das rekurrente Aufmerksamkeitsmodell um ein neuartiges Hebb-Rosenblatt-Arbeitsspeichermodell und erzielt konkurrenzfähige Ergebnisse auf MNIST.
Die Arbeit verwendet variational Auto-Encoding und Netzwerk Konditionierung für Übertragung musikalischer Klangfarben, wir entwickeln und verallgemeinern unsere Architektur für Many-to-Many Instrumenten Transfers zusammen mit Visualisierungen und Bewertungen.
Vorschlagen eines modulierten variationalen Auto-Encoders für die Übertragung musikalischer Klangfarben durch Ersetzen des üblichen adversarial Übersetzungskriteriums durch eine Maximum Mean Discrepancy.
Beschreibt ein Many-to-many Modell für die Übertragung musikalischer Klangfarben, das auf den jüngsten Entwicklungen im Bereich der Übertragung von Bereichen und Stilen aufbaut.
Vorschlag eines hybriden VAE-basierten Modells zur Übertragung von Klangfarben auf Aufnahmen von Musikinstrumenten.
Wir untersuchen das Verhalten von gewichtsgebundenen mehrschichtigen einfachen Autoencodern unter der Annahme zufälliger Gewichte. Durch eine exakte Charakterisierung im Grenzbereich großer Dimensionen zeigt unsere Analyse interessante Phasenübergangsphänomene.
Eine theoretische Analyse von Autoencodern mit zwischen Encoder und Decoder gebundenen Gewichten (weight-tied) mittels Mean Field Analysis.
Analyse der Leistungen von gewichteten gebundenen Autoencodern auf der Grundlage der jüngsten Fortschritte bei der Analyse hochdimensionaler statistischer Probleme und insbesondere des Message-Passing-Algorithmus.
In diesem Beitrag werden Auto-Encoder unter verschiedenen Annahmen untersucht und es wird aufgezeigt, dass dieses Modell eines zufälligen Auto-Encoders elegant und rigoros mit eindimensionalen Gleichungen analysiert werden kann.
Inspiriert von früheren Arbeiten zu Sliced-Wasserstein-Autoencodern (SWAE) und Kernel-Glättung konstruieren wir ein neues generatives Modell - den Cramer-Wold AutoEncoder (CWAE).
In dieser Arbeit wird eine WAE-Variante vorgeschlagen, die auf einem neuen statistischen Abstand zwischen der kodierten Datenverteilung und der latenten Prioritätsverteilung beruht.
Vorstellen einer Variation des Wasserstein-AudoEncoders, eine neuartige regulierte Auto-Encoder Architektur, die eine spezifische Wahl der Divergenzstrafe vorschlägt.
In diesem Beitrag wird der Cramer-Wold-Autoencoder vorgeschlagen, der den Cramer-Wold-Abstand zwischen zwei Verteilungen auf der Grundlage des Cramer-Wold-Theorems verwendet.
Wir verwenden einen GAN-Diskriminator, um am Ausgang des GAN-Generators ein annäherndes Rückweisungsstichprobenverfahren durchzuführen.
 Vorschlagen eines Algorithmus für die Rückweisung von Stichproben aus dem GAN-Generator.
In diesem Beitrag wird ein Post-Processing-Rejection-Sampling-Verfahren für GANs vorgeschlagen, das so genannte Discriminator Rejection Sampling, mit dem gute Beispiele aus dem GAN-Generator herausgefiltert werden können.
Eine einfache und schnelle Methode zur Extraktion visueller Merkmale aus Convolutional Neural Networks.
Vorschlagen eines schnellen Wegs zum Erlernen von Convolutional Features, die später mit jedem Klassifikator verwendet werden können, indem eine geringere Anzahl von Trainings Epocs und spezifische Zeitplanverzögerungen der Lernrate verwendet werden.
Verwenden eines Schema zum Abklingen der Lernrate, das relativ zur Anzahl der beim Training verwendeten Epochen festgelegt ist, und extrahieren der Ausgabe der vorletzten Schicht als Merkmale, um einen herkömmlichen Klassifikator zu trainieren.
Wir bieten neue Einblicke und Interpretationen von RNNs aus der Perspektive der max-affinen Spline-Operatoren.
Schreibt die Gleichungen des Elman RNN in Form von sogenannten max-affine Spline-Operatoren um.
Bereitstellen eines neuen Ansatz zum Verständnis von RNNs mit Max-Affine-Spline-Operatoren (MASO), indem sie mit stückweise affinen und konvexen MASOs Aktivierungen umgeschrieben werden.
Die Autoren bauen auf der Max-Affine-Spline-Operator Interpretation einer umfangreichen Klasse von tiefen Netzen auf und konzentrieren sich dabei auf rekurrente neuronale Netze, bei denen Störungen im anfänglichen verborgenen Zustand als Regularisierung dient.
Wir skalieren Neuronale Theorembeweiser auf große Datensätze, verbessern den Regel-Lernprozess und erweitern ihn, um gemeinsam Schlussfolgerungen über Text und Wissensdatenbanken zu ziehen.
Vorschlagen einer Erweiterung des Systems der neuronalen Theorembeweiser, die die wichtigsten Probleme dieser Modelle angeht, indem die zeitliche und räumliche Komplexität der Modelle verringert wird.
Skalierung von NTPs durch approximierte Suche nach nächsten Nachbarn über Fakten und Regeln während der Vereinheitlichung und Vorschlag zur Parametrisierung von Prädikaten mit Hilfe von Aufmerksamkeit über bekannte Prädikate.
Verbessern des zuvor vorgeschlagenen Ansatz des Neuronalen Theoremprüfers durch die Verwendung der nächste Nachbarn Suche.
Verallgemeinerung der zwischen Bildpaaren erlernten Beziehungen anhand einer kleinen Anzahl von Trainingsdaten auf bisher ungesehene Bildtypen mit Hilfe eines erklärbaren dynamischen Systemmodells, Reservoir Computing, und einer biologisch plausiblen Lerntechnik auf der Grundlage von Analogien.
Behauptet Ergebnisse der "kombinierten Transformationen" im Zusammenhang mit RC unter Verwendung eines Echo-State-Netzes mit Standard-Tanh-Aktivierungen zu erzielen, mit dem Unterschied, dass keine rekurrenten Gewichte trainiert werden.
Neuartige Methode zur Klassifizierung verschiedener Verteilungen von MNIST-Daten.
Die Arbeit verwendet ein Echo-State-Netzwerk, um zu lernen, Bildtransformationen zwischen Bildpaaren in eine von fünf Klassen zu klassifizieren.
Wir stellen Generative Adversarial Privacy and Fairness (GAPF) vor, ein datengesteuertes Verfahren zum Erlernen privater und fairer Repräsentationen mit zertifizierten Garantien für Privatsphäre und Fairness.
In diesem Beitrag wird ein GAN-Modell verwendet, um einen Überblick über die mit dem Private/Fair Representation Learning (PRL) verbundenen Arbeiten zu geben.
In diesem Beitrag wird ein kontradiktorischer Ansatz für private und faire Repräsentationen durch erlernte Verzerrung von Daten vorgestellt, der die Abhängigkeit von sensiblen Variablen minimiert, während der Grad der Verzerrung begrenzt ist.
Die Autoren beschreiben einen Rahmen für das Erlernen einer demografischen Paritätsrepräsentation, die zum Trainieren bestimmter Klassifikatoren verwendet werden kann.
Wir stellen Metriken und einen optimalen Angriff für die Bewertung von Modellen vor, die sich mit Hilfe von Vertrauensschwellen gegen gegnerische Beispiele verteidigen.
In dieser Arbeit wird eine Familie von Angriffen auf Vertrauensschwellenalgorithmen vorgestellt, die sich hauptsächlich auf Bewertungsmethoden konzentrieren.
Vorschlagen einer Bewertungsmethode für Verteidigungsmodelle mit Vertrauensschwellenwerten und einen Ansatz zur Generierung von adversarial Beispielen durch Auswahl der falschen Klasse mit dem größten Vertrauen bei gezielten Angriffen.
In dem Beitrag wird eine Bewertungsmethode für die Beurteilung von Angriffen auf Vertrauensschwellenverfahren vorgestellt und eine neue Art von Angriff vorgeschlagen.
Eine neuartige Methode zum Lernen mit spärlicher Belohnung unter Verwendung von adversarialer Belohnungsumetikettierung.
Vorschlagen der Nutzung einer wettbewerbsorientierten Multi-Agenten Umgebung zur Förderung der Erkundung und zeigen, dass CER + HER > HER ~ CER.
Vorschlag für eine neue Methode zum Lernen aus spärlichen Belohnungen in modellfreien Reinforcement Learning Umgebungen und zur Verdichtung von Belohnungen.
Um die spärlichen Belohnungsprobleme anzugehen und die Exploration in RL-Algorithmen zu fördern, schlagen die Autoren eine Relabeling-Strategie namens Competitive Experience Reply (CER) vor.
Der Aufbau eines TTS-Modells mit Gaussian Mixture VAEs ermöglicht eine feinkörnige Steuerung des Sprechstils, der Geräuschbedingungen und mehr.
Beschreibt das konditionierte GAN-Modell zur Erzeugung von sprecherkonditionierten Mel-Spektren durch Erweiterung des z-Raums entsprechend der Identifizierung.
In dieser Arbeit wird ein zweischichtiges Modell latenter Variablen vorgeschlagen, um eine entwirrte latente Repräsentation zu erhalten, die eine feinkörnige Kontrolle über verschiedene Attribute ermöglicht.
In diesem Beitrag wird ein Modell vorgeschlagen, das nicht-annotierte Attribute wie Sprechstil, Akzent, Hintergrundgeräusche usw. kontrollieren kann.
Ermöglichung der Zählung von Modellen für die visuelle Beantwortung von Fragen durch die Behandlung sich überschneidender Objektvorschläge.
In dieser Arbeit wird eine von Hand entworfene Netzwerkarchitektur auf einem Graphen von Objektvorschlägen vorgeschlagen, um eine weiche nichtmaximale Unterdrückung durchzuführen, um die Objektanzahl zu erhalten.
Konzentriert sich auf ein Zählproblem bei der Beantwortung visueller Fragen unter Verwendung eines Aufmerksamkeitsmechanismus und schlägt einen differenzierbaren Zählkomplex vor, der explizit die Anzahl der Objekte zählt.
Diese Arbeit befasst sich mit dem Problem der Objektzählung bei der Beantwortung visueller Fragen und schlägt mehrere Heuristiken vor, um die richtige Zählung zu finden.
Ein einfacher und trainingsfreier Ansatz für Satzeinbettungen mit konkurrenzfähiger Leistung im Vergleich zu ausgefeilten Modellen, die entweder große Mengen an Trainingsdaten oder eine lange Trainingszeit benötigen.
Vorstellung eines neuen trainingsfreien Verfahrens zur Erzeugung von Satzeinbettungen mit systematischer Analyse
Vorschlag einer neuen, auf Geometrie basierenden Methode zur Satzeinbettung aus Worteinbettungsvektoren, indem die Neuheit, Bedeutung und Korpuseinzigartigkeit jedes Wortes quantifiziert wird.
In diesem Beitrag wird die Satzeinbettung auf der Grundlage einer orthogonalen Zerlegung des aufgespannten Raums durch Worteinbettungen untersucht.
Wir schlagen neuartige Erweiterungen von Prototypischen Netzen vor, die durch die Möglichkeit ergänzt werden, bei der Erstellung von Prototypen unmarkierte Beispiele zu verwenden.
Diese Arbeit ist eine Erweiterung eines prototypischen Netzwerks, das die Verwendung von unmarkierten Beispielen für das Training jeder Episode berücksichtigt.
Untersucht das Problem der halbüberwachten Few-Shot Klassifizierung durch Erweiterung der prototypischen Netze in die Umgebung des halbüberwachten Lernens mit Beispielen aus Ablenkungsklassen.
Erweitert das Prototypische Netzwerk auf die halb-überwachte Einstellung, indem es Prototypen unter Verwendung zugewiesener Pseudo-Labels aktualisiert, mit Distraktoren umgeht und Proben anhand des Abstands zu den ursprünglichen Prototypen gewichtet.
Wir beweisen theoretisch, dass lineare Interpolationen für die Analyse von trainierten impliziten generativen Modellen ungeeignet sind. 
Untersuchung des Problems, wann der lineare Interpolant zwischen zwei Zufallsvariablen der gleichen Verteilung folgt, in Bezug auf die vorherige Verteilung eines impliziten generativen Modells.
In dieser Arbeit geht es um die Frage, wie man bei einem latenten Variablenmodell im latenten Raum interpolieren kann.
Erkennung von Lungenknoten anhand von Projektionsdaten anstelle von Bildern.
DNNs werden für die patchbasierte Erkennung von Lungenknoten in CT-Projektionsdaten verwendet.
Gemeinsame Modellierung von Computertomographie-Rekonstruktion und Läsionserkennung in der Lunge durch Training der Zuordnungen von Rohsinogrammen zu Erkennungsergebnissen in einer Ende-zu-Ende Weise.
Präsentiert ein Ende-zu-Ende Training einer CNN-Architektur, die CT-Bildsignalverarbeitung und Bildanalyse kombiniert.
Wir evaluieren quantitativ und qualitativ tiefgehende, auf Reinforcement Learning basierende Navigationsmethoden unter einer Vielzahl von Bedingungen, um die Frage zu beantworten, inwieweit sie in der Lage sind, klassische Pfadplaner und Zordnungs Algorithmen zu ersetzen.
Evaluierung eines Deep RL-basierten Modells in Trainingslabyrinthen durch Messung der wiederholten Latenzzeit zum Ziel und Vergleich mit dem kürzesten Weg.
Wir evaluieren das Lernen heteroskedastischer Rauschmodelle mit verschiedenen Differentiable Bayes Filtern.
Vorschlag, heteroskedastische Rauschmodelle aus Daten zu lernen, indem die Vorhersagewahrscheinlichkeit von Ende zu Ende durch differenzierbare Bayes'sche Filter und zwei verschiedene Versionen des unscented Kalman Filters optimiert wird.
Überarbeitung der Bayes-Filter und Bewertung des Nutzens des Trainings von Beobachtungs- und Prozessrauschmodellen bei gleichzeitiger Beibehaltung aller anderen Modelle.
In diesem Beitrag wird eine Methode zum Erlernen und Verwenden von zustands- und beobachtungsabhängigem Störungen in herkömmlichen Bayes'schen Filteralgorithmen vorgestellt. Der Ansatz besteht darin, ein neuronales Netzmodell zu konstruieren, das als Eingabe die rohen Beobachtungsdaten nimmt und eine kompakte Darstellung und eine zugehörige diagonale Kovarianz erzeugt.
Wir überdenken die Art und Weise, wie Informationen im Wissensgraphen effizienter genutzt werden können, um die Leistung bei der Zero-Shot Learning Aufgabe zu verbessern, und schlagen zu diesem Zweck ein Dense Graph Propagation (DGP) Modul vor.
Die Autoren schlagen eine Lösung für das Problem der Überglättung in Graph Convolutional Networks vor, indem sie eine dichte Ausbreitung zwischen allen verwandten Knoten, gewichtet nach dem gegenseitigen Abstand, ermöglichen.
Vorschlagen eines neuartigen Graph Convolutional Networks, um das Problem der Zero-Shot Klassifizierung anzugehen, indem relationale Strukturen zwischen Klassen als Input für Graph Convolutional Networks verwendet werden, um Klassifizierer für ungesehene Klassen zu lernen.
Eine kapselbasierte semantische Segmentierung, bei der die Wahrscheinlichkeiten der Klassenbezeichnungen durch die Kapselpipeline zurückverfolgt werden. 
Die Autoren stellen einen Rückverfolgungsmechanismus vor, um die unterste Ebene der Kapseln mit ihren jeweiligen Klassen zu verbinden.
Vorschlagen einer Rückverfolgungsschicht für Kapselnetze, um eine semantische Segmentierung vorzunehmen, und ausdrückliches nutzen der Teil-Ganzes-Beziehung in den Kapselschichten.
Vorschlagen einer Rückverfolgungsmethode, die auf dem CapsNet-Konzept von Sabour basiert, um parallel zur Klassifizierung eine semantische Segmentierung durchzuführen.
Wir betrachten SGD als eine Trajektorie im Raum der Wahrscheinlichkeitsmaße, zeigen ihre Verbindung zu Markov-Prozessen, schlagen ein einfaches Markov-Modell des SGD-Lernens vor und vergleichen es experimentell mit SGD unter Verwendung informationstheoretischer Größen. 
Konstruiert eine Markov-Kette, die einem verkürzten Pfad in der TV-Metrik auf P folgt, und zeigt, dass die Trajektorien von SGD und \alpha-SMLC eine ähnliche bedingte Entropie aufweisen.
Untersuchung des Verlaufs von H(\hat{y}) gegenüber H(\hat{y}|y) auf der Informationsebene für stochastische Gradientenabstiegsmethoden zum Training neuronaler Netze.
Beschreibt SGD unter dem Gesichtspunkt der Verteilung p(y',y), wobei y ein (möglicherweise verfälschtes) wahres Klassenzeichen und y' eine Modellvorhersage ist.
In diesem Beitrag wird eine Methode zur Automatisierung des Entwurfs von stochastischen Gradienten-MCMC-Vorschlägen unter Verwendung eines Meta-Lernansatzes vorgeschlagen. 
Stellt einen Meta-Lernansatz vor, um automatisch MCMC-Sampler auf der Grundlage der Hamilton'schen Dynamik zu entwickeln, die bei Problemen, die den Trainingsproblemen ähneln, schneller mischen.
Parametrisierung von Diffusions- und Curl-Matrizen durch neuronale Netze und Meta-Lernen und Optimierung eines sg-mcmc-Algorithmus. 
Techniken zur Bewertung der Bildqualität verbessern das Training und die Bewertung energiebasierter generativer adversarischer Netzwerke.
Vorschlagen einer energiebasierten Formulierung des BEGAN-Modells und modifizieren des Modells, so dass es einen auf der Bewertung der Bildqualität basierenden Begriff enthält.
Schlägt einige neue Energiefunktionen im BEGAN (boundary equilibrium GAN framework) vor, darunter l_1 score, Gradient magnitude similarity score und chrominance score.
Wir stellen eine einfache Variante der Momentum-Optimierung vor, die in der Lage ist, klassisches Momentum, Nesterov und Adam bei Deep-Learning-Aufgaben mit minimalem Hyperparameter-Tuning zu übertreffen.
Einführung einer Impulsvariante, bei der mehrere Geschwindigkeiten mit unterschiedlichen Dämpfungskoeffizienten zusammengefasst werden, was die Schwingungen deutlich verringert.
Vorschlag einer aggregierten Impulsmethode für die gradientenbasierte Optimierung durch Verwendung mehrerer Geschwindigkeitsvektoren mit unterschiedlichen Dämpfungsfaktoren anstelle eines einzelnen Geschwindigkeitsvektors zur Verbesserung der Stabilität.
Die Autoren kombinieren mehrere Aktualisierungsschritte miteinander, um ein aggregiertes Momentum zu erreichen, und zeigen, dass es stabiler ist als andere Momentum-Methoden.
Wir führen die Recurrent Discounted Unit ein, die die Aufmerksamkeit in linearer Zeit auf eine beliebig lange Sequenz anwendet.
In diesem Papier wird die Recurrent Discounted Attention (RDA) vorgeschlagen, eine Erweiterung des Recurrent Weighted Average (RWA) durch Hinzufügen eines Diskontierungsfaktors.
Erweitert den rekurrenten Gewichtsdurchschnitt, um die Beschränkungen der ursprünglichen Methode zu überwinden und gleichzeitig ihre Vorteile beizubehalten, und schlägt die Methode vor, Elman-Netze als Basis-RNN zu verwenden.
Es ist möglich, eine null-zentrierte Gauß-Verteilung über die Gewichte eines neuronalen Netzes zu lernen, indem man nur die Varianzen lernt, und das funktioniert erstaunlich gut.
Diese Arbeit untersucht die Auswirkungen des Mittelwerts des Variationsposteriores und schlägt eine Varianzschicht vor, die nur die Varianz zur Speicherung von Informationen verwendet.
Studien über neuronale Netze mit Varianz, die das Posterior von neuronalen Netzen nach Bayes mit Gaußverteilungen mit Null-Mittelwert approximieren.
Wir erstellen ein Netzwerk von Graph Convolution Networks, wobei wir jedes mit einer anderen Potenz der Adjazenzmatrix füttern und alle ihre Repräsentationen zu einem Klassifizierungs-Subnetzwerk kombinieren und so den neuesten Stand der Technik bei der halbüberwachten Knotenklassifizierung erreichen.
Vorschlagen eines neuen Netzwerks von GCNs mit zwei Ansätzen: eine vollständig verbundene Schicht auf gestapelten Merkmalen und einen Aufmerksamkeitsmechanismus, der skalare Gewichte pro GCN verwendet.
Stellt ein Netz von Graph Convolutional Networks vor, das mit Hilfe von Random-Walk-Statistiken Informationen von nahen und entfernten Nachbarn im Graphen extrahiert.
Ein schneller Pruning Algroithmus für vollständig verbundene DNN-Schichten mit theoretischer Analyse der Verschlechterung des Generalisierungsfehlers.
Stellt einen günstigen Pruning Algorithmus für dichte Schichten von DNNs vor.
Schlägt eine Lösung für das Problem des Pruning von DNNs vor, indem die Net-trim Zielfunktion als eine Difference of convex (DC) Funktion dargestellt wird.
Wir schlagen ein neues hybrides temporales Netzwerk vor, das die beste Leistung bei der Segmentierung von Videoaktionen in drei öffentlichen Datensätzen erzielt.
Erörtert das Problem der Segmentierung von Handlungen in langen Videos, die bis zu 10 Minuten lang sein können, unter Verwendung einer zeitlichen Convolutional Encoder-Decoder Architektur.
Schlägt eine Kombination aus temporalem Convolutional und rekurrentem Netzwerk für die Segmentierung von Videoaktionen vor.
In dieser Arbeit wird vorgeschlagen, Wissen von einem tiefen Modell auf ein oberflächliches Modell zu übertragen, indem Merkmale Schritt für Schritt nachgeahmt werden.
Erläutert eine Methode zur stufenweisen Wissensvermittlung unter Verwendung verschiedener Netzstrukturen.
In diesem Beitrag wird vorgeschlagen, ein Netz in mehrere Teile zu unterteilen und jeden Teil nacheinander zu destillieren, um die Destillationsleistung in tiefen Lehrernetzen zu verbessern.
Verbesserungen in der Robustheit von adversarialem Training sowie nachweisbare Robustheitsgarantien werden durch die Ergänzung von adversarialem Training mit einer nachvollziehbaren Lipschitz-Regularisierung erreicht.
Untersucht die Erweiterung des Trainingsverlustes mit einem zusätzlichen Gradienten-Regularisierungsterm, um die Robustheit der Modelle gegenüber ungünstigen Beispielen zu verbessern.
Durch einen Trick wird der adversarial Verlust durch einen vereinfacht, bei dem die gegnerische Störung in geschlossener Form erscheint.
Ein Modell, das Eliminierung und Auswahl zur Beantwortung von Multiple-Choice-Fragen kombiniert.
Erläutert den Gated Attention Reader und fügt Gates hinzu, die auf der Eliminierung von Antworten beim Multiple-Choice-Leseverständnis basieren.
Diese Arbeit schlägt die Verwendung eines Eliminationsgatters in Modellarchitekturen für Leseverständnisaufgaben vor, erzielt aber keine State-of-the-Art-Ergebnisse.
In diesem Beitrag wird ein neues Modell für das Leseverständnis mit mehreren Auswahlmöglichkeiten vorgestellt, das auf der Idee basiert, dass einige Optionen eliminiert werden sollten, um bessere Darstellungen der Passagen/Fragen zu erhalten.
Wir haben ein neuartiges probabilisitisches rekursives Schlussfolgerungsmodell (PR2) für Multi-Agenten Aufgaben mit tiefem Reinforcement Learning vorgeschlagen.
Vorschlagen eines neuen Ansatz für vollständig dezentralisiertes Training in Multi-Agenten Reinforcement Learning.
Befasst sich mit dem Problem, RL-Agenten mit rekursiven Argumentationsfähigkeiten in einem Multi-Agenten Umfeld auszustatten, basierend auf der Hypothese, dass rekursive Argumentation für sie vorteilhaft ist, um zu nicht-trivalen Gleichgewichten zu konvergieren.
Die Arbeit stellt eine dezentralisierte Trainingsmethode für Multi-Agenten Reinforcement Learning vor, bei der die Agenten die Strategien anderer Agenten ableiten und die abgeleiteten Modelle zur Entscheidungsfindung verwenden. 
Ein neuer Algorithmus zur Reduzierung des Kommunikations Overheads bei verteiltem Deep Learning durch Unterscheidung von eindeutigen Gradienten.
Vorschlag einer varianzbasierten Gradientenkomprimierung zur Reduzierung des Kommunikationsaufwands beim verteilten Deep Learning.
Schlägt eine neue Methode zur Komprimierung von Gradientenaktualisierungen für verteilte SGD vor, um die Gesamtausführung zu beschleunigen.
Einführung einer varianzbasierten Gradientenkompressionsmethode für effizientes verteiltes Training neuronaler Netze und Messung der Mehrdeutigkeit.
Ein neues unbeaufsichtigtes Verfahren zur Anpassung von tiefen Bereichen, das Korrelationsabgleich und Entropieminimierung effizient vereint.
Verbessert den Korrelationsabgleichsansatz zur Domänenanpassung, indem der euklidische Abstand durch den geodätischen Log-Euklidischen Abstand zwischen zwei Kovarianzmatrizen ersetzt wird und die Ausgleichskosten automatisch anhand der Entropie auf der Zieldomäne ausgewählt werden.
Vorschlag für einen Korrelationsabgleich mit minimaler Entropie, einen unüberwachten Algorithmus zur Bereichsanpassung, der Entropieminimierung und Korrelationsabgleichsmethoden miteinander verbindet.
Wir schlagen eine Variante des Backpropagation-Algorithmus vor, bei der die Gradienten durch Konzeptoren gegen eine Verschlechterung der zuvor gelernten Aufgaben abgeschirmt werden.
In diesem Beitrag wird der Begriff der Konzeptoren, eine Art Regularisierer, verwendet, um das Vergessen beim kontinuierlichen Lernen beim Training neuronaler Netze für sequenzielle Aufgaben zu verhindern.
Es wird eine Methode zum Erlernen neuer Aufgaben vorgestellt, ohne dass frühere Aufgaben beeinträchtigt werden, indem Konzeptoren verwendet werden.
Analysieren Sie den Grund dafür, dass generative Modelle für neuronale Reaktionen universelle Antworten bevorzugen; schlagen Sie eine Methode vor, um dies zu vermeiden.
Untersucht das Problem der universellen Antworten, das die neuronalen Seq2Seq-Generierungsmodelle plagt.
Die Arbeit untersucht die Verbesserung der neuronalen Antwort Generierungsaufgabe durch die Vernachlässigung der gemeinsamen Antworten mit Modifikation der Verlustfunktion und Präsentation der gemeinsamen/universellen Antworten während der Trainingsphase.
Wir nutzen die syntaktische Struktur des Quellcodes, um natürlichsprachliche Sequenzen zu erzeugen.
Vorstellen einer Methode zur Erzeugung von Sequenzen aus Code durch Parsing und Erstellung eines Syntaxbaums.
Diese Arbeit stellt eine AST-basierte Kodierung für Programmiercode vor und zeigt deren Effektivität in den Aufgaben der extremen Codezusammenfassung und des Code Captioning.
In diesem Beitrag wird ein neues Code-zu-Sequenz-Modell vorgestellt, das die syntaktische Struktur von Programmiersprachen nutzt, um Quellcode-Schnipsel zu kodieren und sie anschließend in natürliche Sprache zu dekodieren.
Wir verbessern CNNs mit einem neuartigen Aufmerksamkeitsmechanismus für feinkörnige Erkennung. Bei 5 Datensätzen wird eine überragende Leistung erzielt.
Beschreibt einen neuartigen Aufmerksamkeitsmechanismus, der auf die feinkörnige Erkennung angewandt wird und die Erkennungsgenauigkeit der Grundlinie konsequent verbessert.
In diesem Beitrag wird ein Feed-Forward Attention Mechanismus für die feinkörnige Bildklassifizierung vorgeschlagen.
In diesem Beitrag wird ein interessanter Aufmerksamkeitsmechanismus für die feinkörnige Bildklassifizierung vorgestellt.
Wir ersetzen normale Convolutions durch adaptive Convolutions, um GAN-Generatoren zu verbessern.
Es wird vorgeschlagen, die Convolutions im Generator durch einen adaptiven Convolution Block zu ersetzen, der lernt, Convolution Gewichte und Verzerrungen von Upsampling-Operationen adaptiv pro Pixelposition zu erzeugen.
Verwendet Adaptive Convolution im Kontext von GANs mit einem Block namens AdaConvBlock, der die reguläre Convolution ersetzt. Dies gibt mehr lokalen Kontext pro Kernelgewicht, so dass es lokal flexible Objekte erzeugen kann.
Wir führen groß angelegte Experimente durch, um zu zeigen, dass eine einfache Online-Variante der Destillation uns helfen kann, das Training verteilter neuronaler Netze auf mehr Maschinen zu skalieren.
Vorschlag einer Methode zur Skalierung des verteilten Trainings, die über die derzeitigen Grenzen des stochastischen Mini-Batch Gradientenabstiegs hinausgeht.
Vorschlag für eine Online-Destillationsmethode, die so genannte Co-Destillation, bei der zwei verschiedene Modelle so trainiert werden, dass sie mit den Vorhersagen des anderen Modells übereinstimmen, um den eigenen Verlust zu minimieren.
Einführung einer Online-Destillationstechnik zur Beschleunigung herkömmlicher Algorithmen für das Training groß angelegter verteilter neuronaler Netze.
Wir stellen einen Algorithmus zur Beschleunigung des SVM-Trainings auf massiven Datensätzen vor, indem wir kompakte Darstellungen konstruieren, die eine effiziente und nachweislich ungefähre Inferenz ermöglichen.
Untersucht den Ansatz der Kernmenge für SVM und zielt darauf ab, eine kleine Menge gewichteter Punkte so zu wählen, dass sich die Verlustfunktion über die Punkte nachweislich derjenigen über den gesamten Datensatz annähert.
Die Arbeit schlägt eine auf Wichtigkeitssampling basierende Coreset-Konstruktion zur Darstellung großer Trainingsdaten für SVMs vor.
Wir beweisen eine nicht-konvexe Konvergenzrate für die stochastische Gradientenmethode mit Vorzeichen. Der Algorithmus hat Verbindungen zu Algorithmen wie Adam und Rprop sowie zu Gradientenquantisierungsverfahren, die beim verteilten maschinellen Lernen verwendet werden.
Bereitstellung einer Konvergenzanalyse des Sign SGD-Algorithmus für nicht-konvexe Fälle.
Die Arbeit untersucht einen Algorithmus, der das Vorzeichen der Gradienten anstelle der tatsächlichen Gradienten für das Training tiefer Modelle verwendet.
Wir stellen eine transparente Middleware für die Beschleunigung neuronaler Netze mit eigener Compiler-Engine vor, die eine bis zu 11,8-fache Beschleunigung auf CPUs und eine 2,3-fache Beschleunigung auf GPUs erreicht.
In diesem Beitrag wird eine transparente Middleware-Schicht für die Beschleunigung neuronaler Netze vorgeschlagen und es werden einige Beschleunigungsergebnisse auf einfachen CPU- und GPU-Architekturen erzielt.
Wir schlagen eine konische Convolution und die 2D-DFT vor, um die Rotationsäquivarianz in einem neuronalen Netz zu kodieren.
Im Zusammenhang mit der Bildklassifikation wird in diesem Papier eine Architektur für Cpnvolutional Neural Networks mit rotationsäquivarianten Merkmalskarten vorgeschlagen, die schließlich durch Verwendung des Betrags der diskreten 2D-Fouriertransformation (DFT) rotationsinvariant gemacht werden.
Die Autoren bieten ein rotationsinvariantes neuronales Netzwerk durch die Kombination von konischer Convolution und 2D-DFT.
Wir stellen ein neuartiges Feed-Forward System zur Erzeugung visueller Metamere vor.
Vorschlag eines NeuroFovea Modells für die Erzeugung von Fixpunkt-Metameren unter Verwendung eines Stiltransfer-Ansatzes über eine Encoder-Decoder Stilarchitektur.
Eine Analyse des Metamerismus und ein Modell zur schnellen Herstellung von Metameren, die für die experimentelle Psychophysik und andere Bereiche von Nutzen sind.
Die Arbeit schlägt eine schnelle Methode zur Erzeugung visueller Metamere vor für physikalisch unterschiedliche Bilder, die nicht von einem Original unterschieden werden können mittels foveatierter, schneller, beliebiger Stilübertragung.
Wir zeigen, dass das Training von Feedforward-Relu-Netzen mit einem schwachen Regularizer zu einer maximalen Marge führt und analysieren die Auswirkungen dieses Ergebnisses.
Erforscht die Margentheorie für neuronale Netze und zeigt, dass die maximale Marge mit der Größe des Netzes monoton zunimmt.
Diese Arbeit untersucht die implizite Verzerrung von Minimierern eines regulierten Kreuzentropieverlustes eines zweischichtigen Netzes mit ReLU-Aktivierungen und erhält eine obere Schranke für die Verallgemeinerung, die nicht mit der Netzgröße zunimmt.
Eine verteilte Architektur für Deep Reinforcement Learning im großen Maßstab, die parallele Datengenerierung nutzt, um den Stand der Technik beim Arcade Learning Environment Benchmark in einem Bruchteil der Trainingszeit früherer Ansätze zu verbessern.
Untersucht ein distirbuted Deep RL System, in dem Erfahrungen, anstatt Gradienten, zwischen den parallelen Arbeiten und dem zentralisierten Lernenden geteilt werden.
Ein paralleler Ansatz für das DQN-Training, der auf der Idee beruht, dass mehrere Akteure parallel Daten sammeln, während ein einzelner Lerner das Modell anhand von Erfahrungen aus dem zentralen Wiedergabespeicher trainiert.
Dieser Beitrag schlägt eine verteilte Architektur für Deep Reinforcement Learning im großen Maßstab vor, wobei der Schwerpunkt auf der Parallelisierung des Akteursalgorithmus im Prioritized Experience Replay Framework liegt.
Neuronale Architekturen, die Darstellungen von unregelmäßig beobachteten Signalen liefern und nachweislich eine Signalrekonstruktion ermöglichen.
Beweist, dass Convolutional Neural Networks mit Leaky-ReLU Aktivierungsfunktion nichtlineare Rahmen sind, mit ähnlichen Ergebnissen für ungleichmäßig abgetastete Zeitreihen.
In diesem Artikel werden neuronale Netze über Zeitreihen betrachtet und es wird gezeigt, dass die ersten Convolutional Filter zur Darstellung einer diskreten Wavelet-Transformation gewählt werden können.
Phrasenbasierte Aufmerksamkeitsmechanismen zur Zuweisung von Aufmerksamkeit auf Phrasen, die zusätzlich zu den bestehenden Token-to-Token-Attentionen Phrase-to-Token- und Phrase-to-Phrase-Attention-Alignments erreichen.
Die Arbeit stellt einen Aufmerksamkeitsmechanismus vor, der eine gewichtete Summe nicht nur über einzelne Token, sondern auch über Ngramme (Phrasen) berechnet.
Tiefe Netze liegen mit größerer Wahrscheinlichkeit falsch, wenn sie mit unerwarteten Daten getestet werden. Wir schlagen eine experimentelle Methodik vor, um das Problem zu untersuchen, und zwei Methoden, um die Fehlerwahrscheinlichkeit bei unbekannten Eingangsverteilungen zu verringern.
Es werden zwei Ideen zur Verringerung übermäßiger falscher Vorhersagen vorgeschlagen: "G-Distillation" eines Ensembles mit zusätzlichen unüberwachten Daten und Verringerung des Neuheitsvertrauens mit Hilfe eines Neuheitsdetektors.
Die Autoren schlagen zwei Methoden zur Schätzung der Klassifizierungssicherheit bei neuartigen, ungesehenen Datenverteilungen vor. Die erste Idee besteht darin, Ensemble-Methoden als Basis zu verwenden, um unsichere Fälle zu identifizieren, und dann Destillationsmethoden zu verwenden, um das Ensemble in ein einzelnes Modell zu reduzieren, das das Verhalten des Ensembles nachahmt. Die zweite Idee besteht darin, einen Neuheitsdetektor-Klassifikator zu verwenden und die Netzwerkausgabe nach dem Neuheitswert zu gewichten.
Wir beschreiben einen praktischen Optimierungsalgorithmus für tiefe neuronale Netze, der im Vergleich zu weit verbreiteten Algorithmen schneller arbeitet und bessere Modelle erzeugt.
Schlägt einen neuen Algorithmus vor, bei dem die Hess'sche Formel implizit verwendet wird und eine Motivation aus Potenzreihen verwendet wird.
Stellt einen neuen Algorithmus 2. Ordnung vor, der implizit Krümmungsinformationen verwendet, und zeigt die Intuition hinter den Näherungsschemata in den Algorithmen und validiert die Heuristiken in verschiedenen Experimenten.
Wir führen die Annäherung mit gebrochener Bitbreite ein und zeigen, dass sie erhebliche Vorteile hat.
Vorschlagen einer Methode zur Veränderung des Quantisierungsgrades in einem neuronalen Netz während der Vorwärtsausbreitungsphase.
Beibehaltung der Genauigkeit eines 2-Bit Networds bei Verwendung von weniger als 2-Bit Gewichten.
Mean Replacement ist eine effiziente Methode, um den Verlust nach dem Pruning zu verbessern, und auf Taylor-Approximation basierende Scoring-Funktionen funktionieren besser mit absoluten Werten. 
Vorschlag einer einfachen Verbesserung der Methoden für das Pruning von Einheiten unter Verwendung der "mittleren Ersetzung".
In diesem Beitrag wird eine Strategie zum Pruning des Mittelwerts vorgestellt und die absolutwertige Taylor-Erweiterung als Bewertungsfunktion für das Pruning verwendet.
 Vermeiden des posterior Collapse, indem die Rate nach unten begrenzt wird.
Vorstellung eines Ansatzes zur Verhinderung eines posterior Kollapses bei VAEs durch Begrenzung der Familie der Variationsannäherung auf das Posterior.
In dieser Arbeit wird eine Einschränkung für die Familie der Variationsposterioren eingeführt, so dass der KL-Term kontrolliert werden kann, um den Posterior-Kollaps in tiefen generativen Modellen wie VAEs zu bekämpfen.
Wir haben ein adaptives Batch-Momentum entwickelt, das im Vergleich zu Mini-Batch-Methoden nach dem Scannen derselben Datenepochen geringere Verluste erzielt und robuster gegenüber großen Schrittweiten ist.
Diese Arbeit befasst sich mit dem Problem der automatischen Abstimmung der Batch-Größe während des Deep-Learning-Trainings, und behauptet, Batch-adaptive SGD auf adaptive Dynamik zu erweitern und die Algorithmen auf komplexe neuronale Netz Probleme anzupassen.
In der Arbeit wird vorgeschlagen, einen Algorithmus zu verallgemeinern, der SGD mit adaptiven Losgrößen durchführt, indem der Nutzenfunktion ein Momentum hinzugefügt wird.
Wir verwenden Meta-Gradienten, um das Trainingsverfahren von tiefen neuronalen Netzen für Graphen anzugreifen.
Untersucht das Problem des Erlernens besserer vergifteter Graphparameter, die den Verlust eines neuronalen Graphennetzes maximieren können. 
Ein Algorithmus zur Veränderung der Graphenstruktur durch Hinzufügen/Löschen von Kanten, um die globale Leistung der Knotenklassifizierung zu verschlechtern, und die Idee, Meta-Learning zur Lösung des zweistufigen Optimierungsproblems einzusetzen.
Wir zeigen, dass modular strukturierte Modelle die beste systematische Verallgemeinerung bieten und dass ihre Ende-zu-Ende-Versionen nicht so gut verallgemeinern.
In diesem Beitrag wird die systemische Generalisierung zwischen modularen neuronalen Netzen und anderen generischen Modellen durch die Einführung eines neuen Datensatzes für räumliche Schlussfolgerungen bewertet.
Eine gezielte empirische Evaluierung der Generalisierung in Modellen für visuelles Denken, die sich auf das Problem der Erkennung von (Objekt, Relation, Objekt) Tripeln in synthetischen Szenen mit Buchstaben und Zahlen konzentriert.
Relationale Vorwärtsmodelle für das Lernen von Multi-Agenten machen genaue Vorhersagen über das zukünftige Verhalten der Agenten, sie erzeugen interpretierbare Repräsentationen und können innerhalb der Agenten verwendet werden.
Eine Möglichkeit, die Varianz beim modellfreien Lernen zu verringern, indem ein explizites Modell der Aktionen, die andere Agenten ausführen werden, erstellt wird, das eine graphische, netzartige Architektur verwendet. 
Vorhersage des Verhaltens von Multi-Agenten mit Hilfe eines relationalen Vorwärtsmodells mit einer rekurrenten Komponente, das zwei Basismodelle und zwei Ablationsmodelle übertrifft.
Die normalisierte Lösung des Gradientenabstiegs bei der logistischen Regression (oder ein ähnlich abnehmender Verlust) konvergiert bei trennbaren Daten langsam gegen die L2-Max-Margin-Lösung.
Das Papier bietet einen formalen Beweis dafür, dass der Gradientenabstieg auf dem logistischen Verlust sehr langsam gegen die harte SVM-Lösung konvergiert, wenn die Daten linear separierbar sind. 
Diese Arbeit konzentriert sich auf die Charakterisierung des Verhaltens der Log-Loss Minimierung auf linear trennbaren Daten und zeigt, dass Log-Loss, minimiert mit Gradientenabstieg, zur Konvergenz der Max-Margin Lösung führt.
Aufbauend auf früheren Arbeiten zur Domänengeneralisierung hoffen wir, einen Klassifikator zu entwickeln, der sich auch auf bisher unbekannte Domänen verallgemeinern lässt, selbst wenn während des Trainings keine Domänenidentifikatoren verfügbar sind.
Ein Ansatz zur Domänengeneralisierung, um semantische Informationen auf der Grundlage eines linearen Projektionsschemas von CNN- und NGLCM-Ausgabeschichten aufzudecken.
Die Arbeit schlägt einen unüberwachten Ansatz zur Identifizierung von Bildmerkmalen vor, die für Bildklassifizierungsaufgaben nicht aussagekräftig sind.
Wir schlagen eine Methode zum Erlernen der physischen Fahrzeugtarnung vor, um Objektdetektoren in der freien Natur anzugreifen. Wir finden unsere Tarnung effektiv und übertragbar.
Die Autoren untersuchen das Problem des Erlernens eines Tarnmusters, das, wenn es auf ein simuliertes Fahrzeug angewendet wird, verhindert, dass ein Objektdetektor es erkennt.
In diesem Beitrag geht es um adversariales Lernen für die Erkennung von Störfahrzeugen durch das Lernen von Tarnmustern.
Wir kombinieren differenzierbare Entscheidungsbäume mit überwachten variationalen Autoencodern, um die Interpretierbarkeit der Klassifizierung zu verbessern. 
Diese Arbeit schlägt ein hybrides Modell eines variationalen Autoencoders vor, der mit einem differenzierbaren Entscheidungsbaum und einem begleitenden Trainingsschema zusammengesetzt ist. Experimente zeigen die Klassifizierungsleistung des Baums, die negative Log-Likelihood-Leistung und die Interpretierbarkeit des latenten Raums.
In diesem Beitrag wird versucht, einen interpretierbaren und genauen Klassifikator zu erstellen, indem eine überwachte VAE und ein differenzierbarer Entscheidungsbaum kombiniert werden.
Ein praktischer und nachweislich garantierter Ansatz für das Training effizienter Klassifikatoren in Anwesenheit von Label-Verschiebungen zwischen Quell- und Zieldatensätzen.
Die Autoren schlagen einen neuen Algorithmus zur Verbesserung der Stabilität des Schätzverfahrens für die Klassenbedeutungsgewichtung in einem zweistufigen Verfahren vor.
Die Autoren betrachten das Problem des Lernens unter Label Shifts, bei denen sich die Labelanteile unterscheiden, während die Konditionalitäten gleich sind, und schlagen einen verbesserten Schätzer mit Regularisierung vor.
Ansatz zur Verbesserung der Klassifizierungsgenauigkeit bei Klassen im Heck.
Das Hauptziel dieser Arbeit ist es, einen ConvNet-Klassifikator zu erlernen, der für Klassen im hinteren Teil der Klassenhäufigkeitsverteilung bessere Leistungen erbringt.
Vorschlag für einen Bayes'schen Rahmen mit einem Gauß'schen Mischmodell, um das Problem der unausgewogenen Anzahl von Trainingsdaten aus verschiedenen Klassen bei Klassifizierungsanwendungen zu lösen.
Wir stellen eine Methode vor, mit der sich interessante Zustände für Agenten mit Reinforcement Learning synthetisieren lassen, um ihr Verhalten zu analysieren. 
In diesem Beitrag wird ein generatives Modell für visuelle Beobachtungen in RL vorgeschlagen, das in der Lage ist, Beobachtungen von Interesse zu generieren.
Ein Ansatz zur Visualisierung interessanter Zustände, der einen variationalen Autoencoder umfasst, der lernt, den Zustandsraum zu rekonstruieren, und einen Optimierungsschritt, der Konditionierungsparameter zur Erzeugung synthetischer Bilder findet.
Ein tiefes neuronales Netzwerk, das mit einer neuartigen Verlustfunktion trainiert wird, die Repräsentationen dafür erlernt, wann man sich enthalten sollte, was robustes Lernen in Gegenwart verschiedener Arten von Rauschen ermöglicht.
Eine neue Verlustfunktion für die Ausbildung eines tiefen neuronalen Netzes, die sich enthalten kann, mit der Leistung aus den Blickwinkeln bei Vorhandensein von strukturierten Störungen, bei Vorhandensein von unstrukturierten Störungen, und offene Welt Erkennung betrachtet.
Dieses Manuskript stellt tiefe abstinente Klassifizierer vor, die den klassenübergreifenden Entropieverlust mit einem Abstinenzverlust modifizieren, der dann auf gestörte Bildklassifizierungsaufgaben angewendet wird.
Eine Regularisierungstechnik für TD-Lernen, die zeitliche Übergeneralisierung vermeidet, insbesondere in tiefen Netzen.
Eine Variante des zeitlichen Differenzlernens für den Fall der Funktionsannäherung, die versucht, das Problem der Übergeneralisierung über zeitlich aufeinanderfolgende Zustände hinweg zu lösen.
Das Papier stellt HR-TD vor, eine Variation des TD(0)-Algorithmus, die das Problem der Übergeneralisierung in konventionellem TD verbessern soll.
Wir stellen einen neuen CNN-Kernel für unstrukturierte Gitter für sphärische Signale vor und zeigen einen signifikanten Gewinn an Genauigkeit und Parametereffizienz bei Aufgaben wie 3D-Klassifizierung und omnidirektionaler Bildsegmentierung.
Eine effiziente Methode, die Deep Learning auf sphärischen Daten ermöglicht und mit viel weniger Parametern als gängige Ansätze konkurrenzfähige/aktuelle Zahlen erreicht.
Das Papier schlägt einen neuartigen Convolutional-Kernel für CNN auf unstrukturierten Gittern vor und formuliert die Convolution durch eine lineare Kombination von Differentialoperatoren.
Wenn Sie bei visuellen Vorhersageaufgaben Ihr Vorhersagemodell entscheiden lassen, welche Zeiten vorhergesagt werden sollen, hat dies zwei Vorteile: (i) es verbessert die Qualität der Vorhersage und (ii) es führt zu semantisch kohärenten "Engpasszustand"-Vorhersagen, die für die Planung nützlich sind.
Verfahren zur Vorhersage von Einzelbildern in einem Video, wobei der Ansatz beinhaltet, dass die Zielvorhersage fließend ist, aufgelöst durch ein Minimum des Vorhersagefehlers.
Neuformulierung der Aufgabe der Videovorhersage/Interpolation, so dass ein Vorherseher nicht gezwungen ist, Bilder in festen Zeitintervallen zu erzeugen, sondern trainiert wird, Bilder zu erzeugen, die zu einem beliebigen Zeitpunkt in der Zukunft stattfinden.
Wir haben ein LSTM verwendet, um zu erkennen, wenn ein Smartphone ein Gebäude betritt. Dann sagen wir anhand der Daten von Sensoren an Bord des Smartphones die Bodenhöhe des Geräts voraus.
In diesem Beitrag wird ein System vorgestellt, das anhand der Sensordaten eines mobilen Geräts und unter Verwendung eines LSTM sowie von Änderungen des Luftdrucks eine Schätzung der Bodenhöhe vornimmt.
Vorschlag für ein zweistufiges Verfahren zur Bestimmung des Stockwerks, in dem sich ein Mobiltelefon in einem hohen Gebäude befindet.
Kombinieren Sie die Darstellung von Sprachzielen mit rückblickenden Erfahrungswiederholungen.
In diesem Aufsatz wird die in der Rückschau implizite Annahme berücksichtigt, dass es eine Abbildung von Zuständen auf Ziele gibt, und es wird eine natürlichsprachliche Darstellung von Zielen vorgeschlagen.
In diesem Beitrag wird das Hindsight Experience Replay Framework mit natürlichsprachlichen Zielen verwendet, um die Beispiel-Effizienz von Modellen zur Befolgung von Anweisungen zu verbessern.
Wir schlagen ein gemeinsames Codebuch- und Faktorisierungsschema zur Verbesserung des Poolings zweiter Ordnung vor.
In diesem Beitrag wird eine Möglichkeit vorgestellt, bestehende faktorisierte Darstellungen zweiter Ordnung mit einer harten Zuweisung im Stil eines Codebuchs zu kombinieren.
Vorschlag für eine neuartige bilineare Darstellung auf der Grundlage eines Codebuchmodells und eine effiziente Formulierung, bei der Codebuch-basierte Projektionen durch gemeinsame Projektion faktorisiert werden, um die Parametergröße weiter zu reduzieren.
Wir schlagen eine Meta-Lernmethode vor und wenden sie an, die auf schwacher Überwachung basiert, um halb-überwachtes und Ensemble-Lernen bei der Extraktion biomedizinischer Beziehungen zu kombinieren.
Ein halbüberwachtes Verfahren zur Klassifizierung von Beziehungen, bei dem mehrere Basislerner anhand eines kleinen markierten Datensatzes trainiert werden und einige von ihnen dazu verwendet werden, unmarkierte Beispiele für das halbüberwachte Lernen zu annotieren.
Diese Arbeit befasst sich mit dem Problem der Erzeugung von Trainingsdaten für die Extraktion biologischer Beziehungen und verwendet Vorhersagen von Daten, die von schwachen Klassifikatoren als zusätzliche Trainingsdaten für einen Meta-Lernalgorithmus gekennzeichnet wurden.
In diesem Papier wird eine Kombination aus halbüberwachtem Lernen und Ensemble-Lernen für die Informationsextraktion vorgeschlagen, wobei Experimente an einer biomedizinischen Beziehungsextraktionsaufgabe durchgeführt werden.
Eine Klasse von Netzwerken, die einfache Modelle im Handumdrehen generieren (sogenannte Erklärungen), die als Regularisierer fungieren und eine konsistente Modelldiagnose und Interpretierbarkeit ermöglichen.
Die Autoren behaupten, dass der bisherige Stand der Technik neuronale Netze direkt als Komponenten in die grafischen Modelle integriert, was die Modelle uninterpretierbar macht.
Vorschlag für eine Kombination von neuronalen Netzen und grafischen Modellen durch Verwendung eines tiefen neuronalen Netzes zur Vorhersage der Parameter eines grafischen Modells.
Wir schlagen einen modellfreien Algorithmus für das Imitation Learning vor, der die Anzahl der Interaktionen mit der Umgebung im Vergleich zum modernsten Algorithmus für das Imitation Learning, GAIL, reduzieren kann.
Schlägt vor, den deterministischen Policy-Gradienten-Algorithmus zu erweitern, um aus Demonstrationen zu lernen, während er mit einer Art Dichteabschätzung des Experten kombiniert wird.
In diesem Beitrag wird das Problem des modellfreien Imitation Learnings betrachtet und eine Erweiterung des generativen adversarial Imitation Learning Algorithmus vorgeschlagen, indem die stochastische Politik des Lerners durch eine deterministische ersetzt wird.
Das Papier kombiniert IRL, adversariales Training und Ideen aus deterministischen Policy-Gradienten mit dem Ziel, die Komplexität von Beispielen zu verringern.
Graph-CNN mit geringer Rechenkomplexität (ohne Approximation) und besserer Klassifizierungsgenauigkeit.
Schlägt einen neuen CNN-Ansatz zur Graphenklassifizierung vor, der einen Filter verwendet, der auf ausgehenden Spaziergängen mit zunehmender Länge basiert, um Informationen von weiter entfernten Knotenpunkten in einem Fortpflanzungsschritt einzubeziehen.
Vorschlag für eine neue Architektur eines neuronalen Netzes für die halbüberwachte Graphenklassifizierung, die auf polynomialen Graphenfiltern aufbaut und diese auf aufeinanderfolgenden neuronalen Netzschichten mit ReLU-Aktivierungsfunktionen einsetzt.
Die Arbeit führt Topology Adaptive GCN ein, um Convolutional Networks auf graph-strukturierte Daten zu verallgemeinern.
Wir zeigen, dass katastrophales Vergessen innerhalb einer einzigen Aufgabe auftritt, und stellen fest, dass Beispiele, die nicht zum Vergessen neigen, ohne Generalisierungsverlust aus der Trainingsmenge entfernt werden können.
Untersucht das Vergessensverhalten von Trainingsbeispielen während des SGD und zeigt, dass es beim Training neuronaler Netze in verschiedenen Netzarchitekturen "Unterstützungsbeispiele" gibt.
In dieser Arbeit wird analysiert, inwieweit Netzwerke lernen, bestimmte Beispiele richtig zu klassifizieren und diese Beispiele dann im Laufe des Trainings vergessen.
In dem Beitrag wird untersucht, ob einige Beispiele beim Training neuronaler Netze schwieriger zu lernen sind als andere. Solche Beispiele werden beim Lernen vergessen und mehrfach neu gelernt.
Ein unüberwachter Ansatz für das Erlernen von entflechteten Darstellungen von Objekten aus unmarkierten monokularen Videos.
Entwirft eine Merkmalsdarstellung aus Videosequenzen, die von einer Szene aus verschiedenen Blickwinkeln aufgenommen wurden.
Vorschlag für eine unüberwachte Methode zum Erlernen von Repräsentationen für visuelle Eingaben, die einen metrischen Lernansatz beinhaltet, der die nächstgelegenen Paare von Bildfeldern im Einbettungsraum zusammenführt, während andere Paare auseinandergedrängt werden.
Diese Arbeit untersucht das selbstüberwachte Lernen von Objektrepräsentationen, mit der Hauptidee, Objekte mit ähnlichen Merkmalen zu ermutigen, sich weiter zueinander "hingezogen" zu fühlen.
Wir trainieren mit zustandsorientierten Vektor-Belohnungen einen Agenten, der Zustandsänderungen aus Aktionsverteilungen vorhersagt, indem wir eine neue, von der Quantilsregression inspirierte Reinforcement Learning Technik verwenden.
Stellt einen Algorithmus vor, der darauf abzielt, das Reinforcement Learning in Situationen zu beschleunigen, in denen die Belohnung auf den Zustandsraum abgestimmt ist. 
Diese Arbeit befasst sich mit RL im kontinuierlichen Handlungsraum, unter Verwendung einer neu parametrisierten Strategie und eines neuartigen vektorbasierten Trainingsziels.
In dieser Arbeit wird vorgeschlagen, die Verteilungs-RL mit einem Netz zu mischen, das die Entwicklung der Welt in Form von Quantilen modelliert, um die Effizienz der Stichprobe zu verbessern.
Wir schlagen Episodic Backward Update vor, einen neuartigen Deep Reinforcement Learning-Algorithmus, der Übergänge episodenweise abtastet und Werte rekursiv rückwärts aktualisiert, um schnelles und stabiles Lernen zu erreichen.
Schlägt ein neues DQN vor, bei dem die Ziele für eine vollständige Episode durch eine Rückwärtsaktualisierung (von Ende zu Anfang) berechnet werden, um eine schnellere Ausbreitung der Belohnungen bis zum Ende der Episode zu erreichen.
Die Autoren schlagen vor, den DQN-Algorithmus zu modifizieren, indem sie den Max-Bellman-Operator rekursiv auf eine Trajektorie mit einem gewissen Zerfall anwenden, um die Akkumulation von Fehlern mit dem verschachtelten Max zu verhindern.
In Deep-Q-Netzen werden die Q-Werte ab dem Ende der Episode aktualisiert, um eine schnelle Ausbreitung der Belohnungen entlang der Episode zu ermöglichen.
In dieser Arbeit stellen wir eine neuartige Siamese Deep Neural Network Architektur vor, die in der Lage ist, effektiv aus Daten zu lernen, wenn mehrere unerwünschte Ereignisse vorliegen.
In dieser Arbeit werden Siamesische Neuronale Netze in den Rahmen der konkurrierenden Risiken eingeführt, indem direkt für den c-Index optimiert wird.
Die Autoren befassen sich mit Fragen der Risikoabschätzung in einem Umfeld der Überlebensanalyse mit konkurrierenden Risiken und schlagen vor, den zeitabhängigen Diskriminierungsindex unter Verwendung eines siamesischen Überlebensnetzwerks direkt zu optimieren
Wir stellen ein typbasiertes Zeigernetzmodell zusammen mit einer wertbasierten Verlustmethode vor, um ein neuronales Modell zur Übersetzung natürlicher Sprache in SQL effektiv zu trainieren.
Die Arbeit behauptet, eine neuartige Methode zu entwickeln, um Abfragen in natürlicher Sprache auf SQL abzubilden, indem eine Grammatik zur Anleitung der Dekodierung und eine neue Verlustfunktion für Zeiger/Kopiermechanismen verwendet wird.
Ein unverzerrter und variantenarmer Gradientenschätzer für diskrete latente Variablenmodelle.
Vorschlag einer neuen Technik zur Varianzreduzierung, die bei der Berechnung eines erwarteten Verlustgradienten verwendet werden kann, wenn die Erwartung in Bezug auf unabhängige binäre Zufallsvariablen besteht.
Ein Algorithmus, der Rao-Blackwellization und gewöhnliche Zufallszahlen kombiniert, um die Varianz des Score-Funktions-Gradientenschätzers im speziellen Fall stochastischer binärer Netzwerke zu verringern.
Ein unvoreingenommener und varianzarmer Augment-REINFORCE-Merge (ARM)-Schätzer für die Berechnung und das Backpropagating von Gradienten in binären neuronalen Netzen.
Wir beweisen, dass paralleles lokales SGD eine lineare Beschleunigung mit viel weniger Kommunikation als paralleles Mini-Batch SGD erreicht.
Liefert einen Konvergenzbeweis für lokale SGD und beweist, dass lokale SGD die gleichen Geschwindigkeitsgewinne wie Minibatch bieten kann, aber möglicherweise in der Lage ist, deutlich weniger zu kommunizieren.
Diese Arbeit stellt eine Analyse der lokalen SGD und Grenzen auf, wie häufig die Schätzer, die durch die Ausführung von SGD erhalten werden, gemittelt werden müssen, um lineare Parallelisierung Geschwindigkeitssteigerungen zu erhalten.
Die Autoren analysieren den lokalen SGD-Algorithmus, bei dem K parallele SGD-Ketten ausgeführt werden und die Iterate gelegentlich über Maschinen hinweg durch Mittelwertbildung synchronisiert werden.
Kompakte Wahrnehmung von dynamischen Prozessen.
Untersucht das Problem der kompakten Darstellung des Modells eines komplexen dynamischen Systems bei gleichzeitiger Erhaltung der Informationen durch die Verwendung einer Informationsengpassmethode.
In diesem Beitrag wurde die Gaußsche lineare Dynamik untersucht und ein Algorithmus zur Berechnung der Informationsengpasshierarchie (IBH) vorgeschlagen.
Dichtes RNN, das von jedem versteckten Zustand direkt zu mehreren vorhergehenden versteckten Zuständen aller Schichten vollständige Verbindungen hat.
Schlägt eine neue RNN-Architektur vor, die langfristige Abhängigkeiten besser modelliert, eine multiskalige Darstellung von sequentiellen Daten erlernen kann und das Gradientenproblem durch die Verwendung parametrisierter Gating-Einheiten umgeht.
In diesem Beitrag wird eine vollständig vernetzte dichte RNN-Architektur mit Gated-Verbindungen zu jeder Schicht und Verbindungen zu den vorangehenden Schichten vorgeschlagen und die Ergebnisse auf PTB-Charakterebene modelliert.
SD-GANs entwirren latente Codes anhand bekannter Gemeinsamkeiten in einem Datensatz (z. B. Fotos, die dieselbe Person abbilden).
In diesem Beitrag wird das Problem der kontrollierten Bilderzeugung untersucht und ein Algorithmus vorgeschlagen, der ein Bildpaar mit derselben Identität erzeugt.
Diese Arbeit schlägt SD-GAN vor, eine Methode zum Training von GANs, um die Identitäts- und Nicht-Identitätsinformationen im latenten Eingangsvektor Z zu entflechten.
Wir schlagen einen Lernrahmen für bereichsübergreifende Übersetzungen vor, der exakt zykluskonsistent ist und über adversariales Training, Maximum-Likelihood-Schätzung oder eine Mischform gelernt werden kann.
schlägt AlignFlow vor, eine effiziente Methode zur Umsetzung des Prinzips der Zykluskonsistenz unter Verwendung invertierbarer Flüsse.
Flussmodelle für ungepaarte Bild-zu-Bild-Übersetzung.
In einem Programmsynthesekontext, in dem die Eingabe eine Menge von Beispielen ist, reduzieren wir die Kosten, indem wir eine Teilmenge von repräsentativen Beispielen berechnen.
Vorschlagen einer Methode zur Identifizierung repräsentativer Beispiele für die Programmsynthese, um die Skalierbarkeit bestehender Constraint-Programming Lösungen zu erhöhen.
Eine Methode zur Auswahl einer Teilmenge von Beispielen, auf die ein Constraint Solver angewendet wird, um Probleme der Programmsynthese zu lösen.
In dieser Arbeit wird eine Methode zur Beschleunigung von Mehrzweck-Programmsynthesizern vorgeschlagen.
Wir stellen Recurrent Relational Networks vor, ein leistungsfähiges und allgemeines neuronales Netzmodul für relationales Denken, und verwenden es, um 96,6% der schwierigsten Sudokus und 19/20 BaBi-Aufgaben zu lösen.
Einführung rekurrenter relationaler Netze (RRNs), die zu beliebigen neuronalen Netzen hinzugefügt werden können, um die Fähigkeit zur relationalen Schlussfolgerung zu erhöhen.
Einführung eines tiefen neuronalen Netzes für strukturierte Vorhersagen, das bei Soduku-Rätseln und der BaBi-Aufgabe die beste Leistung erzielt.
In diesem Beitrag wird eine Methode beschrieben, die als relationales Netzwerk bezeichnet wird, um tiefe neuronale Netze mit relationalen Schlussfolgerungen zu versehen.
Drei Klassenprioritäten sind alles, was Sie brauchen, um tiefe Modelle aus nur U-Daten zu trainieren, während zwei nicht ausreichen sollten.
Schlägt einen unvoreingenommenen Schätzer vor, der das Training von Modellen mit schwacher Überwachung auf zwei unbeschrifteten Datensätzen mit bekannten Klassenprioritäten ermöglicht, und erörtert die theoretischen Eigenschaften der Schätzer.
Eine Methodik für das Training eines beliebigen binären Klassifikators aus nur unbeschrifteten Daten und eine empirische Risikominimierungsmethode für zwei Sätze unbeschrifteter Daten, bei denen Klassenprioritäten gegeben sind.
Die Aggregation von Klassenevidenz aus vielen kleinen Bildfeldern reicht aus, um ImageNet zu lösen, führt zu besser interpretierbaren Modellen und kann Aspekte der Entscheidungsfindung von populären DNNs erklären.
In diesem Beitrag wird eine neuartige und kompakte neuronale Netzarchitektur vorgeschlagen, die die Informationen in Bag-of-Words-Merkmalen nutzt. Der vorgeschlagene Algorithmus verwendet nur die Patch-Informationen unabhängig und führt eine Mehrheitsabstimmung mit unabhängig klassifizierten Patches durch.
Aktuelle somatische Mutationsmethoden funktionieren nicht mit Flüssigbiopsien (d.h. Sequenzierung mit geringer Abdeckung). Wir wenden eine CNN-Architektur auf eine eindeutige Darstellung einer Lesung und deren Bewertung an und zeigen eine signifikante Verbesserung gegenüber früheren Methoden in der Niedrigfrequenz-Einstellung.
Vorschlagen einer CNN-basierten Lösung namens Kittyhawk für somatische Mutationserkennung bei extrem niedrigen Allelefrequenzen.
Ein neuer Algorithmus zur Erkennung von Krebsmutationen aus der Sequenzierung zellfreier DNA, der den Sequenzkontext identifiziert, der Sequenzierungsfehler von echten Mutationen unterscheidet.
In dieser Arbeit wird ein Deep Learning Framework zur Vorhersage von somatischen Mutationen bei extrem niedrigen Frequenzen vorgeschlagen, die bei der Erkennung von Tumoren aus zellfreier DNA auftreten.
Die Arbeit stellt ein neues Gold-Standard-Korpus der biomedizinischen wissenschaftlichen Literatur vor, das manuell mit UMLS Konzept Erwähnungen annotiert wurde.
Einzelheiten zum Aufbau eines manuell annotierten Datensatzes für biomedizinische Konzepte, der größer ist und von einer umfangreicheren Ontologie abgedeckt wird als bisherige Datensätze.
Dieses Papier verwendet MedMentions, ein TaggerOne Semi-Markov-Modell für die Ende-zu-Ende Konzepterkennung und -Verknüpfung auf einem Satz von Pubmed-Zusammenfassungen, um Arbeiten mit biomedizinischen Konzepten/Entitäten zu kennzeichnen
Wir schlagen eine tiefe Clustermethode vor, bei der jedes Cluster anstelle eines Centroids durch einen Autoencoder repräsentiert wird.
Stellt ein tiefes Clustering vor, das auf einer Mischung von Autoencodern basiert, wobei die Datenpunkte einem Cluster auf der Grundlage des Darstellungsfehlers zugeordnet werden, wenn das Autoencodernetzwerk zu ihrer Darstellung verwendet wurde.
Ein Deep-Clustering-Ansatz, der ein Autoencoder-Framework verwendet, um eine niedrigdimensionale Einbettung der Daten zu erlernen und gleichzeitig Daten mithilfe eines tiefen neuronalen Netzwerks zu clustern.
Eine tiefe Clustering-Methode, bei der jedes Cluster mit verschiedenen Autoencodern dargestellt wird, funktioniert durchgängig und kann auch zum Clustern neu eingehender Daten verwendet werden, ohne dass das gesamte Clustering-Verfahren erneut durchgeführt werden muss.
Wir definieren eine neue Integrale Wahrscheinlichkeitsmetrik (Sobolev IPM) und zeigen, wie sie für das Training von GANs zur Texterzeugung und zum halbüberwachten Lernen verwendet werden kann.
Schlägt ein neuartiges Regularisierungsschema für GANs vor, das auf einer Sobolev-Norm basiert und Abweichungen zwischen L2-Normen von Ableitungen misst.
Die Autoren stellen einen anderen GAN-Typ vor, der den typischen Aufbau eines GANs verwendet, aber eine andere Funktionsklasse hat, und geben ein Rezept für das Training von GANs mit dieser Art von Funktionsklasse.
In dem Beitrag wird eine andere Gradientenstrafe für GAN-Kritiker vorgeschlagen, die die erwartete quadratische Norm des Gradienten auf 1 setzt.
Wir schlagen einen neuen Ansatz vor, um GANs mit einer Mischung von Generatoren zu trainieren, um das Problem des Zusammenbrechens der Modi zu überwinden.
Bewältigung des Problems des Modus-Kollapses in GANs unter Verwendung einer eingeschränkten Mischungsverteilung für den Generator und eines Hilfsklassifikators, der die Quellmischungskomponente vorhersagt.
In der Arbeit wird eine Mischung von Generatoren vorgeschlagen, um GANs ohne zusätzliche Rechenkosten zu trainieren
Die Autoren zeigen, dass die Verwendung von MGAN, das darauf abzielt, das Problem des Modellzusammenbruchs durch Mischungsgeneratoren zu überwinden, hochmoderne Ergebnisse erzielt.
Wir präsentieren die BabyAI-Plattform zur Untersuchung der Dateneffizienz beim Sprachenlernen mit einem Menschen in der Schleife.
Stellt eine Forschungsplattform mit einem Bot in der Schleife vor, der lernt, Sprachbefehle auszuführen, wobei die Sprache kompositorische Strukturen aufweist.
Stellt eine Plattform für das Erlernen von Sprachen vor, die jeden Menschen in der Lernschleife durch einen heuristischen Lehrer ersetzt und eine synthetische Sprache verwendet, die einer 2D-Gitterwelt zugeordnet ist.
Ein k-means-Prior in Kombination mit einer L1-Regularisierung führt zu hochmodernen Kompressionsergebnissen.
Dieses Papier untersucht die weiche Parameterbindung und Kompression von DNNs/CNNs.
Die SVRG-Methode versagt bei modernen Deep-Learning-Problemen.
In diesem Beitrag wird eine Analyse von SVRG-Methoden vorgestellt, die zeigt, dass Dropout, Batch-Norm und Datenerweiterung (zufällige Ernte/Rotation/Translation) dazu neigen, die Verzerrung und/oder Varianz der Aktualisierungen zu erhöhen.
Diese Arbeit untersucht die Anwendbarkeit von SVGD auf moderne neuronale Netze und zeigt, dass die naive Anwendung von SVGD in der Regel scheitert.
Eine Methode zur Anwendung von Deep Learning auf 3D-Oberflächen unter Verwendung ihrer sphärischen Deskriptoren und alt-az anisotroper Convolution auf der 2-Sphäre.
Stellt ein polares anisotropes Convolution Schema auf einer Einheitskugel vor, bei dem die Filtertranslation durch Filterrotation ersetzt wird.
Diese Arbeit untersucht tiefes Lernen von 3D-Formen mit alt-az anisotroper 2-Sphären-Faltung.
Training von binären/alternären Netzen durch lokale Umparametrisierung mit der CLT-Approximation.
Trainiert binäre und ternäre Gewichtsverteilungsnetze unter Verwendung von Backpropagation, um Neuronenvoraktivierungen mit einem Reparametrisierungstrick zu testen.
In diesem Beitrag wird vorgeschlagen, stochastische Parameter in Kombination mit dem Trick der lokalen Reparametrisierung zu verwenden, um neuronale Netze mit binären oder ternären Gewichten zu trainieren, was zu Ergebnissen auf dem neuesten Stand der Technik führt.
Optimal Completion Distillation (OCD) ist ein Trainingsverfahren zur Optimierung von Sequenz-zu-Sequenz-Modellen auf der Basis von Edit-Distanz, das bei Ende-zu-Ende Spracherkennungsaufgaben den Stand der Technik erreicht.
Alternativer Ansatz für das Training von seq2seq-Modellen unter Verwendung eines dynamischen Programms zur Berechnung optimaler Fortsetzungen von vorhergesagten Präfixen.
Ein Trainingsalgorithmus für autoregressive Modelle, der kein MLE-Vortraining benötigt und direkt aus dem Sampling optimieren kann.
Die Arbeit geht auf einen Mangel von Sequenz-zu-Sequenz-Modellen ein, die mit Hilfe von Maximum-Likelihood-Schätzungen trainiert werden, und schlägt einen Ansatz vor, der auf Edit-Distanzen und der impliziten Verwendung vorgegebener Label-Sequenzen während des Trainings basiert.
Eine gemeinsame Modell- und Gradientensparsamkeitsmethode für föderiertes Lernen.
Wendet Variational Dropout an, um die Kommunikationskosten beim verteilten Training neuronaler Netze zu reduzieren, und führt Experimente mit den Datensätzen mnist, cifar10 und svhn durch. 
Die Autoren schlagen einen Algorithmus vor, der die Kommunikationskosten beim föderierten Lernen reduziert, indem spärliche Gradienten vom Gerät zum Server und zurück gesendet werden.
Kombiniert einen verteilten Optimierungsalgorithmus mit Variational Dropout, um die von den lokalen Lernern an den Master-Server gesendeten Gradienten zu strecken.
Wir beweisen eine Multiklassen-Boosting-Theorie für die ResNet-Architekturen, die gleichzeitig eine neue Technik für Multiklassen-Boosting schafft und einen neuen Algorithmus für ResNet-artige Architekturen bietet.
Präsentiert einen Boosting-Algorithmus für das Training von tiefen Residual Networks, eine Konvergenzanalyse für Trainingsfehler und eine Analyse der Generalisierungsfähigkeit.
Eine Lernmethode für ResNet unter Verwendung des Boosting-Frameworks, die das Lernen komplexer Netzwerke zerlegt und weniger Rechenaufwand erfordert.
Die Autoren schlagen das tiefe ResNet als Boosting-Algorithmus vor und behaupten, dieser sei effizienter als die standardmäßige Ende-zu-Ende Backpropagation.
Die Arbeit analysiert die Optimierungslandschaft von einschichtigen neuronalen Netzen und entwirft ein neues Ziel, das nachweislich kein ungewolltes lokales Minimum aufweist. 
Dieses Papier untersucht das Problem des Lernens von neuronalen Netzen mit einer verborgenen Schicht, stellt eine Verbindung zwischen dem Least squares population loss und dem Hermite-Polynomen her und schlägt eine neue Verlustfunktion vor.
Eine Tensor-Faktorisierungs-Methode zur Verschlankung eines neuronalen Netzes mit einer versteckten Schicht.
Ein offener Korpus zur Informationsextraktion und seine eingehende Analyse.
Erstellt ein neues Korpus für die Informationsextraktion, das größer ist als die bisherigen öffentlichen Korpora und Informationen enthält, die in den bisherigen Korpora nicht vorhanden sind.
Präsentiert einen Datensatz von Open-IE-Triples, die mit Hilfe eines neuen Extraktionssystems aus Wikipedia gesammelt wurden. 
Die Arbeit beschreibt die Erstellung eines Open IE-Korpus über die englische Wikipedia durch eine automatische Methode
Wir definieren eine flexible DSL für die Generierung von RNN-Architekturen, die RNNs unterschiedlicher Größe und Komplexität zulässt, und schlagen eine Ranking-Funktion vor, die RNNs als rekursive neuronale Netze darstellt und ihre Leistung simuliert, um die vielversprechendsten Architekturen auszuwählen.
Es wird eine neue Methode zur Erzeugung von RNN-Architekturen vorgestellt, die eine domänenspezifische Sprache für zwei Arten von Generatoren (zufällig und RL-basiert) zusammen mit einer Ranking-Funktion und einem Evaluator verwendet.
In dieser Arbeit wird die Suche nach guten RNN Cell-Architekturen als Black-Box-Optimierungsproblem dargestellt, bei dem Beispiele als Operatorbaum dargestellt und auf der Grundlage gelernter Funktionen bewertet oder von einem RL-Agenten erzeugt werden.
In diesem Beitrag wird eine Meta-Lernstrategie für die automatische Architektursuche im Kontext von RNN untersucht, indem ein DSL verwendet wird, die rekurrente RNN-Operationen spezifiziert.
Wir trainieren und schlussfolgern nur mit ganzen Zahlen mit geringer Bitbreite in DNNs.
Eine Methode namens WAGE, die alle Operanden und Operatoren in einem neuronalen Netz quantisiert, um die Anzahl der Bits für die Darstellung in einem Netz zu reduzieren.
Die Autoren schlagen diskrete Gewichte, Aktivierungen, Gradienten und Fehler sowohl beim Training als auch beim Testen von neuronalen Netzen vor.
In diesem Beitrag entwickeln wir schnelle, umschulungsfreie Sparsifizierungsmethoden, die für die fliegende Sparsifizierung von CNNs in vielen industriellen Kontexten eingesetzt werden können.
In diesem Papier werden Ansätze für das Pruning von CNNs ohne erneutes Training vorgeschlagen, indem drei Schemata zur Bestimmung der Schwellenwerte für die Pruning-Gewichte eingeführt werden.
Diese Arbeit beschreibt eine Methode zur Sparsifizierung von CNNs ohne Neutraining.
Wir schlagen vor, dass das Training mit stufenweise wachsenden Mengen eine Optimierung für neuronale Netze darstellt.
Die Autoren vergleichen das Curriculum Learning mit dem Lernen in einer zufälligen Reihenfolge mit Phasen, die eine neue Stichprobe von Beispielen zu der zuvor zufällig zusammengestellten Menge hinzufügen.
In diesem Beitrag wird der Einfluss der Reihenfolge im Curriculum und beim selbstgesteuerten Lernen untersucht, und es wird gezeigt, dass die Reihenfolge der Trainingsinstanzen bis zu einem gewissen Grad nicht wichtig ist.
Ein Bild-zu-Bild-Übersetzungsverfahren, das einem Bild den Inhalt eines anderen Bildes hinzufügt und so ein neues Bild erzeugt.
Dieses Arbeit befasst sich mit der Aufgabe der Übertragung von Inhalten, wobei die Neuigkeit im Verlust liegt.
Ein Datensatz zum Testen des mathematischen Denkens (und der algebraischen Verallgemeinerung) sowie Ergebnisse zu aktuellen Sequenz-zu-Sequenz-Modellen.
Es wird ein neuer synthetischer Datensatz zur Bewertung der mathematischen Argumentationsfähigkeit von Sequenz-zu-Sequenz-Modellen vorgestellt und zur Bewertung verschiedener Modelle verwendet.
Modell zum Lösen grundlegender mathematischer Probleme.
In diesem Beitrag werden effiziente und ökonomische Parametrisierungen von Convolutional Neural Networks vorgestellt, die durch partielle Differentialgleichungen motiviert sind.
Es werden vier "kostengünstige" Alternativen zur Standard Convolution Operation vorgestellt, die anstelle der Standard Convolution Operation verwendet werden können, um deren Rechenaufwand zu verringern.
In diesem Beitrag werden Methoden zur Reduzierung der Rechenkosten von CNN-Implementierungen vorgestellt und neue Parametrisierungen von CNN-ähnlichen Architekturen eingeführt, die die Parameterkopplung begrenzen.
Die Arbeit schlägt eine PDE-basierte Perspektive zum Verständnis und zur Parametrisierung von CNNs vor.
Verwendung der Theorie der Ratenverzerrung, um zu bestimmen, wie stark ein Modell mit latenten Variablen verbessert werden kann.
Befasst sich mit Problemen der Optimierung des Priors im Modell der latenten Variablen und der Auswahl der Likelihood-Funktion, indem  Kriterien vorgeschlagen werden, die auf einer Untergrenze für die negative Log-Likelihood basieren.
Stellt ein Theorem vor, das eine untere Schranke für die negative logarithmische Wahrscheinlichkeit der Ratenverzerrung bei der Modellierung latenter Variablen liefert.
Die Autoren argumentieren, dass die Theorie der Ratenverzerrung für verlustbehaftete Kompression ein natürliches Instrumentarium für die Untersuchung von Modellen mit latenten Variablen bietet und schlagen eine untere Grenze vor.
Wir ignorieren Nichtlinearitäten und berechnen keine Gradienten im Rückwärtsdurchlauf, um Berechnungen zu sparen und sicherzustellen, dass Gradienten immer fließen. 
Der Autor schlug lineare Backprop-Algorithmen vor, um den Gradientenfluss für alle Teile während der Backpropagation zu gewährleisten.
Systematisches Verständnis des diskreten Autoencoders VQ-VAE unter Verwendung von EM und dessen Verwendung zum Entwurf eines nicht-autoregressiven Übersetzungsmodells, das einer starken autoregressiven Basislinie entspricht.
In diesem Beitrag wird eine neue Art der Interpretation des VQ-VAE vorgestellt und ein neuer Trainingsalgorithmus auf der Grundlage des Soft EM Clustering vorgeschlagen.
Die Arbeit präsentiert eine alternative Sichtweise auf das Trainingsverfahren für den VQ-VAE unter Verwendung des Soft EM Algorithmus.
In diesem Beitrag wird ein tiefes neuronales Netz vorgestellt, in das eine Verlustfunktion in Bezug auf die optimale Randverteilung eingebettet ist, die das Overfitting-Problem theoretisch und empirisch entschärft.
Präsentiert eine PAC-Bayes'sche Grenze für einen Margenverlust.
Wir versuchen, gelernte Repräsentationen in komprimierten Netzen durch ein experimentelles System zu verstehen, das wir Deep Net Triage nennen.
Vergleich verschiedener Initialisierungs- und Trainingsmethoden zur Übertragung von Wissen von einem VGG-Netz auf ein kleineres Studentennetz durch Ersetzen von Blöcken von Schichten durch einzelne Schichten.
In dieser Arbeit werden fünf Methoden für das Triaging oder die Komprimierung von Blockschichten für tiefe Netze vorgestellt.
Die Arbeit schlägt eine Methode zur Komprimierung eines Blocks von Schichten in einem NN vor, bei der mehrere verschiedene Teilansätze bewertet werden.
Der empirische Nachweis eines neuen Phänomens erfordert neue theoretische Erkenntnisse und ist für die aktive Diskussion in der Literatur über SGD und das Verständnis von Generalisierung von Bedeutung.
In der Arbeit wird ein Phänomen erörtert, bei dem das Training neuronaler Netze in sehr spezifischen Situationen von einem Zeitplan mit hohen Lernraten stark profitieren kann.
Die Autoren analysieren das Training von Residual Networks mit großen zyklischen Lernraten und zeigen schnelle Konvergenz mit zyklischen Lernraten und Beweise für große Lernraten, die als Regularisierung wirken.
Wir schlagen eine Methode für die Konstruktion beliebig tiefer Netze mit unendlicher Breite vor, auf deren Grundlage wir ein neuartiges Gewichtungsinitialisierungsschema für Netze mit endlicher Breite ableiten und dessen konkurrenzfähige Leistung demonstrieren.
Schlägt einen Ansatz zur Initialisierung von Gewichten vor, um unendlich tiefe und unendlich breite Netzwerke zu ermöglichen, mit experimentellen Ergebnissen auf kleinen Datensätzen.
Vorschlagen tiefer neuronaler Netze von unendlicher Breite.
Wir haben biologisch plausible Lernregeln für die synaptische Plastizität eines rekurrenten neuronalen Netzes zur Speicherung von Reizrepräsentationen abgeleitet. 
Ein neuronales Netzmodell, das aus rekurrent verbundenen Neuronen und einem oder mehreren Redouts besteht und darauf abzielt, eine bestimmte Ausgabe über die Zeit hinweg beizubehalten.
In diesem Beitrag wird ein selbstorganisierender Speichermechanismus in einem neuronalen Modell vorgestellt und eine Zielfunktion eingeführt, die die Änderungen des zu speichernden Signals minimiert.
Um das GAN-Training zu verstehen, definieren wir eine einfache GAN-Dynamik und zeigen die quantitativen Unterschiede zwischen optimalen Updates und Updates erster Ordnung in diesem Modell.
Die Autoren untersuchen die Auswirkungen von GANs in Situationen, in denen bei jeder Iteration der Diskriminator bis zur Konvergenz trainiert und der Generator mit Gradientenschritten aktualisiert wird, oder in denen einige wenige Gradientenschritte für den Diskriminator und den Generator durchgeführt werden.
In diesem Beitrag wird die Dynamik des gegnerischen Trainings von GANs auf einem Gaußschen Mischmodell untersucht.
Wir schlagen eine gradientenbasierte Methode vor, um Wissen aus verschiedenen Quellen über unterschiedliche Bereiche und Aufgaben hinweg zu übertragen.
In diesem Beitrag wird vorgeschlagen, die Gradienten der Ausgangsdomänen zu kombinieren, um das Lernen in der Zieldomäne zu unterstützen. 
Die erste Variational-Bayes-Formulierung der phylogenetischen Inferenz, ein anspruchsvolles Inferenzproblem über Strukturen mit verflochtenen diskreten und kontinuierlichen Komponenten.
Erforscht eine Näherungslösung für das Problem der Bayes'schen Inferenz von phylogenetischen Bäumen durch die Nutzung kürzlich vorgeschlagener subsplit Bayes'scher Netzwerke und moderner Gradientenschätzer für VI.
Vorschlagen eines Variationsansatz für die Bayes'sche Posterior-Inferenz in phylogenetischen Bäumen.
Es handelt sich um eine hybride neuronale Architektur zur Beschleunigung des autoregressiven Modells. 
Die Schlussfolgerung lautet, dass ein Modell, das mehrere Zeitschritte gleichzeitig vorhersagt, verwendet werden sollte, um die Modellgröße zu erhöhen, ohne die Inferenzzeit für die sequenzielle Vorhersage zu verlängern.
In diesem Beitrag wird HybridNet vorgestellt, ein neuronales Sprachsynthese- und Audiosynthesesystem, das das WaveNet-Modell mit einem LSTM kombiniert, mit dem Ziel, ein Modell mit schnellerer Inferenzzeit für die Audioerzeugung anzubieten.
Interpretation durch Identifizierung der vom Modell gelernten Merkmale, die als Indikatoren für die interessierende Aufgabe dienen. Erklären von Modellentscheidungen durch Hervorheben der Reaktion dieser Merkmale in Testdaten. Objektive Evaluierung der Erklärungen anhand eines kontrollierten Datensatzes.
In diesem Beitrag wird eine Methode zur Erstellung visueller Erklärungen für die Ergebnisse tiefer neuronaler Netze vorgeschlagen und ein neuer synthetischer Datensatz veröffentlicht.
Eine Methode für tiefe neuronale Netze, die automatisch relevante Merkmale des Klassensatzes identifiziert und die Interpretation und Erklärung unterstützt, ohne auf zusätzliche Annotationen angewiesen zu sein.
Ein Rahmenwerk zum effizienten Erlernen hochwertiger Satzrepräsentationen.
Schlägt einen schnelleren Algorithmus für das Lernen von Satzrepräsentationen im Stil von SkipThought aus Korpora geordneter Sätze vor, der den Decoder auf Wortebene durch einen kontrastiven Klassifikationsverlust ersetzt.
Diese Arbeit schlägt einen Rahmen für das unbeaufsichtigte Lernen von Satzrepräsentationen vor, indem ein Modell der Wahrscheinlichkeit wahrer Kontextsätze relativ zu zufälligen Kandidatensätzen maximiert wird.
Aus der Perspektive des Informationsengpasses leiten wir eine Normstrafe für den Ausgang des neuronalen Netzes ab.
Setzt eine Aktivierungsnorm-Strafe ein, eine Regularisierung vom Typ L_2 auf die Aktivierungen, die sich aus dem Prinzip des Informationsengpasses ableitet.
In diesem Beitrag wird eine Zuordnung zwischen Aktivierungsnorm Strafen und Informationsengpass Frameworks unter Verwendung des Variational Dropout Frameworks erstellt.
Eine vollständig unbeaufsichtigte Methode, die Dimensionalitätsreduktion und zeitliches Clustering auf natürliche Weise in ein einziges Ende-zu-Ende Lernsystem integriert.
Schlägt einen Algorithmus vor, der Autoencoder mit Zeitreihendaten-Clustering unter Verwendung einer Netzwerkstruktur integriert, die für Zeitreihendaten geeignet ist.
Ein Algorithmus für die gemeinsame Durchführung von Dimensionalitätsreduktion und zeitlichem Clustering in einem Deep-Learning-Kontext, der einen Autoencoder und ein Clustering-Ziel verwendet.
Die Autoren schlugen eine unbeaufsichtigte Zeitreihen-Clustermethode vor, die auf tiefen neuronalen Netzen aufbaut und mit einem Encoder-Decoder und einem Clustermodus ausgestattet ist, um die Zeitreihen zu verkürzen, lokale zeitliche Merkmale zu extrahieren und die kodierten Darstellungen zu erhalten.
Ein gedächtniserweitertes neuronales Netzwerk, das das Many-Class Few-Shot Problem angeht, indem es die Klassenhierarchie sowohl beim überwachten Lernen als auch beim Meta-Lernen nutzt.
In diesem Beitrag werden Methoden vorgestellt, mit denen ein Klassifikator durch grobe bis feine Vorhersagen entlang einer Klassenhierarchie und durch das Lernen eines speicherbasierten KNN-Klassifikators, der während des Lernens falsch beschriftete Instanzen verfolgt, induktiv beeinflusst werden kann.
In diesem Beitrag wird das Problem der Many-Class Few-Shot Klassifizierung aus der Perspektive des überwachten Lernens und des Meta-Lernens formuliert.
Eine neuartige Verlustkomponente, die das Netz dazu zwingt, während des Trainings für eine Klassifizierungsaufgabe eine Repräsentation zu erlernen, die für die Clusterbildung gut geeignet ist.
Dieses Arbeit schlägt zwei Regularisierungsbedingungen vor, die auf einem zusammengesetzten Scharnierverlust über die KL-Divergenz zwischen zwei softmax-normalisierten Eingangsargumenten basieren, um das Lernen von entkoppelten Repräsentationen zu fördern.
Vorschlag für zwei Regularisierer, die dafür sorgen sollen, dass die in der vorletzten Schicht eines Klassifikators gelernten Repräsentationen besser mit der inhärenten Struktur der Daten übereinstimmen.
Wir zeigen, wie man gute Darstellungen aus der Sicht der Ähnlichkeitssuche erhält.
Untersucht die Auswirkungen eines Wechsels des Bildklassifikationsteils über dem DNN auf die Fähigkeit, die Deskriptoren mit einem LSH- oder einem kd-Baum-Algorithmus zu indizieren.
Es wird vorgeschlagen, den Softmax-Kreuzentropieverlust zu verwenden, um ein Netzwerk zu lernen, das versucht, die Winkel zwischen den Eingaben und den entsprechenden Klassenvektoren in einem überwachten Rahmen zu reduzieren.
Wir stellen eine Technik vor, die ein gradientenbasiertes Training von quantisierten neuronalen Netzen ermöglicht.
Schlägt eine einheitliche und allgemeine Methode für das Training neuronaler Netze mit quantisierten synaptischen Gewichten und Aktivierungen reduzierter Präzision vor.
Ein neuer Ansatz zur Quantisierung von Aktivierungen, der bei mehreren realen Bildproblemen Stand der Technik oder wettbewerbsfähig ist.
Verfahren zum Lernen neuronaler Netze mit quantisierten Gewichten und Aktivierungen durch stochastische Quantisierung von Werten und Ersetzen der resultierenden kategorischen Verteilung durch eine kontinuierliche Entspannung.
Wir zeigen, dass das Problem des Modus-Kollapses in GANs durch einen Mangel an Informationsaustausch zwischen Beobachtungen in einem Trainingsstapel erklärt werden kann, und schlagen einen verteilungsbasierten Rahmen für den globalen Informationsaustausch zwischen Gradienten vor, der zu einem stabileren und effektiveren adversen Training führt.
Es wird vorgeschlagen, Diskriminatoren, die nur eine einzige Probe berücksichtigen, durch Diskriminatoren zu ersetzen, die explizit mit Verteilungen von Beispielen arbeiten.
Theorie über Two-Sample Tests und MMD und wie sie vorteilhaft in den GAN-Rahmen integriert werden können.
Wir haben ein Ende-zu-Ende Framework entwickelt, das ein Sequenz zu Sequenz Modell für die Standardisierung chemischer Namen verwendet.
Standardisiert nicht systematische Namen in der chemischen Informationsextraktion durch Erstellung eines parallelen Korpus von nicht systematischen und systematischen Namen und Aufbau eines seq2seq-Modells.
In dieser Arbeit wird eine Methode vorgestellt, mit der nicht systematische Namen chemischer Verbindungen durch eine Kombination von Mechanismen in ihre systematischen Äquivalente übersetzt werden können.
Die SGD wird zu Beginn des Trainings in einen Bereich gelenkt, in dem ihr Schritt im Vergleich zur Krümmung zu groß ist, was sich auf den Rest des Trainings auswirkt. 
Analysiert die Beziehung zwischen der Konvergenz/Generalisierung und der Aktualisierung der größten Eigenvektoren der Hessian der empirischen Verluste von DNNs.
In dieser Arbeit wird die Beziehung zwischen der SGD-Schrittgröße und der Krümmung der Verlustfläche untersucht
Wir stellen einen neuartigen Reinforcement Learning Algorithmus vor, der mehrere Handlungen vorhersagt und daraus Proben zieht.
In dieser Arbeit wird eine einheitliche Mischung aus deterministischen Richtlinien eingeführt, und es wird festgestellt, dass diese Parametrisierung stochastischer Richtlinien DDPG bei mehreren OpenAI-Gym-Benchmarks übertrifft.
Die Autoren untersuchen eine Methode zur Verbesserung der Leistung von Netzwerken, die mit DDPG trainiert wurden, und zeigen eine verbesserte Leistung bei einer großen Anzahl von standardmäßigen kontinuierlichen Kontrollumgebungen.
Da wir die Nachteile bei der Anwendung des ursprünglichen Dropout-Verfahrens auf DenseNet erkannt haben, haben wir die Dropout-Methode unter drei Aspekten entwickelt, die auch auf andere CNN-Modelle angewendet werden können.
Anwendung verschiedener binärer Dropout-Strukturen und Zeitpläne mit dem spezifischen Ziel, die DenseNet-Architektur zu regulieren.
Vorschlagen einer Pre-Dropout-Technik für densenet, die den Dropout vor der nichtlinearen Aktivierungsfunktion implementiert.
Beziehungsbewusste tiefe Modelle zu besserem Lernen mit menschlichem Wissen anleiten.
In dieser Arbeit wird eine Variante des Säulennetzes vorgeschlagen, die auf der Injektion menschlicher Führung durch Änderung der Berechnungen im Netz beruht.
Eine Methode zur Einbeziehung menschlicher Ratschläge in das Deep Learning durch die Erweiterung von Column Network, einem graphischen neuronalen Netz für kollektive Klassifizierung.
Die jüngsten Erfolge binärer neuronaler Netze lassen sich auf der Grundlage der Geometrie hochdimensionaler binärer Vektoren verstehen.
Untersucht numerisch und theoretisch die Gründe für den empirischen Erfolg von binarisierten neuronalen Netzen.
In diesem Beitrag wird die Wirksamkeit binärer neuronaler Netze analysiert und erläutert, warum die Binarisierung die Leistung des Modells erhalten kann.
Nachdem wir bewiesen haben, dass ein Neuron als inverser Problemlöser für die Superauflösung fungiert und ein Netzwerk von Neuronen garantiert eine Lösung liefert, haben wir eine doppelte Netzwerkarchitektur vorgeschlagen, die schneller als der Stand der Technik ist.
Erörtert die Verwendung neuronaler Netze für die Superauflösung.
Eine neue Architektur zur Lösung von Bild-Superauflösungsaufgaben und eine Analyse, die darauf abzielt, eine Verbindung zwischen CNNs zur Lösung von Superauflösung und zur Lösung von spärlichen regularisierten inversen Problemen herzustellen.
Dynamisches Modell, das durch schwache Überwachung Teilungs- und Eroberungsstrategien erlernt.
Schlägt vor, der Architektur eines neuronalen Netzes eine neue induktive Verzerrung hinzuzufügen, indem eine Strategie des Teilens und Eroberns angewendet wird.
Diese Arbeit untersucht Probleme, die mit einem dynamischen Programmieransatz gelöst werden können, und schlägt eine neuronale Netzwerkarchitektur zur Lösung solcher Probleme vor, die Sequenz-zu-Sequenz Baselines übertrifft.
Die Arbeit schlägt eine einzigartige Netzwerkarchitektur vor, die Divide-and-Conquer Strategien zur Lösung algorithmischer Aufgaben erlernen kann.
Ein Rep-ähnlicher Gradient für nicht reparametrisierbare kontinuierliche/diskrete Verteilungen; weiter verallgemeinert auf tiefe probabilistische Modelle, was zu statistischer Backpropagation führt.
Stellt einen Gradientenschätzer für erwartungsbasierte Ziele vor, der unvoreingenommen ist, eine geringe Varianz aufweist und sowohl für kontinuierliche als auch für diskrete Zufallsvariablen gilt.
Eine verbesserte Methode zur Berechnung von Ableitungen des Erwartungswerts und ein neuer Gradientenschätzer mit geringer Varianz, der das Training von generativen Modellen ermöglicht, bei denen Beobachtungen oder latente Variablen diskret sind.
Entwirft einen Gradienten mit geringer Varianz für Verteilungen im Zusammenhang mit kontinuierlichen oder diskreten Zufallsvariablen.
Wir zeigen, dass NN-Parameter- und Hyperparameter-Kostenlandschaften als Quantenzustände mit einem einzigen Quantenschaltkreis erzeugt werden können und dass diese für Training und Meta-Training verwendet werden können.
Beschreibt eine Methode, bei der ein Rahmen für tiefes Lernen quantisiert werden kann, indem die Zweizustandsform einer Bloch-Kugel/eines Qubits berücksichtigt und ein binäres neuronales Quantennetzwerk erstellt wird.
In diesem Beitrag wird die Quantenamplitudenverstärkung vorgeschlagen, ein neuer Algorithmus für das Training und die Modellauswahl in binären neuronalen Netzen.
Schlägt eine neuartige Idee zur Ausgabe eines Quantenzustands vor, der eine vollständige Kostenlandschaft aller Parameter für ein gegebenes binäres neuronales Netz darstellt, indem ein binäres neuronales Quantennetz (QBNN) konstruiert wird.
Ein allgemeines Verfahren zur Ausbildung eines zertifizierten, kostensensitiven und robusten Klassifizierers gegen negative Einflüsse.
Berechnet und fügt die Kosten eines adversarial Angriffs in das Optimierungsziel ein, um ein Modell zu erhalten, das kostensensitiv gegen feindliche Angriffe robust ist. 
Baut auf der semnialen Arbeit von Dalvi et al. auf und erweitert den Ansatz zur zertifizierbaren Robustheit um eine Kostenmatrix, die für jedes Paar von Quelle-Ziel-Klassen angibt, ob das Modell gegenüber gegnerischen Beispielen robust sein sollte.
Verwendung von Triplets zum Erlernen einer Metrik für den Vergleich neuronaler Reaktionen und zur Verbesserung der Leistung einer Prothese.
Die Autoren entwickeln neue Spike-Train-Abstandsmetriken, einschließlich neuronaler Netze und quadratischer Metriken. Diese Metriken übertreffen nachweislich die naive Hamming-Distanz-Metrik und erfassen implizit einige Strukturen im neuronalen Code.
Mit der Anwendung der Verbesserung der neuronalen Prothese im Auge, die Autoren vorschlagen, eine Metrik zwischen neuronalen Antworten entweder durch die Optimierung einer quadratischen Form oder ein tiefes neuronales Netz zu lernen.
In diesem Beitrag wird eine Methode vorgestellt, mit der Fragen (Hinweise) und Abfragen (Vorschläge) generiert werden können, um den Benutzer beim Mind-Mapping zu unterstützen.
Stellt ein Werkzeug vor, das das Mind-Mapping durch vorgeschlagenen Kontext in Bezug auf bestehende Knotenpunkte und durch Fragen, die weniger entwickelte Zweige erweitern, unterstützt.
In diesem Beitrag wird ein Ansatz zur Unterstützung von Menschen bei Mindmapping-Aufgaben vorgestellt, eine Schnittstelle und algorithmische Funktionen zur Unterstützung von Mindmapping entwickelt und eine Evaluierungsstudie durchgeführt.
Erkennung von Beispielen außerhalb der Verteilung durch Verwendung von Merkmalsstatistiken niedriger Ordnung, ohne dass eine Änderung des zugrunde liegenden DNN erforderlich ist.
Es wird ein Algorithmus zur Erkennung von Beispielen außerhalb der Verteilung vorgestellt, der die laufende Schätzung von Mittelwert und Varianz innerhalb von BatchNorm-Schichten verwendet, um Merkmalsdarstellungen zu konstruieren, die später in einen linearen Klassifikator eingegeben werden.
Ein Ansatz zur Erkennung von Beispielen außerhalb der Verteilung, bei dem die Autoren vorschlagen, eine logistische Regression über einfache Statistiken jeder Batch-Normalisierungsschicht von CNN zu verwenden.
In dem Beitrag wird vorgeschlagen, Z-Scores für den Vergleich von ID- und OOD-Stichproben zu verwenden, um zu bewerten, was Deep Nets zu tun versuchen.
Wir schlagen eine neue Methode namens Maximal Divergence Sequential AutoEncoder vor, die die variationale AutoEncoder-Darstellung für die Erkennung von Schwachstellen im Binärcode nutzt.
In diesem Beitrag wird eine auf einem variationalen Autoencoder basierende Architektur für Code-Einbettungen zur Erkennung von Schwachstellen in binärer Software vorgeschlagen, wobei gelernte Einbettungen effektiver zwischen anfälligem und nicht anfälligem Binärcode unterscheiden können als Basisprogramme.
In diesem Beitrag wird ein Modell zur automatischen Extraktion von Merkmalen für die Erkennung von Schwachstellen mithilfe von Deep Learning Techniken vorgeschlagen. 
Die Berechnung der Aufmerksamkeit auf der Grundlage der posterioren Verteilung führt zu sinnvollerer Aufmerksamkeit und besserer Leistung.
Diese Arbeit schlägt ein Sequenz-zu-Sequenz Modell vor, bei dem die Aufmerksamkeit als latente Variable behandelt wird, und leitet neuartige Inferenzverfahren für dieses Modell ab, mit denen Verbesserungen in der maschinellen Übersetzung und bei der Generierung morphologischer Flexionen erzielt werden.
Diese Arbeit stellt ein neuartiges posteriores Aufmerksamkeitsmodell für seq2seq-Probleme vor.
Komprimierung trainierter DNN-Modelle durch Minimierung ihrer Komplexität bei gleichzeitiger Begrenzung ihres Verlustes.
In dieser Arbeit wird eine Methode zur Komprimierung von tiefen neuronalen Netzen mit Genauigkeitseinschränkungen vorgeschlagen.
In diesem Beitrag wird eine verlustwertbeschränkte k-means Kodierungsmethode für die Netzwerkkompression vorgestellt und ein iterativer Algorithmus zur Modelloptimierung entwickelt.
Wir entwickeln eine Technik zur Visualisierung von Aufmerksamkeitsmechanismen in beliebigen neuronalen Netzen. 
Schlägt vor, ein latentes Aufmerksamkeitsnetz zu erlernen, das helfen kann, die innere Struktur eines tiefen neuronalen Netzes zu visualisieren.
Die Autoren dieser Arbeit schlagen ein datengesteuertes Black-Box Visualisierungsschema vor. 
Wir untersuchen eine Vielzahl von RL-Algorithmen für die Molekülgenerierung und definieren neue Benchmarks (die als OpenAI Gym veröffentlicht werden), wobei wir feststellen, dass PPO und ein Hill-Climbing MLE-Algorithmus am besten funktionieren.
Betrachtet die Modellevaluation für die Molekülgenerierung, indem 19 Benchmarks vorgeschlagen werden, kleine Datensätze zu einem großen, standardisierten Datensatz erweitert werden und untersucht wird, wie RL-Techniken für das Moleküldesign angewendet werden können.
Dieser Beitrag zeigt, dass die anspruchsvollsten RL-Methoden bei der Modellierung und Synthese von Molekülen weniger effektiv sind als die einfache Hill-Climbing-Technik, mit PPO als Ausnahme.
Die robusteste Fähigkeit zu analogem Denken entsteht, wenn Netze Analogien lernen, indem sie abstrakte relationale Strukturen in ihren Eingabedomänen gegenüberstellen.
Die Arbeit untersucht die Fähigkeit eines neuronalen Netzes, Analogien zu lernen, und zeigt, dass ein einfaches neuronales Netz in der Lage ist, bestimmte Analogieprobleme zu lösen.
In diesem Beitrag wird ein Ansatz zum Training neuronaler Netze für analoge Schlussfolgerungen beschrieben, der insbesondere visuelle Analogien und symbolische Analogien berücksichtigt.
Ein zielorientiertes neuronales Konversationsmodell durch Selbstspiel.
Ein Selbstspielmodell zur zielorientierten Dialoggenerierung, das eine stärkere Kopplung zwischen der Aufgabenbelohnung und dem Sprachmodell erzwingen soll.
Dieser Beitrag beschreibt eine Methode zur Verbesserung eines zielorientierten Dialogsystems durch Selbstspiel. 
Vervollständigung von Suchanfragen in Echtzeit mit LSTM-Sprachmodellen auf Zeichenebene.
In diesem Papier werden Methoden zur Abfragevervollständigung vorgestellt, die eine Präfixkorrektur und einige technische Details zur Erfüllung bestimmter Latenzanforderungen auf einer CPU umfassen.
Die Autoren schlagen einen Algorithmus zur Lösung des Problems der Abfragevervollständigung mit Fehlerkorrektur vor und übernehmen die RNN-basierte Modellierung auf Zeichenebene und optimieren den Inferenzteil, um Ziele in Echtzeit zu erreichen.
In dieser Arbeit beweisen wir die Konvergenz zur Kritikalität von (stochastischem und deterministischem) RMSProp und deterministischem ADAM für glatte, nicht-konvexe Ziele und wir demonstrieren eine interessante beta_1-Sensitivität für ADAM auf Autoencodern. 
In diesem Beitrag wird eine Konvergenzanalyse von RMSProp und ADAM für glatte, nicht-konvexe Funktionen vorgestellt.
Die Entwicklung unüberwachter Verteidigungsmechanismen gegen gegnerische Angriffe ist entscheidend, um die Verallgemeinerbarkeit der Verteidigung zu gewährleisten. 
In dieser Arbeit wird eine Methode zur Erkennung von adversarial Beispielen in einer Deep Learning Klassifizierungsumgebung vorgestellt.
In diesem Beitrag wird eine unbeaufsichtigte Methode zur Erkennung von adversarial Beispielen in neuronalen Netzen vorgestellt.
Proxy-less neuronale Architektur Suche nach direkt lernenden Architekturen auf großen Zielaufgaben (ImageNet) bei gleichzeitiger Reduzierung der Kosten auf das gleiche Niveau des normalen Trainings.
Diese Arbeit befasst sich mit dem Problem der Architektursuche und versucht insbesondere, dies zu tun, ohne auf "Proxy"-Aufgaben trainieren zu müssen, bei denen das Problem durch eine geringere Optimierung, architektonische Komplexität oder Datenmenge vereinfacht ist.
Ein neuer Rahmen auf der Grundlage der Variationsinferenz für die Erkennung von Verteilungsabweichungen.
Beschreibt einen probabilistischen Ansatz zur Quantifizierung der Unsicherheit bei DNN-Klassifizierungsaufgaben, der andere SOTA-Methoden bei der Erkennung von Abweichungen von der Verteilung übertrifft.
Ein neues Framework für die Erkennung von Out-of-Distribution, basierend auf variabler Inferenz und einer priorisierten Dirichlet-Verteilung, der den aktuellen Stand der Technik anhand verschiedener Datensätze darstellt.
Out-of-Distribution Erkennung durch eine neue Methode zur Annäherung an die Konfidenzverteilung der Klassifizierungswahrscheinlichkeit unter Verwendung der Variationsinferenz der Dirichlet-Verteilung.
Wir lernen eine Repräsentation des Handlungsraums eines Agenten aus rein visuellen Beobachtungen. Wir verwenden einen rekurrenten latenten Variablenansatz mit einem neuartigen Kompositionsverlust.
Schlägt ein kompositionelles latentes Variablenmodell vor, um Modelle zu erlernen, die vorhersagen, was als Nächstes in Szenarien geschieht, in denen Handlungskennzeichen nicht in großer Zahl verfügbar sind.
Ein auf variationalem IB basierender Ansatz zum Erlernen von Handlungsrepräsentationen direkt aus Videos von ausgeführten Handlungen, der eine bessere Effizienz nachfolgender Lernmethoden erzielt und gleichzeitig eine geringere Menge an Videos mit Handlungskennzeichnungen erfordert.
In diesem Beitrag wird ein Ansatz zur Videovorhersage vorgeschlagen, der autonom einen Aktionsraum findet, der Unterschiede zwischen aufeinanderfolgenden Bildern kodiert.
Reinforcement Learning kann verwendet werden, um Agenten zu trainieren, die Teambildung über viele Verhandlungsprotokolle hinweg zu verhandeln.
Diese Arbeit untersucht tiefes Multi-Agenten-RL in Umgebungen, in denen alle Agenten kooperieren müssen, um eine Aufgabe zu erfüllen (z.B. Suche und Rettung, Multiplayer-Videospiele), und verwendet einfache kooperative gewichtete Abstimmungsspiele, um die Wirksamkeit von tiefem RL zu untersuchen und um Lösungen, die durch tiefes RL gefunden wurden, mit einer fairen Lösung zu vergleichen.
Ein Ansatz des Reinforcement Learnings für die Aushandlung von Koalitionen in kooperativen spieltheoretischen Kontexten, der in Fällen verwendet werden kann, in denen unbegrenzte Trainingssimulationen verfügbar sind.
Unüberwachte Methoden zum Auffinden, Analysieren und Kontrollieren wichtiger Neuronen in der NMT.
In diesem Beitrag werden unbeaufsichtigte Ansätze zur Entdeckung wichtiger Neuronen in neuronalen maschinellen Übersetzungssystemen vorgestellt und die von diesen Neuronen kontrollierten linguistischen Eigenschaften analysiert.
Unüberwachte Methoden zur Einstufung von Neuronen in der maschinellen Übersetzung, bei denen wichtige Neuronen identifiziert und zur Steuerung der MÜ-Ausgabe verwendet werden.
Wir schlagen einen DRL-Rahmen vor, der aufgaben- und umgebungsspezifisches Wissen voneinander trennt.
Die Autoren schlagen vor, das Reinforcement Learning in eine PATH-Funktion und eine GOAL-Funktion zu zerlegen.
Eine modulare Architektur mit dem Ziel, umweltspezifisches Wissen und aufgabenspezifisches Wissen in verschiedene Module aufzuteilen, die dem Standard-A3C für ein breites Spektrum von Aufgaben entsprechen.
pix2scene: ein tiefgreifender generativer Ansatz zur impliziten Modellierung der geometrischen Eigenschaften einer 3D-Szene aus Bildern.
Untersucht die Erklärung von Szenen mit Surfels in einem neuronalen Erkennungsmodell und demonstriert Ergebnisse zur Bildrekonstruktion, Synthese und mentalen Formrotation.
Die Autoren stellen eine Methode zur Erstellung eines 3D-Szenenmodells anhand eines 2D-Bildes und einer Kameraposition unter Verwendung eines selbst-superfizierten Modells vor.
Die Identifizierung der Beziehungen, die Wörter verbinden, ist für verschiedene NLP-Aufgaben wichtig. Wir modellieren die Darstellung von Beziehungen als überwachtes Lernproblem und lernen parametrisierte Operatoren, die vortrainierte Worteinbettungen auf Beziehungsrepräsentationen abbilden.
In diesem Beitrag wird eine neuartige Methode zur Darstellung lexikalischer Beziehungen als Vektoren vorgestellt, bei der nur vortrainierte Worteinbettungen und eine neuartige Verlustfunktion für Wortpaare verwendet werden.
Eine neuartige Lösung für das Problem der Beziehungskomposition, wenn Sie bereits trainierte Wort-/Entitätseinbettungen haben und nur daran interessiert sind, zu lernen, Beziehungsrepräsentationen zu komponieren.
Wir schlagen vor, zwei identische Kopien eines rekurrenten neuronalen Netzes (mit gemeinsamen Parametern) mit unterschiedlichen Dropout Masken zu trainieren und dabei die Differenz zwischen ihren (pre-softmax) Vorhersagen zu minimieren.
Präsentiert Fraternal Dropout als Verbesserung gegenüber Expectation-linear Dropout in Bezug auf Konvergenz und demonstriert die Nützlichkeit von Fraternal Dropout an einer Reihe von Aufgaben und Datensätzen.
Gewichtung und Verformung von Raum-Zeit-Datensätzen für hocheffiziente Annäherungen an das Flüssigkeitsverhalten lernen.
Ein auf einem neuronalen Netz basierendes Modell wird zur Interpolation von Simulationen für neuartige Szenenbedingungen aus dicht registrierten impliziten 4D-Oberflächen für eine strukturierte Szene verwendet.
In diesem Beitrag wird ein gekoppelter Deep-Learning Ansatz zur Erzeugung realistischer Flüssigkeitssimulationsdaten vorgestellt, die für Echtzeit-Entscheidungsunterstützungsanwendungen nützlich sein können.
In diesem Beitrag wird ein Deep-Learning Ansatz für physikalische Simulationen vorgestellt, der zwei Netzwerke zur Synthese von 4D-Daten kombiniert, die physikalische 3D-Simulationen darstellen.
Wir konstruieren und bewerten farbinvariante neuronale Netze auf einem neuen realistischen Datensatz.
Schlägt eine Methode vor, um neuronale Netze für die Bilderkennung farbinvariant zu machen, und evaluiert sie anhand des cifar 10-Datensatzes.
Die Autoren untersuchen eine modifizierte Eingabeschicht, die zu farbinvarianten Netzen führt, und zeigen, dass bestimmte farbinvariante Eingabeschichten die Genauigkeit bei Testbildern mit einer anderen Farbverteilung als die Trainingsbilder verbessern können.
Die Autoren testen ein CNN auf Bildern mit Farbkanälen, die so verändert wurden, dass sie gegenüber Permutationen invariant sind, wobei die Leistung nicht allzu sehr beeinträchtigt wurde. 
Wir analysieren, wie der Grad der Überlappungen zwischen den rezeptiven Feldern eines Convolutional Networks seine Ausdruckskraft beeinflusst.
Die Arbeit untersucht die Ausdruckskraft, die durch "Überlappung" in Convolution Layers von DNNs bereitgestellt wird, indem lineare Aktivierungen mit Produktpooling berücksichtigt werden.
In diesem Beitrag wird die Ausdrucksfähigkeit von Convolutional Arithmetic Circuits analysiert und gezeigt, dass eine exponentiell große Anzahl von nicht überlappenden ConvACs erforderlich ist, um den Gittertensor eines überlappenden ConvACs zu approximieren.
Ein theoretischer Algorithmus zum Testen der lokalen Optimalität und zum Extrahieren von Abstiegsrichtungen an nicht differenzierbaren Punkten von empirischen Risiken von einschichtigen ReLU-Netzen.
Schlägt einen Algorithmus vor, um zu prüfen, ob ein gegebener Punkt ein verallgemeinerter stationärer Punkt zweiter Ordnung ist.
Ein theoretischer Algorithmus, der die Lösung von konvexen und nicht-konvexen quadratischen Programmen beinhaltet, um die lokale Optimalität zu überprüfen und Sättigungen beim Training von zweischichtigen ReLU-Netzen zu vermeiden.
Der Autor schlägt eine Methode vor, mit der geprüft werden kann, ob ein Punkt ein stationärer Punkt ist oder nicht, und klassifiziert stationäre Punkte dann entweder als lokale Minimalpunkte oder als stationäre Punkte zweiter Ordnung.
Ein neuer, auf relativ harten Negativen basierender Verlust, der die beste Leistung bei der Suche nach Bildunterschriften erzielt.
Erlernen der gemeinsamen Einbettung von Sätzen und Bildern unter Verwendung von Tripelverlusten, die auf die härtesten Negative angewandt werden, anstatt über alle Tripel zu mitteln.
Wir nutzen das Prinzip der alternierenden Minimierung, um eine effektive neue Technik zum Trainieren von Deep Autoencodern bereitzustellen.
Alternierendes Minimierungs Framework für das Training von Autoencoder und Encoder-Decoder Netzwerken.
Die Autoren erforschen einen alternierenden Optimierungsansatz für das Training von Autoencodern, wobei jede Schicht als verallgemeinertes lineares Modell behandelt wird, und schlagen vor, den stochastischen normalisierten GD als Minimierungsalgorithmus in jeder Phase zu verwenden.
Transferlernen zur Schätzung kausaler Effekte unter Verwendung neuronaler Netze.
Entwicklung von Algorithmen zur Schätzung des bedingten durchschnittlichen Behandlungseffekts anhand von Hilfsdatensätzen in verschiedenen Umgebungen, sowohl mit als auch ohne Basis-Lerner.
Die Autoren schlagen Methoden vor, um eine neuartige Aufgabe des Transfer-Lernens für die Schätzung der CATE-Funktion anzugehen, und bewerten sie anhand einer synthetischen Umgebung und eines realen experimentellen Datensatzes.
Verwendung der Regression mit neuronalen Netzen und Vergleich von Transfer-Learning-Konzepten zur Schätzung eines bedingten durchschnittlichen Behandlungseffekts unter der Annahme der String-Ignorierbarkeit.
Wir stellen LeMoNADe vor, eine durchgängig erlernte Methode zur Motiverkennung, die direkt auf Calcium Bildgebungsvideos arbeitet.
Dieses Papier schlägt ein VAE-ähnliches Modell zur Identifizierung von Motiven aus Calcium Bildgebungsvideos vor, das sich auf Bernouli-Variablen stützt und einen Gumbel-Softmax-Trick zur Inferenz benötigt.
Wir schlagen einen Rahmen vor, um eine gute Strategie durch Imitationslernen aus einer verrauschten Demonstrationsmenge zu erlernen, indem wir ein Meta-Training für die Bewertung der Demonstrationseignung durchführen.
Trägt einen MAML-basierten Algorithmus zum Imitation Learning bei, der automatisch feststellt, ob die angebotenen Demonstrationen "geeignet" sind.
Eine Methode zum Imitationslernen aus einer Menge von Demonstrationen, die unbrauchbares Verhalten enthält, die die nützlichen Demonstrationen durch ihre Leistungsgewinne zum Zeitpunkt des Meta-Trainings auswählt.
Wir führen kausale implizite generative Modelle ein, die aus bedingten und intervenierenden Verteilungen abfragen können, und schlagen außerdem zwei neue bedingte GANs vor, die wir für ihr Training verwenden.
Verfahren zur Kombination eines zufälligen Graphen, der die Abhängigkeitsstruktur von Etiketten beschreibt, mit zwei bedingten GAN-Architekturen, die Bilder erzeugen, die sich auf das binäre Label beziehen.
Die Autoren befassen sich mit der Frage des Lernens eines kausalen Modells zwischen Bildvariablen und dem Bild selbst aus Beobachtungsdaten, wenn eine kausale Struktur zwischen Bildbezeichnungen gegeben ist.
Wir beweisen, dass NCE selbstnormiert ist und demonstrieren dies an Datensätzen.
Es wird ein Beweis für die Selbstnormalisierung der NCE als Ergebnis einer rangniedrigen Matrixapproximation der rangniedrigen Approximation der normalisierten bedingten Wahrscheinlichkeitsmatrix vorgelegt.
In diesem Beitrag wird das Problem der selbstnormalisierenden Modelle betrachtet und der Mechanismus der Selbstnormalisierung durch Interpretation der NCE im Sinne der Matrixfaktorisierung erklärt.
Die Anreicherung von Worteinbettungen mit Affektinformationen verbessert deren Leistung bei Sentiment Vorhersageaufgaben.
Es wird vorgeschlagen, Affekt-Lexika zur Verbesserung der Worteinbettung zu verwenden, um die Standardlösungen Word2vec und Glove zu übertreffen.
In diesem Beitrag wird vorgeschlagen, Informationen aus einer semantischen Ressource, die den Affekt von Wörtern quantifiziert, in einen textbasierten Worteinbettungsalgorithmus zu integrieren, um Sprachmodelle besser auf semantische und pragmatische Phänomene abzustimmen.
In diesem Beitrag werden Modifikationen der Verlustfunktionen word2vec und GloVe vorgestellt, um Affekt-Lexika einzubeziehen und das Lernen affektsensitiver Worteinbettungen zu erleichtern.
Für die unbeaufsichtigte und induktive Netzwerkeinbettung schlagen wir einen neuartigen Ansatz vor, um die relevantesten Nachbarn zu erkunden und das zuvor gelernte Wissen über die Knoten zu erhalten, indem wir eine Bi-Attention-Architektur verwenden bzw. eine globale Verzerrung einführen
Hier wird eine Erweiterung von GraphSAGE vorgeschlagen, die eine globale Einbettungsmatrix in den lokalen Aggregationsfunktionen und eine Methode zur Auswahl interessanter Knoten verwendet.
Wir argumentieren, dass die Verallgemeinerung der linearen Grapheneinbettung nicht auf die Dimensionalitätsbeschränkung zurückzuführen ist, sondern vielmehr auf die kleine Norm der Einbettungsvektoren.
Die Autoren zeigen, dass der Generalisierungsfehler von linearen Grapheneinbettungsmethoden durch die Norm der Einbettungsvektoren und nicht durch Dimensionalitätsbeschränkungen begrenzt wird.
Die Autoren schlagen eine theoretische Schranke für die Generalisierungsleistung des Lernens von Grapheneinbettungen vor und argumentieren, dass die Norm der Koordinaten den Erfolg der gelernten Darstellung bestimmt.
Mischen Sie SGD und Momentum (oder machen Sie etwas Ähnliches mit Adam), um große Gewinne zu erzielen.
Die Arbeit schlägt einfache Modifikationen von SGD und Adam vor, sogenannte QH-Varianten, die die Eltern-Methode und eine Reihe anderer Optimierungstricks wiederherstellen können.
Eine Variante des klassischen Impulses, die ein gewichtetes Mittel aus Impuls- und Gradientenaktualisierung verwendet, sowie eine Bewertung der Beziehungen zwischen anderen Impuls-basierten Optimierungsverfahren.
Ein neuartiger Weg zur Verallgemeinerung von Lambda-Renditen, indem der RL-Agent entscheiden kann, wie stark er jede der n-Schritt-Renditen gewichten möchte.
Erweitert den A3C-Algorithmus mit Lambda-Rückgaben und schlägt einen Ansatz zum Lernen der Gewichte der Rückgaben vor.
Die Autoren stellen konfidenzbasierte autodidaktische Rückgaben vor, eine Deep Learning RL-Methode zur Anpassung der Gewichte eines Eignungsvektors in TD(lambda)-ähnlichen Wertschätzungen, um stabilere Schätzungen des Zustands zu begünstigen.
Wir haben ein neues selbstfahrendes Modell vorgeschlagen, das sich aus einem Wahrnehmungsmodul für das Sehen und Denken und einem Fahrmodul für das Verhalten zusammensetzt, um eine bessere Generalisierungs- und Unfallerklärungsfähigkeit zu erreichen.
Vorgestellt wird eine Multitasking-Lernarchitektur für die Schätzung von Tiefen- und Segmentierungskarten und die Fahrvorhersage unter Verwendung eines Wahrnehmungsmoduls und eines Fahrentscheidungsmoduls.
Eine Methode für eine modifizierte Ende-zu-Ende Architektur, die eine bessere Verallgemeinerungs- und Erklärungsfähigkeit aufweist, robuster gegenüber verschiedenen Testumgebungen ist und über eine Decoderausgabe verfügt, die bei der Fehlersuche im Modell helfen kann.
Die Autoren stellen ein neuronales Multi-Task Convolutional Neural Network für durchgängiges Fahren vor und liefern Auswertungen mit dem Open-Source-Simulator CARLA, die eine bessere Generalisierungsleistung unter neuen Fahrbedingungen zeigen als die Grundlinien.
Ein allgemeiner Rahmen zur Skalierung bestehender Grapheneinbettungstechniken auf große Graphen.
In diesem Beitrag wird ein mehrstufiges Einbettungsframework vorgeschlagen, das zusätzlich zu den bestehenden Netzwerkeinbettungsmethoden angewendet werden kann, um große Netzwerke mit höherer Geschwindigkeit zu skalieren.
Die Autoren schlagen ein dreistufiges Framework für die Einbettung großer Graphen mit verbesserter Einbettungsqualität vor.
Eine neuartige Methode zur Erhöhung der Widerstandsfähigkeit von OCSVMs gegen gezielte Integritätsangriffe durch selektive nichtlineare Transformationen von Daten in niedrigere Dimensionen.
Die Autoren schlagen eine Verteidigung gegen Angriffe auf die Sicherheit von einklassigen SVM-basierten Anomalie-Detektoren vor.
In diesem Papier wird untersucht, wie zufällige Projektionen verwendet werden können, um OCSVM robust gegenüber adversarially gestörten Trainingsdaten zu machen.
Lernen einer besseren Darstellung neuronaler Netze mit dem Prinzip des Informationsengpasses.
Schlägt eine Lernmethode vor, die auf dem Informationsengpass Framework basiert, bei dem verborgene Schichten von tiefen Netzen die Eingabe X komprimieren und gleichzeitig genügend Informationen zur Vorhersage der Ausgabe Y beibehalten.
In diesem Beitrag wird eine neue Methode für das Training stochastischer neuronaler Netze vorgestellt, die auf der Grundlage von Informationsrelevanz und -kompression arbeitet, ähnlich wie beim Information Bottleneck.
Wir schlagen einen Schätzer für die maximale mittlere Diskrepanz vor, der geeignet ist, wenn eine Zielverteilung nur über ein verzerrtes Stichprobenauswahlverfahren zugänglich ist, und zeigen, dass er in einem generativen Netzwerk verwendet werden kann, um diese Verzerrung zu korrigieren.
Schlägt einen Wichtigkeits-gewichteten Schätzer der MMD vor, um die MMD zwischen Verteilungen zu schätzen, die auf Stichproben basieren, die nach einem bekannten oder geschätzten unbekannten Schema verzerrt sind.
Die Autoren befassen sich mit dem Problem der Verzerrung der Stichprobenauswahl bei MMD-GANs und schlagen eine Schätzung der MMD zwischen zwei Verteilungen unter Verwendung der gewichteten maximalen mittleren Diskrepanz vor.
In diesem Papier wird eine Modifikation des Ziels für das Training generativer Netze mit einem MMD-Gegner vorgestellt.
Verwendung von Bayes'scher Regression zur Schätzung des Posterior über Q-Funktionen und Einsatz von Thompson Sampling als gezielte Explorationsstrategie mit effizientem Kompromiss zwischen Exploration und Exploitation.
Die Autoren schlagen einen neuen Algorithmus für die Exploration in Deep RL vor, bei dem sie eine lineare Bayes'sche Regression mit Merkmalen aus der letzten Schicht eines DQN-Netzwerks anwenden, um die Q-Funktion für jede Aktion zu schätzen.
Die Autoren beschreiben, wie Bayes'sche neuronale Netze mit Thompson-Sampling für eine effiziente Exploration beim q-Lernen eingesetzt werden können, und schlagen einen Ansatz vor, der die Epsilon-Greedy Explorationsansätze übertrifft.
PolyCNN muss nur einen Seed convolutional Filter auf jeder Schicht lernen. Dies ist eine effiziente Variante des traditionellen CNN mit gleichwertiger Leistung.
Versuche, die Anzahl der Parameter des CNN-Modells zu reduzieren, indem die polynomiale Transformation von Filtern verwendet wird, um die Filterantworten zu vergrößern.
Die Autoren schlagen eine Weight-Sharing-Architektur vor, um die Anzahl der Parameter eines Convolutional Neural Networks mit Seed-Filtern zu reduzieren.
In dieser Arbeit schlagen wir KL-CPD vor, ein neuartiges Kernel-Lernverfahren für Zeitreihen-CPD, das eine untere Schranke der Testleistung über ein generatives Hilfsmodell als Ersatz für die abnormale Verteilung optimiert. 
Beschreibt einen neuartigen Ansatz zur Optimierung der Wahl des Kernels im Hinblick auf eine höhere Testleistung und zeigt, dass er Verbesserungen gegenüber Alternativen bietet.
Clustern, bevor Sie klassifizieren; Verwendung schwacher Kennzeichnungen zur Verbesserung der Klassifizierung.
Vorschlagen der Verwendung einer auf Clustering basierenden Verlustfunktion auf mehreren Ebenen eines Deepnets, sowie die Verwendung einer hierarchischen Struktur des Beschriftungsraums, um bessere Darstellungen zu trainieren.
In diesem Beitrag werden hierarchische Label-Informationen verwendet, um zusätzliche Verluste auf Zwischenrepräsentationen beim Training neuronaler Netze zu erheben.
Advantage-basierte Regret-Minimierung ist ein neuer Deep Reinforcement Learning-Algorithmus, der besonders effektiv bei teilweise beobachtbaren Aufgaben ist, wie z.B. 1st Person Navigation in Doom und Minecraft.
In diesem Beitrag werden die Konzepte der kontrafaktischen Bedauernsminimierung im Bereich des Deep RL und ein Algorithmus namens ARM vorgestellt, der besser mit partieller Beobachtbarkeit umgehen kann.
Die Arbeit bietet eine spieltheoretisch inspirierte Variante des Policy-Gradienten-Algorithmus, die auf der Idee der kontrafaktischen Bedauernsminimierung basiert, und behauptet, dass der Ansatz mit dem teilweise beobachtbaren Bereich besser umgehen kann als Standardmethoden.
Ein tiefes Multi-Task-Lernmodell, das die Tensor-Ring-Darstellung anpasst.
Eine Variante der Tensor-Ring-Formulierung für Multi-Task-Lernen, bei der einige TT-Kerne für das Lernen einer "gemeinsamen Aufgabe" gemeinsam genutzt werden, während für jede einzelne Aufgabe individuelle TT-Kerne gelernt werden.
Ein Regressionsmodell, das die bedingten Verteilungen eines stochastischen Prozesses lernt, indem es die Aufmerksamkeit in neuronale Prozesse einbezieht.
Es wird vorgeschlagen, das Problem der unzureichenden Anpassung bei der neuronalen Prozessmethode zu lösen, indem dem deterministischen Pfad ein Aufmerksamkeitsmechanismus hinzugefügt wird.
Eine Erweiterung des Rahmens der Neuronalen Prozesse, die einen aufmerksamkeitsbasierten Konditionierungsmechanismus hinzufügt, der es dem Modell ermöglicht, Abhängigkeiten in der Konditionierungsmenge besser zu erfassen.
Die Autoren erweitern neuronale Prozesse, indem sie die self-attention zur Anreicherung der Merkmale der Kontextpunkte und die cross-attention zur Erzeugung einer abfragespezifischen Repräsentation einbeziehen. Sie lösen das Underfitting-Problem von NPs und zeigen, dass ANPs besser und schneller konvergieren als NPs.
Lösung des Schachbrettproblems in der Deconvolutional Layer durch Aufbau von Abhängigkeiten zwischen Pixeln.
In dieser Arbeit werden Pixel Deconvolutional Layers für Convolutional Neural Networks vorgeschlagen, um den Schachbretteffekt zu mildern.
Eine neuartige Technik zur Verallgemeinerung von Deconvolution Operationen, die in Standard CNN Architekturen verwendet werden, die eine sequentielle Vorhersage von benachbarten Pixelmerkmalen vorschlägt, was zu räumlich glatteren Ausgaben für Deconvolution Layers führt.
Wir quantisieren und prunen die Gewichte des neuronalen Netzes mit Hilfe von Bayes'scher Variationsinferenz mit einem multimodalen, Seltenheit induzierenden Prior.
Schlägt vor, eine Mischung aus kontinuierlichem Spike-Propto 1/abs als Prior für ein Bayes'sches neuronales Netz zu verwenden und demonstriert die gute Leistung mit relativ sparsamen Convnets für Minist und Cifar-10.
In diesem Beitrag wird ein variationaler Bayes'scher Ansatz vorgestellt, mit dem die Gewichte neuronaler Netze nach dem Training auf prinzipielle Weise auf ternäre Werte quantifiziert werden können.
Wir implementieren einen DNN-Gewichts Pruning Ansatz, der die höchsten Pruning Raten erzielt.
Diese Arbeit konzentriert sich auf das Pruning von Gewichten für die Kompression neuronaler Netze und erreicht eine 30-fache Kompressionsrate für AlexNet und VGG für ImageNet.
Eine progressive Pruning Technik, die den Gewichtsparametern eine strukturelle Seltenheitsbeschränkung auferlegt und die Optimierung als ADMM Framework umschreibt, wodurch eine höhere Genauigkeit als beim projizierten Gradientenabstieg erreicht wird.
In dieser Arbeit wird eine neue Deep Learning Architektur zur Lösung des Problems des überwachten Lernens mit spärlichen und unregelmäßig abgetasteten multivariaten Zeitreihen vorgestellt.
Schlägt einen Rahmen für die Erstellung von Vorhersagen für spärliche, unregelmäßig abgetastete Zeitreihendaten unter Verwendung eines Interpolationsmoduls vor, das die fehlenden Werte durch glatte Interpolation, nicht glatte Interpolation und Intensität modelliert. 
Löst das Problem des überwachten Lernens mit spärlichen und unregelmäßig abgetasteten multivariaten Zeitreihen mit Hilfe eines semiparametrischen Interpolationsnetzwerks, gefolgt von einem Vorhersagenetzwerk.
Permutationsinvariante Verlustfunktion für die Punktmengenvorhersage.
Schlägt einen neuen Verlust für die Punktregistrierung (Ausrichten von zwei Punktmengen) mit vorteilhafter permutationsinvarianter Eigenschaft vor. 
In diesem Beitrag wird eine neuartige Distanzfunktion zwischen Punktmengen eingeführt, zwei weitere Permutationsdistanzen in einer durchgängigen Objekterkennungsaufgabe angewendet und gezeigt, dass in zwei Dimensionen alle lokalen Minima des holographischen Verlustes globale Minima sind.
Vorschlagen permutationsinvarianter Verlustfunktionen, die von der Entfernung der Mengen abhängen.
Wir stellen ein hierarchisches Modell für die effiziente, durchgängige Platzierung von Berechnungsgraphen auf Hardwaregeräten vor.
Es wird vorgeschlagen, Gruppen von Operatoren gemeinsam zu erlernen und auf Geräten zu platzieren, um Operationen für tiefes Lernen durch Reinforcement Learning zu verteilen.
Die Autoren verwenden ein vollständig verbundenes Netzwerk, um den Schritt der Kollokation in einer automatischen Platzierungsmethode zu ersetzen, die zur Beschleunigung der Laufzeit eines TensorFlow-Modells vorgeschlagen wurde.
Schlägt einen Geräteplatzierungsalgorithmus vor, um Operationen von Tensorflow auf Geräten zu platzieren.
Wir schlagen eine Methode vor, die Gruppeneigenschaften nutzt, um eine Darstellung von Bewegung ohne Labels zu erlernen, und demonstrieren die Verwendung dieser Methode zur Darstellung von 2D- und 3D-Bewegungen.
Schlägt vor, die starre Bewegungsgruppe aus einer latenten Repräsentation von Bildsequenzen zu lernen, ohne dass explizite Beschriftungen erforderlich sind, und demonstriert die Methode experimentell an Sequenzen von MINST-Ziffern und dem KITTI-Datensatz.
Diese Arbeit schlägt einen Ansatz für das Erlernen von Video-Bewegungsmerkmalen in einer unbeaufsichtigten Art und Weise, unter Verwendung von Einschränkungen zur Optimierung des neuronalen Netzes, um Merkmale zu erzeugen, die zur Regression der Odometrie verwendet werden können.
In dieser Arbeit wird eine neuartige Convolutional Layer vorgeschlagen, die in einem kontinuierlichen Reproduzierenden Kernel-Hilbert-Raum arbeitet.
Projektion von Beispielen in einen RK-Hilbert-Raum und Durchführung von Convolution und Filterung in diesem Raum.
Diese Arbeit formuliert eine Variante von Convolutional Neural Networks, die beides, Aktivierungen und Filter, als kontinuierliche Funktionen modelliert, die aus Kernel-Basen zusammengesetzt sind.
ImageNet-trainierte CNNs sind auf die Textur von Objekten ausgerichtet (statt auf die Form wie beim Menschen). Die Überwindung dieses großen Unterschieds zwischen menschlichem und maschinellem Sehen führt zu einer verbesserten Erkennungsleistung und einer bisher nicht gekannten Robustheit gegenüber Bildverzerrungen.
Verwendung von Bildstilisierung zur Erweiterung der Trainingsdaten für ImageNet-trainierte CNNs, um die resultierenden Netzwerke besser an die menschlichen Urteile anzugleichen.
Diese Arbeit untersucht CNNs wie AlexNet, VGG, GoogleNet und ResNet50, zeigt, dass diese Modelle in Richtung Textur voreingenommen sind, wenn sie auf ImageNet trainiert werden, und schlägt einen neuen ImageNet-Datensatz vor.
Wir evaluieren die Effektivität von zusätzlichen diskriminierenden Aufgaben, die zusätzlich zu den Statistiken der Posterior-Verteilung durchgeführt werden, die von variativen Autoencodern gelernt wurden, um die Sprecherabhängigkeit zu erzwingen.
Vorschlag eines Autoencoder Modells zum Erlernen einer Repräsentation für die Sprecherverifikation unter Verwendung von Analysefenstern kurzer Dauer.
Eine modifizierte Version des variationalen Autoencoder Modells, das das Problem der Sprechererkennung im Zusammenhang mit kurzen Segmenten angeht.
Die Variationsinferenz ist verzerrt, wir sollten sie entzerren.
Einführung in die Jackknife-Variationsinferenz, eine Methode zur Entschärfung von Monte Carlo Zielen, wie z. B. dem nach Wichtigkeit gewichteten Autoencoder.
Die Autoren analysieren die Verzerrung und Varianz der IWAE-Schranke und leiten einen Jacknife-Ansatz zur Schätzung von Momenten als eine Möglichkeit zur Entlastung von IWAE für endliche, nach Wichtigkeit gewichtete Stichproben ab.
Ein Framework, das eine Strategie für den autonomen Fahrspurwechsel bereitstellt, indem es mit Hilfe von Deep Reinforcement Learning taktische Entscheidungen auf hoher Ebene trifft und eine enge Integration mit einer Low-Level Steuerung aufrechterhält, um Aktionen auf niedriger Ebene durchzuführen.
Betrachtet das Problem des autonomen Spurwechsels für selbstfahrende Autos in einer mehrspurigen Multi-Agenten Slotcar Umgebung und schlägt eine neue Lernstrategie Q-Masking vor, die eine definierte Steuerung auf niedriger Ebene mit taktischen Entscheidungsregeln auf hoher Ebene verbindet.
In diesem Beitrag wird ein Deep-Q-Learning-Ansatz für das Problem des Spurwechsels mit Hilfe von "Q-Masking" vorgeschlagen, der den Aktionsraum entsprechend den Einschränkungen oder dem Vorwissen reduziert.
Die Autoren schlagen eine Methode vor, bei der eine auf Q-Learning basierende High-Level Regelwerk mit einer kontextuellen Maske kombiniert wird, die aus Sicherheitsbedingungen und Low-Level Steuerungen abgeleitet wird, die bestimmte Aktionen in bestimmten Zuständen nicht auswählbar machen. 
Automatische Robotersuche mit graphischen neuronalen Netzen.
Vorschlagen eines Ansatz für die automatische Entwicklung von Robotern auf der Grundlage der Evolution neuronaler Graphen. Die Experimente zeigen, dass die Optimierung von Steuerung und Hardware besser ist als nur die Optimierung der Steuerung.
Die Autoren schlagen ein Schema vor, das auf einer graphischen Darstellung der Roboterstruktur und einem graphisch-neuronalen Netz als Steuerung basiert, um Roboterstrukturen in Kombination mit ihren Steuerungen zu optimieren.  
Wir demonstrieren einen Autoencoder für Graphen.
Erlernen der Erstellung von Graphen mit Hilfe von Deep-Learning-Methoden in "one shot", wobei die Wahrscheinlichkeit des Vorhandenseins von Knoten und Kanten sowie Knotenattributvektoren direkt ausgegeben werden.
Ein automatischer variationaler Autoencoder zur Erzeugung von Graphen.
Wir schlagen einen neuen Algorithmus für das LSTM Training durch Lernen in Richtung binärwertiger Gatter vor, von dem wir zeigen, dass er viele gute Eigenschaften hat.
Eine neue "Gate"-Funktion für LSTM vorschlagen, um die Werte der Gates auf 0 oder 1 zu setzen. 
Die Arbeit zielt darauf ab, LSTM-Gatter zu binären Gattern zu machen, indem es den neuen Gumbel-Softmax Trick anwendet, um eine durchgängig trainierbare kategoriale Verteilung zu erhalten.
Verbesserung der Empfehlungen durch zeitabhängige Modellierung mit neuronalen Netzen in mehreren Produktkategorien auf einer Einzelhandelswebsite.
In dem Beitrag wird eine neue, auf einem neuronalen Netz basierende Methode für Empfehlungen vorgeschlagen.
Die Autoren beschreiben ein Verfahren, mit dem sie ihr Empfehlungssystem von Grund auf neu aufbauen und den zeitlichen Verfall von Käufen in den Lern Frameworks integrieren.
Kopplung des GAN-basierten Rahmens für die Bildwiederherstellung mit einem anderen aufgabenspezifischen Netzwerk, um ein realistisches Bild zu erzeugen und gleichzeitig die aufgabenspezifischen Merkmale zu erhalten.
Eine neuartige Methode der Task-GAN Bildkopplung, die GAN und ein aufgabenspezifisches Netzwerk koppelt, um Halluzinationen oder einen Moduskollaps zu vermeiden.
Die Autoren schlagen vor, die GAN-basierte Bildrestauration mit einem anderen aufgabenspezifischen Zweig, wie z. B. Klassifizierungsaufgaben, zu erweitern, um weitere Verbesserungen zu erzielen.
Ein durchgängig trainiertes tiefes neuronales Netzwerk, das Gaussian Mixture Modeling nutzt, um Dichteabschätzungen und unbeaufsichtigte Anomalieerkennung in einem niedrigdimensionalen Raum durchzuführen, der von einem tiefen Autoencoder gelernt wird.
Die Arbeit präsentiert ein gemeinsames Deep Learning Framework für Dimensionsreduktion Clustering, das zu einer wettbewerbsfähigen Anomalieerkennung führt.
Ein neues Verfahren zur Erkennung von Anomalien, bei dem die Schritte Dimensionsreduzierung und Dichteschätzung gemeinsam optimiert werden.
Wir haben projektive Unterraumnetzwerke für das Few-Shot Learning und das halbüberwachte Few-Shot Learning vorgeschlagen.
In diesem Beitrag wird ein neuer, auf Einbettung basierender Ansatz für das Problem des Few-Shot Learnings und eine Erweiterung dieses Modells auf das semiüberwachte Few-Shot Learning vorgeschlagen.
Neue Methode zur voll- und halbüberwachten Few-Shot Klassifizierung, die auf dem Erlernen einer allgemeinen Einbettung und dem anschließenden Erlernen eines Unterraums davon für jede Klasse basiert.
Wir untersuchen kontingenzbewusste und kontrollierbare Aspekte bei der Erkundung und erreichen die beste Leistung auf Montezumas Rache ohne Expertendemonstrationen.
In diesem Beitrag wird das Problem der Extraktion einer aussagekräftigen Zustandsrepräsentation untersucht, die bei der Erkundung einer spärlichen Belohnungsaufgabe hilft, indem kontrollierbare (erlernte) Merkmale des Zustands identifiziert werden.
In diesem Beitrag wird die neuartige Idee vorgestellt, Kontingenzbewusstsein zur Unterstützung der Exploration bei spärlich belohnten Reinforcement Learning Aufgaben zu verwenden, und es werden Ergebnisse auf dem neuesten Stand der Technik erzielt.
Wir haben einen überwachten Algorithmus, DNA-GAN, vorgeschlagen, um mehrere Attribute von Bildern zu entwirren.
Diese Arbeit untersucht das Problem der attributbedingten Bilderzeugung mit Hilfe von generativen adversen Netzwerken und schlägt vor, Bilder aus Attributen und latentem Code als High-Level-Repräsentation zu erzeugen.
In diesem Beitrag wird eine neue Methode zur Entflechtung verschiedener Bildattribute unter Verwendung einer neuartigen DNA-Struktur GAN vorgeschlagen.
In diesem Beitrag wird ein neuartiges generatives Modellierungsverfahren für latente Variablen vorgestellt, das die Darstellung globaler Informationen in einer latenten Variable und lokaler Informationen in einer anderen latenten Variable ermöglicht.
Die Arbeit stellt eine VAE vor, die Etiketten verwendet, um die gelernte Repräsentation in einen invarianten und einen kovarianten Teil zu trennen.
Wir nähern uns dem Problem des aktiven Lernens als einem Kernmengenauswahlproblem und zeigen, dass dieser Ansatz besonders in der Batch Active Learning Umgebung nützlich ist, die beim Training von CNNs entscheidend ist.
Die Autoren stellen einen Algorithmus-agnostischen aktiven Lernalgorithmus für die Mehrklassen-Klassifizierung vor.
In dem Beitrag wird ein aktiver Lernalgorithmus im Batch-Modus für CNN als Kernmengenproblem vorgeschlagen, der Zufallsstichproben und Unsicherheitsstichproben übertrifft.
Untersucht aktives Lernen für Convolutional Neural Networks und formuliert das aktive Lernproblem als Kernmengenauswahl und stellt eine neue Strategie vor.
Ein einfacher Algorithmus zur Verbesserung der Optimierung und Handhabung langfristiger Abhängigkeiten in LSTM.
In diesem Beitrag wird ein einfacher stochastischer Algorithmus namens h-detach vorgestellt, der speziell für die LSTM-Optimierung entwickelt wurde und auf die Lösung dieses Problems ausgerichtet ist.
Vorschlagen einer einfachen Änderung des LSTM-Trainingsverfahrens, um die Gradientenfortpflanzung entlang der Zellzustände oder den "linearen zeitlichen Pfad" zu erleichtern.
Das richtige Training von CNNs mit der Dustbin-Klasse erhöht ihre Robustheit gegenüber adversarial Angriffen und ihre Fähigkeit, mit out-of-distribution Beispielen umzugehen.
In dieser Arbeit wird vorgeschlagen, ein zusätzliches Label zur Erkennung von OOD-Proben und adversarial Beispielen in CNN-Modellen hinzuzufügen.
In dem Beitrag wird eine zusätzliche Klasse vorgeschlagen, die natürliche out-of-distribution Bilder und interpolierte Bilder für adversarial und Out-of-Distribution Beispiele in CNNs einbezieht.
In einem tiefen Convolutional Neural Network, das mit einem ausreichenden Maß an Datenerweiterung trainiert und durch SGD optimiert wurde, könnten explizite Regularisierer (Gewichtsverfall und Dropout) keine zusätzliche Verbesserung der Generalisierung bewirken.
In diesem Beitrag wird die Datenerweiterung als Alternative zu den gängigen Regularisierungstechniken vorgeschlagen. Es wird gezeigt, dass für einige wenige Referenzmodelle/Aufgaben die gleiche Generalisierungsleistung nur durch Datenerweiterung erreicht werden kann.
In dieser Arbeit wird eine systematische Studie zur Datenerweiterung bei der Bildklassifizierung mit tiefen neuronalen Netzen vorgestellt, die zeigt, dass die Datenerweiterung einige gängige Regularisierer wie Gewichtsverfall und Dropout ersetzen kann.
In dieser Arbeit stellen wir Gedit vor, ein System von Gesten auf der Tastatur zur bequemen mobilen Textbearbeitung.
Berichtet über den Entwurf und die Bewertung der Gedit-Interaktionstechniken.
Präsentiert eine neue Reihe von Touch-Gesten für den nahtlosen Übergang zwischen Texteingabe und Textbearbeitung auf mobilen Geräten.
Wir beweisen, dass DNN eine rekursiv approximierte Lösung für das Prinzip der maximalen Entropie ist.
Es wird eine Herleitung vorgestellt, die ein DNN mit der rekursiven Anwendung der maximalen Entropie-Modellanpassung verbindet.
Die Arbeit zielt darauf ab, das Deep Learning aus der Perspektive des Prinzips der maximalen Entropie zu betrachten.
Wir stellen einen neuartigen Ansatz zum Reinforcement Learning vor, der eine aufgabenunabhängige intrinsische Belohnungsfunktion nutzt, die anhand von peripheren Pulsmessungen trainiert wird, die mit den Reaktionen des menschlichen autonomen Nervensystems korreliert sind. 
Schlägt einen Rahmen für das Reinforcement Learning vor, der auf der menschlichen emotionalen Reaktion im Kontext des autonomen Fahrens basiert.
Die Autoren schlagen vor, Signale, wie z.B. grundlegende autonome viszerale Reaktionen, die die Entscheidungsfindung beeinflussen, innerhalb des RL-Rahmens zu verwenden, indem RL-Belohnungsfunktionen mit einem Modell ergänzt werden, das direkt aus den Reaktionen des menschlichen Nervensystems gelernt wurde.
Schlägt vor, physiologische Signale zu nutzen, um die Leistung von Algorithmen des Reinforcement Learnings zu verbessern und eine intrinsische Belohnungsfunktion zu erstellen, die weniger spärlich ist, indem die Herzpulsamplitude gemessen wird.
Sind CNNs robust oder anfällig für Labelstörungen? Praktisch gesehen sind sie robust.
Die Autoren testen die Robustheit von CNNs gegenüber Labelstörungen anhand des ImageNet 1k-Baums von WordNet.
Eine Analyse der Leistung des Modells eines Convolutional Neural Networks, wenn klassenabhängige und klassenunabhängige Störungen eingeführt werden.
Zeigt, dass CNNs robuster gegenüber klassenrelevantem Labelstörungen sind und argumentiert, dass reale Störungen klassenrelevant sein sollten.
Hochwertige Audiosynthese mit GANs.
Schlägt einen Ansatz vor, der das GAN-Framework nutzt, um Audio durch die Modellierung von logarithmischen Größen und momentanen Frequenzen mit ausreichender Frequenzauflösung im Spektralbereich zu erzeugen. 
Eine Strategie zur Erzeugung von Audio-Samples aus Rauschen mit GANs, mit Änderungen an der Architektur und der Darstellung, die notwendig sind, um überzeugende Audios zu erzeugen, die einen interpretierbaren latenten Code enthalten.
Stellt eine einfache Idee zur besseren Darstellung von Audiodaten vor, so dass Convolutional Modelle wie generative adversarische Netze angewendet werden können.
Graph-Optimierung mit Signalfilterung im Vertex-Bereich.
Die Arbeit untersucht das Lernen der Adjazenzmatrix eines Graphen mit spärlich verbunden ungerichteten Graphen mit nicht-negativen Rand Gewichten unter Verwendung eines projizierten Sub-Gradient Descent Algorithmus.
Entwicklung eines neuen Verfahrens zur Rückkopplung auf der Adjazenzmatrix eines neuronalen Netzes
Dieses Papier beschreibt ein 3D Authoring Tool für die Bereitstellung von AR in Montagelinien der Industrie 4.0.
Das Papier befasst sich mit der Frage, wie AR Authoring Tools das Training von Fließbandsystemen unterstützen und schlägt einen Ansatz vor.
Ein AR-Leitsystem für industrielle Montagelinien, das die Erstellung von AR-Inhalten vor Ort ermöglicht.
Stellt ein System vor, mit dem Fabrikarbeiter mithilfe eines Augmented-Reality-Systems effizienter geschult werden können. 
Wir zeigen, dass der beim Training von GANs weit verbreitete iterative Algorithmus erster Ordnung bei richtiger Wahl der Schrittweite tatsächlich mit einer sublinearen Rate zu einer stationären Lösung konvergiert.
Diese Arbeit verwendet GANs und Multi-Task-Lernen, um eine Konvergenzgarantie für primär-duale Algorithmen auf bestimmten Min-Max-Problemen zu geben.
Analysiert die Lerndynamik von GANs durch Formulierung des Problems als primär-duales Optimierungsproblem unter Annahme einer begrenzten Klasse von Modellen.
Wir zeigen, wie man Deep RL nutzt, um Agenten zu konstruieren, die soziale Dilemmata jenseits von Matrixspielen lösen können.
Lernen von Zwei-Spieler General-Summen-Spielen mit unvollkommenen Informationen.
Spezifiziert eine Auslösestrategie (CCC) und einen entsprechenden Algorithmus, der die Konvergenz zu effizienten Ergebnissen in sozialen Dilemmas zeigt, ohne dass die Agenten die Handlungen der anderen beobachten müssen.
Die Arbeit schlägt zwei Quantisierungsschemata für die Kommunikation von stochastischen Gradienten beim verteilten Lernen vor und analysiert diese, um die Kommunikationskosten im Vergleich zum Stand der Technik zu reduzieren und gleichzeitig die gleiche Genauigkeit zu erhalten. 
Die Autoren schlagen vor, die stochastischen Gradienten, die durch den Trainingsprozess berechnet werden, mit einer Dither-Quantisierung zu versehen, um den Quantisierungsfehler zu verbessern und im Vergleich zu den Grundlinien bessere Ergebnisse zu erzielen, und schlagen ein verschachteltes Schema zur Reduzierung der Kommunikationskosten vor.
Die Autoren stellen eine Verbindung zwischen der Reduzierung der Kommunikation bei der verteilten Optimierung und der Dither-Quantisierung her und entwickeln zwei neue verteilte Trainingsalgorithmen, bei denen der Kommunikationsaufwand erheblich reduziert wird.
Gemeinsames Trainieren eines Netzes zur Erzeugung von Störgeräuschen und eines Klassifizierungsnetzes, um eine bessere Robustheit gegenüber adversarial Angriffen zu erreichen.
Eine GAN-Lösung für tiefe Klassifizierungsmodelle, die gegenüber White- und Blackbox-Angriffen robuste Modelle erzeugt. 
Die Arbeit schlägt einen Verteidigungsmechanismus gegen adversarial Angriffe vor, der GANs verwendet, wobei generierte Störungen als adversarial Beispiele und ein Diskriminator zur Unterscheidung zwischen ihnen verwendet werden.
Verwendung von Ensemble-Methoden zur Verteidigung gegen adversarial Störungen bei tiefen neuronalen Netzen.
In diesem Beitrag wird vorgeschlagen, Ensembling als gegnerischen Verteidigungsmechanismus zu verwenden.
Empirische Untersuchung der Robustheit verschiedener Ensembles tiefer neuronaler Netze gegenüber den beiden Arten von Angriffen, FGSM und BIM, auf zwei populären Datensätzen, MNIST und CIFAR10.
Vorschlag für eine Methode zur Erzeugung von Sätzen, die auf der Fusion von Textinformationen und visuellen Informationen, die mit den Textinformationen verbunden sind, basiert.
Diese Arbeit beschreibt ein Deep Learning Modell für Dialogsysteme, das sich visuelle Informationen zunutze macht.
In diesem Beitrag wird ein neuartiger Datensatz für geerdete Dialoge vorgeschlagen und eine rechnerische Beobachtung gemacht, dass er helfen könnte, auch bei textbasierten Dialogen über das Sehen nachzudenken.
Schlägt vor, die traditionellen textbasierten Ansätze zur Satzgenerierung/Dialog durch die Einbeziehung visueller Informationen zu ergänzen, indem ein Datenpaket gesammelt wird, das sowohl aus Text als auch aus zugehörigen Bildern oder Videos besteht.
Wir haben ein neuartiges kontextuelles rekurrentes Convolutional Network mit robusten Eigenschaften für visuelles Lernen vorgeschlagen.
In diesem Beitrag wird eine Feedback Verbindung vorgestellt, um das Lernen von Merkmalen durch die Einbeziehung von Kontextinformationen zu verbessern.
In der Arbeit wird vorgeschlagen, "rekurrente" Verbindungen in ein Convolution Network mit Gating-Mechanismus einzufügen.
Wir zeigen, dass das Training eines tiefen Netzwerks unter Verwendung von Batch-Normalisierung gleichbedeutend ist mit approximativer Inferenz in Bayes'schen Modellen, und wir demonstrieren, wie diese Erkenntnis es uns ermöglicht, nützliche Schätzungen der Modellunsicherheit in konventionellen Netzwerken vorzunehmen.
In diesem Beitrag wird vorgeschlagen, die Batch-Normalisierung zum Testzeitpunkt zu verwenden, um die Vorhersageunsicherheit zu erhalten, und es wird gezeigt, dass die Monte-Carlo Vorhersage zum Testzeitpunkt unter Verwendung der Batch-Norm besser ist als der Dropout.
Schlägt vor, dass das Regularisierungsverfahren, das als Batch-Normalisierung bezeichnet wird, als Durchführung einer approximativen Bayes'schen Inferenz verstanden werden kann, die in Bezug auf die Schätzungen der Unsicherheit, die sie produziert, ähnlich wie MC-Dropout funktioniert.
Wir verbessern das Gradient Dropping (eine Technik, bei der nur große Gradienten bei verteiltem Training ausgetauscht werden), indem wir lokale Gradienten bei der Aktualisierung der Parameter einbeziehen, um den Qualitätsverlust zu verringern und die Trainingszeit weiter zu verkürzen.
In dieser Arbeit werden 3 Modi für die Kombination von lokalen und globalen Gradienten vorgeschlagen, um mehr Rechenknoten besser zu nutzen.
Befasst sich mit dem Problem der Verringerung des Kommunikationsbedarfs bei der Umsetzung verteilter Optimierungsverfahren, insbesondere SGD.
Exploration mit Distributional RL und trunkierter Varianz.
Stellt eine RL-Methode vor, um mit Hilfe von UCB-Techniken Kompromisse zwischen Entdeckung und Ausnutzen zu verwalten.
Eine Methode zur Verwendung der durch die Quantile Regression DQN gelernten Verteilung für die Entdeckung, anstelle der üblichen Epsilon-Greedy-Strategie.
Schlägt neue Algorithmen (QUCB und QUCB+) vor, um den Kompromiss der Entdeckung bei mehrarmigen Banditen und allgemeiner beim Reinforcement Learning zu bewältigen.
Auf der Grundlage von Sprach- und Kommunikationstheorien stellen wir gemeinschaftsbasierte Autoencoder vor, bei denen mehrere Encoder und Decoder gemeinsam strukturierte und wiederverwendbare Repräsentationen lernen.
Die Autoren befassen sich mit dem Problem des Repräsentationslernens, zielen darauf ab, wiederverwendbare und strukturierte Repräsentationen zu erstellen, argumentieren, dass die Koadaptation zwischen Encoder und Decoder in der traditionellen AE zu einer schlechten Repräsentation führt, und führen gemeinschaftsbasierte Auto-Encoder ein.
In diesem Beitrag wird ein gemeinschaftsbasierter Autoencoder vorgestellt, der sich mit der Koadaptation von Encodern und Decodern befasst und darauf abzielt, bessere Repräsentationen zu erstellen.
Wir stellen MetaMimic vor, einen Algorithmus, der als Input einen Demonstrationsdatensatz nimmt und (i) eine One-Shot High-Fidelity Imitationsstrategie und (ii) eine unbedingte Aufgabenregel ausgibt.
Die Arbeit befasst sich mit dem Problem der One-Shot-Imitation mit hoher Imitationsgenauigkeit, indem es DDPGfD so erweitert, dass nur Zustandstrajektorien verwendet werden.
In diesem Papier wird ein Ansatz für eine One-Shot Imitation mit hoher Genauigkeit vorgeschlagen, der das allgemeine Problem der Exploration beim Imitationslernen angeht.
Präsentiert eine RL-Methode zum Lernen aus Videodemonstrationen ohne Zugang zu Expertenhandlungen.
Wir stellen eine neuartige Normalisierungsmethode für tiefe neuronale Netze vor, die robust gegenüber Multimodalitäten in den dazwischenliegenden Merkmalsverteilungen ist.
Normalisierungsmethode, die eine multimodale Verteilung im Merkmalsraum lernt.
Vorschlagen einer Verallgemeinerung der Batch-Normalisierung unter der Annahme, dass die Statistik der Aktivierungen der Einheiten über die Batches und über die räumlichen Dimensionen nicht unimodal ist.
Wir haben eine auf Wissensdestillation basierende Methode vorgeschlagen, um die Genauigkeit der mehrsprachigen neuronalen maschinellen Übersetzung zu erhöhen.
Ein mehrsprachiges neuronales maschinelles Übersetzungsmodell, das zunächst separate Modelle für jedes Sprachpaar trainiert und dann eine Destillation durchführt.
Ziel der Arbeit ist es, ein maschinelles Übersetzungsmodell zu trainieren, indem der standardmäßige Cross-Entropie-Verlust durch eine Destillationskomponente ergänzt wird, die auf individuellen Lehrermodellen (für einzelne Sprachpaare) basiert.
Wir untersuchen die verschiedenen Arten von Vorwissen, die das menschliche Lernen unterstützen, und stellen fest, dass allgemeine Vorannahmen über Objekte die wichtigste Rolle bei der Steuerung des menschlichen Spielverhaltens spielen.
Die Autoren untersuchen experimentell, welche Aspekte der menschlichen Vorurteile für das Reinforcement Learning in Videospielen wichtig sind.
Die Autoren stellen eine Studie über die von Menschen beim Spielen von Videospielen verwendeten Vorurteile vor und zeigen, dass es eine Taxonomie von Merkmalen gibt, die sich in unterschiedlichem Maße auf die Fähigkeit auswirken, Aufgaben im Spiel zu erledigen.
Aufgrund des Bedarfs an parallelisierbaren, offenen Hyperparameter-Optimierungsverfahren schlagen wir die Verwendung von k-determinanten Punktprozessen in der Hyperparameter-Optimierung mittels Zufallssuche vor.
Schlägt die Verwendung des k-DPP zur Auswahl von Kandidatenpunkten bei der Suche nach Hyperparametern vor.
Die Autoren schlagen k-DPP als Open-Loop-Methode für die Hyperparameter-Optimierung vor und bieten eine empirische Studie und einen Vergleich mit anderen Methoden.
Betrachtet die nicht-sequenzielle und uninformierte Hyperparametersuche unter Verwendung determinanter Punktprozesse, die Wahrscheinlichkeitsverteilungen über Teilmengen einer Grundmenge sind, mit der Eigenschaft, dass Teilmengen mit "vielfältigeren" Elementen eine höhere Wahrscheinlichkeit haben.
Beim induktiven Transferlernen übertrifft die Feinabstimmung von vortrainierten Convolutional Networks das Training von Grund auf erheblich.
Befasst sich mit dem Problem des Transferlernens in tiefen Netzwerken und schlägt einen Regularisierungsterm vor, der die Abweichung von der Initialisierung bestraft.
schlägt eine Analyse verschiedener adaptiver Regularisierungstechniken für tiefes Transferlernen vor und konzentriert sich dabei auf die Verwendung einer L@-SP-Bedingung.
Wir betrachten neuronale Netze mit blockdiagonalen inneren Produktschichten aus Gründen der Effizienz.
In diesem Beitrag wird vorgeschlagen, die inneren Schichten in einem neuronalen Netz blockdiagonal zu gestalten, und es wird erörtert, dass blockdiagonale Matrizen effizienter sind als Pruning und dass blockdiagonale Schichten zu effizienteren Netzen führen.
Ersetzen von vollständig zusammenhängenden Schichten durch blockdiagonale vollständig zusammenhängende Schichten.
Wir schlagen eine neue Technik zur Gewichtsnormalisierung vor, die als spektrale Normalisierung bezeichnet wird, um das Training des Diskriminators von GANs zu stabilisieren.
In dieser Arbeit wird die spektrale Regularisierung zur Normalisierung von GAN-Zielen verwendet, und das daraus resultierende GAN, SN-GAN genannt, gewährleistet im Wesentlichen die Lipschitz-Eigenschaft des Diskriminators.
In dieser Arbeit wird eine "spektrale Normalisierung" vorgeschlagen, die einen großen Schritt nach vorne bei der Verbesserung des Trainings von GANs darstellt.
Übergangsstrategien ermöglichen es den Agenten, komplexe Fertigkeiten zusammenzustellen, indem sie zuvor erworbene primitive Fertigkeiten nahtlos miteinander verbinden.
Schlägt ein Schema für den Übergang zu günstigen Strategiezuständen für die Ausführung gegebener Optionen in kontinuierlichen Domänen vor. Dabei kommen zwei gleichzeitig ablaufende Lernprozesse zum Einsatz.
Vorgestellt wird eine Methode zum Erlernen von Strategien für den Übergang von einer Aufgabe zu einer anderen mit dem Ziel, komplexe Aufgaben unter Verwendung eines Schätzers für die Zustandsnähe zur Belohnung für die Übergangsstrategie abzuschließen.
Schlägt ein neues Trainingsschema mit einer erlernten Hilfsbelohnungsfunktion zur Optimierung von Übergangsstrategien vor, die den Endzustand einer vorherigen Makroaktion/Option mit guten Anfangszuständen der folgenden Makroaktion/Option verbinden.
Wir klassifizieren die dynamischen Merkmale, die eine und zwei GRU-Zellen in kontinuierlicher Zeit erfassen können und die nicht erfasst werden können, und verifizieren unsere Ergebnisse experimentell mit k-schrittigen Zeitreihenvorhersagen. 
Die Autoren analysieren GRUs mit versteckten Größen von eins und zwei als zeitkontinuierliche dynamische Systeme und behaupten, dass die Ausdruckskraft der versteckten Zustandsdarstellung Vorwissen darüber liefern kann, wie gut eine GRU bei einem bestimmten Datensatz abschneiden wird.
In diesem Beitrag werden GRUs aus der Perspektive dynamischer Systeme analysiert und es wird gezeigt, dass 2d-GRUs so trainiert werden können, dass sie eine Vielzahl von Fixpunkten annehmen und Linienattraktoren annähern können, aber keinen Ringattraktor nachahmen können.
Konvertiert GRU-Gleichungen in kontinuierliche Zeit und nutzt Theorie und Erfahrungen, um 1- und 2-dimensionale GRU-Netzwerke zu untersuchen und jede Vielfalt der dynamischen Topologie in diesen Systemen zu zeigen.
Differenzierte Eingaben führen zu einer funktionalen Differenzierung des Netzes, und die Interaktion von Verlustfunktionen zwischen Netzen kann den Optimierungsprozess beeinflussen.
Eine Abwandlung des ursprünglichen Sanduhrnetzes für die Schätzung von Einzelposen, die Verbesserungen gegenüber der ursprünglichen Basislinie bringt.
Die Autoren erweitern ein gestapeltes Sanduhr-Netzwerk mit Inception-Resnet-A-Modulen und schlagen einen mehrskaligen Ansatz zur Schätzung der menschlichen Pose in RGB-Standbildern vor.
Um eine Satzeinbettung anhand von technischen Dokumenten zu trainieren, berücksichtigt unser Ansatz die Dokumentstruktur, um einen breiteren Kontext zu finden und Wörter außerhalb des Vokabulars zu behandeln.
Präsentiert Ideen zur Verbesserung der Satzeinbettung durch die Einbeziehung von mehr Kontext.
Lernen von Satzrepräsentationen mit Informationen über Satzabhängigkeiten.
Erweitert die Idee der Bildung einer unüberwachten Repräsentation von Sätzen, wie sie im SkipThough-Ansatz verwendet wird, durch die Verwendung einer breiteren Reihe von Beweisen für die Bildung der Repräsentation eines Satzes.
Wir erforschen die Struktur neuronaler Verlustfunktionen und die Auswirkung von Verlustlandschaften auf die Generalisierung, indem wir eine Reihe von Visualisierungsmethoden verwenden.
Diese Arbeit schlägt eine Methode zur Visualisierung der Verlustfunktion eines NN vor und gibt Einblicke in die Trainierbarkeit und Generalisierung von NNs.
Untersucht die Nicht-Konvexität der Verlustfläche und der Optimierungspfade.
Wir zeigen, dass wir tiefe Modelle robuster gegen adversarial Angriffe machen können, indem wir eine Multi-way Output Encoding anstelle des weit verbreiteten One-Hot Encoding verwenden.
In diesem Beitrag wird vorgeschlagen, die abschließende Cross-Entropie-Schicht, die auf One-Hot-Labels in Klassifikatoren trainiert wird, durch die Kodierung jedes Labels als hochdimensionalen Vektor zu ersetzen und den Klassifikator so zu trainieren, dass der L2-Abstand zur Kodierung der richtigen Klasse minimiert wird.
Die Autoren schlagen eine neue Methode zur Abwehr von Angriffen vor, die im Vergleich zu den Basislinien erhebliche Vorteile bietet.
Wir stellen das erste NMT-Modell mit vollständig paralleler Dekodierung vor, das die Inferenzlatenz um das 10-fache reduziert.
Diese Arbeit schlägt einen nicht-autoregressiven Decoder für den Encoder-Decoder-Rahmen vor, bei dem die Entscheidung, ein Wort zu erzeugen, nicht von der vorherigen Entscheidung der erzeugten Wörter abhängt.
Diese Arbeit beschreibt einen Ansatz zur nicht-autoregressiven Dekodierung für neuronale maschinelle Übersetzung mit der Möglichkeit einer paralleleren Dekodierung, die zu einer erheblichen Geschwindigkeitssteigerung führen kann.
Schlägt die Einführung einer Reihe latenter Variablen vor, die die Fruchtbarkeit jedes Ausgangswortes darstellen, um die Generierung des Zielsatzes nicht autoregressiv zu gestalten.
Wir demonstrieren ein zertifizierbares, trainierbares und skalierbares Verfahren zur Verteidigung gegen adversarial Beispiele.
Schlägt eine neue Verteidigung gegen Sicherheitsangriffe auf neuronale Netze mit dem attack-Modell vor, das ein Sicherheitszertifikat für den Algorithmus ausgibt.
Ableitung einer oberen Schranke für adversarial Störungen für neuronale Netze mit einer versteckten Schicht.
Wir schlagen einen Regularisierer vor, der die Klassifizierungsleistung neuronaler Netze verbessert.
Die Autoren schlagen vor, ein Modell unter dem Gesichtspunkt der Maximierung der gegenseitigen Information zwischen den Vorhersagen und den wahren Ausgaben zu trainieren, mit einem Regularisierungsterm, der irrelevante Informationen beim Lernen minimiert.
Schlägt vor, die Parameter in eine invertierbare Merkmalskarte F und eine lineare Transformation w in der letzten Schicht zu zerlegen, um die gegenseitige Information I(Y, \hat{T}) zu maximieren und gleichzeitig irrelevante Informationen einzuschränken.
In diesem Beitrag wird ein neuartiger generativer Modellierungsrahmen vorgestellt, der den Zusammenbruch latenter Variablen vermeidet und die Verwendung bestimmter Ad-hoc-Faktoren beim Training von variationalen Autoencodern verdeutlicht.
Die Arbeit schlägt vor, das Problem eines variationalen Autoencoders zu lösen, der die latenten Variablen ignoriert.
Diese Arbeit schlägt vor, einen stochastischen Autoencoder zum ursprünglichen VAE-Modell hinzuzufügen, um das Problem zu lösen, dass der LSTM Decoder eines Sprachmodells zu stark sein könnte, um die Informationen der latenten Variablen zu ignorieren.
In diesem Beitrag wird AutoGen vorgestellt, das einen generativen variationalen Autoencoder mit einem auf Autoencoder basierenden High-Fidelity-Rekonstruktionsmodell kombiniert, um die latente Repräsentation besser zu nutzen.
Diese Arbeit untersucht das Problem der Domänenaufteilung durch Segmentierung von Instanzen, die aus verschiedenen  probabilistischen Verteilungen stammen.  
Dieser Beitrag befasst sich mit dem Problem der Neuheitserkennung beim Lernen mit offenen Mengen und beim verallgemeinerten Zero-Shot Lernen und schlägt eine mögliche Lösung vor.
Ein Ansatz zur Trennung von Bereichen, der auf Bootstrapping basiert, um Ähnlichkeitsschwellenwerte für bekannte Klassen zu ermitteln, gefolgt von einem Kolmogorov-Smirnoff-Test zur Verfeinerung der Bootstrapping-In-Distributionszonen.
Schlägt vor, einen neuen Bereich, den unsicheren Bereich, einzuführen, um die Unterscheidung zwischen gesehenen und ungesehenen Bereichen beim Open-Set und verallgemeinerten Zero-Shot Lernen besser zu handhaben.
SGD führt implizit eine Variationsinferenz durch; das Gradientenrauschen ist in hohem Maße nicht isotrop, so dass SGD nicht einmal zu kritischen Punkten des ursprünglichen Verlusts konvergiert.
Diese Arbeit bietet eine Variationsanalyse von SGD als Nicht-Gleichgewichtsprozess.
Dieser Beitrag diskutiert die regulierte Zielfunktion, die durch Standard-SGD im Kontext neuronaler Netze minimiert wird, und bietet eine Perspektive der Variationsinferenz unter Verwendung der Fokker-Planck-Gleichung.
Entwicklung einer Theorie zur Untersuchung der Auswirkungen von stochastischem Gradientenrauschen für SGD, insbesondere für tiefe neuronale Netzmodelle.
Agenten können lernen, ausschließlich visuelle Demonstrationen (ohne Handlungen) zur Testzeit zu imitieren, nachdem sie zur Trainingszeit aus eigener Erfahrung und ohne jegliche Form der Überwachung gelernt haben.
In diesem Beitrag wird ein Ansatz für Zero-Shot visuelles Lernen durch das Erlernen parametrischer Fähigkeitsfunktionen vorgeschlagen.
Eine Arbeit über die Imitation einer Aufgabe, die nur während der Inferenz präsentiert wird, wobei das Lernen auf selbstüberwachte Weise erfolgt und der Agent während des Trainings verwandte, aber unterschiedliche Aufgaben erforscht.
Schlägt eine Methode zur Umgehung des Problems der kostspieligen Expertendemonstration vor, indem die zufällige Erkundung eines Agenten genutzt wird, um verallgemeinerbare Fähigkeiten zu erlernen, die ohne spezielles Vortraining angewendet werden können.
Die Arbeit beschreibt ein Verfahren zur Verbesserung von Wortvektorraummodellen mit einer Bewertung von Paragram- und GloVe-Modellen für Ähnlichkeitsbenchmarks.
In diesem Beitrag wird ein neuer Algorithmus vorgeschlagen, der GloVe-Wortvektoren anpasst und dann eine nicht-euklidische Ähnlichkeitsfunktion zwischen ihnen verwendet.
Die Autoren stellen Beobachtungen zu den Schwächen der bestehenden Vektorraummodelle vor und nennen einen 6-stufigen Ansatz zur Verfeinerung bestehender Wortvektoren.
Wir schlagen eine neue Quantisierungsmethode vor und wenden sie zur Quantisierung von RNNs sowohl für die Kompression als auch für die Beschleunigung an.
In dieser Arbeit wird eine Multibit Quantisierungsmethode für rekurrente neuronale Netze vorgeschlagen.
Eine Technik zur Quantisierung von Gewichtsmatrizen neuronaler Netze und ein alternierendes Optimierungsverfahren zur Schätzung der Menge von k binären Vektoren und Koeffizienten, die den ursprünglichen Vektor am besten repräsentieren.
Wir ersetzen die vollständig verknüpften Schichten eines neuronalen Netzes durch den Multiskalen-Verschränkungs-Renormalisierungssatz, eine Art Quantenoperation, die Korrelationen über große Entfernungen beschreibt. 
In dem Beitrag schlagen die Autoren vor, die MERA-Tensorisierungstechnik zur Komprimierung neuronaler Netze zu verwenden.
Eine neue Parametrisierung linearer Zuordnungen für den Einsatz in neuronalen Netzen, die eine hierarchische Faktorisierung der linearen Zuordnung verwendet, die die Anzahl der Parameter reduziert und gleichzeitig die Modellierung relativ komplexer Wechselwirkungen ermöglicht.
Studien zur Komprimierung von Feed-Forward-Schichten unter Verwendung von Tensor-Zerlegungen niedrigen Ranges und Erforschung einer baumartigen Zerlegung.
Wir präsentieren eine Single-Shot Analyse eines trainierten neuronalen Netzes, um Redundanzen zu entfernen und die optimale Netzstruktur zu ermitteln.
In dieser Arbeit wird eine Reihe von Heuristiken zur Identifizierung einer guten Architektur für neuronale Netze vorgeschlagen, die auf der PCA der Aktivierungen der Einheiten über den Datensatz basieren.
In diesem Beitrag wird ein Rahmen für die Optimierung von Architekturen neuronaler Netze durch die Identifizierung redundanter Filter in verschiedenen Schichten vorgestellt.
Wir führen die erste eingehende Sicherheitsanalyse von DNN-Fingerprinting Angriffen durch, die Cache Seitenkanäle ausnutzen, was einen Schritt zum Verständnis der Anfälligkeit von DNNs für Seitenkanalangriffe darstellt.
In diesem Beitrag wird das Problem des Fingerprints von neuronalen Netzwerkarchitekturen unter Verwendung von Cache-Seitenkanälen betrachtet und es werden Verteidigungsmaßnahmen gegen Sicherheit durch Unklarheit diskutiert.
In diesem Beitrag werden Cache-Seitenkanalangriffe durchgeführt, um Attribute eines Opfermodells zu extrahieren und auf dessen Architektur zu schließen. Außerdem wird gezeigt, dass sie eine nahezu perfekte Klassifizierungsgenauigkeit erreichen können.
Wir schlagen Complement Objective Training (COT) vor, ein neues Trainingsparadigma, das sowohl die primären als auch die komplementären Ziele optimiert, um die Parameter neuronaler Netze effektiv zu lernen.
Es wird erwogen, das Ziel der Cross-Entropy durch eine Maximierung des "Komplement"-Ziels zu ergänzen, das darauf abzielt, die vorhergesagten Wahrscheinlichkeiten von Klassen, die nicht der Grundwahrheit entsprechen, zu neutralisieren.
Die Autoren schlagen ein sekundäres Ziel für die Softmax-Minimierung vor, das auf der Auswertung der von den falschen Klassen gesammelten Informationen beruht und zu einem neuen Trainingsansatz führt.
Befasst sich mit dem Training neuronaler Netze für Klassifizierungs- oder Sequenzerstellungsaufgaben unter Verwendung von Cross-Entropie-Verlusten.
Unsicherheitsabschätzung in einem einzigen Vorwärtsdurchlauf ohne zusätzliche lernbare Parameter.
Eine neue Methode zur Berechnung von Output-Unsicherheitsschätzungen in DNNs für Klassifizierungsprobleme, die mit den modernsten Methoden zur Unsicherheitsschätzung übereinstimmt und diese bei Aufgaben zur Erkennung von Abweichungen von der Verteilung übertrifft.
Die Autoren stellen den gehemmte nSoftmax vor, eine Modifikation des Softmax durch Hinzufügen einer konstanten Aktivierung, die ein Maß für die Unsicherheit darstellt. 
Wir haben ein funktionsreiches System für Deep Learning mit verschlüsselten Eingaben entwickelt, das verschlüsselte Ausgaben erzeugt und die Privatsphäre wahrt.
Ein Framework für private Deep Learning Modellinferenz unter Verwendung von FHE-Schemata, die schnelles Bootstrapping unterstützen und somit die Rechenzeit reduzieren können.
In dem Beitrag wird eine Möglichkeit vorgestellt, ein neuronales Netz mit Hilfe homomorpher Verschlüsselung sicher zu bewerten.
Wir stellen ein System namens GamePad vor, um die Anwendung von Methoden des maschinellen Lernens auf das Theorembeweisen im Coq-Beweisassistenten zu untersuchen.
Dieser Artikel beschreibt ein System zur Anwendung von maschinellem Lernen auf interaktive Theorembeweise, konzentriert sich auf die Aufgaben der Taktikvorhersage und der Positionsbewertung und zeigt, dass ein neuronales Modell ein SVM bei beiden Aufgaben übertrifft.
Schlägt vor, dass Techniken des maschinellen Lernens bei der Erstellung von Beweisen im Theorembeweiser Coq eingesetzt werden.
In dieser Arbeit haben wir das effiziente Training von verlustbewussten gewichtsquantisierten Netzen mit quantisiertem Gradienten in einer verteilten Umgebung sowohl theoretisch als auch empirisch untersucht.
Diese Arbeit untersucht die Konvergenzeigenschaften der verlustbewussten Gewichtsquantisierung mit verschiedenen Gradientenpräzisionen in einer verteilten Umgebung und bietet eine Konvergenzanalyse für die Gewichtsquantisierung mit voll-präzisen, quantisierten, und quantisierten abgeschnittenen Gradienten.
Die Autoren schlagen eine Analyse der Auswirkungen der gleichzeitigen Quantisierung der Gewichte und Gradienten beim Training eines parametrisierten Modells in einer vollständig synchronisierten verteilten Umgebung vor.
Eine Regularisierungsstrategie zur Verbesserung der Leistung des sequentiellen Lernens.
Ein neuartiger, auf Regularisierung basierender Ansatz für das sequentielle Lernproblem unter Verwendung eines Modells fester Größe, das zusätzliche Terme zum Verlust hinzufügt, das die Seltenheit der Darstellung fördert und katastrophales Vergessen bekämpft.
Diese Arbeit befasst sich mit dem Problem des katastrophalen Vergessens beim lebenslangen Lernen, indem es regulierte Lernstrategien vorschlägt.
Ein synaptisches neuronales Netzwerk mit Synapsengraphen und Lernen, das die Eigenschaft der topologischen Konjugation und der Bose-Einstein-Verteilung im Surprisal-Raum aufweist.  
Die Autoren schlagen ein hybrides neuronales Netzwerk vor, das aus einem Synapsengraphen besteht, der in ein standardmäßiges neuronales Netzwerk eingebettet werden kann.
stellt ein biologisch inspiriertes neuronales Netzmodell vor, das auf den erregenden und hemmenden Ionenkanälen in den Membranen echter Zellen basiert.
Verallgemeinerte Modelle zur Grapheneinbettung.
Ein verallgemeinerter Ansatz zur Einbettung von Wissensgraphen, der die Einbettungen auf der Grundlage von drei verschiedenen gleichzeitigen Zielen erlernt und gleich gut oder sogar besser abschneidet als die bestehenden State-of-the-Art-Ansätze.
Bewältigt die Aufgabe, Einbettungen von multirelationalen Graphen mit Hilfe eines neuronalen Netzes zu lernen.
Schlägt eine neue Methode, GEN, zur Berechnung von Einbettungen von Multibeziehungsgraphen vor, insbesondere, dass so genannte E-Zellen und R-Zellen Anfragen der Form (h,r,?),(?r,t) und (h,?,t) beantworten können.
Minimax Curriculum Learning ist eine maschinelle Lernmethode, bei der die wünschenswerte Härte erhöht und die Vielfalt planmäßig reduziert wird.
Ein Curriculum Lernansatz, der eine submodulare Mengenfunktion verwendet, die die Vielfalt der während des Trainings ausgewählten Beispiele erfasst. 
In dem Beitrag wird das MiniMax Curriculum Lernen als ein Ansatz für das adaptive Training von Modellen durch die Bereitstellung verschiedener Teilmengen von Daten vorgestellt.
Implizite Modelle, angewandt auf Kausalität und Genetik.
Die Autoren schlagen vor, das implizite Modell zu verwenden, um das Genom-weite Assoziationsproblem anzugehen.
In diesem Beitrag werden Lösungen für die Probleme bei genomweiten Assoziationsstudien vorgeschlagen, die durch die Populationsstruktur und das potenzielle Vorhandensein nichtlinearer Wechselwirkungen zwischen verschiedenen Teilen des Genoms entstehen, und es werden Brücken zwischen statistischer Genetik und ML geschlagen.
Vorstellung eines nichtlinearen generativen Modells für GWAS, das die Populationsstruktur modelliert, wobei Nichtlinearitäten mit Hilfe von neuronalen Netzen als nichtlineare Funktionsapproximatoren modelliert werden und die Inferenz mit Hilfe von likelihood-freier Variationsinferenz durchgeführt wird.
Few-Shot Lernen durch Ausnutzung der Beziehung auf Objektebene, um die Beziehung auf Bildebene zu lernen (Ähnlichkeit).
Diese Arbeit befasst sich mit dem Problem des Few-Shot Lernens, indem es einen auf Einbettung basierenden Ansatz vorschlägt, der lernt, Merkmale auf Objektebene zwischen Support- und Query-Set-Beispielen zu vergleichen.
Schlägt eine Few-Shot Lernmethode vor, die die Beziehung zwischen verschiedenen Bildern auf Objektebene auf der Grundlage der Suche nach nahen Nachbarn ausnutzt und Merkmalskarten von zwei Eingabebildern zu einer Merkmalszuordnung zusammenfügt.
Forscher, die Techniken zur Verarbeitung natürlicher Sprache auf Quellcode anwenden, verwenden keine Form von vortrainierten Einbettungen, wir zeigen, dass sie dies tun sollten.
In diesem Beitrag wird untersucht, ob das Pretraining von Worteinbettungen für Programmiersprachencode mit Hilfe von NLP-ähnlichen Sprachmodellen einen Einfluss auf die Aufgabe der Zusammenfassung von extremem Code hat.
Diese Arbeit zeigt, wie das Pretraining von Wortvektoren anhand von Code-Korpussen zu Repräsentationen führt, die besser geeignet sind als zufällig initialisierte und trainierte Repräsentationen für die Vorhersage von Funktions-/Methodennamen.
Wir lösen den Rubik's Cube mit reinem Reinforcement Learning.
Lösung zum Lösen des Rubik Cube durch Reinforcement Learning (RL) mit Monte Carlo Baumsuche (MCTS) durch autodidaktische Iteration. 
In dieser Arbeit wird der Rubik's Cube mit Hilfe einer annähernden Iterationsmethode, der so genannten autodidaktischen Iteration, gelöst, wobei das Problem der spärlichen Belohnungen durch die Schaffung eines eigenen Belohnungssystems überwunden wird.
Einführung eines tiefen RL-Algorithmus zur Lösung des Rubik-Würfels, der den riesigen Zustandsraum und die sehr spärliche Belohnung des Rubik-Würfels bewältigt.
Wir beschreiben ein differenzierbares Ende-zu-Ende Modell für QA, das lernt, Textabschnitte in der Frage als Denotationen im Wissensgraphen darzustellen, indem es sowohl neuronale Module für die Komposition als auch die syntaktische Struktur des Satzes lernt.
In diesem Beitrag wird ein Modell für die Beantwortung visueller Fragen vorgestellt, das sowohl Parameter als auch Strukturvorherseher für ein modulares neuronales Netz erlernen kann, ohne überwachte Strukturen oder Unterstützung durch einen syntaktischen Parser.
Schlägt vor, ein Modell zur Beantwortung von Fragen nur aus Antworten und einer KB zu trainieren, indem latente Bäume gelernt werden, die die Syntax erfassen und die Semantik von Wörtern lernen.
Wir stellen eine neuartige Compiler-Infrastruktur vor, die die Unzulänglichkeiten bestehender Deep-Learning-Frameworks behebt.
Vorschlag zur Umstellung von Ad-hoc Code Generierung in Deep Learning Engines auf bewährte Verfahren für Compiler und Sprachen.
In diesem Beitrag wird ein Compiler-Framework vorgestellt, das die Definition von domänenspezifischen Sprachen für Deep Learning Systeme ermöglicht und Kompilierungsphasen definiert, die Standardoptimierungen und spezielle Optimierungen für neuronale Netze nutzen können.
In diesem Beitrag wird ein DLVM vorgestellt, das die Vorteile eines Tensor-Compilers nutzt.
Aufmerksamkeitsbasierte Architektur für das Erlernen von Sprache durch Reinforcement Learning in einer neuen anpassbaren 2D-Gitterumgebung.
Der Beitrag befasst sich mit dem Problem der Navigation anhand einer Anweisung und schlägt einen Ansatz vor, der textliche und visuelle Informationen über einen Aufmerksamkeitsmechanismus kombiniert.
In diesem Beitrag wird das Problem der Befolgung von Anweisungen in natürlicher Sprache bei einer Ich-Perspektive einer a priori unbekannten Umgebung betrachtet und eine Methode der neuronalen Architektur vorgeschlagen.
Untersucht das Problem der Navigation zu einem Zielobjekt in einer 2D-Gitterumgebung, indem man einer gegebenen natürlichsprachlichen Beschreibung folgt und visuelle Informationen als rohe Pixel erhält.
Eine einfache Architektur, die aus Convolutions und Aufmerksamkeit besteht, erzielt Ergebnisse, die mit den am besten dokumentierten rekurrenten Modellen vergleichbar sind.
Ein schnelles, leistungsstarkes Paraphrasierungsverfahren auf der Grundlage von Datenerweiterung und ein nicht-rekurrentes Leseverständnismodell, das nur Convolutions und Aufmerksamkeit verwendet.
In dieser Arbeit wird vorgeschlagen, CNNs und Selbstaufmerksamkeitsmodule anstelle von LSTMs einzusetzen und das RC-Modelltraining mit Passagenparaphrasen zu erweitern, die von einem neuronalen Paraphrasierungsmodell generiert werden, um die RC-Leistung zu verbessern.
In diesem Beitrag wird ein Modell für das Leseverständnis vorgestellt, das Convolutions und Aufmerksamkeit verwendet, und es wird vorgeschlagen, zusätzliche Trainingsdaten durch Paraphrasierung auf der Grundlage von standardmäßiger neuronaler Maschinenübersetzung zu ergänzen.
Wir stellen Spherical CNNs vor, ein Convolutional Network für sphärische Signale, und wenden es auf die Erkennung von 3D-Modellen und die Regression molekularer Energie an.
Die Arbeit schlägt einen Rahmen für die Konstruktion von sphärischen Convolutional Networks vor, der auf einer neuartigen Synthese mehrerer bestehender Konzepte beruht.
In diesem Beitrag geht es darum, wie Convolutional Neural Networks so erweitert werden können, dass sie über eine eingebaute sphärische Invarianz verfügen, und es werden Werkzeuge aus der nicht-abelschen harmonischen Analyse verwendet, um dieses Ziel zu erreichen.
Die Autoren entwickeln ein neuartiges Schema zur Darstellung sphärischer Daten von Grund auf.
Ein Verfahren für die automatisierte Planung von realen Objekten wie Kühlkörpern und Tragflächenprofilen, das neuronale Netze und Gradientenabstieg einsetzt.
Neuronales Netz (Parametrisierung und Vorhersage) und Gradientenabstieg (Backpropogation) für die automatische Planung von technischen Aufgaben. 
In diesem Beitrag wird die Verwendung eines tiefen Netzwerks vorgestellt, um das Verhalten eines komplexen physikalischen Systems zu approximieren und dann optimale Geräte zu entwerfen, indem dieses Netzwerk im Hinblick auf seine Eingaben optimiert wird.
Wir schlagen eine duale Version der logistischen adversen Distanz für den Merkmalsabgleich vor und zeigen, dass sie stabilere Gradientenschritt-Iterationen liefert als das Min-Max-Ziel.
Die Arbeit befasst sich mit der Fixierung von GANs auf der Berechnungsebene.
Diese Arbeit untersucht eine duale Formulierung eines adversarial Verlusts, die auf einer Obergrenze des logistischen Verlusts basiert, und verwandelt das Standard-Min-Max-Problem des adversarial Trainings in ein einziges Minimierungsproblem.
Schlägt vor, das GAN-Sattelpunktziel (für einen logistischen Regressionsdiskriminator) als Minimierungsproblem durch Dualisierung des Maximum-Likelihood-Ziels für die regularisierte logistische Regression neu zu formulieren.
Implementierung von binären neuronalen Netzen auf dem neuesten Stand der Rechenleistung.
Die Arbeit stellt eine in C/CUDA geschriebene Bibliothek vor, die alle für die Forward Propagation von BCNNs erforderlichen Funktionen enthält.
Dieser Beitrag baut auf Binary-NET auf und erweitert es auf CNN-Architekturen, bietet Optimierungen, die die Geschwindigkeit des Vorwärtspasses verbessern, und stellt optimierten Code für Binary CNN bereit.
Wir schlagen einen Algorithmus zur Auswahl von Teilmengen vor, der mit gradientenbasierten Methoden trainiert werden kann und durch submodulare Optimierung eine nahezu optimale Leistung erzielt.
Vorschlagen eines auf einem neuronalen Netz basierenden Modells, das eine submodulare Funktion integriert, indem es eine gradientenbasierte Optimierungstechnik mit einem submodularen Rahmen namens 'Differentiable Greedy Network' (DGN) kombiniert.
Vorschlagen eines neuronalen Netz, das eine Teilmenge von Elementen auswählt (z. B. die Auswahl von k Sätzen, die sich am meisten auf eine Behauptung beziehen, aus einer Menge von abgerufenen Dokumenten).
Wir führen das hierarchisch geclusterte Repräsentationslernen (HCRL) ein, das gleichzeitig das Repräsentationslernen und das hierarchische Clustering im Einbettungsraum optimiert.
In dem Beitrag wird vorgeschlagen, das verschachtelte CRP als Clustermodell und nicht als Themenmodell zu verwenden.
stellt eine neuartige hierarchische Clustering-Methode über einem Einbettungsraum vor, bei der sowohl der Einbettungsraum als auch das hierarchische Clustering gleichzeitig erlernt werden.
Wir entwickeln ein statistisch-geometrisches, unüberwachtes Lernverfahren für tiefe neuronale Netze, um sie robust gegen adversarial Angriffe zu machen.
Umwandlung traditioneller tiefer neuronaler Netze in robuste adversarial Kalssifizierer unter Verwendung von GRNs.
Vorschlagen einer Verteidigung, die auf klassenbedingten Merkmalsverteilungen basiert, um tiefe neuronale Netze in robuste Klassifizierer zu verwandeln.
Effizienteres tiefes Reinforcement Learning in großen Zustands-Aktionsräumen durch strukturierte Exploration mit tiefen hierarchischen Richtlinien.
Eine Methode zur Koordinierung des Agentenverhaltens unter Verwendung von Strategien, die eine gemeinsame latente Struktur haben, eine Methode zur variablen Optimierung der Strategien, um die koordinierten Strategien zu optimieren, und eine Ableitung der variablen, hierarchischen Aktualisierung der Autoren.
Diese Arbeit schlägt eine algorithmische Innovation vor, die aus hierarchischen latenten Variablen für die koordinierte Exploration in Multi-Agenten Umgebungen besteht.
Wir bieten viele Einblicke in die Generalisierung neuronaler Netze aus dem theoretisch nachvollziehbaren linearen Fall.
Die Autoren untersuchen ein einfaches Modell linearer Netzwerke zum Verständnis von Generalisierung und Transferlernen.
Durch die Batch-Normalisierung bleibt die Gradientenvarianz während des gesamten Trainings erhalten, wodurch die Optimierung stabilisiert wird.
In dieser Arbeit wurde die Auswirkung der Batch-Normalisierung auf die Gradienten Backpropagation in residualen Netzen analysiert.
Menschliche Verhaltensbeurteilungen werden verwendet, um spärliche und interpretierbare Darstellungen von Objekten zu erhalten, die sich auf andere Aufgaben übertragen lassen.
Dieser Beitrag beschreibt ein groß angelegtes Experiment zu menschlichen Objekt-/Semantikrepräsentationen und ein Modell solcher Repräsentationen.
In diesem Beitrag wird ein neues Repräsentationssystem für Objektrepräsentationen entwickelt, das auf der Grundlage von Daten trainiert wird, die aus menschlichen Beurteilungen von Bildern gewonnen wurden.
Ein neuer Ansatz zum Erlernen eines spärlichen, positiven, interpretierbaren semantischen Raums, der die menschlichen Ähnlichkeitsurteile maximiert, indem er so trainiert wird, dass die Vorhersage menschlicher Ähnlichkeitsurteile speziell maximiert wird.
Wir schlagen einen Agenten vor, der zwischen dem Benutzer und einem Black-Box-System zur Beantwortung von Fragen sitzt und lernt, Fragen so umzuformulieren, dass die bestmöglichen Antworten herauskommen.
In diesem Beitrag wird eine aktive Beantwortung von Fragen durch einen Ansatz des verstärkten Lernens vorgeschlagen, der lernt, Fragen so umzuformulieren, dass sie die bestmöglichen Antworten liefern.
Beschreibt anschaulich, wie die Forscher zwei Modelle für die Umformulierung von Fragen und die Auswahl von Antworten während der Beantwortung von Fragen entwickelt und aktiv trainiert haben.
Lernen von Prioritäten für adversarial Autoencoder.
Schlägt eine einfache Erweiterung der adversarial Autoencoder für die bedingte Bilderzeugung vor.
Konzentriert sich auf adversarial Autoencoder und führt ein Codegenerator-Netzwerk ein, um einen einfachen Prior in einen umzuwandeln, der zusammen mit dem Generator die Datenverteilung besser abbilden kann.
Generierung von Text unter Verwendung von Satzeinbettungen aus Skip-Thought Vektoren mit Hilfe von generativen adversarial Netzen.
Beschreibt die Anwendung von generativen adversarial Netzen zur Modellierung von Textdaten mit Hilfe von Skigedankenvektoren und Experimenten mit verschiedenen GANs für zwei unterschiedliche Datensätze.
Stellt eine unvoreingenommene und leicht zu implementierende Online-Gradientenschätzung für rekurrente Modelle vor.
Die Autoren stellen einen neuen Ansatz für das Online-Lernen der Parameter rekurrenter neuronaler Netze aus langen Sequenzen vor, der die Imitation der abgeschnittenen Backpropagation durch die Zeit überwindet.
In diesem Beitrag wird das Online-Training von RNNs auf prinzipielle Weise angegangen, und es wird eine Modifikation von RTRL und die Verwendung eines Forward Ansatzes für die Gradientenberechnung vorgeschlagen.
Superauflösung grober Labels in Beschriftungen auf Pixelebene, angewandt auf Luftbilder und medizinische Scans.
Eine Methode zur Superauflösung von groben, niedrig aufgelösten Segmentierungslabels, wenn die gemeinsame Verteilung von niedrig und hoch aufgelösten Labels bekannt ist.
Wir schlagen eine Methode vor, um die latenten Merkmale, die aus verschiedenen Datensätzen gelernt wurden, mithilfe harmonischer Korrelationen abzugleichen.
Es wird vorgeschlagen, Merkmalskorrespondenzen zu verwenden, um einen vielfältigen Abgleich zwischen Datenstapeln aus denselben Proben vorzunehmen, um die Sammlung gesstörter Messungen zu vermeiden.
Die Entwicklung der Körperform bei RL-gesteuerten Agenten verbessert deren Leistung (und hilft beim Lernen).
PEOM-Algorithmus, der den Shapley-Wert einbezieht, um die Entwicklung zu beschleunigen, indem der Beitrag jedes Körperteils ermittelt wird.
Gestalten Sie die Belohnung mit intrinsischer Motivation, um katastrophale Zustände zu vermeiden und das katastrophale Vergessen abzuschwächen.
Ein RL-Algorithmus, der den DQN-Algorithmus mit einem parallel trainierten Angstmodell kombiniert, um katastrophale Zustände vorherzusagen.
Die Arbeit untersucht katastrophales Vergessen in RL, indem es Aufgaben hervorhebt, bei denen ein DQN in der Lage ist, zu lernen, katastrophale Ereignisse zu vermeiden, solange es das Vergessen vermeidet.
Ein neuartiger Convolution Operator für automatisches Repräsentationslernen in der Einheitskugel.
Diese Arbeit steht im Zusammenhang mit den jüngsten Arbeiten über sphärische CNN und äquivariante SE(n)-Netzwerke und erweitert frühere Ideen auf volumetrische Daten in der Einheitskugel.
Er schlägt vor, volumetrische Convolutions auf Convolution Networks zu verwenden, um die Einheitskugel zu lernen, und erörtert die Methodik und die Ergebnisse des Prozesses.
Wir trainieren Strategien des Reinforcement Learning mit Hilfe von Belohnungserweiterung, Curriculum-Lernen und Meta-Lernen, um erfolgreich durch Webseiten zu navigieren.
Entwickelt eine Lehrplan-Lernmethode für das Training eines RL-Agenten zur Navigation in einem Web, basierend auf der Idee, eine Anweisung in mehrere Unteranweisungen zu zerlegen.
Sprachübergreifende Textklassifizierung durch universelle Kodierung.
In diesem Beitrag wird ein Ansatz zur sprachübergreifenden Textklassifikation durch die Verwendung vergleichbarer Korpora vorgeschlagen.
Lernen von sprachenübergreifenden Einbettungen und Trainieren eines Klassifizierers unter Verwendung etikettierter Daten in der Ausgangssprache, um das Lernen eines sprachenübergreifenden Textkategorisierers ohne etikettierte Informationen in der Zielsprache anzugehen.
In dieser Arbeit schlagen wir Deep Inside-Outside Recursive Auto-Encoder (DIORA) vor, eine vollständig unbeaufsichtigte Methode zur Entdeckung der Syntax bei gleichzeitigem Lernen von Repräsentationen für die entdeckten Konstituenten. 
Ein neuronales latentes Baummodell, das mit einem Auto-Encoding Ziel trainiert wird, das den Stand der Technik beim unüberwachten Konstituenten Parsing erreicht und die syntaktische Struktur besser erfasst als andere latente Baummodelle.
Die Arbeit schlägt ein Modell für unüberwachtes Dependency Parsing (latente Bauminduktion) vor, das auf einer Kombination des Inside-Outside-Algorithmus mit neuronaler Modellierung (rekursive Auto-Encoder) basiert. 
Wir untersuchen die Verzerrung des kurzsichtigen Meta-Optimierungsziels.
In diesem Beitrag werden ein vereinfachtes Modell und ein Problem vorgeschlagen, um die kurzfristige Verzerrung der Meta-Optimierung der Lernrate zu demonstrieren.
In diesem Beitrag wird die Frage der verkürzten Backpropagation für die Meta-Optimierung anhand einer Reihe von Experimenten an einem Spielzeugproblem untersucht.
Eine hierarchische und kompositorische Methode zur Erstellung von Beschriftungen.
In dieser Arbeit wird eine besser interpretierbare Methode für Bildunterschriften vorgestellt.
Wir entwickeln ein neues topologisches Komplexitätsmaß für tiefe neuronale Netze und zeigen, dass es die wichtigsten Eigenschaften dieser Netze erfasst.
In diesem Beitrag wird der Begriff der neuronalen Persistenz vorgeschlagen, ein topologisches Maß für die Zuordnung von Punktwerten zu vollständig verbundenen Schichten in einem neuronalen Netz.
In der Arbeit wird vorgeschlagen, die Komplexität eines neuronalen Netzes anhand seiner nullten persistenten Homologie zu analysieren.
Die Betrachtung der Entscheidungsgrenzen um eine Eingabe herum gibt Ihnen mehr Informationen als eine feste kleine Nachbarschaft.
Die Autoren stellen einen neuartigen Angriff zur Generierung von adversarial Beispielen vor, bei dem sie Klassifikatoren angreifen, die durch zufällige Klassifizierung von kleinen L2-Störungen erstellt wurden.
Ein neuer Ansatz zur Erzeugung von adversarial Angriffen auf ein neuronales Netz und eine Methode zur Verteidigung eines neuronalen Netzes gegen diese Angriffe.
Wir trainieren ein neuronales Netz, um annähernd optimale Gewichte in Abhängigkeit von den Hyperparametern auszugeben.
Hyper-Netzwerke für die Optimierung von Hyper-Parametern in neuronalen Netzwerken.
Schätzung der Kovarianzmatrix von Finanzanlagen mit Gauß-Prozess-Modellen mit latente Variablen.
Veranschaulicht, wie das Gaussian Process Latent Variable Model (GP-LVM) klassische lineare Faktormodelle für die Schätzung von Kovarianzmatrizen in Portfolio-Optimierungsproblemen ersetzen kann.
In dieser Arbeit werden Standard-GPLVMs verwendet, um die Kovarianzstruktur und eine latente Raumdarstellung von S&P500-Finanzzeitreihen zu modellieren, um Portfolios zu optimieren und fehlende Werte vorherzusagen.
In diesem Papier wird vorgeschlagen, ein GPLVM zur Modellierung von Finanzerträgen zu verwenden.
Wir führen meta-adversariales Lernen ein, eine neue Technik zur Regularisierung von GANs, und schlagen eine Trainingsmethode vor, die explizit die Ausgangsverteilung des Diskriminators kontrolliert.
Die Arbeit schlägt Varianz regularisierendes adversarial Lernen für das Training von GANs vor, um sicherzustellen, dass der Gradient für den Generator nicht verschwindet.
Ein Deep Reinforcement Learning Agent, dessen Gewichte mit parametrischen Störungen versehen sind, kann zur Unterstützung einer effizienten Erkundung eingesetzt werden.
In diesem Beitrag werden NoisyNets vorgestellt, neuronale Netze, deren Parameter durch eine parametrische Störfunktion gestört werden, und die eine erhebliche Leistungsverbesserung gegenüber grundlegenden Algorithmen für tiefes Reinforcement Learning erzielen.
Neue Explorationsmethode für tiefe RL durch Einspeisung von Störungen in die Gewichte der tiefen Netze, wobei die Störungen verschiedene Formen annehmen können.
Active Neural Localizer, ein vollständig differenzierbares neuronales Netz, das mit Hilfe von Deep Reinforcement Learning eine effiziente Lokalisierung lernt.
In diesem Beitrag wird das Problem der Lokalisierung auf einer bekannten Karte unter Verwendung eines Glaubensnetzes als RL-Problem formuliert, bei dem das Ziel des Agenten darin besteht, die Anzahl der Schritte zu minimieren, um sich selbst zu lokalisieren.
Dies ist eine klare und interessante Arbeit, die ein parametrisiertes Netzwerk zur Auswahl von Aktionen für einen Roboter in einer simulierten Umgebung aufbaut.
Wir entwickeln einen Trainingsalgorithmus für nicht-autoregressive maschinelle Übersetzungsmodelle, der eine vergleichbare Genauigkeit wie starke autoregressive Grundmodelle erreicht, aber eine Größenordnung schneller in der Inferenz ist.  
Entnimmt Wissen aus versteckten Zwischenzuständen und Aufmerksamkeitsgewichten, um die nicht-autoregressive neuronale maschinelle Übersetzung zu verbessern.
Schlägt vor, ein gut trainiertes autoregressives Modell zu nutzen, um die versteckten Zustände und die Wortausrichtung von nicht-autoregressiven neuronalen maschinellen Übersetzungsmodellen zu informieren.
Mit Hilfe mophologischer Operationen (Dilatation und Erosion) haben wir eine Klasse von Netzen definiert, die eine beliebige kontinuierliche Funktion annähern können. 
In dieser Arbeit wird vorgeschlagen, die Standard RELU/tanh Einheiten durch eine Kombination von Dilatations- und Erosionsoperationen zu ersetzen, wobei festgestellt wird, dass der neue Operator mehr Hyperebenen erzeugt und eine größere Ausdruckskraft hat.
Die Autoren stellen Morph-Net vor, ein einschichtiges neuronales Netz, bei dem die Abbildung durch morphologische Dilatation und Erosion erfolgt.
Diese Arbeit erweitert die DNN Kompression über die Gewichte hinaus auf die Aktivierungen, durch Integration des Prunings von Aktivierungen in das Pruning der Gewichte. 
Eine integrale Modellkompressionsmethode, die sowohl Gewichts- als auch Aktivierungs- Pruning handhabt, was zu einer effizienteren Netzberechnung und einer effektiven Reduzierung der Anzahl von Multiplikationen und Akkumulationen führt.
In diesem Artikel wird ein neuartiger Ansatz zur Verringerung der Rechenkosten von tiefen neuronalen Netzen durch die Integration von Aktivierungs-Pruning zusammen mit Gewichts-Pruning vorgestellt. Es wird gezeigt, dass gängige Techniken des ausschließlichen Gewichts-Pruning die Anzahl der Aktivierungen ungleich Null nach ReLU erhöhen.
Wir schlagen eine einfache Methode zum Trainieren von variationalen Auto Encoders (VAE) mit diskreten latenten Repräsentationen unter Verwendung von Wichtigkeitssampling vor.
Einführung einer Wichtigkeits-Sampling Verteilung und Verwendung von Stichproben aus der Verteilung zur Berechnung der Wichtigkeitsgewichteten Schätzung des Gradienten
In dieser Arbeit wird vorgeschlagen, wichtige Stichproben zur Optimierung der VAE mit diskreten latenten Variablen zu verwenden.
Ein neuer verteilter asynchroner SGD Algorithmus, der auf bestehenden Architekturen ohne zusätzliches Tuning oder Overhead die höchste Genauigkeit erreicht.
Vorschlagen einer Verbesserung bestehender ASGD-Ansätze bei mittlerer Skalierung, die Momentum mit SGD für asynchrones Training über einen verteilten Worker-Pool verwendet.
Diese Arbeit befasst sich mit der Staleness des Gradienten im Vergleich zur parallelen Leistung Problem beim verteilten Deep Learning-Training und schlägt einen Ansatz zur Schätzung zukünftiger Modellparameter an den Slaves vor, um die Auswirkungen der Kommunikationslatenz zu reduzieren.
Wir schlagen einen neuen Lernalgorithmus für tiefe neuronale Netze vor, der die schichtweise Abhängigkeit der Backpropagation aufhebt.
Ein alternatives Trainingsparadigma für DNIs, bei dem das Hilfsmodul so trainiert wird, dass es die endgültige Ausgabe des ursprünglichen Modells direkt annähert, bietet Nebeneffekte.
Beschreibt eine Methode zum Training neuronaler Netze ohne Aktualisierungssperre.
Bietet eine unverzerrte Version der verkürzten Backpropagation durch Stichproben der Verkürzungslängen und entsprechende Neugewichtung.
Vorschlagen einer stochastische Methoden zur Bestimmung von Abbruchpunkten in der Backpropagation durch die Zeit.
Eine neue Annäherung an die Backpropagation durch die Zeit, um die Rechen- und Speicherbelastung zu überwinden, die entsteht, wenn man aus langen Sequenzen lernen muss.
nicht gezielte und gezielte Angriffe auf GCN durch Hinzufügen gefälschter Knotenpunkte
Die Autoren schlagen eine neue Technik vor, mit der "falsche" Knoten hinzugefügt werden können, um einen GCN-basierten Klassifikator zu täuschen.
Transferlernen für Sequenzen durch Lernen, um Informationen auf Zellebene über Domänen hinweg abzugleichen.
In dem Papier wird vorgeschlagen, RNN/LSTM mit Kollokationsabgleich als Repräsentationslernmethode für Transferlernen/Domänenanpassung in NLP zu verwenden.
Wir formulieren die Modellunsicherheit beim Reinforcement Learning als einen kontinuierlichen Bayes Adaptiven Markov Entscheidungsprozess und präsentieren eine Methode zur praktischen und skalierbaren Bayes'schen Optimierung von Strategien.
Mit einem Bayes'schen Ansatz lässt sich ein besseres Gleichgewicht zwischen Erkundung und Ausbeutung in RL erreichen.
Wir argumentieren, dass GAN-Benchmarks eine große Stichprobe des Modells erfordern müssen, um das Auswendiglernen zu bestrafen, und untersuchen, ob die Divergenzen neuronaler Netze diese Eigenschaft aufweisen.
Die Autoren schlagen ein Kriterium für die Bewertung der Qualität der von einem generativen adversarial Netzwerk erzeugten Beispielen vor.
Erzeugung von Dialogen im offenen Bereich mit Dialogakten.
Die Autoren verwenden eine Fernüberwachungsmethode, um Tags für Dialoghandlungen als Konditionierungsfaktor für die Generierung von Antworten in Dialogen mit offener Domäne hinzuzufügen.
Das Papier beschreibt eine Technik zur Einbindung von Dialoghandlungen in neuronale Konversationsagenten.
Unsere Hypothese ist, dass bei zwei Bereichen die Abbildung mit der geringsten Komplexität und der geringsten Diskrepanz der Zielabbildung am nächsten kommt.
Die Arbeit befasst sich mit dem Problem des Lernens von Zuordnungen zwischen verschiedenen Domänen ohne jegliche Überwachung und stellt drei Vermutungen auf.
Es wird gezeigt, dass es beim unüberwachten Lernen auf unausgerichteten Daten möglich ist, die Abbildung zwischen den Domänen nur mit GAN ohne Rekonstruktionsverlust zu lernen.
Wir verfeinern die Ergebnisse der Über-Approximation von unvollständigen Verifikatoren mit MILP-Lösern, um mehr Robustheitseigenschaften als der Stand der Technik zu beweisen. 
Vorstellen eines Verifizierer, der die Genauigkeit von unvollständigen Verifizierern und die Skalierbarkeit von vollständigen Verifizierern durch Überparametrisierung, gemischt ganzzahlige lineare Programmierung und Entspannung der linearen Programmierung verbessert.
Eine gemischte Strategie zur Erzielung einer besseren Präzision bei Robustheitsüberprüfungen von Feed-Forward neuronalen Netzen mit stückweise linearen Aktivierungsfunktionen, wobei eine bessere Präzision als bei unvollständigen Überprüfern und eine bessere Skalierbarkeit als bei vollständigen Überprüfern erreicht wird.
Sind HMMs ein Spezialfall von RNNs? Wir untersuchen eine Reihe von architektonischen Transformationen zwischen HMMs und RNNs, sowohl durch theoretische Ableitungen als auch durch empirische Hybridisierung und liefern neue Erkenntnisse.
In diesem Beitrag wird untersucht, ob HMMs ein Spezialfall von RNNs sind, die Sprachmodellierung und POS-Tagging verwenden.
Wir schlagen eine neuartige Regularisierungsmethode vor, die die Kovarianz zwischen den Dimensionen der versteckten Schichten in einem Netzwerk bestraft.
Diese Arbeit stellt einen Regularisierungsmechanismus vor, der die Kovarianz zwischen allen Dimensionen in der latenten Repräsentation eines neuronalen Netzes bestraft, um die latente Repräsentation zu entflechten.
Das vorgeschlagene System ahmt den Klassifizierungsprozess nach, der durch eine Reihe von Ein-Komponenten Pickings vermittelt wird.
Eine Methode zur Erhöhung der Genauigkeit von Deep-Nets bei Mehrklassen-Klassifizierungsaufgaben, die scheinbar durch eine Reduktion von Mehrklassen- auf Binärklassifizierung erfolgt.
Ein neuartiges Klassifizierungsverfahren mit Unterscheidung, Maximalantwort und Mehrfachprüfung zur Verbesserung der Genauigkeit mittelmäßiger Netzwerke und zur Verbesserung von Feedforward Netzwerken.
Die Erfahrung zeigt, dass größere Modelle in weniger Trainingsschritten trainiert werden können, da sich alle Faktoren bei der Durchquerung des Gewichtsraums verbessern.
Diese Arbeit zeigt, dass breitere RNNs die Konvergenzgeschwindigkeit verbessern, wenn sie auf NLP-Probleme angewandt werden, und dass die Auswirkung der Erhöhung der Breiten in tiefen neuronalen Netzen auf die Konvergenz der Optimierung.
In dieser Arbeit werden die Auswirkungen einer Überparametrisierung auf die Anzahl der Wiederholungen, die ein Algorithmus benötigt, um zu konvergieren, beschrieben und weitere empirische Beobachtungen zu den Auswirkungen einer Überparametrisierung beim Training neuronaler Netze vorgestellt.
Mehrköpfige Zeigernetzwerke für gemeinsames Lernen zur Lokalisierung und Behebung von Fehlern durch Variablenmissbrauch.
Schlägt ein LSTM basiertes Modell mit Zeigern vor, um das Problem des VarMisuse in mehrere Schritte aufzuteilen.
In diesem Beitrag wird ein LSTM basiertes Modell zur Fehlererkennung und -behebung des VarMisuse Fehlers vorgestellt, das im Vergleich zu früheren Ansätzen in mehreren Datensätzen deutliche Verbesserungen aufweist.
Menschenähnliches Clustering mit CNNs.
Das Papier bestätigt die Idee, dass tiefe Convolutional Neural Networks lernen könnten, Eingabedaten besser zu clustern als andere Clustering-Methoden, indem sie den Kontext jedes Eingabepunkts aufgrund eines großen Sichtfelds interpretieren können.
Diese Arbeit kombiniert Deep Learning für die Merkmalsdarstellung mit der Aufgabe der menschenähnlichen, unbeaufsichtigten Gruppierung.
Wir entwickeln zwei Algorithmen mit linearer Komplexität für die modellagnostische Modellinterpretation auf der Grundlage des Shapley-Wertes, wenn der Beitrag der Merkmale zum Ziel durch eine graphisch-strukturierte Faktorisierung gut angenähert ist.
In der Arbeit werden zwei Annäherungen an den Shapley-Wert vorgeschlagen, der zur Erstellung von Merkmalsbewertungen für die Interpretierbarkeit verwendet wird.
In dieser Arbeit werden zwei Methoden für die instanzielle Bewertung der Wichtigkeit von Merkmalen unter Verwendung von Shapely-Werten vorgeschlagen und zwei effiziente Methoden zur Berechnung von ungefähren Shapely-Werten bereitgestellt, wenn eine bekannte Struktur zwischen den Merkmalen vorhanden ist.
Lokale Codes wurden in neuronalen Feed-Forward Netzen gefunden.
Eine Methode zur Bestimmung, inwieweit einzelne Neuronen in einer versteckten Schicht eines MLP einen lokalistischen Code kodieren, der für verschiedene Eingabedarstellungen untersucht wird.
Untersucht die Entwicklung von lokalistischen Darstellungen in den verborgenen Schichten von neuronalen Feed-Forward-Netzen.
Erweiterung der relationalen Modellierung zur Unterstützung multimodaler Daten unter Verwendung neuronaler Kodierer.
In diesem Beitrag wird vorgeschlagen, Link-Vorhersagen in Wissensdatenbanken durchzuführen, indem die ursprünglichen Entitäten durch multimodale Informationen ergänzt werden, und es wird ein Modell vorgestellt, das in der Lage ist, bei der Bewertung von Triples alle Arten von Informationen zu kodieren.
In dem Beitrag geht es um die Einbeziehung von Informationen aus verschiedenen Modalitäten in Ansätzen zur Vorhersage von Verbindungen.
Wir schlagen eine neuartige Methode vor, die SG-MCMC-Sampling, Gruppensparsamkeit und Netzwerk-Pruning integriert, um ein Sparse Structured Ensemble (SSE) mit verbesserter Leistung und deutlich geringeren Kosten als traditionelle Methoden zu lernen. 
Die Autoren schlagen ein Verfahren zur Erzeugung eines Ensembles von spärlich strukturierten Modellen vor.
Ein neuer Rahmen für das Training von neuronalen Netzwerken, der SG-MCMC-Methoden innerhalb des Deep Learning verwendet und dann die Recheneffizienz durch Gruppensparsamkeit und Pruning erhöht.
In diesem Beitrag wird die Verwendung von FNN und LSTMs untersucht, um die Durchschnittsbildung von bayesianischen Modellen rechnerisch machbar zu machen und die durchschnittliche Modellleistung zu verbessern.
Neuartiger Rahmen für das Meta-Lernen, der eine breite Klasse bestehender Few-Shot Lernmethoden vereinheitlicht und erweitert. Erzielt eine starke Leistung bei Few-Shot Learning Benchmarks, ohne iterative Testzeitinferenz zu benötigen.   
Diese Arbeit befasst sich mit dem Few-Shot Learning aus der Sicht der probabilistischen Inferenz und erreicht den neuesten Stand der Technik trotz eines einfacheren Aufbaus als viele Wettbewerber.
Definition eines sich teilweise gegenseitig ausschließenden Softmax-Verlustes für positive Daten und Implementierung eines kooperativen Stichprobenverfahrens.
In diesem Beitrag wird Cooperative Importance Sampling vorgestellt, um das Problem zu lösen, dass die sich gegenseitig ausschließende Annahme der traditionellen Softmax verzerrt ist, wenn negative Stichproben nicht explizit definiert sind.
In dieser Arebit werden PMES-Methoden vorgeschlagen, um die Annahme des ausschließlichen Ergebnisses bei Softmax-Verlusten zu lockern, und es wird der empirische Nutzen zur Verbesserung von Einbettungsmodellen des Typs word2vec aufgezeigt.
Lehrer-Schüler Framework für eine effiziente Videoklassifizierung mit weniger Bildern.
In der Arbeit wird eine Idee vorgeschlagen, aus einem vollständigen Videoklassifizierungsmodell ein kleines Modell zu destillieren, das nur eine geringere Anzahl von Einzelbildern erhält.
Die Autoren stellen ein Lehrer-Schüler-Netzwerk zur Lösung von Videoklassifizierungsproblemen vor und schlagen serielle und parallele Trainingsalgorithmen vor, um die Rechenkosten zu reduzieren.
Eine einheitliche statistische Sicht auf die breite Klasse der tiefen generativen Modelle.
Die Arbeit entwickelt einen Rahmen, in dem GAN-Algorithmen als eine Form der Variationsinferenz auf einem generativen Modell interpretiert werden, das eine Indikatorvariable rekonstruiert, die angibt, ob eine Stichprobe zu den wahren generativen Datenverteilungen gehört.
Ein Verfahren, das das Lernen von Regeln und das Lernen von Prototypen kombiniert. 
Es wird ein neuer Framework für interpretierbare Vorhersagen vorgestellt, der regelbasiertes Lernen, Prototyp-Lernen und NNs kombiniert und besonders auf longitudinale Daten anwendbar ist.
Diese Arbeit zielt darauf ab, den Mangel an Interpretierbarkeit von Deep Learning Modellen zu beheben, und schlägt Prototype lEArning via Rule Lists (PEARL) vor, das Regellernen und Prototyplernen kombiniert, um eine genauere Klassifizierung zu erreichen und die Aufgabe der Interpretierbarkeit zu vereinfachen.
In diesem Beitrag wird ein neues Generatives Adversariales Netzwerk vorgeschlagen, das stabiler und effizienter ist und bessere Bilder erzeugt als die bisherigen .
Diese Arbeit kombiniert Fisher-GAN und Deli-GAN.
Diese Arbeit kombiniert Deli-GAN, das eine gemischte Prioritätsverteilung im latenten Raum hat, und Fisher GAN, das Fisher IPM anstelle von JSD als Ziel verwendet.
Wir stellen eine modulare Multisensornetzwerk-Architektur mit einem Aufmerksamkeitsmechanismus vor, der eine dynamische Sensorauswahl auf realen, verrauschten Daten von CHiME-3 ermöglicht.
Eine generische neuronale Architektur, die in der Lage ist, die Aufmerksamkeit zu erlernen, die den verschiedenen Eingangskanälen in Abhängigkeit von der relativen Qualität der einzelnen Sensoren im Vergleich zu den anderen zukommen muss.
Betrachtet die Verwendung von Aufmerksamkeit für die Sensor- oder Kanalauswahl mit Ergebnissen zu TIDIGITS und GRID, die einen Vorteil von Aufmerksamkeit gegenüber der Zusammenstellung von Merkmalen zeigen.
Um ein Cloud-basiertes DNN-Training zu ermöglichen und gleichzeitig den Datenschutz zu gewährleisten, schlagen wir vor, die Zwischendaten-Darstellungen zu nutzen, indem die DNNs aufgeteilt und separat auf lokalen Plattformen und in der Cloud bereitgestellt werden.
In diesem Beitrag wird eine Technik zur Privatisierung von Daten durch das Erlernen einer Merkmalsrepräsentation vorgeschlagen, die für die Bildrekonstruktion schwierig, aber für die Bildklassifizierung hilfreich ist.
Bedingte rekurrente GANs für die Generierung reellwertiger medizinischer Sequenzen, mit neuartigen Bewertungsansätzen und einer empirischen Datenschutzanalyse.
Schlägt vor, synthetische Daten, die von GANs generiert werden, als Ersatz für persönlich identifizierbare Daten beim Training von ML-Modellen für datenschutzsensible Anwendungen zu verwenden.
Die Autoren schlagen eine neuartige rekurrente GAN-Architektur vor, die kontinuierliche Domänensequenzen erzeugt, und evaluieren sie anhand mehrerer synthetischer Aufgaben und einer Aufgabe mit ICU-Zeitreihen.
Schlägt vor, RGANs und RCGANs zu verwenden, um synthetische Sequenzen aus aktuellen Daten zu erzeugen.
Unsere Studien und empirischen Modelle liefern wertvolle neue Informationen für Designer, die verstehen und kontrollieren wollen, wie Betonungseffekte von Nutzern wahrgenommen werden.
In diesem Beitrag wird untersucht, welche visuellen Hervorhebungen bei der Datenvisualisierung schneller wahrgenommen werden und wie verschiedene Hervorhebungsmethoden im Vergleich zueinander abschneiden.
Zwei Studien über die Wirksamkeit von Betonungseffekten, von denen eine die Niveaus nützlicher Unterschiede bewertet, und eine weitere, die für eine ökologisch validere Untersuchung tatsächlich unterschiedliche Visualisierungen verwendet.
Eine einfache, auf dem Gedächtnisnetzwerk (MemNN) und dem Beziehungsnetzwerk (RN) basierende Argumentationsarchitektur, die die Zeitkomplexität im Vergleich zum RN reduziert und ein State-of-the-Art-Ergebnis für die auf bAbI-Geschichten basierende QA und den bAbI-Dialog erzielt.
Einführung des Related Memory Network (RMN), einer Verbesserung gegenüber dem Relationship Network (RN).
Wir zeigen, dass die Aufteilung eines neuronalen Netzes in parallele Zweige die Leistung verbessert und dass die richtige Kopplung der Zweige die Leistung noch weiter erhöht.
In der Arbeit wird eine Rekonfiguration des bestehenden CNN-Modells nach dem Stand der Technik vorgeschlagen, die eine neue Verzweigungsarchitektur mit besserer Leistung verwendet.
Diese Arbeit zeigt die Vorteile der Parametereinsparung durch gekoppeltes Ensembling.
Stellt eine Deep-Network-Architektur vor, die Daten mit Hilfe mehrerer paralleler Zweige verarbeitet und die Ergebnisse dieser Zweige kombiniert, um die endgültigen Ergebnisse zu berechnen.
Kombination von Störungsinjektion, schrittweiser Quantisierung und Aktivierungsklemmung zur Erzielung modernster 3, 4 und 5 Bit Quantisierung.
Schlägt vor, während des Trainings Störungen zu injizieren und die Parameterwerte in einer Schicht sowie die Aktivierungsausgabe in der Quantisierung des neuronalen Netzes zu klammern.
Eine Methode zur Quantisierung von tiefen neuronalen Netzen für Klassifizierung und Regression, die Störungsinjektion, Clamping mit gelernten maximalen Aktivierungen und graduelle Blockquantisierung verwendet, um gleichwertige oder bessere Leistungen als die modernsten Methoden zu erzielen.
Wir schlagen Leap vor, ein System, das Wissen über Lernprozesse hinweg überträgt, indem es die erwartete Distanz minimiert, die der Trainingsprozess auf der Verlustfläche einer Aufgabe zurücklegt.
Der Artikel schlägt ein neuartiges Meta-Lernziel vor, das darauf abzielt, bei der Bewältigung von Aufgabensammlungen, die eine beträchtliche Vielfalt zwischen den einzelnen Aufgaben aufweisen, die modernsten Ansätze zu übertreffen.
Eine Alternative zum Transfer-Lernen, die schneller lernt, viel weniger Parameter benötigt (3-13 %), in der Regel bessere Ergebnisse erzielt und die Leistung bei alten Aufgaben genau beibehält.
Controller Module für inkrementelles Lernen auf Bildklassifizierungsdatensätzen.
Wir stellen ein allgemeines Verfahren zur 8-Bit Inferenz mit niedriger Präzision für Convolutional Neural Networks vor. 
In diesem Beitrag wird ein System zur automatischen Quantisierung der vortrainierten CNN-Modelle entwickelt.
Wir schlagen vor, induktive Verzerrungen und Operationen aus der hyperbolischen Geometrie einzubeziehen, um den Aufmerksamkeitsmechanismus der neuronalen Netze zu verbessern.
In dieser Arbeit wird die in den Aufmerksamkeitsmechanismen verwendete Punktproduktähnlichkeit durch die negative hyperbolische Distanz ersetzt und auf das bestehende Transformer-Modell, Graph-Attention-Networks und Relation-Networks angewendet.
Die Autoren schlagen einen neuartigen Ansatz zur Verbesserung der relationalen Aufmerksamkeit vor, indem sie die Anpassungs- und Aggregationsfunktionen so ändern, dass sie hyperbolische Geometrie verwenden. 
In diesem Beitrag wird gezeigt, wie die H-Infinity-Control-Theorie dazu beitragen kann, robuste, tiefgreifende Strategien für Robotermotoren zu entwickeln.
Es wird vorgeschlagen, Elemente der robusten Kontrolle in die Forschung zu gelenkten Strategien einzubeziehen, um eine Methode zu entwickeln, die gegenüber Störungen und Modellfehlanpassungen widerstandsfähig ist.
Die Arbeit stellt eine Methode zur Bewertung der Sensitivität und Robustheit von Deep RL Strategien vor und schlägt einen dynamischen Spielansatz zum Erlernen robuster Strategien vor.
Analyse der Anfälligkeit von Klassifikatoren für universelle Störungen und Beziehung zur Krümmung der Entscheidungsgrenze.
Die Arbeit bietet eine interessante Analyse, die die Geometrie der Entscheidungsgrenzen von Klassifizierern mit kleinen universellen Störfaktoren in Verbindung bringt.
In diesem Beitrag werden universelle Störungen erörtert - Störungen, die einen trainierten Klassifikator in die Irre führen können, wenn sie den meisten Eingabedatenpunkten hinzugefügt werden.
Es werden Modelle entwickelt, die versuchen, die Existenz universeller Störungen zu erklären, die neuronale Netze täuschen.
Wir schlagen eine Meta-Lernmethode für die interaktive Korrektur von Richtlinien mit natürlicher Sprache vor.
Diese Arbeit bietet ein Meta Learning Framework, das zeigt, wie neue Aufgaben in einer interaktiven Umgebung gelernt werden können. Jede Aufgabe wird durch eine Reinforcement Learning Umgebung gelernt, und dann wird die Aufgabe durch die Beobachtung neuer Anweisungen aktualisiert.
In diesem Beitrag wird Agenten beigebracht, Aufgaben mittels natürlichsprachlicher Anweisungen in einem iterativen Prozess zu erledigen.
Wir untersuchen die Modularität von tiefen generativen Modellen.
Die Arbeit bietet eine Möglichkeit, die modulare Struktur des tiefen generativen Modells zu untersuchen, mit dem Schlüsselkonzept, über Kanäle von Generatorarchitekturen zu verteilen.
Wir stellen Seq2SQL vor, das Fragen in SQL-Abfragen übersetzt, indem es Belohnungen aus der Online-Abfrageausführung nutzt, und WikiSQL, einen SQL-Tabellen-/Fragen-/Abfragedatensatz, der um Größenordnungen größer ist als bestehende Datensätze.
Ein neuer semantischer Parsing-Datensatz, der sich auf die Generierung von SQL aus natürlicher Sprache unter Verwendung eines auf Reinforcement Learning basierenden Modells konzentriert.
Die Modellierung von Störungen am Eingang während des diskriminierenden Trainings verbessert die Robustheit gegenüber nachteiligen Einflüssen. Vorschlag einer PCA-basierten Bewertungsmetrik für die adversial Robustheit.
Diese Arbeit schlägt ExL vor, eine adversarische Trainingsmethode, die multiplizierte Störungen verwendet und sich als hilfreich bei der Abwehr von Blackbox-Angriffen auf drei Datensätze erweist.
In diesem Beitrag werden multiplikative Störungen N in die Trainingsdaten aufgenommen, um adversial Robustheit zu erreichen, wenn sowohl auf die Modellparameter theta als auch auf den Störungen selbst trainiert wird.
Eine Methode zur Beantwortung der Frage "Warum nicht Klasse B?" zur Erklärung von tiefen Netzen.
Die Arbeit schlägt einen Ansatz vor, der kontrastive visuelle Erklärungen für tiefe neuronale Netze liefert.
Wir analysieren die Invertierbarkeit von tiefen neuronalen Netzen, indem wir die Präimages von ReLU-Schichten und die Stabilität der Inverse untersuchen.
Diese Arbeit untersucht das Volumen der Vorabbildung der Aktivierung eines ReLU-Netzwerks in einer bestimmten Schicht und baut auf der stückweisen Linearität der Forward Funktion eines ReLU-Netzwerks auf. 
Dieses Arbeit stellt eine Analyse der inversen Invarianz von ReLU-Netzen vor und liefert obere Schranken für singuläre Werte eines Zugnetzes.
Das adversial Training von Ensembles bietet eine Robustheit gegenüber adversial Beispielen, die über die von gegnerisch trainierten Modellen und unabhängig trainierten Ensembles beobachtete Robustheit hinausgeht.
 Es wird vorgeschlagen, ein Ensemble von Modellen gemeinsam zu trainieren, wobei in jedem Zeitschritt eine Reihe von Beispielen, die für das Ensemble selbst adversial sind, in das Lernen einbezogen wird.
Routing-Netzwerke: eine neue Art von neuronalem Netzwerk, das lernt, seine Eingaben adaptiv zu routen, um Multi-Task-Lernen zu ermöglichen.
In dem Beitrag wird vorgeschlagen, ein modulares Netz mit einem Controller zu verwenden, der bei jedem Zeitschritt Entscheidungen über den nächsten zu verwendenden Knoten trifft.
Die Arbeit stellt eine neuartige Formulierung für das Lernen der optimalen Architektur eines neuronalen Netzes in einem Multi-Task-Learning-Rahmen vor, indem es Multi-Agenten-Reinforcement Learning verwendet, um eine Strategie zu finden, und zeigt eine Verbesserung gegenüber fest kodierten Architekturen mit geteilten Schichten.
Wir zeigen, wie man die erwartete L_0-Norm parametrischer Modelle mit Gradientenabstieg optimieren kann, und führen eine neue Verteilung ein, die das Hard Gating erleichtert.
Die Autoren stellen einen gradientenbasierten Ansatz zur Minimierung einer Zielfunktion mit einem L0 Sparse Penalty vor, um das Erlernen spärlicher neuronaler Netze zu unterstützen.
Wir schlagen eine neuartige, auf Aufmerksamkeit basierende, interpretierbare Graph Neural Network-Architektur vor, die den aktuellen Stand der Technik in Standard-Benchmark-Datensätzen übertrifft.
Die Autoren schlagen zwei Erweiterungen von GCNs vor, indem sie zwischengeschaltete Nichtlinearitäten aus der GCN-Berechnung entfernen und einen Aufmerksamkeitsmechanismus in der Aggregationsschicht hinzufügen.
Die Arbeit schlägt einen halbüberwachten Lernalgorithmus für die Klassifizierung von Graphknoten vor, der von Graph Neural Networks inspiriert ist.
Ein Framework für das Training von generativen Modellen auf der Basis von Autoencodern, mit nicht-adversen Verlusten und uneingeschränkten neuronalen Netzwerkarchitekturen.
In diesem Beitrag werden Autoencoder verwendet, um eine Verteilungsanpassung im hochdimensionalen Raum vorzunehmen.
Produktvielfältige Einbettungsräume mit heterogener Krümmung liefern im Vergleich zu traditionellen Einbettungsräumen verbesserte Darstellungen für eine Vielzahl von Strukturen.
Schlägt eine Methode zur Dimensionalitätsreduktion vor, die Daten in eine Produktmannigfaltigkeit von sphärischen, euklidischen und hyperbolischen Mannigfaltigkeiten einbettet. Der Algorithmus basiert auf dem Abgleich der geodätischen Abstände auf der Produktmannigfaltigkeit mit Graphenabständen.
Wir integrieren symbolische (deduktive) und statistische (neural-basierte) Methoden, um eine Programmsynthese in Echtzeit mit nahezu perfekter Generalisierung von einem Eingabe-Ausgabe-Beispiel zu ermöglichen.
Die Arbeit präsentiert einen Branch-and-Bound-Ansatz zum Erlernen guter Programme, bei dem ein LSTM verwendet wird, um vorherzusagen, welche Zweige im Suchbaum zu guten Programmen führen sollten.
Schlägt ein System vor, das aus einem einzigen Beispiel Programme synthetisiert, die besser verallgemeinert werden können als der bisherige Stand der Technik.
Wir untersuchen die Überschneidung von VAEs und Sparse Coding.
Diese Arbeit schlägt eine Erweiterung von VAEs mit spärlichen Prioren und Posterioren vor, um spärliche interpretierbare Repräsentationen zu lernen.
Die prinzipielle Einstellung von Skip-Verbindungen verhindert eine Verschlechterung in tiefen Feed-Forward-Netzen.
Die Autoren stellen eine neue Trainingsstrategie, VAN, für das Training sehr tiefer Feed-Forward-Netze ohne Skip-Verbindungen vor.
Die Arbeit stellt eine Architektur vor, die linear zwischen ResNets und Vanilla Deep Nets interpoliert, ohne Verbindungen zu überspringen.
Komprimierung von tiefen neuronalen Netzen, die auf eingebetteten Geräten eingesetzt werden.
Die Autoren stellen einen l-1 regularisierten SVRG-basierten Trainingsalgorithmus vor, der in der Lage ist, viele Gewichte des Netzwerks auf 0 zu setzen.
Diese Arbeit reduziert den Speicherbedarf.
Ein auf vorhersehender Kodierung basierender Lernalgorithmus für den Aufbau tiefer neuronaler Netzmodelle des Gehirns.
Der Beitrag befasst sich mit dem Lernen eines generativen neuronalen Netzes unter Verwendung eines prädiktiven Kodierungssystems.
Die Erkennung von Objektinstanzen mit adversial Autoencodern wurde mit einem neuartigen "mentalen Bild" durchgeführt, das eine kanonische Repräsentation des Eingangsbildes darstellt.
In dem Beitrag wird eine Methode zum Erlernen von Merkmalen für die Objekterkennung vorgeschlagen, die gegenüber verschiedenen Transformationen des Objekts, insbesondere der Objektposition, invariant ist.
Diese Arbeit untersuchte die Aufgabe der Few-Shot Erkennung mittels eines generierten mentalen Bildes als Zwischendarstellung des Eingangsbildes.
Kombinieren Sie zeitliche Logik mit hierarchischem Reinforcement Learning für die Komposition von Fähigkeiten.
Die Arbeit bietet eine Strategie zur Konstruktion eines Produkt-MDPs aus einem ursprünglichen MDP und dem mit einer LTL-Formel verbundenen Automaten.
Es wird vorgeschlagen, zeitliche Logik mit hierarchischem Reinforcement Learning zu verbinden, um die Zusammensetzung von Fähigkeiten zu vereinfachen.
Wir schlagen ein Quantisierungsschema für Gewichte und Aktivierungen von tiefen neuronalen Netzen vor. Dadurch wird der Speicherbedarf erheblich reduziert und die Inferenz beschleunigt.
CNN-Modellkomprimierung und Beschleunigung der Inferenz durch Quantisierung.
Wenn ein Roboter in einer Umgebung eingesetzt wird, in der Menschen gehandelt haben, ist der Zustand der Umgebung bereits für die Wünsche der Menschen optimiert, und wir können dies nutzen, um auf menschliche Präferenzen zu schließen.
Die Autoren schlagen vor, die explizit angegebene Belohnungsfunktion eines RL-Agenten durch zusätzliche Belohnungen/Kosten zu ergänzen, die aus dem Anfangszustand und einem Modell der Zustandsdynamik abgeleitet werden.
In dieser Arbeit wird ein Weg vorgeschlagen, die implizite Information im Ausgangszustand mit Hilfe von IRL abzuleiten und die abgeleitete Belohnung mit einer vorgegebenen Belohnung zu kombinieren.
Systematische Kategorisierung von Regularisierungsmethoden für Deep Learning, die ihre Gemeinsamkeiten aufzeigt.
Versuch, eine Taxonomie für Regularisierungstechniken zu erstellen, die beim Deep Learning eingesetzt werden.
Wir beweisen die exponentielle Effizienz von rekurrenten neuronalen Netzen gegenüber flachen Netzen.
Die Autoren vergleichen die Komplexität von Tensor-Train Netzen mit Netzen, die durch CP-Zerlegung strukturiert sind.
Eine neue probabilistische Behandlung für GAN mit theoretischer Garantie.
In diesem Beitrag wird ein Bayes'sches GAN vorgeschlagen, das theoretische Garantien für die Konvergenz zur realen Verteilung hat und Likelihoods über den Generator und den Diskriminator mit Logarithmen proportional zu den traditionellen GAN-Zielfunktionen setzt.
Verteidigung gegen nachteilige Störungen neuronaler Netze aufgrund vielfältiger Annahmen.
In dem Manuskript werden zwei Zielfunktionen vorgeschlagen, die auf der Annahme der Vielfältigkeit basieren und als Abwehrmechanismen gegen adversarial Beispiele dienen.
Verteidigung gegen adversarial Angriffe auf der Grundlage der Annahme, dass natürliche Daten vielfältig sind.
Suche nach einer neuronalen Architektur in einem einzigen Schritt durch direkte spärliche Optimierung.
Es wird eine Methode zur Architektursuche vorgestellt, bei der Verbindungen mit spärlicher Regularisierung entfernt werden.
In diesem Beitrag wird die direkte Sparse Optimierung vorgeschlagen, eine Methode, die es ermöglicht, neuronale Architekturen für spezifische Probleme zu einem vernünftigen Rechenaufwand zu erhalten.
In dieser Arebit wird eine Suchmethode für neuronale Architekturen vorgeschlagen, die auf einer direkten Sparse Optimierung basiert.
Erzielt modernste Genauigkeit für quantisierte, flache Netze durch Nutzung der Destillation. 
Vorschläge für kleine und kostengünstige Modelle durch die Kombination von Destillation und Quantisierung für Experimente zur Bildverarbeitung und neuronaler maschineller Übersetzung.
In diesem Beitrag wird ein Rahmen für die Verwendung des Lehrermodells zur Unterstützung der Kompression für das Deep Learning Modell im Rahmen der Modellkompression vorgestellt.
NMT mit latenten Bäumen verbessern.
Diese Arbeit beschreibt eine Methode zur Induktion von quellenseitigen Abhängigkeitsstrukturen im Dienste der neuronalen maschinellen Übersetzung.
Lernen indem von einer einzigen Demonstration aus rückwärts gearbeitet wird, selbst wenn diese ineffizient ist, und dem Agenten progressiv erlauben nach und nach mehr Aufgaben selbst zu lösen.
In diesem Beitrag wird eine Methode zur Steigerung der Effizienz von spärlichen Belohnungs-RL-Methoden durch ein Rückwärts Curriculum auf Expertendemonstrationen vorgestellt. 
Die Arbeit stellt eine Strategie zur Lösung von spärlichen Belohnungsaufgaben mit RL vor, indem Anfangszustände aus Demonstrationen gesampelt werden.
Externer Speicher für Online-Reinforcement Learning basierend auf der Schätzung von Gradienten über eine neuartige Reservoir-Sampling-Technik.
Die Arbeit schlägt einen modifizierten Ansatz für RL vor, bei dem ein zusätzliches "episodisches Gedächtnis" vom Agenten gehalten wird und ein "Abfragenetzwerk" verwendet wird, das auf dem aktuellen Zustand basiert.
Wir erreichen eine Bias-Varianz-Zerlegung für Boltzmann-Maschinen mit Hilfe einer informationsgeometrischen Formulierung.
Das Ziel dieses Artikels ist es, die Effektivität und Verallgemeinerbarkeit von Deep Learning zu analysieren, indem eine theoretische Analyse der Bias-Varianz-Zerlegung für hierarchische Modelle, insbesondere Boltzmann-Maschinen, vorgestellt wird.  
Die Arbeit kommt zu dem Schluss, dass es möglich ist, sowohl die Verzerrung als auch die Varianz in einem hierarchischen Modell zu reduzieren.
Kombination von Netzwerk Pruning und persistenten Kernels zu einer praktischen, schnellen und genauen Netzimplementierung.
In diesem Beitrag werden spärliche persistente RNNs vorgestellt, ein Mechanismus, der die bestehende Arbeit des Speicherns von RNN-Gewichten auf einem Chip durch Pruning ergänzt.
Wir stellen eine neue Pruning-Methode und ein spärliches Matrixformat vor, um eine hohe Indexkomprimierung und eine parallele Indexdekodierung zu ermöglichen.
Die Autoren verwenden die Viterbi-Codierung, um den Index der Sparse-Matrix eines Pruned Netzwerks drastisch zu komprimieren, wodurch einer der wichtigsten Speicher-Overheads reduziert und die Inferenz in der parallelen Umgebung beschleunigt wird.
Ein neuartiges hierarchisches Regelnetzwerk, das bereits erlernte Fähigkeiten neben und als Teilkomponenten von neuen Fähigkeiten wiederverwenden kann, indem es die zugrunde liegenden Beziehungen zwischen den Fähigkeiten entdeckt.
Diese Arbeit zielt darauf ab, hierarchische Regeln zu lernen, indem eine rekursive Regelstruktur verwendet wird, die durch eine stochastische zeitliche Grammatik geregelt wird.
In diesem Beitrag wird ein Ansatz zum Erlernen hierarchischer Strategien in einem Kontext des lebenslangen Lernens vorgeschlagen, indem Strategien gestapelt und dann eine explizite "Switch"-Strategie verwendet wird.
Wir schlagen vor, explizite vektoralgebraische Formelprojektionen als alternative Methode zur Visualisierung von Einbettungsräumen zu verwenden, die speziell auf zielgerichtete Analyseaufgaben zugeschnitten ist und in unserer Benutzerstudie t-SNE übertrifft.
Analyse der Einbettung von Psaces auf nicht-parametrische Weise (anhand von Beispielen).
Wir schlagen einen prinzipiellen Ansatz vor, der Klassifizierer mit der Fähigkeit ausstattet, größeren Abweichungen zwischen Trainings- und Testdaten auf intelligente und effiziente Weise zu widerstehen.
Introspektives Lernen zum Umgang mit Datenvariationen zur Testzeit.
Diese Arbeit schlägt die Verwendung von gelernten Transformationsnetzwerken vor, die in introspektive Netzwerken eingebettet sind, um die Klassifizierungsleistung mit synthetisierten Beispielen zu verbessern.
Diskretisierung der Eingaben führt zu Robustheit gegenüber adversarial Beispielen.
Die Autoren präsentieren eine eingehende Studie über die Diskretisierung / Quantisierung der Eingabe als Schutz gegen adversarial Beispiele.
Wir haben dimensionsunabhängige Schranken für Trainingsalgorithmen mit geringer Genauigkeit bewiesen.
In diesem Beitrag werden Bedingungen diskutiert, unter denen die Konvergenz von Trainingsmodellen mit niedrigpräzisen Gewichten nicht von der Modelldimension abhängt.
Änderungen an MAML und RL2, die eine bessere Erkundung ermöglichen sollen. 
Die Arbeit schlägt einen Trick vor, um die Zielfunktionen zu erweitern, um die Exploration in Meta-RL auf der Grundlage von zwei neuen Meta RL Algorithmen voranzutreiben.
Ein probabilistisches neuronales symbolisches Modell mit einem latenten Programmraum für besser interpretierbare Fragebeantwortung.
In diesem Beitrag wird ein diskretes, strukturiertes latentes Variablenmodell für die Beantwortung visueller Fragen vorgeschlagen, das eine kompositionelle Generalisierung und Schlussfolgerung mit erheblichem Leistungs- und Fähigkeitsgewinn beinhaltet.
Wir nutzen die formale Verifikation, um die Effektivität von Techniken zum Auffinden von Gegenbeispielen oder zur Abwehr von Gegenbeispielen zu bewerten.
In diesem Beitrag wird eine Methode zur Berechnung von adversarial Beispielen mit minimalem Abstand zu den ursprünglichen Eingaben vorgeschlagen.
Die Autoren schlagen vor, Beispiele mit nachweislich minimalem Abstand als Instrument zur Bewertung der Robustheit eines trainierten Netzes zu verwenden.
Die Arbeit beschreibt eine Methode zur Generierung von adversarial Beispielen, die einen minimalen Abstand zu dem Trainingsbeispiel haben, das zu ihrer Generierung verwendet wurde.
Die Interaktion zwischen Paragraphen Retriever und maschinellem Lesegerät erfolgt über Reinforcement Learning, um große Verbesserungen bei offenen Datensätzen zu erzielen.
Die Arbeit stellt einen neuen Rahmen für die bidirektionale Interaktion zwischen Dokumentensuchmaschine und Leser für die Beantwortung von Fragen in offenen Bereichen mit der Idee des 'Leserzustands' von Leser zu Suchmaschine vor.
In dem Beitrag wird ein Modell für das maschinelle Lesen von mehreren Dokumenten vorgeschlagen, das aus drei verschiedenen Teilen und einem Algorithmus besteht.
Es wird eine neue Architektur vorgestellt, die die Informationsglobalisierungskraft von U-Netzen in einem tieferen Netz nutzt und ohne Schnickschnack aufgabenübergreifend gute Leistungen erbringt.
Eine Netzarchitektur für die semantische Bildsegmentierung, die auf der Zusammenstellung eines Stapels grundlegender U-Netz-Architekturen basiert, die die Anzahl der Parameter reduziert und die Ergebnisse verbessert.
Hier wird eine gestapelte U-Netz-Architektur für die Bildsegmentierung vorgeschlagen.
Wir schlagen ein neuronales Netz vor, das in der Lage ist, themenspezifische Fragen zu generieren.
Präsentiert einen auf neuronalen Netzen basierenden Ansatz zur Generierung themenspezifischer Fragen mit der Begründung, dass themenspezifische Fragen in praktischen Anwendungen sinnvoller sind.
Schlägt eine themenbasierte Generierungsmethode vor, die ein LSTM zur Extraktion von Themen mittels einer zweistufigen Kodierungstechnik verwendet.
Wir implementieren ein adversariales Domänenanpassungsnetzwerk, um eine feste Gehirn-Maschine-Schnittstelle gegen allmähliche Veränderungen der aufgezeichneten neuronalen Signale zu stabilisieren.
Beschreibt einen neuen Ansatz für implantierte Gehirn-Maschine Schnittstellen, um Kalibrierungsprobleme und Kovariatenverschiebungen zu lösen. 
Die Autoren definieren ein BMI, das einen Autoencoder verwendet, und gehen dann auf das Problem der Datendrift im BMI ein.
Analyse und Verständnis der Art und Weise, wie Agenten in neuronalen Netzen lernen, einfache, grundierte Sprache zu verstehen.
Die Autoren verbinden psychologische Versuchsmethoden mit dem Verständnis, wie die Blackbox der Deep Learning Methoden Probleme löst.
In diesem Beitrag wird eine Analyse der Agenten vorgestellt, die durch Reinforcement Learning in einer einfachen Umgebung, die verbale Anweisungen mit visuellen Informationen kombiniert, eine grundierte Sprache lernen.
Erlernen von Objektteilen, hierarchischer Struktur und Dynamik durch Beobachten ihrer Bewegung.
Schlägt ein unbeaufsichtigtes Lernmodell vor, das lernt, Objekte in Teile zu zerlegen, eine hierarchische Struktur für die Teile vorherzusagen und auf der Grundlage der zerlegten Teile und der Hierarchie Bewegungen vorherzusagen.
Wir entwickeln ein Verständnis für ressourceneffiziente Techniken zur Super-Resolution.
Die Arbeit schlägt eine detaillierte empirische Bewertung der Kompromisse vor, die von verschiedenen Convolutional Neural Networks bei dem Problem der Superauflösung erzielt werden.
In diesem Beitrag wird vorgeschlagen, die Effizienz der Systemressourcen für Netze mit hoher Auflösung zu verbessern.
Wir untersuchen ReLU-Netzwerke in der Fourier-Domäne und zeigen ein merkwürdiges Verhalten.
Fourier-Analyse von ReLU-Netzwerken, bei der festgestellt wurde, dass sie auf das Lernen niedriger Frequenzen ausgerichtet sind. 
Diese Arbeit enthält theoretische und empirische Beiträge zum Thema Fourier-Koeffizienten von neuronalen Netzen.
In dem Beitrag wird vorgeschlagen, Wahrscheinlichkeitsverteilungen anstelle von Punkten für Einbettungsaufgaben wie Erkennung und Überprüfung zu verwenden.
In der Arbeit wird eine Alternative zur derzeitigen Punkteinbettung und eine Technik zu deren Schulung vorgeschlagen.
Die Arbeit schlägt ein Modell vor, das unsichere Einbettungen verwendet, um Deep Learning auf Bayes'sche Anwendungen zu erweitern.
Wir schlagen Tensorkontraktion und Tensorregressionsschichten mit niedrigem Rang vor, um die multilineare Struktur im gesamten Netzwerk zu erhalten und zu nutzen, was zu einer enormen Platzersparnis mit geringen bis keinen Auswirkungen auf die Leistung führt.
In diesem Beitrag werden neue Schichtenarchitekturen für neuronale Netze vorgeschlagen, die eine Low-Rank-Darstellung von Tensoren verwenden.
In dieser Arbeit werden Tensorzerlegung und Tensorregression in CNN integriert, indem eine neue Tensorregressionsschicht verwendet wird.
Wir verwenden zweisprachige Wörterbücher zur Datenerweiterung für die neuronale maschinelle Übersetzung.
In diesem Beitrag wird die Verwendung zweisprachiger Wörterbücher zur Erstellung synthetischer Quellen für einsprachige Zieldaten untersucht, um NMT-Modelle zu verbessern, die mit kleinen Mengen paralleler Daten trainiert wurden.
Wir schlagen ein neuartiges Modell der Neugier vor, das auf dem episodischen Gedächtnis und der Idee der Erreichbarkeit basiert und uns erlaubt, die bekannten "Couch-Potato" Probleme früherer Arbeiten zu überwinden.
Schlägt vor, Explorationsboni in RL-Algorithmen zu vergeben, indem größere Boni für Beobachtungen vergeben werden, die in Umgebungsschritten weiter entfernt sind.
Die Autoren schlagen einen Explorationsbonus vor, der bei spärlichen Belohnungs-RL-Problemen helfen soll, und betrachten viele Experimente in komplexen 3D-Umgebungen.
Wir führen einen neuen Datensatz mit logischen Folgerungen ein, um die Fähigkeit von Modellen zu messen, die Struktur von logischen Ausdrücken zu erfassen und zu nutzen, und zwar anhand einer Aufgabe zur Vorhersage von Folgerungen.
Die Arbeit schlägt ein neues Modell vor, um tiefe Modelle für die Erkennung von logischen Folgerungen als Produkt von kontinuierlichen Funktionen über mögliche Welten zu verwenden.
Schlägt ein neues Modell für maschinelles Lernen mit Vorhersage der logischen Folgerung vor.
Die Arbeit handelt von einer neuen energieeffizienten Methode für inkrementelles Lernen.
Er schlägt ein Verfahren für inkrementelles Lernen als Transferlernen vor.
In diesem Beitrag wird eine Methode vorgestellt, mit der tiefe Convolutional Neural Networks inkrementell trainiert werden können, wobei die Daten in kleinen Batches über einen bestimmten Zeitraum hinweg verfügbar sind.
Stellt einen Ansatz für klasseninkrementelles Lernen mit tiefen Netzen vor, indem er drei verschiedene Lernstrategien für den endgültigen/besten Ansatz vorschlägt.
Paralleles Scannen zur Parallelisierung linearer rekurrenter neuronaler Netze verwenden. Modell mit einer Länge von 1 Million Abhängigkeiten trainieren.
Schlägt vor, RNN durch Anwendung der Methode von Blelloch zu beschleunigen.
Die Autoren schlagen einen parallelen Algorithmus für lineare Surrogate-RNNs vor, der Geschwindigkeitssteigerungen gegenüber den bestehenden Implementierungen von Quasi-RNN, SRU und LSTM ermöglicht.
GAN in natürlicher Sprache zum Ausfüllen von Lücken.
In diesem Beitrag wird vorgeschlagen, Text mithilfe von GANs zu generieren.
Generierung von Textproben mit Hilfe von GAN und einem Mechanismus zum Auffüllen fehlender Wörter in Abhängigkeit vom umgebenden Text.
Vergleich von psychophysischen und CNN-kodierten Texturrepräsentationen in einer Anwendung zur Erkennung von Neuheiten in einem neuronalen Einklassen-Netzwerk.
Dieser Artikel konzentriert sich auf die Erkennung von Neuheiten und zeigt, dass psychophysikalische Darstellungen die VGG-Encoder-Merkmale in einigen Bereichen dieser Aufgabe übertreffen können.
In diesem Beitrag wird die Erkennung von Anomalien in Texturen untersucht und eine originelle Verlustfunktion vorgeschlagen.
Schlägt vor, zwei Anomalie-Detektoren aus drei verschiedenen Modellen zu trainieren, um Wahrnehmungsanomalien in visuellen Texturen zu erkennen.
Wir schlagen einen neuartigen Regularisierungsansatz vor, der die Nicht-Überlappung beim Repräsentationslernen fördert, um die Interpretierbarkeit zu verbessern und die Überanpassung zu reduzieren.
Die Arbeit führt einen Matrix-Regulierer ein, um gleichzeitig sowohl Seltenheit als auch annähernde Orthogonalität zu induzieren.
Der Artikel untersucht eine Regularisierungsmethode, um die Seltenheit zu fördern und die Überlappung zwischen den Trägern der Gewichtsvektoren in den gelernten Darstellungen zu reduzieren, um die Interpretierbarkeit zu verbessern und eine Überanpassung zu vermeiden.
In dem Beitrag wird ein neuer Regularisierungsansatz vorgeschlagen, der gleichzeitig dazu führt, dass die Gewichtsvektoren (W) spärlich und orthogonal zueinander sind.
Beweist, dass Gating-Mechanismen gegenüber Zeittransformationen invariant sind. Einführung und Test einer neuen Initialisierung für LSTMs auf der Grundlage dieser Erkenntnis.
Die Arbeit verbindet die Entwicklung von rekurrenten Netzwerken und deren Auswirkungen auf die Reaktion des Netzwerks auf Zeittransformationen und nutzt dies, um ein einfaches Initialisierungsschema für die Verzerrung zu entwickeln.
Wir schlagen ein neues Framework für die automatische Steuerung des maschinellen Lernprozesses vor und überprüfen die Wirksamkeit des "Learning to teach".
Diese Arbeit konzentriert sich auf das "maschinelle Lernen" und schlägt vor, das Reinforcement Learning zu nutzen, indem die Belohnung als die Lerngeschwindigkeit des Lernenden definiert wird und die Parameter des Lehrers mit Hilfe des Policy-Gradienten aktualisiert werden.
Die Autoren definieren ein Deep Learning Modell, das aus vier Komponenten besteht: einem Studentenmodell, einem Lehrermodell, einer Verlustfunktion und einem Datensatz. 
Vorschlagen eines Frameworks für das "Lernen zu lehren", das der Auswahl der Daten entspricht, die dem Lernenden präsentiert werden.
Ein differenzierbarer Verlust für logische Beschränkungen beim Training und bei der Abfrage neuronaler Netze.
Ein Rahmenwerk für die Umwandlung von Abfragen über Parameter und Eingabe-/Ausgabepaare für neuronale Netze in differenzierbare Verlustfunktionen und eine zugehörige deklarative Sprache für die Spezifikation dieser Abfragen.
Diese Arbeit befasst sich mit dem Problem der Kombination logischer Ansätze mit neuronalen Netzen, indem eine logische Formel in eine nicht-negative Verlustfunktion für ein neuronales Netz übersetzt wird.
Auf genetischen Algorithmen basierender Ansatz zur Optimierung von Strategien für tiefe neuronale Netze.
Die Autoren stellen einen Algorithmus für das Training von Ensembles von Strategienetzwerken vor, der regelmäßig verschiedene Strategien im Ensemble miteinander mischt.
In diesem Beitrag wird eine vom genetischen Algorithmus inspirierte Methode zur Optimierung von Richtlinien vorgeschlagen, die die Mutations- und Crossover-Operatoren in Richtliniennetzwerken nachahmt.
Ein prinzipielles Framework für die Modellquantisierung unter Verwendung der proximalen Gradientenmethode, mit empirischer Bewertung und theoretischen Konvergenzanalysen.
Schlägt die ProxQuant-Methode zum Trainieren neuronaler Netze mit quantisierten Gewichten vor.
Schlägt vor, binäre Netze und ihre Varianten mit Hilfe des proximalen Gradientenabstiegs zu lösen.
Schlechte lokale Minima verschwinden in einem mehrschichtigen neuronalen Netz: Ein Beweis mit vernünftigeren Annahmen als bisher.
In Netzen mit einer einzigen verborgenen Schicht nimmt das Volumen der suboptimalen lokalen Minima im Vergleich zu den globalen Minima exponentiell ab.
In diesem Beitrag wird untersucht, warum Standard-SGD-Algorithmen auf der Grundlage neuronaler Netze zu "guten" Lösungen konvergieren.
Wir schlagen eine aufmerksamkeitsinvariante Angriffsmethode vor, um mehr übertragbare adversarial Beispiele für Blackbox-Angriffe zu generieren, die modernste Verteidigungsmaßnahmen mit einer hohen Erfolgsquote überlisten können.
Die Arbeit schlägt einen neuen Weg vor, um den Stand der Technik bei der Abwehr von gegnerischen Angriffen auf CNN zu überwinden.
In diesem Beitrag wird die Vermutung geäußert, dass die "Aufmerksamkeitsverschiebung" eine Schlüsseleigenschaft ist, die dazu führt, dass adversarial Angriffe nicht übertragen werden können, und es wird eine aufmerksamkeitsinvariante Angriffsmethode vorgeschlagen.
Wie man 100.000 Klassen auf einem einzigen Grafikprozessor trainiert.
Vorschlagen einer effizienten Hashing-Methode MACH für die Softmax-Approximation im Kontext eines großen Ausgaberaums, die sowohl Speicher als auch Rechenzeit spart.
Eine Methode zur Klassifizierung von Problemen mit einer großen Anzahl von Klassen in einem Mehrklassenumfeld, demonstriert an ODP- und Imagenet-21K Datensätzen.
Die Arbeit stellt ein auf Hashing basierendes Verfahren zur Verringerung der Speicher- und Berechnungszeit für die K-Wege-Klassifizierung vor, wenn K groß ist.
Wir stellen eine allgemeine Methode zur unverzerrten Schätzung von Gradienten von Black-Box-Funktionen von Zufallsvariablen vor. Wir wenden diese Methode auf diskrete Variationsinferenz und Reinforcement Learning an. 
Schlägt einen neuen Ansatz zur Durchführung des Gradientenabstiegs für die Blackbox-Optimierung oder das Training diskreter latenter Variablenmodelle vor.
Wir schlagen einen Schätzer für die Unterstützungsgröße der gelernten Verteilung von GANs vor, um zu zeigen, dass sie in der Tat unter einem Mode-Kollaps leiden, und wir beweisen, dass Encoder-Decoder GANs dieses Problem ebenfalls nicht vermeiden.
In dem Beitrag wird versucht, die Größe der Unterstützung für Lösungen, die von typischen GANs erzeugt werden, experimentell zu schätzen. 
Diese Arbeit schlägt einen cleveren neuen Test vor, der auf dem Geburtstags-Paradoxon basiert, um die Vielfalt in generativen Beispielen zu messen. Die Ergebnisse des Experiments werden dahingehend interpretiert, dass der Modus-Kollaps in einer Reihe von modernen generativen Modellen stark ist.
Die Arbeit verwendet das Geburtstagsparadoxon, um zu zeigen, dass einige GAN-Architekturen Verteilungen mit relativ geringer Unterstützung erzeugen.
Wir schlagen ein Framework vor, um natürliche Gegner gegen Black-Box-Klassifikatoren sowohl für visuelle als auch für textuelle Domänen zu generieren, indem wir die Suche nach Gegnern im latenten semantischen Raum vornehmen.
Schlägt eine Methode zur Erstellung von semantischen Gegenbeispielen vor.
Schlägt einen Rahmen zur Erzeugung natürlicher Gegenbeispiele durch die Suche nach Gegenspielern in einem latenten Raum mit dichter und kontinuierlicher Datendarstellung vor.
Wir erweitern die K-FAC Methode auf RNNs, indem wir eine neue Familie von Fisher Approximationen entwickeln.
Die Autoren erweitern die K-FAC-Methode auf RNNs und stellen drei Möglichkeiten der Annäherung von F vor. Sie zeigen Optimierungsergebnisse für drei Datensätze, die ADAM sowohl in der Anzahl der Aktualisierungen als auch in der Rechenzeit übertreffen.
Schlägt vor, die Optimierungsmethode mit dem Kronecker-Faktor Appropriate Curvature auf rekurrente neuronale Netze zu erweitern.
Die Autoren stellen eine Methode zweiter Ordnung vor, die speziell für RNNs konzipiert ist.
Das generative Modell für Kernel von Convolutional Neural Networks, das beim Training auf neuen Datensätzen als Prior-Verteilung fungiert.
Eine Methode zur Modellierung von Convolutional Neural Networks unter Verwendung einer Bayes-Methode.
Die Idee ist, einen Prior für einen Hilfsdatensatz zu ermitteln und diesen Prior dann über die CNN-Filter zu verwenden, um die Inferenz für einen Datensatz von Interesse zu starten.
Diese Arbeit untersucht das Erlernen informativer Prioritäten für Convolutional Neural Network Modelle mit ähnlichen Problemdomänen, indem es Autoencoder verwendet, um eine aussagekräftige Priorität für die gefilterten Gewichte der trainierten Netze zu erhalten.
Wir haben Deep Learning Techniken auf die Segmentierung hyperspektraler Bilder und die iterative Auswahl von Merkmalen angewandt.
Schlägt ein gieriges Verfahren zur Auswahl einer Teilmenge hochkorrelierter Spektralmerkmale bei einer Klassifizierungsaufgabe vor.
Die Arbeit untersucht die Verwendung neuronaler Netze für die Klassifizierung und Segmentierung der hyperspektralen Bildgebung (HSI) von Zellen.
Klassifizierung von Zellen und Implementierung von Zellsegmentierung auf der Grundlage von Deep-Learning-Techniken mit Reduzierung der Eingangsmerkmale.
Wir haben einen neuen Datensatz für die Dateninterpretation über Parzellen erstellt und schlagen auch eine Basislinie für dieselben vor.
Die Autoren schlagen eine Pipeline zur Lösung des DIP-Problems vor, die das Lernen aus Datensätzen mit Tripletts der Form {Plot, Frage, Antwort} beinhaltet.
Schlägt einen Algorithmus vor, der die in wissenschaftlichen Diagrammen dargestellten Daten interpretieren kann.
Verbesserung prädiktiver rekurrenter neuronaler Netze durch orthogonale Zufallsmerkmale.
Schlägt vor, die Leistung von rekurrenten neuronalen Netzen mit Vorhersagecharakter durch Berücksichtigung orthogonaler Zufallsmerkmale zu verbessern.
Die Arbeit befasst sich mit dem Problem der Ausbildung prädiktiver rekurrenter neuronaler Netze und liefert zwei Beiträge.
Wir schlagen Fidelity-weighted Learning vor, einen halb-überwachten Lehrer-Schüler-Ansatz für das Training neuronaler Netze mit schwach beschrifteten Daten.
Diese Arbeit schlägt einen Ansatz für das Lernen mit schwacher Überwachung vor, indem es einen sauberen und einen gestörten Datensatz verwendet und von einem Lehrer- und einem Schülernetzwerk ausgeht.
In dem Beitrag wird versucht, tiefe neuronale Netzmodelle mit wenigen gelabelten Trainingsbeispielen zu trainieren.
Die Autoren schlagen einen Ansatz für das Training von Deep-Learning-Modellen für Situationen vor, in denen es nicht genügend zuverlässige kommentierte Daten gibt.
Wir haben zwei neue Ansätze vorgeschlagen, die inkrementelle geschnittene inverse Regression und die inkrementelle überlappende geschnittene inverse Regression, um eine überwachte Dimensionsreduktion in einer Online-Lernmethode zu implementieren.
Untersucht das Problem der ausreichenden Dimensionsreduzierung und schlägt einen inkrementellen Algorithmus zur invertierten Regression vor.
In diesem Beitrag wird ein Online-Lernalgorithmus für die überwachte Dimensionsreduzierung vorgeschlagen, die so genannte inkrementelle geschnittene inverse Regression.
Wir schlagen ein Lernmodell vor, das es DNN ermöglicht, mit nur 2 Bit/Gewicht zu lernen, was besonders nützlich für das Lernen auf dem Gerät ist.
Schlägt eine Methode zur schrittweisen Diskretisierung eines NN vor, um Speicher und Leistung zu verbessern.
Das Lernen von Transportoperatoren auf Mannigfaltigkeiten bildet eine wertvolle Darstellung für Aufgaben wie Transferlernen.
Verwendet ein Wörterbuch-Lernsystem, um vielfältige Transportoperatoren auf erweiterten USPS-Ziffern zu lernen.
Die Arbeit berücksichtigt den Rahmen des vielfältigen Transportoperator-Lernens von Culpepper und Olshausen (2009), und interpretiert es als Erhalt einer MAP-Schätzung unter einem probabilistischen generativen Modell.
Wir stellen ein neuronales Variationsmodell zum Erlernen sprachgeleiteter kompositorischer visueller Konzepte vor.
Schlägt eine neuartige neuronale Netzarchitektur vor, die Objektkonzepte durch die Kombination von Beta-VAE und SCAN erlernt.
In diesem Beitrag wird ein VAE-basiertes Modell für die Übersetzung zwischen Bildern und Text vorgestellt, dessen latente Repräsentation sich gut für die Anwendung symbolischer Operationen eignet, was ihnen eine aussagekräftigere Sprache für die Auswahl von Bildern aus Texten verleiht. 
Diese Arbeit schlägt ein neues Modell namens SCAN (Symbol-Konzept-Assoziationsnetzwerk) für hierarchisches Konzeptlernen vor und ermöglicht die Verallgemeinerung auf neue Konzepte, die mit Hilfe logischer Operatoren aus bestehenden Konzepten zusammengesetzt werden.
Latent Topic Conversational Model, eine Mischung aus seq2seq und neuronalem Themenmodell, um vielfältigere und interessantere Antworten zu generieren.
In diesem Beitrag wird eine Kombination aus Themenmodell und seq2seq-Konversationsmodell vorgeschlagen.
Schlägt ein Gesprächsmodell mit thematischen Informationen durch die Kombination seq2seq Modell mit neuronalen Thema Modelle und zeigt das vorgeschlagene Modell übertrifft einige der Grundlinien seq2seq Modelle und andere latente Variablen seq2seq Modell Varianten.
Der Beitrag befasst sich mit dem Problem der anhaltenden Aktualität in Gesprächsmodellen und schlägt ein Modell vor, das eine Kombination aus einem neuronalen Themenmodell und einem seq2seq-basierten Dialogsystem ist. 
Das Problem der Graphenanalyse wird in ein Problem der Punktwolkenanalyse umgewandelt. 
Schlägt ein tiefes GNN-Netzwerk für Graphenklassifizierungsprobleme vor, das eine adaptive Graphenpooling-Schicht verwendet.
Die Autoren schlagen eine Methode zum Lernen von Darstellungen für Graphen vor.
Wir schlagen vor, adversarische Beispiele auf der Grundlage generativer adversarischer Netze in einer Semi-Whitebox- und Blackbox-Umgebung zu generieren.
Beschreibt AdvGAN, ein Conditional GAN plus Adversarial Loss, und evaluiert AdvGAN in Semi-White-Box- und Black-Box-Settings und berichtet über den aktuellen Stand der Technik.
In dieser Arbeit wird eine Methode zur Erzeugung von Gegenbeispielen vorgeschlagen, die Klassifizierungssysteme täuschen und die MadryLab mnist-Herausforderung gewinnen.
In diesem Beitrag wird gezeigt, wie tiefe Autoencoder durchgängig trainiert werden können, um SoA-Ergebnisse auf einem zeitlich geteilten Netflix-Datensatz zu erzielen.
In diesem Beitrag wird ein Deep Autoencoder Modell für die Vorhersage von Bewertungen vorgestellt, das andere State-of-the-Art-Ansätze auf dem Netflix Preisdatensatz übertrifft. 
Schlägt vor, eine tiefe AE für die Vorhersage von Bewertungen in Empfehlungssystemen zu verwenden.
Die Autoren stellen ein Modell für genauere Netflix-Empfehlungen vor, das zeigt, dass ein Deep Autoencoder komplexere RNN-basierte Modelle mit zeitlichen Informationen übertreffen kann.
Wir stellen den Universal Transformer vor, ein selbstaufmerksames, parallel-in-time rekurrentes Sequenzmodell, das Transformers und LSTMs bei einer Vielzahl von Sequenz-zu-Sequenz-Aufgaben, einschließlich maschineller Übersetzung, übertrifft.
Schlägt ein neues Modell UT vor, das auf dem Transformer-Modell basiert, mit zusätzlicher Rekursion und dynamischem Stoppen der Rekursion.
Diese Arbeit erweitert Transformer durch die rekursive Anwendung eines Mult-Head Self-Attention Blocks, anstatt mehrere Blöcke im Standard Transformer zu stapeln.
In dem Beitrag wird ein interpretierbarer Rahmen für kontinuierliches Lernen entwickelt, in dem Erklärungen zu den abgeschlossenen Aufgaben verwendet werden, um die Aufmerksamkeit des Lernenden bei zukünftigen Aufgaben zu erhöhen, und in dem auch eine Erklärungsmetrik vorgeschlagen wird. 
Die Autoren schlagen ein Framework für kontinuierliches Lernen vor, das auf Erklärungen für durchgeführte Klassifizierungen von zuvor gelernten Aufgaben basiert.
In diesem Beitrag wird eine Erweiterung des Rahmens für kontinuierliches Lernen vorgeschlagen, die auf der Grundlage der bestehenden Methode des variablen kontinuierlichen Lernens und der Beweiskraft der Daten beruht.
Trainingspipeline mit gemischter Genauigkeit unter Verwendung von 16-Bit Ganzzahlen auf Allzweck Hardware; SOTA-Genauigkeit für CNNs der ImageNet-Klasse; beste gemeldete Genauigkeit für ImageNet-1K Klassifizierungsaufgabe mit Training mit reduzierter Genauigkeit;
In diesem Beitrag wird gezeigt, dass eine sorgfältige Implementierung der dynamischen Festkommaberechnung mit gemischter Genauigkeit den neuesten Stand der Technik unter Verwendung eines Deep Learning Modells mit reduzierter Genauigkeit und einer 16-Bit Ganzzahldarstellung erreichen kann.
Schlägt ein "dynamisches Festkomma" Schema vor, das den Exponententeil für einen Tensor teilt, und entwickelt Verfahren, um NN-Berechnungen mit diesem Format durchzuführen, und demonstriert dies für Training mit begrenzter Genauigkeit.
Ein buchstabenbasiertes akustisches ConvNet-Modell führt zu einer einfachen und konkurrenzfähigen Spracherkennungspipeline.
Diese Arbeit wendet Gated Convolutional Neural Networks auf die Spracherkennung an, wobei das Trainingskriterium ASG verwendet wird.
Ein neuartiger GAN-Rahmen, der transformationsinvariante Merkmale nutzt, um umfangreiche Repräsentationen und starke Generatoren zu erlernen.
Schlägt ein modifiziertes GAN-Ziel vor, das aus einem klassischen GAN-Term und einem invarianten Codierungsterm besteht.
In diesem Beitrag wird das IVE-GAN vorgestellt, ein Modell, das einen Encoder in das Generative Adversarial Network Framework einführt.
Wir schlagen eine Methode zum Erlernen latenter Abhängigkeitsstrukturen in variablen Autoencodern vor.
Verwendet eine Matrix binärer Zufallsvariablen zur Erfassung von Abhängigkeiten zwischen latenten Variablen in einem hierarchischen tiefen generativen Modell.
In dieser Arbeit wird ein VAE-Ansatz vorgestellt, bei dem während des Trainings eine Abhängigkeitsstruktur auf der latenten Variable gelernt wird.
Die Autoren schlagen vor, den latenten Raum einer VAE um eine autoregressive Struktur zu erweitern, um die Aussagekraft sowohl des Inferenznetzwerks als auch des latenten Priors zu verbessern.
Wir stellen eine skaleninvariante neuronale Netzwerkarchitektur für die Erkennung von Veränderungspunkten in multivariaten Zeitreihen vor.
Die Arbeit nutzt das Konzept der Wavelet-Transformation innerhalb einer tiefen Architektur, um die Erkennung von Änderungspunkten zu lösen.
In diesem Beitrag wird ein pyramidenbasiertes neuronales Netz vorgeschlagen und auf 1D-Signale angewendet, deren zugrunde liegende Prozesse auf verschiedenen Zeitskalen ablaufen, wobei die Aufgabe darin besteht, Veränderungen zu erkennen.
RL findet bessere Heuristiken für automatische Schlussfolgerungsalgorithmen.
Ziel ist es, eine Heuristik für einen Backtracking-Suchalgorithmus unter Verwendung von Reinforcement Learning zu erlernen, und vorschlagen eines Modells, das grafische neuronale Netze verwendet, um die Einbettung von Wörtern und Klauseln zu erzeugen und sie zur Vorhersage der Qualität jedes Worts zu verwenden, um die Wahrscheinlichkeit jeder Aktion zu bestimmen.
Die Arbeit schlägt einen Ansatz zum automatischen Lernen von Variablenauswahlheuristiken für QBF unter Verwendung von Deep Learning vor.
Beurteilen Sie, ob Ihr GAN tatsächlich etwas anderes tut, als sich die Trainingsdaten zu merken oder nicht.
Ziel ist es, ein Qualitätsmaß/einen Test für GANs bereitzustellen, und es wird vorgeschlagen, die aktuelle Annäherung an eine von einem GAN gelernte Verteilung zu bewerten, indem der Wasserstein Abstand zwischen zwei Verteilungen, die aus einer Summe von Diracs bestehen, als Basisleistung verwendet wird. 
In diesem Beitrag wird ein Verfahren zur Bewertung der Leistung von GANs vorgeschlagen, bei dem der Beobachtungsschlüssel erneut berücksichtigt wird. Das Verfahren wird verwendet, um die aktuellen GANs zu testen und zu verbessern.
Wir verwenden Suchtechniken, um neue Aktivierungsfunktionen zu entdecken, und unsere beste entdeckte Aktivierungsfunktion, f(x) = x * sigmoid(beta * x), übertrifft ReLU bei einer Reihe von anspruchsvollen Aufgaben wie ImageNet.
Schlägt einen auf Reinforcement Learning basierenden Ansatz zum Auffinden von Nichtlinearität vor, indem er Kombinationen aus einer Reihe von unären und binären Operatoren durchsucht.
In dieser Arbeit wird das Reinforcement Learning genutzt, um die Kombination einer Reihe von unären und binären Funktionen zu suchen, die zu einer neuen Aktivierungsfunktion führen.
Der Autor verwendet Reinforcement Learning, um neue potenzielle Aktivierungsfunktionen aus einer Vielzahl von möglichen Kandidaten zu finden. 
Ein Bottom-up-Algorithmus, der CNNs, die mit einem Merkmal pro Schicht beginnen, zu Architekturen mit ausreichender Darstellungskapazität ausbaut.
Es wird vorgeschlagen, die Tiefe der Merkmalszuordnungen eines voll Convolutional Neural Networks dynamisch anzupassen, ein Maß für die Selbstähnlichkeit zu formulieren und die Leistung zu steigern.
Einführung einer einfachen korrelationsbasierten Metrik zur Messung der effektiven Nutzung von Filtern in neuronalen Netzen als Indikator für die effektive Kapazität.
Ziel ist es, das Problem der Suche nach Deep Learning Architekturen durch inkrementelles Hinzufügen und Entfernen von Kanälen in den Zwischenschichten des Netzwerks zu lösen.
Wir trainieren ein Feedforward Netzwerk ohne Backpropagation, indem wir ein energiebasiertes Modell verwenden, um lokale Ziele zu liefern.
Diese Arbeit zielt darauf ab, die iterative Inferenzprozedur in energiebasierten Modellen, die mit Equilibrium Propagation (EP) trainiert werden, zu beschleunigen, indem vorgeschlagen wird, ein Feedforward Netzwerk zu trainieren, um einen Fixpunkt des "equilibrating network" vorherzusagen. 
Training eines separaten Netzes zur Initialisierung von rekurrenten Netzen, die mit Gleichgewichtsvermehrung trainiert wurden.
Lernen von Darstellungen für Bilder, die ein einzelnes Attribut herausrechnen.
Diese Arbeit baut auf bedingten VAE GANs auf, um die Manipulation von Attributen während des Syntheseprozesses zu ermöglichen.
Diese Arbeit schlägt ein generatives Modell vor, um die Repräsentation zu erlernen, die die Identität eines Objekts von einem Attribut trennen kann, und erweitert den Autoencoder adversarial durch Hinzufügen eines Hilfsnetzwerks.
Wir stellen ein Modell für eine konsistente 3D-Rekonstruktion und eine sprunghafte Videovorhersage vor, d.h. die Erzeugung von Bildern in mehreren Zeitschritten in der Zukunft, ohne Zwischenbilder zu erzeugen.
In diesem Beitrag wird eine allgemeine Methode zur Modellierung indizierter Daten vorgeschlagen, bei der die Indexinformationen zusammen mit der Beobachtung in einem neuronalen Netz kodiert werden und dann die Beobachtungsbedingung anhand des Zielindexes dekodiert wird.
Es wird vorgeschlagen, eine VAE zu verwenden, die das Eingangsvideo auf eine permutationsinvariante Weise kodiert, um zukünftige Bilder eines Videos vorherzusagen.
Analyse des beliebten Adam-Optimierers.
In der Arbeit wird versucht, Adam auf der Grundlage der Varianzanpassung mit Schwung zu verbessern, indem zwei Algorithmen vorgeschlagen werden.
Dieser Beitrag analysiert die Skaleninvarianz und die besondere Form der in Adam verwendeten Lernrate und argumentiert, dass Adams Update eine Kombination aus einem Sign-Update und einer varianzbasierten Lernrate ist.
Die Arbeit teilt den ADAM-Algorithmus in zwei Komponenten: Stochastische Richtung im Zeichen der Steigung und adaptive schrittweise mit relativer Varianz, und zwei Algorithmen werden vorgeschlagen, um jede von ihnen zu testen.
Wir schlagen ein neues Framework vor, um die Dropout-Raten für das tiefe neuronale Netz auf der Grundlage einer Rademacher-Komplexitätsgrenze adaptiv anzupassen.
Die Autoren verbinden Dropout-Parameter mit einer Begrenzung der Rademacher-Komplexität des Netzes.
Bezieht die Komplexität der Lernfähigkeit von Netzen auf die Abbruchraten bei der Backpropagation.
Es werden optimierte Gated Deep Learning Architekturen für die Sensorfusion vorgeschlagen.
Die Autoren verbessern mehrere Einschränkungen der grundlegenden negierten Architektur, indem sie eine grobkörnigere Gated-Fusion Architektur und eine zweistufige Gated-Fusion Architektur vorschlagen.
Er schlägt zwei Gated Deep Learning Architekturen für die Sensorfusion vor und zeigt durch die gruppierten Merkmale eine verbesserte Leistung, insbesondere bei zufälligen Sensorstörungen und Ausfällen.
Batch-Normalisierung verursacht explodierende Gradienten in einfachen Feedforward Netzen.
Entwickelt eine Mean Field Theorie für Batch Normalisierung (BN) in vollständig verbundenen Netzwerken mit zufällig initialisierten Gewichten.
Bietet eine dynamische Perspektive auf tiefe neuronale Netze unter Verwendung der Entwicklung der Kovarianzmatrix zusammen mit den Schichten.
Wir trainieren ein Graphennetz zur Vorhersage boolescher Erfüllbarkeit und zeigen, dass es lernt, nach Lösungen zu suchen, und dass die gefundenen Lösungen aus seinen Aktivierungen entschlüsselt werden können.
Die Arbeit beschreibt eine allgemeine neuronale Netzarchitektur zur Vorhersage der Erfüllbarkeit.
In diesem Beitrag wird die NeuroSAT-Architektur vorgestellt, die ein tiefes neuronales Netz zur Vorhersage der Erfüllbarkeit von CNF-Instanzen verwendet.
Ein neuronales Sequenzmodell, das lernt, Prognosen auf einem gerichteten Graphen zu erstellen.
Die Arbeit schlägt die Diffusion Convolutional Recurrent Neural Network Architektur für das räumlich-zeitliche Verkehrsprognose Problem vor.
Es wird vorgeschlagen, ein Verkehrsprognosemodell unter Verwendung eines Diffusionsprozesses für Convolutional Recurrent Neural Networks zu erstellen, um die saptio-temporale Autokorrelation zu berücksichtigen.
Wir trainieren neuronale Netze so, dass sie bei verrauschten Eingaben unsicher sind, um übermäßige Vorhersagen außerhalb der Trainingsverteilung zu vermeiden.
Es wird ein Ansatz zur Ermittlung von Unsicherheitsschätzungen für Vorhersagen in neuronalen Netzen vorgestellt, der eine gute Leistung bei der Quantifizierung der Vorhersageunsicherheit an Punkten außerhalb der Trainingsverteilung aufweist.
Die Arbeit befasst sich mit dem Problem der Unsicherheitsabschätzung von neuronalen Netzen und schlägt vor, einen Bayes'schen Ansatz mit einem kontrastiven Prior zu verwenden.
Diese Studie hebt einen wesentlichen Unterschied zwischen dem menschlichen Sehen und CNNs hervor: Während die Objekterkennung beim Menschen auf der Analyse der Form beruht, haben CNNs keine solche Formvorliebe.
Versucht anhand einer Reihe von gut konzipierten Experimenten nachzuweisen, dass CNNs, die für die Bildklassifizierung trainiert wurden, keine Formverzerrungen wie das menschliche Sehen kodieren.
In diesem Beitrag wird die Tatsache hervorgehoben, dass CNNs nicht unbedingt lernen, Objekte anhand ihrer Form zu erkennen, und dass sie bei auf Rauschen basierenden Merkmalen überreagieren.
Wir beschreiben ein neuartiges generatives Modell mit mehreren Ansichten, das mehrere Ansichten desselben Objekts oder mehrere Objekte in derselben Ansicht generieren kann, ohne dass eine Kennzeichnung der Ansichten erforderlich ist.
In diesem Beitrag wird eine GAN basierte Methode zur Bilderzeugung vorgestellt, die versucht, latente Variablen, die den Bildinhalt beschreiben, von denen zu trennen, die die Eigenschaften der Ansicht beschreiben.
In diesem Beitrag wird eine GAN Architektur vorgeschlagen, die darauf abzielt, die zugrunde liegende Verteilung einer bestimmten Klasse in "Inhalt" und "Ansicht" zu zerlegen.
Es wird ein neues generatives Modell auf der Grundlage des Generative Adversarial Network (GAN) vorgeschlagen, das den Inhalt und die Ansicht von Objekten ohne Überwachung der Ansicht entkoppelt und GMV zu einem bedingten generativen Modell erweitert, das ein Eingabebild nimmt und verschiedene Ansichten des Objekts im Eingabebild erzeugt. 
Es wird ein verlustsensitiver Gewichtsquantisierungsalgorithmus vorgeschlagen, der seine Auswirkungen auf den Verlust direkt berücksichtigt.
Schlägt eine Methode zur Komprimierung von Netzen durch Gewichtsternarisierung vor. 
Die Arbeit schlägt eine neue Methode vor, DNNs mit quantisierten Gewichten zu trainieren, indem die Quantisierung als Einschränkung in einen proximalen Quasi-Newton-Algorithmus aufgenommen wird, der gleichzeitig eine Skalierung für die quantisierten Werte lernt.
Die Arbeit erweitert das verlustbewusste Gewichts-Binarisierungsschema auf Terarisierung und beliebige m-Bit Quantisierung und demonstriert eine vielversprechende Leistung.
Wir entwickeln eine neuartige Gradientenmethode für das automatische Lernen von Regeln mit Optionen unter Verwendung eines differenzierbaren Inferenzschritts.
In der Arbeit wird eine neue Gradiententechnik für das Lernen von Optionen vorgestellt, bei der eine einzige Stichprobe zur Aktualisierung aller Optionen verwendet werden kann.
Schlägt eine Methode zum Lernen von Optionen bei komplexen kontinuierlichen Problemen vor.
Unüberwachte Merkmalsauswahl durch Erfassung der lokalen linearen Struktur von Daten.
Schlägt eine lokal lineare unüberwachte Merkmalsauswahl vor.
In dem Papier wird die LLUFS-Methode für die Merkmalsauswahl vorgeschlagen.
Anhand einer einfachen sprachgesteuerten Navigationsaufgabe untersuchen wir die kompositorischen Fähigkeiten moderner rekurrenter seq2seq-Netzwerke.
Dieser Beitrag konzentriert sich auf die kompositorischen Fähigkeiten moderner Sequenz-zu-Sequenz RNNs und zeigt die Schwächen der aktuellen seq2seq RNN-Architekturen auf.
Die Arbeit analysiert die Komposition Fähigkeiten von RNNs, insbesondere die Verallgemeinerung Fähigkeit der RNNs auf zufällige Teilmenge von SCAN-Befehle, auf längere SCAN-Befehle, und der Zusammensetzung über primitive Befehle. 
Die Autoren stellen einen neuen Datensatz vor, der die Analyse eines Seq2Seq Lernfalls erleichtert.
Wir befassen uns mit dem Problem des Ähnlichkeitslernens für strukturierte Objekte mit Anwendungen insbesondere im Bereich der Computersicherheit und schlagen ein neues Modell für Graph-Matching Netzwerke vor, das sich bei dieser Aufgabe auszeichnet.
Die Autoren stellen ein Graph-Matching Netzwerk für die Wiederauffindung und das Matching von graphisch strukturierten Objekten vor.
Die Autoren gehen das Problem des Graphenabgleichs an, indem sie eine Erweiterung der Grapheneinbettungsnetze vorschlagen.
Die Autoren stellen zwei Methoden zum Erlernen einer Ähnlichkeitsbewertung zwischen Graphenpaaren vor und zeigen die Vorteile der Einführung von Ideen aus dem Graphenabgleich in neuronale Netze.
Wir haben ein RNN-CNN Encoder-Decoder Modell für schnelles unbeaufsichtigtes Lernen von Satzrepräsentationen vorgeschlagen.
Modifikationen des Skip-thought Frameworks für das Lernen von Satzeinbettungen.
In diesem Beitrag wird ein neues hybrides Design für RNN-Encoder und CNN-Decoder vorgestellt, das beim Pretraining von Encodern keinen autoregressiven Decoder benötigt.
Die Autoren erweitern Skip-thought, indem sie nur einen Zielsatz mit einem CNN-Decoder dekodieren.
Ein statistischer Ansatz zur Berechnung von Beispielwahrscheinlichkeiten in generativen adversarial Netzen.
Zeigen Sie, dass WGAN mit entropischer Regularisierung eine untere Schranke für die Wahrscheinlichkeit der beobachteten Datenverteilung maximiert.
Die Autoren behaupten, dass es möglich ist, die obere Schranke eines entropie-regulierten optimalen Transports zu nutzen, um ein Maß für die "Stichprobenwahrscheinlichkeit" zu finden.
Wir stellen geomstats vor, ein effizientes Python-Paket für Riemannsche Modellierung und Optimierung über Mannigfaltigkeiten, das sowohl mit numpy als auch mit tensorflow kompatibel ist.
Die Arbeit stellt das Softwarepaket geomstats vor, das die einfache Nutzung von Riemannschen Mannigfaltigkeiten und Metriken in maschinellen Lernmodellen ermöglicht.
Schlägt ein Python-Paket für Optimierung und Anwendungen auf Riemannschen Mannigfaltigkeiten vor und hebt die Unterschiede zwischen dem Geomstats-Paket und anderen Paketen hervor.
Stellt eine geometrische Toolbox, Geomstats, für maschinelles Lernen auf Riemannschen Mannigfaltigkeiten vor.
Wir konstruieren einen dynamischen spärlichen Graphen mittels Dimensionsreduktionssuche, um die Rechen- und Speicherkosten sowohl beim DNN-Training als auch bei der Inferenz zu reduzieren.
Die Autoren schlagen vor, einen dynamischen, spärlichen Berechnungsgraphen zu verwenden, um die Speicher- und Zeitkosten in tiefen neuronalen Netzen (DNN) zu reduzieren.
In diesem Beitrag wird eine Methode zur Beschleunigung des Trainings und der Inferenz von tiefen neuronalen Netzen durch dynamisches Pruning des Berechnungsgraphen vorgeschlagen.
Wir entwickeln eine praktische Erweiterung des Information-Directed Sampling für Reinforcement Learning, die parametrische Unsicherheit und Heteroskedastizität in der Renditeverteilung für Exploration berücksichtigt.
Die Autoren schlagen einen Weg vor, Information-Directed Sampling auf Reinforcement Learning zu erweitern, indem sie zwei Arten von Unsicherheiten kombinieren, um eine einfache, auf IDS basierende Explorationsstrategie zu erhalten. 
Diese Arbeit untersucht sophistische Explorationsansätze für Reinforcement Learning, die auf Information Direct Sampling und auf Distributional Reinforcement Learning aufbauen.
Verwendung eines neuronalen Graphen Netzes zur Modellierung struktureller Informationen der Agenten, um die Politik und die Übertragbarkeit zu verbessern.
Eine Methode zur Darstellung und zum Erlernen strukturierter Strategien für kontinuierliche Steuerungsaufgaben unter Verwendung neuronaler Graphennetze.
In der Vorlage wird vorgeschlagen, zusätzliche Strukturen in Probleme des Reinforcement Learnings einzubeziehen, insbesondere die Struktur der Morphologie des Agenten.
Vorschlag für eine Anwendung graph neuronaler Netze zum Erlernen von Strategien zur Steuerung von "Tausendfüßler"-Robotern unterschiedlicher Länge.
In diesem Beitrag wird ein hierarchischer Rahmen für das Reinforcement Learning vorgestellt, der auf deterministischen Optionsstrategien und der Maximierung der gegenseitigen Information basiert. 
Schlägt einen HRL-Algorithmus vor, der versucht, Optionen zu lernen, die ihre gegenseitige Information mit der Zustands-Aktions Dichte unter optimalen Regeln maximieren.
In diesem Beitrag wird ein HRL-System vorgeschlagen, bei dem die wechselseitige Information der latenten Variablen und der Zustands-Aktions-Paare annähernd maximiert wird.
Schlägt ein Kriterium vor, das darauf abzielt, die gegenseitige Information zwischen Optionen und Zustands-Aktions Paaren zu maximieren, und zeigt empirisch, dass die gelernten Optionen den Zustands-Aktions Raum zerlegen, nicht aber den Zustandsraum. 
Wir führen hierarchische lokale Interpretationen ein und validieren sie. Dies ist die erste Technik, die automatisch nach wichtigen Interaktionen für individuelle Vorhersagen von LSTMs und CNNs sucht und diese anzeigt.
Ein neuartiger Ansatz zur Erklärung der Vorhersagen neuronaler Netze durch das Erlernen hierarchischer Darstellungen von Gruppen von Eingangsmerkmalen und deren Beitrag zur endgültigen Vorhersage.
Erweitert eine bestehende Merkmalsinterpretationsmethode für LSTMs auf allgemeinere DNNs und führt ein hierarchisches Clustering der Eingangsmerkmale und die Beiträge jedes Clusters zur endgültigen Vorhersage ein.
In diesem Beitrag wird eine hierarchische Erweiterung der kontextuellen Dekomposition vorgeschlagen.
Wir schlagen eine einfach zu implementierende, aber effektive Methode zur Komprimierung neuronaler Netze vor. PFA nutzt die intrinsische Korrelation zwischen den Filterantworten innerhalb der Netzwerkschichten, um einen kleineren Netzwerkfußabdruck zu empfehlen.
Schlägt vor, Convolutional Networks durch Analyse der beobachteten Korrelation zwischen den Filtern einer Schicht zu Prunen, ausgedrückt durch das Eigenwertspektrum ihrer Kovarianzmatrix.
In diesem Beitrag wird ein Ansatz zur Komprimierung neuronaler Netze vorgestellt, bei dem die Korrelation der Filterantworten in jeder Schicht durch zwei Strategien berücksichtigt wird.
In diesem Papier wird eine auf der Spektralanalyse basierende Komprimierungsmethode vorgeschlagen.
Mehrere verschiedene Agenten für die Umformulierung von Suchanfragen, die mit Reinforcement Learning trainiert wurden, um Suchmaschinen zu verbessern.
Parellelisierung der Ensemble-Methode beim Reinforcement Learning für die Umformulierung von Anfragen, Beschleunigung des Trainings und Verbesserung der Vielfalt der gelernten Formulierungen.
Die Autoren schlagen vor, mehrere verschiedene Agenten zu trainieren, jeder mit einer anderen Teilmenge der Trainingsmenge.
Die Autoren schlagen einen Ensemble-Ansatz für die Reformulierung von Anfragen vor.
Wir stellen eine Methode zur Einbettung von Netzwerken vor, die Vorabinformationen über das Netzwerk berücksichtigt und dadurch eine bessere empirische Leistung erzielt.
Die Arbeit schlägt vor, eine Prioritätsverteilung zu verwenden, um die Netzwerkeinbettung einzuschränken. Für die Formulierung dieser Arbeit wurden sehr eingeschränkte Gaußsche Verteilungen verwendet.
Schlägt vor, unbeaufsichtigte Knoteneinbettungen zu lernen, indem die strukturellen Eigenschaften von Netzwerken berücksichtigt werden.
Wir analysieren die Konvergenz von Algorithmen des Adam-Typs und stellen milde hinreichende Bedingungen zur Verfügung, um ihre Konvergenz zu garantieren. Wir zeigen auch, dass eine Verletzung der Bedingungen dazu führen kann, dass ein Algorithmus divergiert.
Präsentiert eine Konvergenzanalyse im nicht-konvexen Umfeld für eine Familie von Optimierungsalgorithmen.
In diesem Beitrag wird die Konvergenzbedingung von Optimierern des Adam-Typs bei ungebundenen nicht-konvexen Optimierungsproblemen untersucht.
Auto-Encoder mit gebundenen Gewichten und abs-Funktion als Aktivierungsfunktion, lernt die Klassifizierung in Vorwärtsrichtung und die Regression in Rückwärtsrichtung aufgrund einer speziell definierten Kostenfunktion.
In dem Beitrag wird die Verwendung der Absolutwert-Aktivierungsfunktion in einer Autoencoder-Architektur mit einem zusätzlichen überwachten Lernterm in der Zielfunktion vorgeschlagen.
In dieser Arbeit wird ein reversibles Netz mit dem absoluten Wert als Aktivierungsfunktion eingeführt.
Wir schlagen ein auf Transformer basierendes Modell zur Extraktion von Beziehungen vor, das anstelle von expliziten linguistischen Merkmalen vortrainierte Sprachrepräsentationen verwendet.
Es wird ein Transformer basiertes Modell zur Extraktion von Beziehungen vorgestellt, das ein Vortraining auf unbeschriftetem Text mit einem Sprachmodellierungsziel nutzt.
Dieser Artikel beschreibt eine neuartige Anwendung von Transformer-Netzwerken zur Extraktion von Beziehungen.
Die Arbeit stellt eine auf Transformer basierende Architektur zur Ausspannungsextraktion vor, die an zwei Datensätzen evaluiert wurde.
Wir schlagen ein einfaches und effizientes Verfahren zur Architektursuche für Convolutional Neural Networks vor.
Es wird eine Suchmethode für neuronale Architekturen vorgeschlagen, die bei CIFAR10 eine Genauigkeit erreicht, die dem neuesten Stand der Technik entspricht, und die viel weniger Rechenressourcen benötigt.
Es wird eine Methode zur Suche von Architekturen für neuronale Netze zur gleichen Zeit wie das Training vorgestellt, die eine erhebliche Zeitersparnis beim Training und bei der Architektursuche mit sich bringt.
Schlägt eine Variante der neuronalen Architektursuche unter Verwendung von Netzwerkmorphismen vor, um einen Suchraum unter Verwendung von CNN-Architekturen zu definieren, der die CIFAR-Bildklassifizierungsaufgabe erfüllt.
Verwendung von GANs zur Erzeugung von Graphen über Random Walks.
Die Autoren schlagen ein generatives Modell von Random Walks auf Graphen vor, das modellagnostisches Lernen, kontrollierbare Anpassung und die Erzeugung von Ensemble-Graphen ermöglicht.
Schlägt eine WGAN-Formulierung zur Erzeugung von Graphen auf der Grundlage von Random Walks unter Verwendung von Knoteneinbettungen und einer LSTM-Architektur zur Modellierung vor.
Wir schlagen vor, ein Problem der gleichzeitigen Klassifizierung und Neuheitserkennung im GAN Framework zu lösen.
Er schlägt ein GAN vor, um Klassifizierung und Neuheitserkennung zu vereinen.
In diesem Beitrag wird eine Methode zur Neuheitserkennung vorgestellt, die auf einem Mehrklassen GAN basiert, das auf die Ausgabe von Bildern trainiert wird, die aus einer Mischung von nominalen und neuartigen Verteilungen erzeugt werden.
In der Arbeit wird ein GAN für die Erkennung von Neuheiten vorgeschlagen, das einen Mischungsgenerator mit Verlusten bei der Merkmalsanpassung verwendet.
Die Leistung der Sprecherverifikation kann durch die Anpassung des Modells an In-Domain-Daten mit Hilfe generativer adversarialer Netze erheblich verbessert werden. Außerdem kann die Anpassung auf unüberwachte Weise erfolgen.
Vorschlag einer Reihe von GAN-Varianten für die Aufgabe der Sprechererkennung unter der Bedingung, dass die Domäne nicht übereinstimmt.
Minimierung der synergetischen wechselseitigen Information innerhalb der Latenzen und der Daten für die Aufgabe der Entflechtung unter Verwendung des VAE-Rahmens.
Schlägt eine neue Zielfunktion für das Erlernen von verschränkten Darstellungen in einem Variationsrahmen vor, indem die Synergie der bereitgestellten Informationen minimiert wird.
Die Autoren zielen darauf ab, eine VAE zu trainieren, die latente Repräsentationen in einer "synergetischen" maximalen Weise entwirrt. 
In diesem Beitrag wird ein neuer Ansatz zur Durchsetzung der Entflechtung in VAEs vorgeschlagen, der einen Term verwendet, der die synergetische gegenseitige Information zwischen den latenten Variablen bestraft.
Beschleunigung von SGD durch eine andere Anordnung der Beispiele.
Die Arbeit stellt eine Methode zur Verbesserung der Konvergenzrate des stochastischen Gradientenabstiegs für das Lernen von Einbettungen vor, indem ähnliche Trainingsbeispiele gruppiert werden.
Schlägt eine ungleichmäßige Sampling-Strategie zur Konstruktion von Minibatches in SGD für die Aufgabe des Lernens von Einbettungen für Objektassoziationen vor.
Kombinieren von Informationen zwischen vorgefertigten Worteinbettungen und aufgabenspezifischen Wortdarstellungen, um das Problem des fehlenden Vokabulars zu lösen.
Diese Arbeit schlägt einen Ansatz vor, um die Vorhersage der Einbettung außerhalb des Vokabulars für die Aufgabe der Modellierung von Dialoggesprächen mit beträchtlichen Gewinnen gegenüber den Basislinien zu verbessern.
Schlägt vor, externe vortrainierte Worteinbettungen und vortrainierte Worteinbettungen auf Trainingsdaten zu kombinieren, indem sie als zwei Ansichten beibehalten werden.
Schlägt eine Methode zur Erweiterung der Abdeckung von vortrainierten Worteinbettungen vor, um das OOV-Problem zu bewältigen, das bei der Anwendung auf Gesprächsdatensätze auftritt, und wendet neue Varianten von LSTM basierten Modellen auf die Aufgabe der Antwortauswahl bei der Dialogmodellierung an.
Wir analysieren Probleme beim Training von gelernten Optimierern, lösen diese Probleme durch Variationsoptimierung unter Verwendung von zwei komplementären Gradientenschätzern und trainieren Optimierer, die in der Wanduhrzeit 5x schneller sind als Basisoptimierer (z.B. Adam).
In diesem Beitrag wird die ungerollte Optimierung verwendet, um neuronale Netze für die Optimierung zu lernen.
Diese Arbeit befasst sich mit dem Problem des Lernens eines Optimierers, insbesondere konzentrieren sich die Autoren darauf, sauberere Gradienten aus dem aufgerollten Trainingsverfahren zu erhalten.
Stellt eine Methode zum "Lernen eines Optimierers" vor, indem eine Variationsoptimierung für den "äußeren" Optimiererverlust verwendet wird, und schlägt die Idee vor, sowohl den reparametrisierten Gradienten als auch den Score-Funktionsschätzer für das Variationsziel zu kombinieren und sie mit Hilfe einer Produkt-Gauß-Formel für den Mittelwert zu gewichten.
Eine Methode für ein effizientes asynchrones verteiltes Training von Deep-Learning-Modellen zusammen mit theoretischen Regret-Grenzen.
Die Arbeit schlägt einen Algorithmus zur Begrenzung der Staleness in asynchronen SGD und bietet eine theoretische Analyse.
Schlägt einen Hybrid-Algorithmus vor, um die Gradientenverzögerung von asynchronen Methoden zu eliminieren.
Wir haben eine neuartige Quantisierungsmethode entwickelt, um die Effizienz und Robustheit von Deep-Learning-Modellen gemeinsam zu optimieren.
Schlägt ein Regularisierungsschema vor, um quantisierte neuronale Netze vor adversarial Angriffen zu schützen, indem es eine Lipschitz-Konstante zur Filterung des Inpout-Outputs der inneren Schichten verwendet.
Eine Modifikation für bestehende RNN-Architekturen, die es ihnen ermöglicht, Zustandsaktualisierungen zu überspringen und dabei die Leistung der ursprünglichen Architekturen beizubehalten.
Schlägt das Skip-RNN-Modell vor, das es einem rekurrenten Netzwerk ermöglicht, die Aktualisierung seines verborgenen Zustands für einige Eingaben selektiv zu überspringen, was zu einer reduzierten Berechnung zur Testzeit führt.
Schlägt ein neuartiges RNN-Modell vor, bei dem sowohl die Eingabe als auch die Zustandsaktualisierung der rekurrenten Zellen adaptiv für einige Zeitschritte übersprungen wird.
Ein schneller Solver zweiter Ordnung für Deep Learning, der bei ImageNet-Problemen ohne Abstimmung der Hyperparameter funktioniert.
Wahl der Richtung durch Verwendung eines einzelnen Schritts des Gradientenabstiegs "in Richtung Newton-Schritt" von einer ursprünglichen Schätzung aus, und dann Übernahme dieser Richtung anstelle des ursprünglichen Gradienten.
Ein neues approximatives Optimierungsverfahren zweiter Ordnung mit geringem Rechenaufwand, das die Berechnung der Hessian Matrix durch einen einzigen Gradientenschritt und eine Warmstart Strategie ersetzt.
Aufmerksamkeitsbasiertes Modell, das mit REINFORCE trainiert wurde, um Heuristiken mit konkurrenzfähigen Ergebnissen bei TSP und anderen Routing-Problemen zu lernen.
Präsentiert einen aufmerksamkeitsbasierten Ansatz zum Erlernen einer Strategie zur Lösung von TSP und anderen kombinatorischen Optimierungsproblemen vom Typ Routing.
In diesem Beitrag wird versucht, Heuristiken für die Lösung kombinatorischer Optimierungsprobleme zu erlernen.
Ein Algorithmus zur Optimierung von Regularisierungs-Hyperparametern während des Trainings.
Die Arbeit schlägt einen Weg vor, um y bei jeder Aktualisierung von Lambda neu zu initialisieren und ein Clipping-Verfahren von y, um die Stabilität des dynamischen Systems zu erhalten.
Schlägt einen Algorithmus zur Hyperparameter-Optimierung vor, der als Erweiterung von Franceschi 2017 angesehen werden kann, bei dem einige Schätzungen warm neu gestartet werden, um die Stabilität der Methode zu erhöhen.
Schlägt eine Erweiterung einer bestehenden Methode zur Optimierung von Regularisierungshyperparametern vor.
Zeigen Sie, dass LSTMs genauso gut oder besser sind als die jüngsten Innovationen für LM und dass die Modellbewertung oft unzuverlässig ist.
Diese Arbeit beschreibt eine umfassende Validierung von LSTM basierten Wort- und Zeichensprachmodellen, die zu einem bedeutenden Ergebnis in der Sprachmodellierung und einem Meilenstein im Deep Learning führt.
Wir zeigen, wie die Verwendung von Skip Verbindungen Sprachverbesserungsmodelle interpretierbarer machen kann, da sie ähnliche Mechanismen verwenden, die in der DSP Literatur erforscht worden sind.
Die Autoren schlagen vor, Residual-, Highway- und Maskierungsblöcke in eine vollständig gefaltete Pipeline einzubauen, um zu verstehen, wie die iterative Inferenz des Outputs und der Maskierung in einer Sprachverbesserungsaufgabe durchgeführt wird
Die Autoren interpretieren Autobahn-, Rest- und Verdeckungsverbindungen. 
Die Autoren erzeugen ihre eigene verrauschte Sprache, indem sie künstlich Rauschen aus einem gut etablierten Rauschdatensatz zu einem weniger bekannten Datensatz mit sauberer Sprache hinzufügen.
Eine Methode zur Eliminierung der Gradientenvarianz und zur automatischen Abstimmung von Prioritäten für ein effektives Training von Bayes'schen Neuronalen Netzen.
Schlägt einen neuen Ansatz vor, um deterministische Variationsinferenz für Feed-Forward BNN mit spezifischen nichtlinearen Aktivierungsfunktionen durch Annäherung der schichtweisen Momente durchzuführen.
Die Arbeit betrachtet einen rein deterministischen Ansatz zum Erlernen von Variationsapproximationen für Bayes'sche neuronale Netze.
Ein formaler Methodenansatz für die Zusammensetzung von Fähigkeiten in Reinforcement Learning Aufgaben.
Die Arbeit kombiniert RL und Constraints, die durch logische Formeln ausgedrückt werden, indem es eine Automatisierung aus scTLTL-Formeln einrichtet.
Schlägt eine Methode vor, die hilft, Richtlinien aus gelernten Teilaufgaben zum Thema Kombination von RL-Aufgaben mit linearen zeitlogischen Formeln zu konstruieren.
Ableitung einer allgemeinen Formulierung einer multimodalen VAE aus der gemeinsamen marginalen Log-Likelihood.
Vorschlag einer multimodalen VAE mit einer aus der Kettenregel abgeleiteten Variationsschranke.
In diesem Beitrag wird ein Ziel, M^2VAE, für multimodale VAEs vorgeschlagen, das eine aussagekräftigere latente Raumrepräsentation erlernen soll.
Wir bauen auf der sequenziellen Monte Carlo Methode mit automatischer Kodierung auf, gewinnen neue theoretische Erkenntnisse und entwickeln ein verbessertes Trainingsverfahren auf der Grundlage dieser Erkenntnisse.
Die Arbeit schlägt eine Version des IWAE-Trainings vor, die SMC anstelle des klassischen Wichtigkeits-Samplings verwendet.
In dieser Arbeit wird die automatische Kodierung sequentieller Monte Carlo Verfahren (SMC) vorgeschlagen, die den VAE Rahmen um ein neues Monte Carlo Ziel auf der Grundlage von SMC erweitert. 
Wir schlagen eine Architektur für das Lernen von Wertfunktionen vor, die den Einsatz beliebiger linearer Algorithmen zur Bewertung von Richtlinien in Verbindung mit nichtlinearem Merkmalslernen ermöglicht.
Die Arbeit schlägt einen Rahmen mit zwei Zeitskalen für das Lernen der Wertfunktion und einer Zustandsdarstellung mit nichtlinearen Approximatoren vor.
In diesem Beitrag werden Two-Timescale Networks (TTNs) vorgeschlagen und die Konvergenz dieser Methode mit Methoden der stochastischen Approximation auf zwei Zeitskalen nachgewiesen. 
In diesem Beitrag wird ein Two-Timescale Network (TTN) vorgestellt, mit dem lineare Methoden zum Lernen von Werten verwendet werden können. 
Wir schlagen einfache, aber effektive Algorithmen zur Matrixfaktorisierung (MF) mit niedrigem Rang vor, um die Laufzeit zu beschleunigen, Speicher zu sparen und die Leistung von LSTMs zu verbessern.
Er schlägt vor, LSTM durch die Verwendung von MF als Nachbearbeitungs-Kompressionsstrategie zu beschleunigen und führt umfangreiche Experimente durch, um die Leistung zu zeigen.
Eine forensische Metrik zur Bestimmung, ob ein bestimmtes Bild eine Kopie (mit möglicher Manipulation) eines anderen Bildes aus einem bestimmten Datensatz ist.
Einführung des siamesischen Netzwerks zur Identifizierung von doppelten und kopierten/veränderten Bildern, das zur Verbesserung der Überwachung der veröffentlichten und begutachteten Literatur eingesetzt werden kann.
Die Arbeit präsentiert eine Anwendung von tiefen Convolutional Networks für die Aufgabe der Erkennung von Bildduplikaten.
Diese Arbeit befasst sich mit dem Problem des Auffindens von doppelten/fast doppelten Bildern aus biomedizinischen Veröffentlichungen und schlägt ein Standard-CNN und Verlustfunktionen vor und wendet sie auf diesen Bereich an.
Stabiles GAN-Training in hohen Dimensionen durch Verwendung eines Arrays von Diskriminatoren, jeder mit einer niedrigdimensionalen Ansicht der erzeugten Beispiele.
In dem Beitrag wird vorgeschlagen, das GAN-Training zu stabilisieren, indem ein Ensemble von Diskriminatoren verwendet wird, von denen jeder auf einer zufälligen Projektion der Eingabedaten arbeitet, um das Trainingssignal für das Generatormodell zu liefern.
In der Arbeit wird eine GAN-Trainingsmethode zur Verbesserung der Trainingsstabilität vorgeschlagen. 
In dem Beitrag wird ein neuer Ansatz für das GAN-Training vorgeschlagen, der stabile Gradienten für das Training des Generators liefert.
Zeigen wir eine geometrische Methode zur perfekten Kodierung von Kategoriebaum-Informationen in vortrainierte Worteinbettungen.
Die Arbeit schlägt eine N-Ball-Einbettung für taxonomische Daten vor, wobei ein N-Ball ein Paar aus einem Schwerpunktvektor und dem Radius vom Zentrum ist.
In diesem Beitrag wird eine Methode vorgestellt, mit der bestehende Vektoreinbettungen von kategorialen Objekten (wie z. B. Wörtern) so verändert werden können, dass sie in Balleinbettungen umgewandelt werden, die Hierarchien folgen.
Konzentriert sich auf die Anpassung der vortrainierten Worteinbettungen, so dass sie die Hypernymie/Hyponymie-Beziehung durch geeignete n-ball Kapselung respektieren.
Wir schlagen Convolutional CRFs als schnelle, leistungsstarke und trainierbare Alternative zu Fully Connected CRFs vor.
Die Autoren ersetzen den großen Filterschritt im permutoförmigen Gitter durch einen räumlich variierenden Convolutional Kernel und zeigen, dass die Inferenz effizienter und das Training einfacher ist. 
Schlägt vor, die Nachrichtenübermittlung auf einer CRF mit abgeschnittenem Gauß-Kernel unter Verwendung eines definierten Kernels und parallelisierter Nachrichtenübermittlung auf einer GPU durchzuführen.
Wir schlagen einen modellunabhängigen Ansatz zur Validierung der Robustheit von Q&A-Systemen vor und demonstrieren die Ergebnisse anhand modernster Q&A-Modelle.
Befasst sich mit dem Problem der Robustheit gegenüber gegnerischen Informationen bei der Beantwortung von Fragen.
Verbesserung der Robustheit des maschinellen Verstehens/Fragenbeantwortens.
Multi-Generator zur Erfassung von Pdata, zur Lösung des Wettbewerbs und des One-beat-all-Problems.
Schlägt parallele GANs vor, um durch eine Kombination mehrerer schwacher Generatoren einen Moduskollaps in GANs zu vermeiden. 
Schwach überwachte Bildsegmentierung unter Verwendung der kompositorischen Struktur von Bildern und generativen Modellen.
In diesem Beitrag wird eine mehrschichtige Darstellung erstellt, um die Segmentierung von unbeschrifteten Bildern besser zu lernen.
In dieser Arbeit wird ein generatives Modell auf GAN-Basis vorgeschlagen, das Bilder in mehrere Schichten zerlegt, wobei das Ziel des GAN darin besteht, echte Bilder von Bildern zu unterscheiden, die durch die Kombination der Schichten entstehen.
In diesem Beitrag wird eine neuronale Netzarchitektur vorgeschlagen, die auf der Idee einer mehrschichtigen Szenenkomposition basiert.
Wir stellen einen geometrischen Rahmen für den Nachweis von Robustheitsgarantien vor und heben die Bedeutung der Kodimension in gegnerischen Beispielen hervor. 
In dieser Arbeit wird eine theoretische Analyse von adversarial Beispielen durchgeführt, die zeigt, dass es einen Kompromiss zwischen der Robustheit in verschiedenen Normen gibt, dass adversarial Training ineffizient ist und dass der Nearest Neighbor Classifier unter bestimmten Bedingungen robust sein kann.
CharNMT ist spröde.
In dieser Arbeit werden die Auswirkungen von Rauschen auf Zeichenebene auf 4 verschiedene neuronale maschinelle Übersetzungssysteme untersucht.
In dieser Arbeit wird die Leistung von NMT-Systemen auf Zeichenebene angesichts von synthetischen und natürlichen Geräuschen auf Zeichenebene empirisch untersucht.
Diese Arbeit untersucht die Auswirkungen von verrauschten Eingaben auf die maschinelle Übersetzung und testet Möglichkeiten, NMT-Modelle robuster zu machen.
Wir lernen tiefe Netzwerke von Einheiten mit harten Schwellenwerten, indem wir Ziele für versteckte Einheiten durch kombinatorische Optimierung und Gewichte durch konvexe Optimierung festlegen, was zu einer verbesserten Leistung bei ImageNet führt.
Die Arbeit erklärt und verallgemeinert Ansätze zum Lernen neuronaler Netze mit harter Aktivierung.
In dieser Arbeit wird das Problem der Optimierung von tiefen Netzen mit hartschwelligen Einheiten untersucht.
Die Arbeit erörtert das Problem der Optimierung neuronaler Netze mit harten Schwellenwerten und schlägt eine neuartige Lösung mit einer Sammlung von Heuristiken/Annäherungen dafür vor.
Anhand einer neuartigen, kontrollierten visuellen Beziehungsaufgabe zeigen wir, dass gleich-unterschiedliche Aufgaben die Kapazität von CNNs kritisch belasten; wir argumentieren, dass visuelle Beziehungen mit Hilfe von Aufmerksamkeits-Gedächtnisstrategien besser gelöst werden können.
Zeigt, dass Convolutional und relationale neuronale Netze visuelle Beziehungsprobleme nicht lösen können, indem die Netze mit künstlich erzeugten visuellen Beziehungsdaten trainiert werden. 
In diesem Beitrag wird untersucht, wie aktuelle CNNs und Relationale Netzwerke visuelle Beziehungen in Bildern nicht erkennen können.
Wir schlagen AD-VAT vor, bei dem der Tracker und das Zielobjekt, die als zwei lernfähige Agenten betrachtet werden, Gegner sind und sich während des Trainings gegenseitig verbessern können.
Diese Arbeit zielt darauf ab, das visuelle aktive Verfolgungsproblem mit einem Trainingsmechanismus anzugehen, bei dem der Tracker und das Ziel als gegenseitige Gegner dienen.
In diesem Beitrag wird eine einfache Multi-Agenten-Deep-RL-Aufgabe vorgestellt, bei der ein beweglicher Tracker versucht, einem beweglichen Ziel zu folgen.
Er schlägt eine neuartige Belohnungsfunktion vor - eine "partielle Nullsumme", die den Wettbewerb zwischen Tracker und Ziel nur dann fördert, wenn sie sich nahe beieinander befinden, und bestraft, wenn sie zu weit entfernt sind.
Wir präsentierten eine Methode zum gemeinsamen Erlernen einer hierarchischen Worteinbettung (Hierarchical Word Embedding, HWE) unter Verwendung eines Korpus und einer Taxonomie zur Identifizierung der hypernymischen Beziehungen zwischen Wörtern.
In diesem Beitrag wird eine Methode zum gemeinsamen Lernen von Worteinbettungen unter Verwendung von Koinzidenzstatistiken und unter Einbeziehung von hierarchischen Informationen aus semantischen Netzwerken vorgestellt.
In dieser Arbeit wird eine gemeinsame Lernmethode für Hypernyme aus Rohtext und überwachten Taxonomiedaten vorgeschlagen. 
In diesem Beitrag wird vorgeschlagen, dem GloVE Ziel ein Maß für die "distributionelle Einschlussdifferenz" hinzuzufügen, um Hypernym-Relationen darzustellen.
Integration von Selbstorganisation und überwachtem Lernen in einem hierarchischen neuronalen Netz.
Der Beitrag diskutiert das Lernen in einem neuronalen Netz mit drei Schichten, wobei die mittlere Schicht topographisch organisiert ist, und untersucht das Zusammenspiel von unbeaufsichtigtem und hierarchisch überwachtem Lernen im biologischen Kontext.
Eine überwachte Variante der selbstorganisierenden Karte (SOM) von Kohonen, bei der jedoch die lineare Ausgabeschicht mit quadratischem Fehler durch eine Softmax-Schicht mit Cross-Entropy ersetzt wird.
Schlägt ein Modell mit versteckten Neuronen mit selbstorganisierender Aktivierungsfunktion vor, deren Ausgänge einen Klassifikator mit Softmax-Ausgangsfunktion speisen. 
Präzisionsautobahn; ein verallgemeinertes Konzept des hochpräzisen Informationsflusses für Sub 4-Bit Quantisierung .
Untersucht das Problem der Quantisierung neuronaler Netze durch den Einsatz eines Präzisions-Highways von Ende-zu-Ende, um den akkumulierten Quantisierungsfehler zu reduzieren und eine extrem niedrige Präzision in tiefen neuronalen Netzen zu ermöglichen. 
Diese Arbeit untersucht Methoden zur Verbesserung der Leistung quantisierter neuronaler Netze.
In diesem Beitrag wird vorgeschlagen, einen hohen Aktivierungs-/Gradientenfluss in zwei Arten von Netzwerkstrukturen, ResNet und LSTM, beizubehalten.
Ein Algorithmus zum effizienten Training neuronaler Netze auf zeitlich redundanten Daten.
Die Arbeit beschreibt ein neuronales Kodierungsschema für spike-basiertes Lernen in tiefen neuronalen Netzen.
In diesem Beitrag wird eine Methode für spike-basiertes Lernen vorgestellt, die darauf abzielt, den Rechenaufwand beim Lernen und Testen zu reduzieren, wenn zeitlich redundante Daten klassifiziert werden.
In dieser Arbeit wird eine prädiktive Kodierungsversion des Sigma-Delta-Kodierungsschemas angewandt, um die Rechenlast eines Deep-Learning-Netzes zu verringern, wobei die drei Komponenten in einer bisher unbekannten Weise kombiniert werden.
Informationsengpässe verhalten sich auf überraschende Weise, wenn der Output eine deterministische Funktion des Inputs ist.
Argumentiert, dass die meisten realen Klassifizierungsprobleme eine solche deterministische Beziehung zwischen den Klassenbezeichnungen und den Eingaben X aufweisen, und untersucht mehrere Probleme, die sich aus solchen Pathologien ergeben.
Untersuchung von Problemen, die bei der Anwendung von Konzepten des Informationsengpasses auf deterministische überwachte Lernmodelle auftreten.
Die Autoren erläutern mehrere kontraintuitive Verhaltensweisen der Informationsengpass-Methode für das überwachte Lernen einer deterministischen Regel.
Wir beweisen, dass idealisierte Bayes'sche neuronale Netze keine gegnerischen Beispiele haben können, und geben empirische Beweise mit realen BNNs.
Die Arbeit untersucht die Robustheit von Bayes'schen Klassifizierern gegenüber widrigen Umständen und nennt zwei Bedingungen, die nachweislich ausreichen, damit "idealisierte Modelle" auf "idealisierten Datensätzen" keine adversarial Beispiele haben.
Die Arbeit stellt eine Klasse von diskriminativen Bayes'schen Klassifikatoren vor, die keine gegnerischen Beispiele haben.
Wir stellen die erste Instanz von adversarial Angriffen vor, die das Zielmodell so umprogrammieren, dass es eine vom Angreifer gewählte Aufgabe ausführt - ohne dass der Angreifer die gewünschte Ausgabe für jede Testzeiteingabe angeben oder berechnen muss.
Die Autoren stellen ein neuartiges adversarial Angriffsschema vor, bei dem ein neuronales Netz umfunktioniert wird, um eine andere Aufgabe zu erfüllen als die, für die es ursprünglich trainiert wurde.
In diesem Beitrag wird die "adversarial Umprogrammierung" von gut trainierten und festen neuronalen Netzen vorgeschlagen und gezeigt, dass die adversarial Umprogrammierung bei untrainierten Netzen weniger effektiv ist.
Die Arbeit erweitert die Idee der "adversarial Angriffe" beim überwachten Lernen von NNs auf eine vollständige Umwidmung der Lösung eines trainierten Netzes.
Wir entwickeln ein neues Verfahren, um die Generalisierungslücke in tiefen Netzen mit hoher Genauigkeit vorherzusagen.
Die Autoren schlagen vor, eine geometrische Marge und eine schichtweise Margenverteilung zur Vorhersage der Generalisierungslücke zu verwenden.
Empirisch zeigt sich ein interessanter Zusammenhang zwischen den vorgeschlagenen Margin-Statistiken und der Generalisierungslücke, der genutzt werden kann, um einige präskriptive Erkenntnisse zum Verständnis der Generalisierung in tiefen neuronalen Netzen zu liefern. 
Wir schlagen einen Algorithmus zur beweisbaren Wiederherstellung von Parametern (Convolutional und Ausgangsgewichte) eines Convolutional Networks mit überlappenden Patches vor.
In dieser Arbeit wird das theoretische Lernen von einschichtigen Convolutional Neural Networks untersucht. Das Ergebnis ist ein Lernalgorithmus und nachweisbare Garantien, die diesen Algorithmus verwenden.
In diesem Beitrag wird ein neuer Algorithmus für das Lernen eines zweischichtigen neuronalen Netzes vorgestellt, der einen einzigen Convolutional Filter und einen Gewichtsvektor für verschiedene Orte umfasst.
Ein neuer Regularisierungsbegriff kann das Training von Wasserstein GANs verbessern.
Die Arbeit schlägt ein Regularisierungsschema für Wasserstein GAN vor, das auf einer Lockerung der Beschränkungen der Lipschitz-Konstante von 1 basiert.
Der Artikel befasst sich mit der Regularisierung/Bestrafung bei der Anpassung von GANs, wenn diese auf einer L_1 Wasserstein-Metrik basieren.
Wir schlagen die Wasserstein-Proximal-Methode für das Training von GANs vor. 
Schlägt ein neues GAN-Verfahren vor, das die in der vorangegangenen Iteration erzeugten Punkte berücksichtigt und den Generator aktualisiert, der l-mal auszuführen ist.
Betrachtet das natürliche Gradientenlernen beim GAN-Lernen, wobei die durch den Wasserstein-2-Abstand induzierte Riemannsche Struktur verwendet wird.
Die Arbeit beabsichtigt, den durch die Wasserstein-2-Distanz induzierten natürlichen Gradienten zu nutzen, um den Generator im GAN zu trainieren, und die Autoren schlagen den Wasserstein-Proximaloperator als Regularisierung vor.
In diesem Beitrag wird eine auf Eliminierung basierende heuristische Funktion für die sequentielle Entscheidungsfindung vorgestellt, die sich zur Steuerung von AND/OR-Suchalgorithmen zur Lösung von Einflussdiagrammen eignet.
Verallgemeinert die Minibuckets-Inferenzheuristik auf Einflussdiagramme.
Annäherung von Mittelwert und Varianz des NN-Outputs bei verrauschtem Input / Dropout / unsicheren Parametern. Analytische Approximationen für Argmax-, Softmax- und Max-Schichten.
Die Autoren konzentrieren sich auf das Problem der Unsicherheitsfortpflanzung in DNN.
In diesem Beitrag wird die Feed-Forward Ausbreitung von Mittelwert und Varianz in Neuronen neu betrachtet, indem das Problem der Ausbreitung von Unsicherheit durch Max-Pooling Schichten und Softmax behandelt wird.
Ein Diskriminator, der sich nicht so leicht durch adversarial Beispiele täuschen lässt, macht das GAN-Training robuster und führt zu einem glatteren Ziel.
Dieser Beitrag schlägt einen neuen Weg vor, um den Trainingsprozess von GAN zu stabilisieren, indem der Diskriminator so reguliert wird, dass er gegenüber adversarial Beispielen robust ist.
Die Arbeit schlägt eine systematische Methode für das Training von GANs mit Robustheits-Regularisierungs Terms vor, die ein reibungsloseres Training von GANs ermöglicht. 
Es wird die Idee vorgestellt, dass das GAN Ziel durch die Robustheit eines Diskriminators gegenüber negativen Störungen geglättet werden kann, was zu besseren Ergebnissen sowohl visuell als auch in Bezug auf die FID führt.
Wir modellieren die Aktivierungsfunktion jedes Neurons als Gaußschen Prozess und lernen sie zusammen mit dem Gewicht mit Variational Inference.
Es wird vorgeschlagen, die funktionale Form jeder Aktivierungsfunktion im neuronalen Netz mit Gaußschen Prozessprioritäten zu versehen, um die Form der Aktivierungsfunktionen zu lernen.
Wir bieten eine theoretische Studie über die Eigenschaften von tiefen zirkulant-diagonalen ReLU-Netzen und zeigen, dass sie universelle Approximatoren mit begrenzter Breite sind.
In dem Beitrag wird vorgeschlagen, zirkulierende und diagonale Matrizen zu verwenden, um die Berechnung zu beschleunigen und den Speicherbedarf in neuronalen Netzen zu verringern.
Diese Arbeit beweist, dass diagonal-zirkulierende ReLU-Netze mit begrenzter Breite (DC-ReLU) universelle Approximatoren sind.
StarHopper ist ein neuartiges Touchscreen-Interface für die effiziente und flexible objektzentrierte Kameradrohnen-Navigation.
Die Autoren skizzieren die von ihnen entwickelte neue Drohnensteuerungsschnittstelle StarHopper, die automatisierte und manuelle Steuerung in einer neuen hybriden Navigationsschnittstelle kombiniert und sich durch den Einsatz einer zusätzlichen Überkopfkamera von der Annahme befreit, dass sich das Zielobjekt bereits im Blickfeld der Drohne befindet.
In diesem Beitrag wird StarHopper vorgestellt, ein System zur halbautomatischen Drohnennavigation im Rahmen von Ferninspektionen.
Stellt StarHopper vor, eine Anwendung, die Computer-Vision-Techniken mit Touch-Eingabe nutzt, um die Drohnensteuerung mit einem objektzentrierten Ansatz zu unterstützen.
Ein Selbstbeobachtungsnetzwerk für RNN/CNN-freie Sequenzkodierung mit geringem Speicherverbrauch, hochgradig parallelisierbarer Berechnung und modernster Leistung bei verschiedenen NLP-Aufgaben.
Es wird vorgeschlagen, die Selbstaufmerksamkeit auf zwei Ebenen anzuwenden, um den Speicherbedarf in aufmerksamkeitsbasierten Modellen mit vernachlässigbaren Auswirkungen auf die Geschwindigkeit zu begrenzen.
In diesem Beitrag wird ein bidirektionales Block-Selbstaufmerksamkeitsmodell als Allzweck-Encoder für verschiedene Sequenzmodellierungsaufgaben im NLP vorgestellt.
Ein neues hochmodernes Modell für die Beantwortung von Fragen mit mehreren Beweisen unter Verwendung grobkörniger, feinkörniger, hierarchischer Aufmerksamkeit.
Schlägt eine Methode für die Multi-Hop-QS vor, die auf zwei getrennten Modulen (grobkörnige und feinkörnige Module) basiert.
Dieses Arbeit schlägt eine interessante Grobkorn-Feinkorn Co-Attention Netzwerk Architektur zur Beantwortung von Fragen mit mehreren Evidenzen vor.
Konzentriert sich auf Multi-Choice QA und schlägt ein Framework für die Grob- bis Feinbewertung vor.
Wir schlagen eine neue Methode für den unausgewogenen optimalen Transport unter Verwendung generativer adversarialer Netzwerke vor.
Die Autoren betrachten das unausgewogene optimale Transportproblem zwischen zwei Maßnahmen mit unterschiedlicher Gesamtmasse unter Verwendung eines stochastischen Min-Max Algorithmus und lokaler Skalierung.
Die Autoren schlagen einen Ansatz zur Schätzung des unausgewogenen optimalen Transports zwischen Stichprobenmaßen vor, der gut in der Dimension und in der Anzahl der Stichproben skaliert.
Die Arbeit führt eine statische Formulierung für unausgewogenen optimalen Transport durch gleichzeitiges Lernen einer Transportzoprdnung T und eines Skalierungsfaktors xi ein.
Wir schlagen eine neue Methode zur Extraktion von Auffälligkeitszuordnungen vor, die zu einer höheren Qualität der Zuordnungen führt.
Schlägt eine klassifikatorunabhängige Methode zur Extraktion von Saliency Zuordnungen vor.
In diesem Beitrag wird ein neuer Saliency Zuordnungen Extraktor vorgestellt, der die Ergebnisse des Standes der Technik zu verbessern scheint.
Die Autoren argumentieren, dass eine extrahierte Saliency Zuordnung, die direkt von einem Modell abhängt, für einen anderen Klassifikator möglicherweise nicht nützlich ist, und schlagen ein Schema zur Annäherung der Lösung vor.
Wir schlagen eine neue Methode vor, um die Menge der Instanzattribute für die Bild-zu-Bild Übersetzung einzubeziehen.
Diese Arbeit schlägt eine Methode - InstaGAN - vor, die auf CycleGAN aufbaut, indem sie Instanzinformationen in Form von Segmentierungsmasken pro Instanz berücksichtigt, mit Ergebnissen, die mit CycleGAN und anderen Baselines vergleichbar sind.
 Schlägt vor, instanzspezifische Segmentierungsmasken für das Problem der ungepaarten Bild-zu-Bild Übersetzung hinzuzufügen.
Die Parameter-Funktionszuordnung von tiefen Netzwerken ist stark verzerrt; dies kann erklären, warum sie verallgemeinern. Wir verwenden PAC-Bayes und Gauß-Prozesse, um nicht-variable Grenzen zu erhalten.
Die Arbeit untersucht die Generalisierungsfähigkeiten von tiefen neuronalen Netzen mit Hilfe der PAC-Bayesianischen Lerntheorie und empirisch gestützten Intuitionen.
In diesem Beitrag wird eine Erklärung für das Generalisierungsverhalten von großen, überparametrisierten neuronalen Netzen vorgeschlagen, indem behauptet wird, dass die Parameter-Funktionszuordnung in neuronalen Netzen auf "einfache" Funktionen ausgerichtet ist und das Generalisierungsverhalten gut ist, wenn das Zielkonzept ebenfalls "einfach" ist.
Wir stellen Evolutionary EM als einen neuartigen Algorithmus für das unbeaufsichtigte Training generativer Modelle mit binären latenten Variablen vor, der eine enge Verbindung zwischen variationalem EM und evolutionärer Optimierung herstellt.
Der Beitrag stellt eine Kombination aus evolutionärer Berechnung und Variations-EM für Modelle mit binären latenten Variablen vor, die durch eine partikelbasierte Approximation dargestellt werden.
In diesem Beitrag wird der Versuch unternommen, Trainingsalgorithmen mit Erwartungsmaximierung und evolutionäre Algorithmen eng zu integrieren.
Wir schlagen ein effizientes rekurrentes Netzwerkmodell für Forward Prediction bei zeitlich variierenden Verteilungen vor.
In diesem Beitrag wird eine Methode zur Erstellung neuronaler Netze vorgeschlagen, die historische Verteilungen auf Verteilungen abbildet, und die Methode wird auf verschiedene Aufgaben der Verteilungsvorhersage angewendet.
Schlägt ein Reccurent Distribution Regression Network vor, das eine rekurrente Architektur auf einem früheren Distribution Regression Network Modell verwendet.
Diese Arbeit befasst sich mit der Regression über Wahrscheinlichkeitsverteilungen durch die Untersuchung zeitlich variierender Verteilungen in einem rekurrenten neuronalen Netz.
Ein neuartiger Ansatz zur Verarbeitung graphenstrukturierter Daten durch neuronale Netze, der die Aufmerksamkeit auf die Nachbarschaft eines Knotens lenkt. Erzielt Spitzenergebnisse bei transduktiven Zitationsnetzwerk Aufgaben und einer induktiven Protein-Protein Interaktionsaufgabe.
In diesem Beitrag wird eine neue Methode zur Klassifizierung von Knoten in einem Graphen vorgeschlagen, die in halbüberwachten Szenarien und auf einem völlig neuen Graphen eingesetzt werden kann. 
Die Arbeit stellt eine neuronale Netzarchitektur vor, die mit graphisch strukturierten Daten arbeitet, die Graph Attention Networks.
Bietet eine faire und nahezu umfassende Diskussion über den Stand der Technik beim Lernen von Vektordarstellungen für die Knoten eines Graphen.
Ein neuartiger, auf Reinforcement Learning basierender Ansatz zur Komprimierung tiefer neuronaler Netze mit Wissensdestillation.
In diesem Beitrag wird vorgeschlagen, anstelle von vordefinierten Heuristiken Reinforcement Learning einzusetzen, um die Struktur des komprimierten Modells im Prozess der Wissensdestillation zu bestimmen.
Stellt eine prinzipielle Methode der Netz-zu-Netz Komprimierung vor, die Policy-Gradienten zur Optimierung von zwei Strategien verwendet, die ein starkes Lehrermodell in ein starkes, aber kleineres Schülermodell komprimieren.
Wir beweisen, dass der Modus-Kollaps in bedingten GANs größtenteils auf ein Missverhältnis zwischen Rekonstruktionsverlust und GAN-Verlust zurückzuführen ist, und stellen eine Reihe neuartiger Verlustfunktionen als Alternativen zum Rekonstruktionsverlust vor.
Die Arbeit schlägt eine Modifikation des traditionellen bedingten GAN-Ziels vor, um eine vielfältige, multimodale Erzeugung von Bildern zu fördern. 
In diesem Beitrag wird eine Alternative zu L1/L2-Fehlern vorgeschlagen, die beim Training von bedingten GANs als Ergänzung zu den Verlusten der Gegner verwendet werden.
Wir verwenden kausale Schlussfolgerungen, um die Architektur von generativen Modellen zu charakterisieren.
In diesem Beitrag wird die Beschaffenheit von Convolutional Filtern im Kodierer und Dekodierer einer VAE sowie in einem Generator und einem Diskriminator eines GAN untersucht.
Diese Arbeit nutzt das Kausalitätsprinzip, um zu quantifizieren, wie sich die Gewichte aufeinanderfolgender Schichten aneinander anpassen.
Ein Ansatz zum Erlernen eines gemeinsamen Einbettungsraums zwischen visuell unterschiedlichen Spielen.
Ein neuer Ansatz zum Erlernen der zugrundeliegenden Struktur visuell unterschiedlicher Spiele, der Convolutional Layers zur Verarbeitung von Eingabebildern, asynchrone Advantage Actor Critic für tiefes Reinforcement Learning und einen gegnerischen Ansatz kombiniert, um die Einbettungsrepräsentation unabhängig von der visuellen Repräsentation von Spielen zu machen.
Es wird eine Methode zum Erlernen einer Strategie für visuell unterschiedliche Spiele durch die Anpassung von Deep Reinforcement Learning vorgestellt.
In diesem Beitrag wird eine Agentenarchitektur diskutiert, die eine gemeinsame Darstellung verwendet, um mehrere Aufgaben mit unterschiedlichen visuellen Statistiken auf Sprite-Ebene zu trainieren.
Wir untersuchen die Zustandsgleichung eines rekurrenten neuronalen Netzes. Wir zeigen, dass SGD die unbekannte Dynamik aus wenigen Input/Output-Beobachtungen unter geeigneten Annahmen effizient erlernen kann.
Die Arbeit untersucht zeitdiskrete dynamische Systeme mit einer nichtlinearen Zustandsgleichung und beweist, dass die Ausführung von SGD auf einer Trajektorie fester Länge logarithmische Konvergenz ergibt.
Diese Arbeit befasst sich mit dem Problem des Lernens eines nichtlinearen dynamischen Systems, bei dem die Ausgabe gleich dem Zustand ist. 
In diesem Beitrag wird die Fähigkeit von SGD untersucht, die Dynamik eines linearen Systems und die nichtlineare Aktivierung zu erlernen.
Eine neue Methode zur Wissensdestillation für das Transferlernen.
In der Arbeit wird eine Methode zur Wissensdestillation vorgestellt, die das vorgeschlagene Konzept der Neuronenverteiler nutzt. 
Schlägt eine Methode zur Wissensdestillation vor, bei der die neuronalen Verteiler als übertragenes Wissen betrachtet werden.
Wir stellen eine einfache und allgemeine Methode vor, um ein einzelnes neuronales Netz zu trainieren, das mit verschiedenen Breiten (Anzahl der Kanäle in einer Schicht) ausgeführt werden kann, was sofortige und adaptive Kompromisse zwischen Genauigkeit und Effizienz zur Laufzeit ermöglicht.
In dem Beitrag wird vorgeschlagen, verschiedene Größenmodelle in einem gemeinsamen Netz zu kombinieren, was die Erkennungsleistung erheblich verbessert.
In dieser Arbeit wird ein einziges ausführbares Netz mit unterschiedlichen Breiten trainiert.
Ähnlichkeitsnetz zum Erlernen einer nicht-metrischen visuellen Ähnlichkeitsschätzung zwischen einem Bildpaar.
Die Autoren schlagen ein lernendes Ähnlichkeitsmaß für visuelle Ähnlichkeit vor und erzielen damit eine Verbesserung in sehr bekannten Datensätzen von Oxford und Paris für die Bildwiedererkennung.
In dem Beitrag wird argumentiert, dass es besser ist, nicht-metrische Entfernungen anstelle von metrischen Entfernungen zu verwenden.
Ein neues zyklisches kontradiktorisches Lernen, ergänzt durch ein Modell für Hilfsaufgaben, das die Leistung der Bereichsanpassung in überwachten und unbeaufsichtigten Situationen mit geringen Ressourcen verbessert.
Es wird eine Erweiterung der zykluskonsistenten adversen Anpassungsmethoden vorgeschlagen, um die Domänenanpassung zu bewältigen, wenn nur begrenzte überwachte Zieldaten verfügbar sind.
In diesem Beitrag wird ein Ansatz zur Domänenanpassung vorgestellt, der auf der Idee des zyklischen GAN basiert, und es werden zwei verschiedene Algorithmen vorgeschlagen.
Wir entwickeln eine Methode zum Erlernen von strukturellen Signaturen in Netzwerken, die auf der Diffusion von Spektralgraphen-Wavelets basiert.
Verwendung spektraler Graph-Wavelet Diffusionsmuster der lokalen Nachbarschaft eines Knotens zur Einbettung des Knotens in einen niedrigdimensionalen Raum.
In der Arbeit wird eine Methode zum Vergleich von Knoten in einem Graphen auf der Grundlage der Wavelet-Analyse des Graphen-Laplacian abgeleitet. 
Ein Mixed-Reality Fahrsimulator mit Stereokameras und Passthrough-VR wurde in einer Nutzerstudie mit 24 Teilnehmern evaluiert.
Er schlägt ein kompliziertes System zur Fahrsimulation vor.
In diesem Beitrag wird ein Mixed-Reality Fahrsimulator vorgestellt, der das Gefühl der Anwesenheit verstärkt.
Er schlägt einen Mixed-Reality Fahrsimulator vor, der die Erzeugung von Verkehr einbezieht und eine verbesserte "Präsenz" durch ein MR-System verspricht.
Quadraturregeln für die Kernel-Approximation.
In dem Beitrag wird vorgeschlagen, die Kernel-Approximation von Zufallsmerkmalen durch die Verwendung von Quadraturregeln wie stochastischen sphärisch-radialen Regeln zu verbessern.
Die Autoren schlagen eine neue Version des Random-Feature-Map Ansatzes zur näherungsweisen Lösung großer Kernel-Probleme vor.
In diesem Beitrag wird gezeigt, dass die Techniken von Genz & Monahan (1998) verwendet werden können, um einen geringen Fehler bei der Kernel Approximation im Rahmen eines zufälligen Fourier Merkmals zu erreichen, eine neue Methode zur Anwendung von Quadraturregeln zur Verbesserung der Kernel Approximation.
Wie kann man neuronale Sprecher / Hörer entwickeln, die anhand von Referenzsprache feinkörnige Merkmale von 3D-Objekten lernen?
Die Autoren stellen eine Studie über das Lernen von 3D Objekten vor, in der sie einen Datensatz von referenziellen Ausdrücken sammeln und verschiedene Modelle trainieren, indem sie mit einer Reihe von architektonischen Entscheidungen experimentieren.
Wir stellen einen Rahmen für das Erlernen von objektzentrierten Repräsentationen vor, die sich für die Planung von Aufgaben eignen, die ein Verständnis der Physik erfordern.
Die Arbeit stellt eine Plattform für die Vorhersage von Bildern von Objekten vor, die unter der Wirkung von Gravitationskräften miteinander interagieren.
Die Arbeit stellt eine Methode vor, die lernt, "Blocktürme" aus einem gegebenen Bild zu reproduzieren.
Schlägt eine Methode vor, mit der man lernt, über die physische Interaktion verschiedener Objekte nachzudenken, ohne dass die Eigenschaften der Objekte überwacht werden.
Wir liefern effizient überprüfbare notwendige und hinreichende Bedingungen für globale Optimalität in tiefen linearen neuronalen Netzen, mit einigen ersten Erweiterungen auf nichtlineare Bedingungen.
Der Artikel enthält Bedingungen für die globale Optimalität der Verlustfunktion von tiefen linearen neuronalen Netzen.
Die Arbeit enthält theoretische Ergebnisse über die Existenz lokaler Minima in der Zielfunktion von tiefen neuronalen Netzen.
Untersuchung einiger theoretischer Eigenschaften von tiefen linearen Netzen.
Verwendung eines rekurrenten Auto-Encoder Modells zur Extraktion mehrdimensionaler Zeitreihenmerkmale
Dieser Text beschreibt eine Anwendung des rekurrenten Autoencoders zur Analyse von mehrdimensionalen Zeitreihen.
Die Arbeit beschreibt ein Sequenz zu Sequenz Auto-Encoder Modell, das verwendet wird, um zu lernen, Darstellungen von Sequenzen, die zeigen, dass für ihre Anwendung, eine bessere Leistung erzielt wird, wenn das Netzwerk nur trainiert wird, um eine Teilmenge der Daten Messungen zu rekonstruieren. 
Schlägt eine Strategie vor, die sich am rekurrenten Autoencoder Modell orientiert, so dass ein Clustering mehrdimensionaler Zeitreihendaten auf der Grundlage von Kontextvektoren durchgeführt werden kann.
Wir stellen ein Graph-zu-Graph Encoder-Decoder Framework für das Lernen verschiedener Graphübersetzungen vor.
Schlägt ein Graph-zu-Graph Übersetzungsmodell für die Moleküloptimierung vor, das von der Analyse übereinstimmender Molekülpaare inspiriert ist.
Erweiterung von JT-VAE auf das Szenario der Übersetzung von Graphen in Graphen durch Hinzufügen der latenten Variable zur Erfassung der Multimodalität und einer adversen Regularisierung im latenten Raum.
Er schlägt ein recht komplexes System vor, das viele verschiedene Entscheidungen und Komponenten umfasst, um ausgehend von einem gegebenen Korpus chemische Zusammensetzungen mit verbesserten Eigenschaften zu erhalten.
Wir lernen einen schnellen neuronalen Löser für PDEs, der Konvergenzgarantien hat.
Entwickelt eine Methode zur Beschleunigung der Finite-Differenzen Methode bei der Lösung von PDEs und schlägt einen überarbeiteten Rahmen für die Festpunktiteration nach der Diskretisierung vor.
Die Autoren schlagen eine lineare Methode zur Beschleunigung von PDE-Lösern vor.
Wir führen funktionale Variationsinferenz auf stochastischen Prozessen durch, die durch Bayes'sche neuronale Netze definiert sind.
Anpassung von variationalen Bayesian Neural Network Approximationen in funktionaler Form und unter Berücksichtigung der Anpassung an einen stochastischen Prozess Prior implizit über Stichproben.
Stellt ein neuartiges ELBO Ziel für das Training von BNNs vor, das es ermöglicht, aussagekräftigere Priors im Modell zu kodieren als die weniger informativen Gewichts Priors, die in der Literatur beschrieben werden.
Stellt einen neuen Variationsinferenzalgorithmus für Bayes'sche neuronale Netzmodelle vor, bei dem der Prior funktional und nicht über einen Prior über Gewichte spezifiziert wird. 
Wir betten Wörter in den hyperbolischen Raum ein und stellen die Verbindung zu den Gaußschen Worteinbettungen her.
In diesem Beitrag wird die Glove Worteinbettung an einen hyperbolischen Raum angepasst, der durch das Poincare Halbebenenmodell gegeben ist.
In diesem Beitrag wird ein Ansatz zur Implementierung eines GLOVE basierten hyperbolischen Worteinbettungsmodells vorgeschlagen, das mit Hilfe der Riemannschen Optimierungsmethoden optimiert wird.
Gedächtnisnetze lernen kein Multi-Hop Denken, es sei denn, wir beaufsichtigen sie.
Die Behauptung, dass Multi-Hop-Denken nicht einfach direkt zu erlernen ist und eine direkte Überwachung erfordert, und dass ein gutes Abschneiden bei WikiHop nicht unbedingt bedeutet, dass das Modell tatsächlich lernt zu hüpfen.
Die Arbeit schlägt vor, das bekannte Problem des Lernens von Gedächtnisnetzwerken zu untersuchen, genauer gesagt, die Schwierigkeit der Überwachung des Aufmerksamkeitslernens mit solchen Modellen.
In diesem Beitrag wird argumentiert, dass das Gedächtnisnetz nicht in der Lage ist, vernünftiges Multi-Hop-Denken zu lernen.
Wir stellen generative Netze vor, die nicht mit einem Diskriminator oder einem Encoder gelernt werden müssen; sie werden durch Invertierung eines speziellen Einbettungsoperators erhalten, der durch eine Wavelet-Streuungstransformation definiert ist.
Stellt Scattering-Transformationen als generative Bildmodelle im Kontext von Generative Adversarial Networks vor und legt dar, warum sie als Gaussianisierungstransformationen mit kontrolliertem Informationsverlust und Invertierbarkeit angesehen werden können. 
Die Arbeit schlägt ein generatives Modell für Bilder vor, das weder einen Diskriminator (wie bei GANs) noch eine erlernte Einbettung benötigt.
Wir schlagen ein neues Modell für neuronales Schnelllesen vor, das die inhärente Interpunktionsstruktur eines Textes nutzt, um effektives Sprung- und Überspringverhalten zu definieren.
Die Arbeit schlägt ein Structural-Jump LSTM Modell zur Beschleunigung des maschinellen Lesens mit zwei Agenten anstelle eines Agenten vor.
Schlägt ein neues Modell für neuronales Schnelllesen vor, bei dem der neue Leser die Möglichkeit hat, ein Wort oder eine Wortfolge zu überspringen.
Der Artikel schlägt eine Schnelllesemethode vor, die Skip- und Jump-Aktionen verwendet, und zeigt, dass die vorgeschlagene Methode genauso genau ist wie LSTM, aber viel weniger Rechenaufwand benötigt.
Wir schlagen die IRN-Architektur vor, um die spärliche und verzögerte Kaufbelohnung für sitzungsbasierte Empfehlungen zu erweitern.
In dem Beitrag wird vorgeschlagen, die Leistung von Empfehlungssystemen durch Reinforcement Learning zu verbessern, indem ein Imaginations Rekonstruktions Netzwerk verwendet wird.
Die Arbeit stellt einen sitzungsbasierten Empfehlungsansatz vor, der sich auf die Käufe der Nutzer statt auf die Klicks konzentriert. 
Theoretische und empirische Erklärung der Verallgemeinerung von stochastischen Deep Learning Algorithmen durch Ensemble-Robustheit.
Diese Arbeit stellt eine Anpassung der algorithmischen Robustheit von Xu & Mannor '12 vor und präsentiert Lerngrenzen und ein Experiment, das die Korrelation zwischen empirischer Ensemble Robustheit und Generalisierungsfehler zeigt. 
Schlägt eine Untersuchung der Generalisierungsfähigkeit von Deep Learning Algorithmen unter Verwendung einer Erweiterung des Stabilitätsbegriffs vor, die als Ensemble Robustheit bezeichnet wird, und gibt Grenzen für den Generalisierungsfehler eines randomisierten Algorithmus in Bezug auf den Stabilitätsparameter an und bietet eine empirische Studie, die versucht, Theorie und Praxis zu verbinden.
Die Arbeit untersuchte die Verallgemeinerungsfähigkeit von Lernalgorithmen unter dem Gesichtspunkt der Robustheit in einem Deep Learning Kontext.
Few-Shot Lernen eines PixelCNN.
In dem Beitrag wird vorgeschlagen, die Dichteschätzung bei geringer Verfügbarkeit von Trainingsdaten mit Hilfe eines Meta-Lernmodells zu verwenden.
Dieser Beitrag befasst sich mit dem Problem der One / Few Shot Dichteschätzung, unter Verwendung von Metalearning-Techniken, die auf One / Few Shot überwachtes Lernen angewendet wurden.
Die Arbeit konzentriert sich auf das Few-Shot Learning mit autoregressiver Dichteschätzung und verbessert PixelCNN mit neuronaler Aufmerksamkeit und Meta Lerntechniken.
Wir zeigen, dass SGD zweischichtige überparametrisierte neuronale Netze mit Leaky ReLU Aktivierungen lernt, die nachweislich auf linear trennbaren Daten generalisieren.
Die Arbeit untersucht überparametrisierte Modelle, die in der Lage sind, gut generalisierende Lösungen zu erlernen, indem sie ein Netz mit einer verborgenen Schicht und einer festen Ausgangsschicht verwenden.
In diesem Beitrag wird gezeigt, dass SGD auf einem überparametrisierten Netzwerk bei linear trennbaren Daten immer noch zu einem Klassifikator führen kann, der nachweislich generalisiert.
Das Training von Agenten mit zielgerichteten Informationsengpässen fördert den Transfer und führt zu einem starken Explorationsbonus.
Schlägt vor, Standard RL Verluste mit der negativen bedingten gegenseitigen Information für die Suche nach Strategien in einer Mehrziel RL Umgebung zu regulieren.
Diese Arbeit schlägt das Konzept des Entscheidungszustandes vor und schlägt eine KL Divergenzregulierung vor, um die Struktur der Aufgaben zu erlernen und diese Informationen zu nutzen, um die Politik zu ermutigen, die Entscheidungszustände zu besuchen.
In dem Beitrag wird eine Methode zur Regularisierung zielbezogener Strategien mit einem Term der gegenseitigen Information vorgeschlagen. 
Wir schlagen eine Optimierungsmethode für den Fall vor, dass nur verzerrte Gradienten verfügbar sind - wir definieren einen neuen Gradientenschätzer für dieses Szenario, leiten die Verzerrung und Varianz dieses Schätzers ab und wenden ihn auf Beispielprobleme an.
Die Autoren schlagen einen Ansatz vor, der die zufällige Suche mit der Surrogate-Gradienteninformation kombiniert, und erörtern den Kompromiss zwischen Varianz und Vorspannung sowie die Optimierung der Hyperparameter.
In dem Beitrag wird eine Methode zur Verbesserung der Zufallssuche vorgeschlagen, bei der ein Unterraum aus den vorherigen k Surrogate Gradienten gebildet wird.
In dieser Arbeit wird versucht, die Entwicklung des OpenAI Typs zu beschleunigen, indem eine nicht isotrophe Verteilung mit einer Kovarianzmatrix der Form I + UU^t und externen Informationen wie einem Surrogategradienten zur Bestimmung von U eingeführt werden.
Ein GAN, der Graph Convolutional Operationen mit dynamisch berechneten Graphen aus verborgenen Merkmalen verwendet.
Die Arbeit schlägt vor, eine Version von GANs speziell für die Erzeugung von Punktwolken mit dem Kernbeitrag der Upsampling-Operation.
In diesem Beitrag werden Graph Convolutional GANs für unregelmäßige 3D-Punktwolken vorgeschlagen, die gleichzeitig die Domäne und die Merkmale lernen.
Wir stellen einen schnellen und einfach zu implementierenden Algorithmus vor, der robust gegenüber Datenrauschen ist.
Die Arbeit zielt darauf ab, potenzielle Beispiele mit Labelrauschen zu entfernen, indem die Beispiele mit großen Verlusten im Trainingsverfahren verworfen werden.
Gradientenbasierte Angriffe auf binarisierte neuronale Netze sind aufgrund der Nichtdifferenzierbarkeit solcher Netze nicht wirksam; unser IPROP-Algorithmus löst dieses Problem durch ganzzahlige Optimierung.
Schlägt einen neuen Algorithmus im Stil der Zielverbreitung vor, um starke adversarial Angriffe auf binarisierte neuronale Netze zu erzeugen.
In diesem Beitrag wird ein neuer Angriffsalgorithmus auf der Grundlage von MILP für binäre neuronale Netze vorgeschlagen.
In dieser Arbeit wird ein Algorithmus zur Suche nach adversarial Angriffen auf binäre neuronale Netze vorgestellt, der iterativ die gewünschten Repräsentationen Schicht für Schicht von der Spitze bis zum Eingang findet und effizienter ist als die vollständige Lösung der gemischt-ganzzahligen linearen Programmierung (MILP).
Die Dekodierung des letzten Tokens im Kontext unter Verwendung der vorhergesagten Verteilung der nächsten Token wirkt als Regularisierer und verbessert die Sprachmodellierung.
Die Autoren führen die Idee der Dekodierung in der Vergangenheit zum Zweck der Regularisierung zur Verbesserung der Komplexität in der Penn Treebank ein.
Vorschlagen eines zusätzlichen Verlustterms, der beim Training eines LSTM LM verwendet werden kann, und zeigt, dass durch Hinzufügen dieses Verlustterms eine SOTA-Perplexität bei einer Reihe von LM-Benchmarks erreicht werden kann.
Schlägt eine neue Regularisierungstechnik vor, die mit geringem Aufwand zu der in AWD-LSTM von Merity et al. (2017) verwendeten hinzugefügt werden kann.
Vorschlag zur Beobachtung impliziter Ordnungen in Datensätzen unter dem Gesichtspunkt eines generativen Modells.
Die Autoren befassen sich mit dem Problem der impliziten Ordnung in einem Datensatz und der Herausforderung, diese wiederherzustellen, und schlagen vor, ein abstandsmetrikfreies Modell zu erlernen, das eine Markov-Kette als generativen Mechanismus der Daten annimmt.
In dem Beitrag wird Generative Markov Networks vorgeschlagen - ein Deep Learning basierter Ansatz zur Modellierung von Sequenzen und zur Entdeckung von Ordnung in Datensätzen.
Schlägt vor, die Ordnung einer ungeordneten Datenstichprobe durch Lernen einer Markov-Kette zu erlernen.
Programmsynthese aus natürlichsprachlicher Beschreibung und Eingabe-/Ausgabebeispielen mittels Tree-Beam Search über Seq2Tree-Modelle.
Es wird ein seq2Tree-Modell zur Übersetzung einer Problemstellung in natürlicher Sprache in das entsprechende funktionale Programm in DSL vorgestellt, das eine Verbesserung gegenüber dem seq2seq-Basisansatz darstellt.
Diese Arbeit befasst sich mit dem Problem der Programmsynthese, wenn eine Problembeschreibung und eine kleine Anzahl von Eingabe-Ausgabe Beispielen vorliegt.
In diesem Beitrag wird eine Technik zur Programmsynthese vorgestellt, die eine eingeschränkte Grammatik von Problemen beinhaltet, die mit Hilfe eines aufmerksamkeitsgesteuerten Encoder-Decoder Netzwerks durchsucht wird.
In diesem Beitrag werden die Diskriminierungs- und Generalisierungseigenschaften von GANs untersucht, wenn die Diskriminatormenge eine eingeschränkte Funktionsklasse wie neuronale Netze ist.
Gleicht die Kapazitäten von Generator- und Diskriminatorklassen in GANs aus, indem es garantiert, dass induzierte IPMs Metriken und keine Pseudo-Metriken sind.
Diese Arbeit bietet eine mathematische Analyse der Rolle der Größe der Gegner-/Diskriminatormenge in GANs.
Alles, was Sie zum Trainieren tiefer Residualnetze brauchen, ist eine gute Initialisierung; Normalisierungsschichten sind nicht erforderlich.
Es wird eine Methode zur Initialisierung und Normalisierung von tiefen Residual Netzwerken vorgestellt. Diese basiert auf Beobachtungen der Forward- und Backward Explosion in solchen Netzen. Die Leistung der Methode entspricht den besten Ergebnissen, die mit anderen Netzen mit expliziterer Normalisierung erzielt wurden.
Die Autoren schlagen eine neuartige Methode zur Initialisierung von Residual Networks vor, die durch die Notwendigkeit begründet ist, explodierende/verschwindende Gradienten zu vermeiden.
Schlägt eine neue Initialisierungsmethode vor, um sehr tiefe RedNets ohne Batch-Norm zu trainieren.
Diese Arbeit zielt darauf ab, eine bessere Metrik für unüberwachtes Lernen, wie z.B. Textgenerierung, zu erlernen und zeigt eine deutliche Verbesserung gegenüber SeqGAN.
Beschreibt einen Ansatz zur Erzeugung von Zeitsequenzen durch Lernen von Zustands-Aktionswerten, wobei der Zustand die bisher erzeugte Sequenz und die Aktion die Wahl des nächsten Wertes ist. 
Diese Arbeit befasst sich mit dem Problem der Verbesserung der Sequenzgenerierung durch das Erlernen besserer Metriken, insbesondere mit dem Problem des Expositionsbias.
Zerlegen Sie die Aufgabe des Lernens eines generativen Modells in das Erlernen von unentwirrten latenten Faktoren für Teilmengen der Daten und das anschließende Erlernen der Verbindung über diese latenten Faktoren.  
Lokal entwirrte Faktoren für ein hierarchisches generatives Modell für latente Variablen, das als hierarchische Variante der adversarial erlernten Inferenz betrachtet werden kann.
Der Beitrag untersucht das Potenzial hierarchischer latenter Variablenmodelle für die Generierung von Bildern und Bildsequenzen und schlägt vor, mehrere übereinander gestapelte ALI-Modelle zu trainieren, um eine hierarchische Darstellung der Daten zu erzeugen.
Die Arbeit zielt darauf ab, die Hierarchien für das Training von GAN in einem hierarchischen Optimierungsplan direkt zu lernen, anstatt von einem Menschen entworfen zu werden.
Wir schlagen ein gemeinsames Modell vor, um visuelles Wissen in Satzrepräsentationen einzubeziehen.
Die Arbeit schlägt eine Methode vor, um Videos in Verbindung mit Untertiteln zu verwenden, um die Satzeinbettung zu verbessern.
In diesem Beitrag wird ein Modell für das Lernen von Satzrepräsentationen vorgeschlagen, die auf der Grundlage von Videodaten fundiert sind.
Vorschlagen einer Methode zur Verbesserung textbasierter Satzeinbettungen durch ein gemeinsames multimodales Framework.
