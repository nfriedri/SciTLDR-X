Wir bieten notwendige und ausreichend analytische Formulare für die kritischen Punkte der quadratischen Verlustfunktionen für verschiedene neuronale Netze und nutzen die analytischen Formulare, um die Landschaftseigenschaften für die Verlustfunktionen dieser neuronalen Netzwerke zu charakterisieren.
Biologisch plausible Lernalgorithmen, insbesondere Zeichensymmetrie, funktionieren gut auf ImageNet
Wir führen den 2-simplical Transformer ein und zeigen, dass diese Architektur ein nützlicher induktiver Bias für logisches Argumentation im Kontext von Deep Reinforcement Learning.
Genaue Vorhersagen über sehr lange Zeithorizonte mit Tensor-Train-RNNs
Wir schlagen einen variationalen Message-Passing-Algorithmus für Modelle vor, die sowohl das tiefe Modell als auch das probabilistische grafische Modell enthalten.
Eine einfache Modifikation der Low-Rank-Faktorisierung, die die Leistung (sowohl bei Bild- als auch bei Sprachaufgaben) verbessert und gleichzeitig kompakt ist.
Wir schlagen ein einfaches, allgemeines und speichersparendes Datenformat vor, um das Training von Deep Learning zu beschleunigen, indem die Sample Fidelity dynamisch zur Trainingszeit ausgewählt werden kann.
Robustes diskriminatives Repräsentationslernen mittels Gradienten-Reskalierung: Eine Perspektive der Regulierung von Schwerpunkten
Sind GANs erfolgreich, weil sie nachteilig trainiert werden oder weil sie ConvNets verwenden? Wir zeigen, dass ein ConvNet-Generator, der mit einem einfachen Rekonstruktionsverlust und erlernbaren Rauschvektoren trainiert wird, viele der wünschenswerten Eigenschaften eines GANs aufweist.
MMD GANs mit einem neuen Random-Forest-Kernel ausrüsten.
Eine Methode für genauere kritische Schätzungen beim Reinforcement Learning.
Wir stellen einen systematischen Rahmen für die Quantifizierung der Robustheit von Klassifikatoren gegenüber natürlich auftretenden Störungen von Bildern in Videos vor.
Deep Learning für strukturierte tabellarische Daten im maschinellen Lernen unter Verwendung eines vortrainierten CNN-Modell aus ImageNet.
Die Dekodierung von Pixeln kann immer noch für das Repräsentationslernen auf Bildern funktionieren
Schnelle, wirklich skalierbare Vollmatrix AdaGrad / Adam, mit Theorie für adaptive stochastische nicht konvexe Optimierung
In dieser Arbeit schlagen wir vor, ein Dialogsystem zu erlernen, das unabhängig voneinander verschiedene Dialogfähigkeiten parametrisiert, und lernt jede von ihnen durch Attention over Parameters (AoP) auszuwählen und zu kombinieren. 
Wir schlagen vor, einen großen Datensatz in einen kleinen Satz synthetischer Daten zu destillieren, mit denen Netze nahe an der ursprünglichen Leistung trainiert werden können. 
Wir schlagen eine primär-duale Subgradienten-Methode für das Training von GANs vor, und diese Methode mildert den Modus-Kollaps effektiv.
Wir stellen fest, dass die Irrationalität eines Experten-Vorführers dem Lernenden helfen kann, seine Präferenzen zu erkennen. 
Wir vergleichen die Robustheit von Modellen aus 4 beliebten NLP-Aufgaben: Q&A, NLI, NER und Sentiment Analysis, indem wir ihre Leistung bei gestörten Eingaben testen.
Ein neuartiger cluster-basierter Algorithmus des Curriculum-Lernens wird vorgeschlagen, um das robuste Training generativer Modelle zu lösen.
Wir haben einen neuartigen verteilten Backdoor-Angriff auf föderiertes Lernen vorgeschlagen und zeigen, dass er im Vergleich zu zentralisierten Standardangriffen nicht nur effektiver ist, sondern auch schwieriger durch bestehende robuste FL-Methoden abgewehrt werden kann
Neuronales Netz für graphenbasiertes halbüberwachtes Lernen; greift die Klassiker wieder auf und propagiert *Labels* statt Merkmalsdarstellungen
Neuronale Architektur Suche für eine Reihe von Aufgaben zum Verstehen natürlicher Sprache. Entwurf des Suchraums für NLU-Aufgaben. Anwendung differenzierbarer Architektursuche zur Entdeckung neuer Modelle
In dieser Arbeit stellen wir EvalNE vor, eine Python-Toolbox zur automatisierten Bewertung von Netzwerkeinbettungsmethoden zur Link-Vorhersage und zur Gewährleistung der Reproduzierbarkeit der Ergebnisse.
Wiederherstellungsgarantie des stochastischen Gradientenabstiegs mit zufälliger Initialisierung für das Lernen eines zweischichtigen neuronalen Netzes mit zwei versteckten Knoten, Gewichten mit Einheitsnorm, ReLU-Aktivierungsfunktionen und Gaußschen Eingaben.
Jumpout wendet drei einfache, aber effektive Modifikationen des Dropouts an, die auf neuen Erkenntnissen über die Generalisierungsleistung von DNN mit ReLU in lokalen Regionen basieren.
Wir untersuchen die natürliche Entstehung von Sparsity in den Aktivierungen und Gradienten für einige Layer eines dichten LSTM-Sprachmodells während des Trainings.
Herkömmliche Speichernetzwerke erzeugen viele redundante latente Vektoren, was zu einer Überanpassung und dem Bedarf größerer Speicherkapazitäten führt. Wir führen Memory Dropout als eine automatische Technik ein, die die Vielfalt im latenten Raum fördert.
Wir leiten die Nesterov-Methode als eine direkte Diskretisierung einer ODE ab, die sich von der Su-Boyd-Candes-Methode unterscheidet, und beweisen die Beschleunigung im stochastischen Fall
Wir schlagen Learning to Transfer-Learn (L2TL) vor, um das Transfer-Lernen auf einem Zieldatensatz durch gezielte Extraktion von Informationen aus einem Quelldatensatz zu verbessern.
In Deep RL können ordnungsinvariante Funktionen in Verbindung mit Standardspeichermodulen verwendet werden, um den Gradientenabfall und die Widerstandsfähigkeit gegen Rauschen zu verbessern.
In dieser Arbeit wird ein Algorithmus zur Behandlung von Optimierungsproblemen mit mehreren Nebenbedingungen unter Berücksichtigung der Vielfältigkeit vorgestellt.
Eine Methode zum Q-Learning auf kontinuierlichen Aktionsräumen durch Vorhersage einer Sequenz diskretisierter 1-D Aktionen.
Unsere Methode beinhaltet WGAN, um eine Anpassung der Belegungsmessungen für das transition leraning zu erreichen.
Die Gaußsche Normalisierung führt während der Backpropagation eine Anpassung nach dem Prinzip der kleinsten Quadrate durch, die die partiellen Ableitungen der normalisierten Aktivierungen in die Nulllinie bringt und dekorreliert.
Wir geben eine theoretische Analyse der Fähigkeit der Batch-Normalisierung zur automatischen Abstimmung der Lernraten im Kontext der Suche nach stationären Punkten für ein Deep Learning Ziel.
Wir schlagen DVD-GAN vor, ein großes generatives Videomodell, das bei mehreren Aufgaben den neusten Stand darstellt und hochkomplexe Videos produziert, wenn es auf großen realen Datensätzen trainiert wird.
Wir schlagen eine neue wiederkehrende Speicherarchitektur vor, die Zustandsänderungen von Entitäten im Sinne des gesunden Menschenverstands verfolgen kann, indem sie die kausalen Auswirkungen von Aktionen simuliert.
Wir untersuchen die Raumeffizienz von speichererweiterten neuronalen Netzen beim Lernen von Mengenzugehörigkeit.
Wir konstruieren eine Kronecker-faktorisierte Laplace-Approximation für neuronale Netze, die zu einer effizienten Matrixnormalverteilung über die Gewichte führt.
Die Graphenregulierung zwingt die spektrale Einbettung dazu, sich auf die größten Cluster zu konzentrieren, wodurch die Darstellung weniger empfindlich gegenüber Rauschen ist. 
Wir zeigen, dass der Expositionsbias viel weniger gravierend sein könnte, als er derzeit für das MLE-LM-Training angenommen wird.
Eine kontrollierte Studie über die Rolle von Umgebungen in Bezug auf Eigenschaften in aufkommenden Kommunikationsprotokollen.
Rasterbasierte Dokumentendarstellung mit kontextualisierten Einbettungsvektoren für Dokumente mit 2D-Layout
Deep-RL-Methoden können von anderen Akteueren angegriffen werden, die Aktionen durchführen, um natürliche Beobachtungen zu erzeugen, die gegnerisch sind.
Wir stellen einen neuen iterativen Algorithmus vor, der auf verallgemeinerten Low-Rank-Modellen zur Berechnung und Interpretation von Worteinbettungsmodellen basiert.
Wir lernen einen bedingten autoregressiven Ablauf, um Störungen vorzuschlagen, die kein Simulatorversagen hervorrufen, was die Inferenzleistung verbessert.
Wir verbessern die Beantwortung von Fragen, die Multi-Hop-Argumentation erfordern, indem wir eine Zwischenkette von Sätzen extrahieren.
Wir entwickeln eine neue Methode zur Schätzung von Normalisierungskonstanten (Bayes'sche Evidenz) unter Verwendung von Optimal Bridge Sampling und eines neuartigen Normalisierungsflusses, der nachweislich die bestehenden Methoden in Bezug auf Genauigkeit und Rechenzeit übertrifft.
Wir überprüfen DNN-Modelle auf katastrophales Vergessen anhand eines neuen Bewertungsschemas, das typische Anwendungsbedingungen widerspiegelt, mit überraschenden Ergebnissen.
Federated Averaging ist bereits ein Meta-Learning-Algorithmus, während in Rechenzentren trainierte Methoden wesentlich schwieriger zu personalisieren sind.
Wir identifizieren Downsampling als einen Mechanismus zur Erinnerung in Convolutional-Autoencodern.
Wir schlagen einen Algorithmus zum inversen Reinforcement Learning vor, der in der Lage ist, Belohnungsfunktionen zu erlernen, die auf neue, ungesehene Umgebungen übertragen werden können.
Die Generalisierung ist stark mit der Bayes'schen Evidenz korreliert, und das Gradientenrauschen minimiert die SGD, deren Evidenz groß ist.
Adversarial Nets, Aufmerksamkeitsmechanismus, Positronenbilder, Datenknappheit
Inspiriert von der neurowissenschaftlichen Forschung, lösen von drei Hauptschwächen des viel zitierten Recurrent Attention Model, indem Sie einfach zwei Terme zur Zielfunktion hinzufügen.
Diese Arbeit stellt einen Clustering-basierten aktiven Lernalgorithmus für Graphen vor.
Wir schlagen die InfoCNF vor, eine effiziente bedingte CNF, die Gating-Netzwerke einsetzt, um die Fehlertoleranzen der ODE-Solver zu lernen  
Eine unbeaufsichtigte Lernmethode, die Meta-Lernen verwendet, um effizientes Lernen von nachgelagerten Bildklassifizierungsaufgaben zu ermöglichen, und dabei die modernsten Methoden übertrifft.
Bedingte VAE auf latenten Räumen von vortrainierten generativen Modellen, die den Transfer zwischen drastisch unterschiedlichen Domänen unter Beibehaltung der Lokalität und der semantischen Ausrichtung ermöglicht.
Eine neuartige Methode des induktiven Transfer-Lernens, die adversariales Lernen und Multi-Task-Lernen einsetzt, um die Diskrepanz zwischen Input- und Output-Raum zu beheben
Eine neue, leistungsstarke Architektur für durchgängige Named Entity Recognition und Relationsextraktionen, die schnell zu trainieren ist.
Variationales Bayes-Schema für Recurrent Neural Networks
Fallstudie über optimale Deep Learning Modelle für UAVs
Wir zeigen die erste erfolgreiche Verwendung von Transformers bei der Erzeugung von Musik, die eine langfristige Struktur aufweist.
Wir schlagen einen neuen Monte-Carlo-Baum-Such- / Rollout-Algorithmus vor, der auf Breiten-basierte Suche stützt, um eine Lookahead zu konstruieren.
Wir schlagen eine neuartige Deep Neural Network Schicht zur Normalisierung der klasseninternen Kovarianz einer internen Repräsentation in einem neuronalen Netzwerk vor, die zu einer deutlichen Verbesserung der Generalisierung der gelernten Repräsentationen führt.
Die Hinzufügung eines Diversitätskriteriums, das von DPP inspiriert ist, in das GAN-Ziel vermeidet den Moduszusammenbruch und führt zu besseren Generationen. 
Wir schlagen eine Generalisierungsgrenze vor, die die Verbesserung der Generalisierungen durch Überparametrisierung teilweise erklären könnte.
Wir stellen drei generische Cloud Verarbeitungsblöcke vor, die sowohl die Genauigkeit als auch den Speicherverbrauch mehrerer moderner Netzwerke verbessern und somit die Entwicklung tieferer und genauerer Netzwerke ermöglichen.
Methoden zum Erlernen kontextueller akustischer Worteinbettungen aus einem Ende-zu-Ende-Spracherkennungsmodell, die mit textbasierten Worteinbettungen konkurrieren können.
In dieser Arbeit wird eine Maskenmethode vorgeschlagen, die die bisherigen unscharfen Ergebnisse der unüberwachten monokularen Tiefenschätzung, die durch Verdeckung verursacht werden, löst.
Wir stellen eine neuartige Methode zur Darstellung von Graphen als mehrkanalige bildähnliche Strukturen vor, die es ermöglicht, sie mit einfachen 2D CNNs zu verarbeiten.
Wir schlagen ein Maß für das Langzeitgedächtnis vor und beweisen, dass tiefe Recurrent Networks viel besser geeignet sind, langfristige zeitliche Abhängigkeiten zu modellieren als untiefe Netze.
Wir vergleichen Wahrnehmungs-, neuronale und modellierte Darstellungen der tierischen Kommunikation mit maschinellem Lernen, Verhalten und Physiologie.
Das Prinzip des Informationsengpasses, angewandt auf ResNets, unter Verwendung von PixelCNN++ Modellen zur Dekodierung der gegenseitigen Information und zur bedingten Erzeugung von Bildern zur Informationsdarstellung
Die Anpassung eines RL-Agenten in einer Zielumgebung mit unbekannter Dynamik ist schnell und sicher, wenn wir vorherige Erfahrungen in einer Vielzahl von Umgebungen übertragen und dann risikoaverse Aktionen während der Anpassung auswählen.
Wir schlagen den Neuro-Symbolic Concept Learner (NS-CL) vor, ein Modell, das visuelle Konzepte, Wörter und das semantische Parsing von Sätzen ohne explizite Überwachung erlernt.
Wir führen einen Gauß'schen Prozessprior über Gewichte in einem neuronalen Netz ein und erforschen seine Fähigkeit, eingabeabhängige Gewichte zu modellieren, mit Vorteilen für verschiedene Aufgaben, einschließlich Unsicherheitsabschätzung und Generalisierung in einer Umgebung mit geringer Stichprobe.
Wir führen eine eingehende Untersuchung der Eignung von Selg-Attention Modellen für die neuronale maschinelle Übersetzung auf Zeichenebene durch.
Wir präsentieren die modernsten Ergebnisse der Verwendung neuronaler Netze zur Diagnose von Röntgenaufnahmen des Brustkorbs
Wir demonstrieren den Nutzen einer neuen KI-Erklärungstechnik, indem wir die gelernten Merkmale eines CNN, das auf die binäre Klassifizierung von Zebrafischbewegungen trainiert wurde, visualisieren.
Interne Konsistenzbeschränkungen verbessern die Fähigkeit von Agenten, emergente Protokolle zu entwickeln, die sich über kommunikative Rollen hinweg verallgemeinern lassen.
Wir stellen eine neue Analysetechnik vor, die interpretierbare kompositorische Strukturen in bekanntermaßen schwer zu interpretierenden Recurrent Neural Networks erkennt.
Wir haben die neuronalen Repräsentationen biologischer visueller Systeme reproduziert, indem wir deren neuronale Ressourcenbeschränkungen in einem tiefen Convolutional Modell simuliert haben.
Eine Theorie, die die Hessian der Lösung und die Verallgemeinerungsfähigkeit des Modells verbindet
Wir haben die Entropiemaximierung in GANs eingeführt, was zu einer Neuinterpretation der Kritik als Energiefunktion führt.
Wir stellen einen langzeit skalierten Algorithmus zur Übertragung von musikalischen Audiostilen  vor, der Audio im Zeitbereich synthetisiert, aber Zeit-Frequenz-Darstellungen von Audio verwendet.
Wir schlagen eine wiederholte Referenz-Benchmark-Aufgabe und einen regulierten, kontinuierlichen Lernansatz für die adaptive Kommunikation mit Menschen in unbekannten Domänen vor
Sortieren im Kodierer und Rückgängigmachen der Sortierung im Dekodierer zur Vermeidung von Verantwortungsproblemen bei eingestellten Auto-Enkodierern
Wir präsentieren einen hierarchischen Lernrahmen für die Navigation in einer enthaltenen Lernumgebung
Attribution kann manchmal irreführend sein
Effizienter Transformer mit ortsabhängigem Hashing und reversiblen Schichten
Wir zeigen, dass Sprachverständnis durch Lesen ein vielversprechender Weg ist, um Regeln zu lernen, die sich auf neue Umgebungen übertragen lassen.
Wir stellen eine Hypothese auf, warum Gradient-Descent verallgemeinert, und zwar auf der Grundlage der Interaktion zwischen den Gradienten pro Beispiel.
Neue Architektur für die Synthese stereoskopischer Ansichten bei beliebigen Kameraverschiebungen unter Verwendung adaptiver t-förmiger Kernel mit adaptiven Dilatationen.
In diesem Beitrag werden eine grundlegende Theorie und optimale Algorithmen für das DNN-Training vorgeschlagen, die den Trainingsspeicher für gängige DNNs um bis zu 80% reduzieren.
Dieses Papier beweist die universelle Approximierbarkeit von quantisierten neuronalen ReLU-Netzen und legt die Komplexitätsgrenze bei beliebigem Fehler dar.
Ein allgemeiner Rahmen für wertbasiertes Reinforcement Learning für kontinuierliche Kontrolle
Generative Adversarial Network Training ist ein kontinuierliches Lernproblem.
Ein vereinheitlichter Rahmen für Few-shot Learning und Zero-shot Learning auf der Grundlage einer Reparametrisierung des Netzes
GraphQA ist eine graphenbasierte Methode zur Bewertung der Qualität von Proteinen, die den Stand der Technik sowohl bei handwerklichen als auch bei repräsentationsbasierten Ansätzen verbessert.
Wir befassen uns mit aktivem Lernen in einer Batch-Umgebung mit verrauschten Orakeln und verwenden Modellunsicherheit, um die Entscheidungsqualität des aktiven Lernalgorithmus während der Erfassung zu kodieren.
Stochastische Übertragung mit einstellbaren Merkmalen. 
Wir schlagen AGILE vor, ein Framework für das Training von Bearbeitern zur Ausführung von Anweisungen anhand von Beispielen für entsprechende Zielzustände.
In Hierarchical RL führen wir den Begriff einer "weichen", d.h. anpassungsfähigen Option ein und zeigen, dass dies das Lernen in Multitasking-Umgebungen erleichtert.
Erlernen eines kontrollierbaren generativen Modells durch Entflechtungslernen der latenten Repräsentation.
Verbessern des Sprachmodells für überwachte Lernaufgaben 
Dynamische Erzeugung von Filtern, die vom Eingangsbild abhängig sind, für CNNs in jedem Vorwärtsdurchlauf 
Wir schlagen ein neues, jederzeit einsetzbares neuronales Netz vor, das eine Teilauswertung durch Teilnetze mit unterschiedlicher Breite und Tiefe ermöglicht.
Wir schlagen ein neues Modell vor, um verallgemeinerbare und vielfältige retrosynthetische Reaktionsvorhersagen zu machen.
Disentanglement-PyTorch ist eine Bibliothek für das Lernen variabler Darstellungen
Wir erweitern die jüngsten Erkenntnisse im Zusammenhang mit der Softmax-Beständigkeit, um modernste Ergebnisse in der kontinuierlichen Steuerung zu erzielen.
Wir passen eine Familie von kombinatorischen Spielen mit abstimmbarer Schwierigkeit und einem optimalen Regelwerk an, die als lineares Netzwerk ausgedrückt werden kann. Wir entwickeln sie als eine reichhaltige Umgebung für das Reinforcement Learning, zeigen Kontraste in der Leistung mit dem überwachten Lernen und analysieren das Multiagentenlernen und die Verallgemeinerung. 
Ein zusätzliches Verfahren für Deep Learning zur Erkennung von Ausreißern während der Vorhersagezeit
Eine vollständig vernetzte Architektur wird verwendet, um Worteinbettungen aus Zeichendarstellungen zu erzeugen, übertrifft traditionelle Einbettungen und bietet Einblicke in Spärlichkeit und Dropout.
Wir führen gegnerische Angriffe gegen binarisierte neuronale Netze durch und zeigen, dass wir die Auswirkungen der stärksten Angriffe reduzieren, während wir eine vergleichbare Genauigkeit in einer Blackbox-Umgebung beibehalten.
Eine empirische Untersuchung des GAN-basierten Alignments von Wortvektorräumen mit Schwerpunkt auf Fällen, in denen lineare Transformationen nachweislich existieren, das Training aber instabil ist.
Unser Ziel in diesem Papier ist es, einen neuen Ansatz für das Problem des Transfer-Lernens von gelabelten auf nicht gelabelte Software-Projekten im Kontext von SVD vorzuschlagen, um das Problem des Modus-Kollabierens zu lösen, mit dem bisherige Ansätze konfrontiert sind.
Eine schnellere Methode zur Erzeugung von Knoteneinbettungen, die eine Reihe von Permutationen über die unmittelbare Nachbarschaft eines Knotens als Kontext verwendet, um seine Darstellung zu erzeugen.
Wir zeigen, wie man rekurrente Architekturen mit der geschlossenen Formlösung eines linearen Autoencoders für Sequenzen initialisiert. Wir zeigen die Vorteile dieses Ansatzes im Vergleich zu orthogonalen RNNs.
Wir bieten ein aufschlussreiches Verständnis der Sequenzkennzeichnung von NER und schlagen die Verwendung von zwei Arten von Kreuzstrukturen vor, die beide theoretische und empirische Verbesserungen bringen.
Wir stellen ein theoretisch bewährtes generatives Modell der Wissensgrapheneinbettung vor. 
Wir untersuchen empirisch, wie schwierig es ist, fehlende Teile von trainierten Modellen wiederherzustellen
In diesem Papier wird die variable Domänenanpassung vorgeschlagen, ein einheitliches, skalierbares und einfaches Rahmenwerk für das Lernen mehrerer Verteilungen durch Variationsinferenz.
Wir schlagen eine neuartige Kombination aus gegnerischem Training und beweisbaren Verteidigungsmaßnahmen vor, die ein Modell mit modernster Genauigkeit und zertifizierter Robustheit auf CIFAR-10 hervorbringt. 
Programme haben eine Struktur, die als Graphen dargestellt werden kann, und neuronale Graphen-Netzwerke können lernen, Fehler in solchen Graphen zu finden.
Wir führen mehrere Kriterien zur Erkennung von Überanpassung ein und analysieren sie.
Wir entwickeln einen punktbasierten Wert-Iterationslöser für POMDPs mit aktiven Wahrnehmungs- und Planungsaufgaben.
Wir stellen ein unterparametrisiertes, nicht convolutional, und einfaches tiefes neuronales Netzwerk vor, das ohne Training natürliche Bilder effektiv darstellen und Bildverarbeitungsaufgaben wie Kompression und Entrauschung konkurrenzfähig lösen kann.
Dieses Papier 1) charakterisiert Funktionen, die durch ReLU-DNNs dargestellt werden können, 2) untersucht formal den Nutzen der Tiefe in solchen Architekturen, 3) gibt einen Algorithmus zur Implementierung empirischer Risikominimierung zur globalen Optimalität für zweischichtige ReLU-Netze.
Benchmarks für biologisch plausible Lernalgorithmen auf komplexen Datensätzen und Architekturen
Wir haben vorgeschlagen, den neuen GrOWL-Regulierer für die gleichzeitige Spärlichkeit und Bindung von Parametern beim DNN-Lernen zu verwenden. 
Eine Analyse der Lern- und Optimierungsstrukturen der Architektursuche in neuronalen Netzen und darüber hinaus.
Wir führen eine verlustfreie Komprimierung großer Bilddatensätze mit einer VAE durch und übertreffen damit bestehende Kompressionsalgorithmen.
Auf Bayes'scher Optimierung basierende Online-Hyperparameter-Optimierung.
In diesem Papier schlagen wir das Latent Question Reformulation Network (LQR-net) vor, ein Multi-Hop- und paralleles, aufmerksames Netzwerk, das für die Beantwortung von Fragen entwickelt wurde, die logische Fähigkeiten erfordern.
Erklärung von multivariaten Zeitreihenmodellen durch Auffinden wichtiger Beobachtungen in der Zeit mit Hilfe von kontrafaktischen Daten
Wir verwenden Selbstüberwachung für beide Domänen, um sie für die unbeaufsichtigte Domänenanpassung abzugleichen.
Das Minimum einer Menge exponentiell verteilter Hashes hat eine sehr nützliche Kollisionswahrscheinlichkeit, die den Jaccard-Index auf Wahrscheinlichkeitsverteilungen verallgemeinert.
Ein graphisches neuronales Netzmodell mit Parametern, die aus natürlichen Sprachen generiert werden, das Multi-Hop-Reasoning durchführen kann. 
Wir stellen Meta-Critic vor, ein Hilfskritikmodul für Off-Policy-Actor-Critic-Methoden, das online während des Lernens einer einzelnen Aufgabe erlernt werden kann.
Durch die Kombination einer originalen PAC-Bayes-Schranke und einer handelsüblichen Komprimierungsmethode für neuronale Netze erhalten wir nicht-vakante Generalisierungsgrenzen für tiefe neuronale Netze auf ImageNet-Skala.
Wir schlagen ein alternatives Maß zur Bestimmung der Effektivität von gegnerischen Angriffen in NLP-Modellen vor, das auf einem Abstandsmaß basiert, ähnlich dem inkrementellen L2-Gewinn in der Kontrolltheorie.
Wir schlagen das Warped Residual Network vor, das einen parallelisierbaren Warp-Operator für die Vorwärts- und Rückwärtspropagation zu entfernten Schichten verwendet, der schneller trainiert als das ursprüngliche Residual Neural Network. 
Wir schlagen eine Reihe von Metriken vor, die die gewünschten Eigenschaften von Erklärungsalgorithmen erfassen und verwenden sie, um solche Methoden objektiv zu vergleichen und zu bewerten
Eine neue Methode zur Erkennung von Verteilungsfehlern hilft dabei, die Zuverlässigkeit von RNN-Vorhersagen für einige NLP-Aufgaben zu messen
Tiefe-2-vs-3-Trennung für sigmoidale neuronale Netze über allgemeine Verteilungen
Wir schlagen eine skalierbare Methode zur Annäherung der Eigenvektoren des Laplacian im Kontext des Reinforcement Learning vor und zeigen, dass die erlernten Darstellungen die Leistung eines RL-Agenten verbessern können.
Simulation von realen Bild-Übersetzungen und Video-Erzeugung
Wir schlagen PocketFlow vor, ein automatisiertes Framework zur Modellkomprimierung und -beschleunigung, um den Einsatz von Deep Learning-Modellen auf mobilen Geräten zu erleichtern.
Wie man GANs aus verrauschten, verzerrten, partiellen Beobachtungen lernt
Siehe die Zusammenfassung.  (Für die Überarbeitung ist die Arbeit identisch, mit Ausnahme eines 59-seitigen Zusatzmaterials, das als eigenständige Version des technischen Berichts des Papiers dienen kann).
Wir führen einen Aufmerksamkeitsmechanismus ein, um die Merkmalsextraktion für tiefes aktives Lernen (AL) in der halbüberwachten Umgebung zu verbessern.
Wir wenden kanonische Formen von Gradientenkomplexen (Barcodes) an, um die Verlustflächen neuronaler Netze zu untersuchen.
Verwendung asynchroner Gradientenaktualisierungen zur Beschleunigung des dynamischen Trainings neuronaler Netze
Wir untersuchen das Problem der Belohnungsgestaltung in kooperativen MARL auf der Grundlage von Paket-Routing-Umgebungen. Die experimentellen Ergebnisse erinnern uns daran, die Belohnungen sorgfältig zu gestalten, da sie wirklich wichtig sind, um das Verhalten der Agenten zu steuern.
Neumann-Netzwerke sind ein durchgängiger, stichprobeneffizienter Lernansatz zur Lösung linearer inverser Probleme in Abbildungen, der mit dem MSE-Optimum-Ansatz kompatibel ist und eine Erweiterung auf patch-basiertes Lernen zulässt.
GLMP: Globale Memory Encoder (Kontext-RNN, globaler Zeiger) und lokale Memory Decoder (Skizzen-RNN, lokaler Zeiger), die externes Wissen (MemNN) gemeinsam nutzen, werden vorgeschlagen, um die Antwortgenerierung im aufgabenorientierten Dialog zu stärken.
Wir schlagen ein neuartiges Aritificial Checkerboard Enhancer (ACE)-Modul vor, das Angriffe auf einen vorgegebenen Pixelraum lenkt und diesen mit einer einfachen Auffüllungsoperation erfolgreich verteidigt.
Wir analysieren systematisch das Konvergenzverhalten populärer Gradientenalgorithmen zur Lösung bilinearer Spiele, sowohl mit simultanen als auch mit alternierenden Updates.
Wir verwenden VAEs, um eine gemeinsame latente Raumeinbettung zwischen Bildmerkmalen und -attributen zu erlernen, und erreichen dadurch modernste Ergebnisse beim generalisierten Zero-Shot-Lernen.
Räumliche Informationen auf den letzten Ebenen sind für eine gute Klassifizierungsgenauigkeit nicht erforderlich.
Wir verwenden Siamesische Netze, um den Generierungsprozess in GANs ohne gelabelte Daten zu steuern und zu entwirren.
Wir stellen Predicted Variables vor, einen Ansatz, der maschinelles Lernen in Programmiersprachen zu einem First Class Citizen macht.
Wir zeigen Wege auf, wie man ein hierarchisches Videovorhersagemodell trainieren kann, ohne Posenmarkierungen zu benötigen.
In dieser Arbeit untersuchen wir das Problem des Lernens von Repräsentationen zur Identifizierung neuer Objekte durch die Erkundung von Objekten mit Hilfe des Tastsinns. Der springende Punkt dabei ist, dass die Abfrage im Bildbereich erfolgt.
Wir verwenden lineare homomorphe Komprimierungsverfahren, um die hinreichenden Statistiken eines bedingten Zufallsfeldmodells der Koreferenz darzustellen, was uns erlaubt, die Inferenz zu skalieren und die Geschwindigkeit um eine Größenordnung zu verbessern.
Wir geben eine theoretische Analyse der Messung und Optimierung der gegenseitigen Information.
Durch das Aufbrechen der Schichtenhierarchie schlagen wir einen dreistufigen Ansatz für die Konstruktion von Neuronen-Hierarchienetzen vor, die NAS, SMASH und die hierarchische Darstellung mit weniger Parametern und kürzerer Suchzeit übertreffen.
Wir schlagen einen Algorithmus vor, der automatisch die Parameter einer Simulationsmaschine anpasst, um Trainingsdaten für ein neuronales Netz zu erzeugen, so dass die Validierungsgenauigkeit maximiert wird.
Ein modellagnostisches Regularisierungsschema für die auf neuronalen Netzen basierende Schätzung der bedingten Dichte.
Eine patch-basierte Bottleneck-Formulierung in einem VAE-Rahmen, die unüberwachte Repräsentationen lernt, die besser für die visuelle Erkennung geeignet sind.
Um das Problem des Verschwindens/Explodierens des Gradienten zu lösen, schlagen wir eine effiziente Parametrisierung der Übergangsmatrix von RNN vor, die keine Ausdruckskraft verliert, schneller konvergiert und eine gute Generalisierung aufweist.
Ein Papier, das eine Methode zur Umwandlung des Bildstils mithilfe von tiefen neuronalen Netzen vorschlägt.
Wir verbessern bestehende Dialogsysteme zur Beantwortung von Menschen, die persönliche Geschichten erzählen, indem wir Darstellungen zur Vorhersage von Emotionen einbeziehen und außerdem einen neuen Benchmark und Datensatz für einfühlsame Dialoge veröffentlichen.
Eine neue rekurrente neuronale Netzarchitektur zur Erkennung paarweiser Granger-Kausalität zwischen nichtlinear interagierenden Zeitreihen. 
Ein auf Kontrollvariablen basierender stochastischer Trainingsalgorithmus für graphische Faltungsnetzwerke, bei dem das rezeptive Feld nur zwei Nachbarn pro Knoten sein kann.
Monte-Carlo-Methoden zur Quantisierung von vortrainierten Modellen ohne zusätzliches Training.
Informationstheoretischer Ansatz für das unüberwachte Lernen eines Hybrids aus diskreten und kontinuierlichen Repräsentationen, 
Es ist wichtig, die Optimierung im Funktionsraum und nicht nur im Parameterraum zu betrachten. Wir führen eine Lernregel ein, die den im Funktionsraum zurückgelegten Weg reduziert, genau wie SGD den im Parameterraum zurückgelegten Weg begrenzt.
Konvergenztheorie für verzerrte (aber konsistente) Gradientenschätzer in der stochastischen Optimierung und Anwendung auf Graph Convolutional Networks
Wir verwenden Schnappschüsse aus dem Trainingsprozess, um jede Methode zur Unsicherheitsabschätzung eines DNN-Klassifikators zu verbessern.
Ein neuer Gesichtsbilddatensatz für ausgewogene Rasse, Geschlecht und Alter, der für die Messung und Abschwächung von Vorurteilen verwendet werden kann
Wir versuchen, den Zeichenprozess von Schriftarten zu modellieren, indem wir sequentielle generative Modelle von Vektorgrafiken (SVGs), einer hochstrukturierten Darstellung von Schriftzeichen, erstellen.
Wir entwickeln eine "dynamische neuronale relationale Inferenz", ein Variations-Auto-Codierer-Modell, das die verborgenen dynamischen Beziehungen zwischen Neuronen explizit und interpretierbar darstellen kann.
Unseres Wissens nach ist DeePa das erste Deep-Learning-Framework, das die Parallelität von CNNs in allen parallelisierbaren Dimensionen auf der Granularität der einzelnen Schichten kontrolliert und optimiert.
Wir kombinieren die Kernel-Methode mit konnektiven Modellen und zeigen, dass die resultierenden tiefen Architekturen schichtweise trainiert werden können und eine transparentere Lerndynamik aufweisen. 
Wir schlagen eine Methode zur stochastischen Optimierung von Strafen zweiter Ordnung vor und zeigen, wie diese für das Training von fairnessbewussten Klassifikatoren angewendet werden kann.
Wir zeigen, dass die Ableitungen von Deep-Learning-Netzwerken eine Low-Rank-Struktur haben, und diese Struktur ermöglicht es uns, Ableitungsinformationen zweiter Ordnung zu verwenden, um die Lernraten adaptiv und auf eine rechnerisch machbare Weise zu berechnen.
Ein einheitlicher Min-Max-Optimierungsrahmen für gegnerische Angriffe und Verteidigung
Dimensionalitätsreduktion für Fälle, in denen Beispiele als weiche Wahrscheinlichkeitsverteilungen dargestellt werden können
Wir schlagen eine neuartige Architektur für intrinsisch motivierte Zielexploration mit unbeaufsichtigtem Lernen von Zielraumrepräsentationen vor und bewerten, wie verschiedene Implementierungen die Entdeckung einer Vielfalt von Strategien ermöglichen.
Wir schlagen einen zusätzlichen Trainingsschritt vor, das so genannte Post-Training, bei dem die optimalen Gewichte für die letzte Schicht des Netzes berechnet werden.
Komprimierung der Worteinbettungen um über 94% ohne Leistungseinbußen.
OE lehrt Anomalie-Detektoren, Heuristiken für die Erkennung ungesehener Anomalien zu erlernen; Experimente sind in der Klassifizierung, Dichte-Schätzung und Kalibrierung in NLP und Vision-Einstellungen; wir tunen nicht auf Test-Verteilung Proben, im Gegensatz zu früheren Arbeiten
Verfahren zum Lernen einer Transformation zwischen einem Paar von Quell-/Zieldatensätzen und deren Anwendung auf einen separaten Quelldatensatz, für den es keinen Zieldatensatz gibt
Kombination von Imitationslernen und Reinforcement Learning, um zu lernen, den Experten zu übertreffen
Ein unüberwachter Lernansatz zur Trennung von zwei strukturierten Signalen aus ihrer Überlagerung
Wir untersuchen die Beziehung zwischen normalisierenden Flows und Variations- und Denoising-Autoencodern und schlagen ein neues Modell vor, das diese verallgemeinert.
Wir verwenden Reinforcement Learning für die Umformulierung von Anfragen bei zwei Aufgaben und stellen überraschenderweise fest, dass beim Training mehrerer Agenten die Vielfalt der Umformulierungen wichtiger ist als die Spezialisierung.
Ein allgemeiner Rahmen für die Einbeziehung langfristiger Sicherheitseinschränkungen in regelbasiertes Reinforcement Learning
Bewertung von generativen Netzen durch ihre Fähigkeit zur Datenerweiterung bei diskriminativen Modellen.
Neu: Anwendung der seq2seq-Modellierung zur Automatisierung des Wissenschaftsjournalismus; hoch abstrakter Datensatz; Transfer-Learning-Tricks; automatisches Bewertungsmaß.
Wir stellen einen Ansatz vor, um die Umgebung so umzugestalten, dass uninterpretierbares Agentenverhalten minimiert oder eliminiert wird.
Wir schlagen eine neue Klasse von Inferenzmodellen vor, die iterativ Gradienten kodieren, um ungefähre nachträgliche Verteilungen zu schätzen.
Wir stellen eine Lernregel für Rückkopplungsgewichte in einem spikenden neuronalen Netz vor, die das Problem des Gewichtstransports angeht.
Wir kombinieren Variationsinferenz und vielfältiges Lernen (insbesondere VAEs und Diffusionskarten), um ein generatives Modell zu erstellen, das auf einem zufälligen Diffusions-Walk auf einer Datenvielfalt basiert; wir erzeugen Beispiele, indem wir aus der stationären Verteilung des Walks ziehen.
Wir entwickeln einen einfachen und allgemeinen Ansatz zur Vermeidung von Interferenzen zwischen Gradienten aus verschiedenen Aufgaben, der die Leistung von Multi-Task-Lernen sowohl im Bereich des überwachten als auch des Reinforcement Learnings verbessert.
Lernen, zu prüfen, indem die Akzeptanzrate des Metropolis-Hastings-Algorithmus nach unten begrenzt wird
Verallgemeinerter BERT für kontinuierliche und cross-modale Eingaben; modernste selbstüberwachte Videodarstellungen.
Eine generische dynamische Architektur, die einen problemspezifischen differenzierbaren Forking-Mechanismus zur Kodierung harter Datenstrukturannahmen einsetzt. Angewandt auf CLEVR VQA und Ausdrucksbewertung.
Wir vereinen die Support-Schätzung mit der Familie der Adversarial Imitation Learning Algorithmen zu Support-guided Adversarial Imitation Learning, einem robusteren und stabileren Imitations-Lernsystem.
Wir wenden gradientenbasiertes Meta-Lernen auf den Bereich der Graphen an und führen eine neue graphen-spezifische Transferfunktion ein, um den Prozess weiter zu beschleunigen.
Wir schlagen eine neue Methode zum inkrementellen Training eines gemischt generativen Modells vor, um die Informationsprojektion der realen Datenverteilung zu approximieren.
Wiederherstellung von Videos aus komprimierten Messungen durch Erlernen einer niedrigdimensionalen Darstellung (mit niedrigem Rang) direkt aus den Messungen beim Training eines tiefen Generators. 
Wir untersuchen eine mehrschichtige Verallgemeinerung des magnitudenbasierten Prunings.
Wir führen die Hypervolumen-Maximierung für das Training von GANs mit mehreren Diskriminatoren ein und zeigen Leistungsverbesserungen in Bezug auf die Qualität und Vielfalt der Beispiele. 
Ein neuer Stand der Technik auf Imagenet für mobile Umgebungen
Ein Rahmenwerk für Hochleistungs-Robotiksimulation und Algorithmenentwicklung.
Ein neuartiges Paradigma der unüberwachten Domänenanpassung - Durchführung der Anpassung ohne Zugriff auf die Quelldaten ("quellenfrei") und ohne Annahmen über die Quelle-Ziel-Kategorienlücke ("universell").
Lässt man einen Meta-Learner entscheiden, welche Aufgabe ein Agent in einer Multitasking-Umgebung trainieren soll, verbessert sich die Multitasking-Fähigkeit erheblich
Wir schlagen Answer-containing Sentence Generation (ASGen) vor, eine neuartige Pre-Training-Methode zur Erzeugung synthetischer Daten für das maschinelle Leseverstehen.
Soft-Quantisierungs-Ansatz zum Erlernen reiner Festkomma-Darstellungen tiefer neuronaler Netze
Wir präsentieren eine neuartige Netzwerkarchitektur für das Lernen kompakter und effizienter tiefer neuronaler Netze
Wir untersuchen zum ersten Mal die Angriffe auf die Mechanismen des Multiple Object Tracking durch maschinelles Lernen. 
Kopplung des halb-überwachten Lernens mit dem selbst-überwachten Lernen und explizite Modellierung der selbst-überwachten Aufgabe unter Berücksichtigung der halb-überwachten Aufgabe
Eine Zeigernetzwerk-Architektur für die Neubewertung von Artikeln, die aus Klickprotokollen gelernt wurde.
Stochastische Gradientenmethode mit Momentum generalisiert.
Lernen, einen Experten zu imitieren, wenn es keine optimalen Aktionen gibt, indem man ein dynamisches Modell lernt, während man die Umgebung erkundet.
Wir zeigen die Möglichkeit des Pruning, um ein kleines Teilnetz mit deutlich höherer Konvergenzrate als das vollständige Modell zu finden.
Wir schlagen einen auf Variationsinferenz basierenden Ansatz vor, um die Inferenz von entwirrten Latenzen zu fördern. Außerdem schlagen wir eine neue Metrik zur Quantifizierung der Entflechtung vor. 
Die von uns vorgeschlagene ASN charakterisiert den Einfluss verschiedener Aktionen auf andere Agenten mit Hilfe neuronaler Netze, die auf der Aktionssemantik zwischen ihnen basieren.
 Wir demonstrieren ein gated recurrent asynchronous spiking neural Network, das einer LSTM-Einheit entspricht.
Effiziente Videoklassifizierung mit einem Frame-basierten Conditional-Gating-Modul zur Auswahl der dominantesten Frames, gefolgt von einer zeitlichen Modellierung und einem Klassifikator.
Differenzierbare dynamische Programmierung über gestörte Eingangsgewichte mit Anwendung auf semi-supervised VAE
Ohne Lernen ist es unmöglich, die Entscheidungen eines maschinellen Lernmodells zu erklären.
Einfaches und effektives neuronales Graphennetz mit einer Mischung aus Zufallsschritten und Aufmerksamkeit
Wir stellen eine unbeaufsichtigte Deep Learning-Rekonstruktion für inverse Bildgebungsprobleme vor, die neuronale Netze mit modellbasierten Einschränkungen kombiniert.
Wir stellen eine diagnostische Aufgabe vor, die eine Variation des "few-shot learning" ist und stellen einen Datensatz dafür vor.
Wir stellen die Verwendung eines sekundären Kodierers/Dekodierers als Verlustfunktion vor, um das Training eines Zusammenfassers zu unterstützen.
Ein zeitlich konsistentes und modalitätsflexibles unüberwachtes Video-zu-Video-Übersetzungssystem, das auf selbstüberwachte Weise trainiert wird.
Argumentationsrahmen werden verwendet, um die Kausalität von Plänen/Modellen darzustellen, die für Erklärungen herangezogen werden sollen.
Wir schlagen einen generativen neuronalen Netzwerkansatz für zeitlich kohärente Punktansammlungen vor.
Wir stellen eine vereinheitlichende Sichtweise auf Black-Box-Angriffe als Gradientenschätzungsproblem vor und präsentieren dann einen Rahmen (basierend auf Bandits-Optimierung), um Prioritäten in die Gradientenschätzung zu integrieren, was zu einer signifikanten Leistungssteigerung führt.
Verbesserung der Kennzeichnungseffizienz durch Multitasking-Lernen bei auditiven Daten
In diesem Beitrag werden zwei Techniken vorgestellt, die eine Struktur auf hoher Ebene in die Erzeugung von prozeduralem Text aus einer Bildsequenz einbeziehen.
Wir haben einen neuartigen Gradientenschätzer nach der Stein-Methode eingeführt und mit anderen Methoden zum Erlernen impliziter Modelle für ungefähre Schlussfolgerungen und Bilderzeugung verglichen.
Ideale Methode, um während des CNN-Trainings Rauschen in die Eingangsdaten zu injizieren
Wir schlagen vor, tiefe Merkmalsverteilungen von Quell- und Zieldaten explizit als Gaußsche Mischverteilungen für die unüberwachte Domänenanpassung (UDA) zu modellieren und erzielen bei mehreren UDA-Aufgaben bessere Ergebnisse als State-of-the-Art-Methoden.
Wir erlernen eine aufgabenunabhängige Weltgraphenabstraktion der Umgebung und zeigen, wie deren Verwendung für die strukturierte Exploration die nachgelagerte aufgabenspezifische RL erheblich beschleunigen kann.
Wir stellen ein Computerprogramm durch eine Reihe von einfacheren Programmen dar und verwenden diese Darstellung zur Verbesserung von Programmsynthesetechniken.
Wie können wir künstliche Agenten entwickeln, die soziale Dilemmasituationen lösen (Situationen, in denen Individuen versucht sind, ihren Nutzen auf Kosten des Gesamtwohls zu erhöhen)?
Wir schlagen ein neuartiges verallgemeinertes transformationsbasiertes Gradientenmodell vor und entwickeln einen polynomialen Gradientenschätzer, der auf diesem Modell basiert.
Semi-überwachtes und Transfer-Lernen zur Klassifizierung von Paketflüssen über ein System kooperativer oder adversarischer neuronaler Blöcke
Unsere Arbeit stellt eine Kronecker-Faktorisierung von rekurrenten Gewichtsmatrizen für parameter-effiziente und gut konditionierte Recurrent Neural Networks vor.
Wir schlagen eine Methode zur Berechnung von adversarial robusten Repräsentationen in einer völlig unbeaufsichtigten Weise vor.
Wir schlagen einen neuen, auf Punkten basierenden Ansatz für strukturelles/kausales Lernen vor, der neuronale Netze und eine neue kontinuierliche, eingeschränkte Formulierung für dieses Problem nutzt
Das Papier analysiert das Problem des Entwurfs von Angriffen gegen mehrere Klassifizierer und stellt Algorithmen vor, die für lineare Klassifizierer optimal sind und die modernste Ergebnisse für Deep Learning liefern.
Wir präsentieren eine allgemeine Closed-Loop-Analyse für Markov-Potential-Spiele und zeigen, dass Deep Reinforcement Learning zum Erlernen eines approximativen Closed-Loop-Nash-Gleichgewichts verwendet werden kann.
Wir erhöhen die verlustfreie Komprimierung mit latenten Variablen und übertreffen damit bestehende Ansätze für ImageNet-Bilder in voller Größe.
Wir stellen die Hypothese auf, dass die Anfälligkeit von Bildmodellen für kleine Störungen eine natürliche Folge der hochdimensionalen Geometrie der Datenvielfalt ist. Wir untersuchen und beweisen diese Hypothese theoretisch für einen einfachen synthetischen Datensatz.
Wir stellen Variational Intrinsic Successor FeatuRes (VISR) vor, einen neuartigen Algorithmus, der kontrollierbare Merkmale erlernt, die genutzt werden können, um schnelle Aufgabeninferenz durch den Rahmen der Nachfolgermerkmale zu ermöglichen.
Eine Methode zur Anpassung von strukturierten Ausgaben durch das Erlernen von diskriminierenden Merkmalsdarstellungen auf Patch-Ebene
Ein neuartiger feinflicher Angriff, der reale Black-Box-Modelle des maschinellen Lernens ohne Übertragung direkt angreifen kann.
Differenzierter Datenschutz auf Benutzerebene für Recurrent Neural Network-Sprachmodelle ist mit einem ausreichend großen Datensatz möglich.
Das Training von Convnets mit gemischten Bildgrößen kann die Ergebnisse bei der Auswertung über mehrere Größen hinweg verbessern
In diesem Beitrag wird eine Methode zur Ausbildung generativer Recurrent Networks vorgestellt, die eine Vorausplanung ermöglicht. Wir lassen ein zweites RNN in umgekehrter Richtung laufen und machen eine weiche Einschränkung zwischen zeitgleichen Vorwärts- und Rückwärtszuständen.
Wir schlagen vor, den Generator eines GAN so zu strukturieren, dass er Objekte und ihre Beziehungen explizit berücksichtigt und Bilder durch Komposition erzeugt
Wir schaffen es, die Kommunikation mit egoistischen Agenten hervorzubringen, im Gegensatz zur aktuellen Ansicht in ML
Wir schlagen einen neuen Rahmen für datenabhängige DNN-Regularisierung vor, der verhindern kann, dass DNNs sich zufälligen Daten oder zufälligen Bezeichnungen zu sehr anpassen.
Multitasking-Lernen verbessert die Spracherkennung auf Wort- und Zeichenebene durch Interpolation der Präferenzvorlieben seiner Komponenten: Frequenz- und Wortlängenpräferenz.
Wir lernen ein neuronales Netz, das die Eingabeverteilung vereinheitlicht, was zu einer konkurrenzfähigen Indexierungsleistung im hochdimensionalen Raum führt
Dieses Papier bietet eine rigorose Studie des varianzreduzierten TD-Lernens und charakterisiert seinen Vorteil gegenüber dem einfachen TD-Lernen
Ein Multiflow-Netzwerk ist eine dynamische Architektur für die Domänenanpassung, die potenziell unterschiedliche Berechnungsgraphen pro Domäne erlernt, um sie auf eine gemeinsame Darstellung abzubilden, in der Inferenzen auf domänenunabhängige Weise durchgeführt werden können.
IMPACT hilft RL-Agenten, schneller zu trainieren, indem es die Trainingszeit an der Wall-Clock verkürzt und gleichzeitig die Effizienz der Beispiele erhöht.
In diesem Papier wird ein Färbungsschema für die Knoten-Disambiguierung in graphischen neuronalen Netzen vorgestellt, das auf Trennbarkeit basiert und sich als universelle MPNN-Erweiterung erweist.
Wenn wir Melodien als Bilder mit semantischen Einheiten darstellen, können wir sie mit einem DCGAN ohne rekurrente Komponenten erzeugen.
Wir analysieren und stellen das Phänomen der "Halluzinationen" in der NMT vor, auch falsche Übersetzungen ohne Bezug zum Quelltext, und schlagen Methoden vor, um deren Häufigkeit zu reduzieren.
Ein Bewertungsrahmen auf der Grundlage eines realen neuronalen Netzes für post-hoc-Erklärungsmethoden
Wir stellen eine von den Neurowissenschaften inspirierte Methode vor, die auf neuronalen Netzen für die Suche im latenten Raum basiert
Durch die Einführung des Begriffs eines optimalen Repräsentationsraums liefern wir ein theoretisches Argument und eine experimentelle Validierung, dass ein unbeaufsichtigtes Modell für Sätze sowohl bei überwachten Ähnlichkeitsaufgaben als auch bei unbeaufsichtigten Transferaufgaben gut abschneiden kann.
Ein Lückentext-Datensatz, der von Lehrern zur Bewertung von Sprachkenntnissen entwickelt wurde
Künstliche neuronale Netze, die mit Gradientenabstieg trainiert werden, sind in der Lage, sowohl realistische neuronale Aktivität als auch die anatomische Organisation eines biologischen Kreises zu rekapitulieren.
Prune und ReLU im Winograd-Bereich für effiziente Convolutional Neural Networks
Ein neuer Algorithmus zum Trainieren tiefer neuronaler Netze. Getestet an Optimierungsfunktionen und MNIST.
Wir stellen Flipout vor, eine effiziente Methode zur Dekorrelation der Gradienten, die durch stochastische neuronale Netzgewichte innerhalb eines Mini-Batches berechnet werden, indem implizit pseudo-unabhängige Gewichtsstörungen für jedes Beispiel gesammelt werden.
Ein neues Modell eines Latently Invertible Autoencoders wird vorgeschlagen, um das Problem der variationalen Inferenz in VAE mit dem invertible Netzwerk und zweistufige adversarial Ausbildung zu lösen.
Wir stellen das PHP-Modell für die hierarchische Darstellung neuronaler Programme und einen Algorithmus zum Lernen von PHPs aus einer Mischung von starker und schwacher Überwachung vor.
Wir schlagen eine Methode zum Wissenstransfer zwischen verwandten RL-Aufgaben unter Verwendung visueller Mappings vor und demonstrieren ihre Effektivität an visuellen Varianten des Atari Breakout-Spiels und verschiedenen Levels von Road Fighter, einem Nintendo-Autofahrspiel.
Richtig angewandte adaptive Gradientenmethoden sind nicht mit einem Generalisierungsnachteil verbunden. 
Wir stellen ein Modell vor, das ausgehend von wenigen Beobachtungen schnell verallgemeinert, indem es überraschende Informationen speichert und zu jedem Zeitpunkt die wichtigsten Daten berücksichtigt.
Wir entwickeln einen durchgängig trainierbaren Ansatz für das Überfliegen, Wiederlesen und frühzeitige Beenden von Klassifikationsaufgaben. 
Wir betrachten Exploration in RL als ein Problem der Anpassung einer Randverteilung über Zustände.
Wir stellen G-HexaConv vor, ein gruppenäquivariantes Convolutional Neural Network auf hexagonalen Gittern.
Vorschlag eines neuartigen Ansatzes zur Objektlokalisierung (Erkennung) auf der Grundlage der Interpretation des tiefen CNN unter Verwendung der internen Darstellung und der Gedanken des Netzwerks
Trellis-Netzwerke sind eine neue Architektur zur Sequenzmodellierung, die rekurrente und konvolutionale Modelle miteinander verbindet und einen neuen Stand der Technik für die Sprachmodellierung auf Wort- und Zeichenebene darstellt.
Eine hohe Objekterkennungsgenauigkeit kann durch das Training von domänenspezifischen kompakten Modellen erreicht werden, und das Training kann sehr kurz sein.
Wir vergleichen tiefe modellbasierte und modellfreie RL-Algorithmen, indem wir die Approximierbarkeit von $Q$-Funktionen, Richtlinien und Dynamik durch neuronale Netze untersuchen. 
Wir stellen einen neuartigen Ansatz für physikalische Schlussfolgerungen mit gesundem Menschenverstand vor, der lernt, Objekte zu erkennen und ihre physikalischen Interaktionen aus visuellen Rohbildern zu modellieren, und zwar auf rein unüberwachte Weise
Bei vielen einfachen Input-Output-Zuordnungen ist eine sehr starke Tendenz zu einfachen Outputs zu beobachten. Die Parameter-Funktions Zuordnung von tiefen Netzen ist auf die gleiche Weise verzerrt.
In diesem Beitrag schlagen wir Imitative Modelle vor, um die Vorteile von IL und zielgerichteter Planung zu kombinieren: probabilistische Vorhersagemodelle für wünschenswertes Verhalten, die in der Lage sind, interpretierbare expertenähnliche Trajektorien zu planen, um bestimmte Ziele zu erreichen.
Neuartige Architektur eines speicherbasierten Aufmerksamkeitsmechanismus für Multi-Agenten-Kommunikation.
Diese Arbeit erzwingt eine Hamilton-Dynamik mit Steuerung, um Systemmodelle aus eingebetteten Positions- und Geschwindigkeitsdaten zu lernen, und nutzt diese physikalisch konsistente Dynamik, um eine modellbasierte Steuerung durch Energieformung zu synthetisieren.
Wir untersuchen die internen Gründe für unsere Beobachtungen, nämlich die abnehmenden Auswirkungen der bekannten Hyperparameter-Optimierungsmethoden auf das föderierte Lernen aus dezentralen Nicht-IID-Daten.
Ein neuartiger Nachahmungsangriff zur Täuschung von Modellen des maschinellen Lernens.
Training mit großen Batches unter Verwendung von adversarialem Training und Informationen zweiter Ordnung
erstes tiefes neuronales Netz zur Modellierung des egozentrischen räumlichen Gedächtnisses, inspiriert durch neurophysiologische Entdeckungen von Navigationszellen im Säugetiergehirn
Wir beweisen die Generalisierung von DNNs durch Hinzufügen eines Lipschitz-Regularisierungsterms zum Trainingsverlust. Wir lösen eine Frage, die in Zhang et al. (2016) gestellt wurde.
Wir trainieren breite Residualnetzwerke, die sofort eingesetzt werden können, indem wir nur ein einziges Bit für jedes Convolutional Gewicht verwenden, und zwar mit deutlich besserer Genauigkeit als bisherige Methoden.
Immersive Visualisierung der klassischen nicht-euklidischen Räume mit Echtzeit-Raytracing.
Wir schlagen den Set-Autoencoder vor, ein Modell zum unbeaufsichtigten Lernen von Repräsentationen für Mengen von Elementen.
Wir schlagen eine autonome Methode für sicheres und effizientes Reinforcement Learning vor, die gleichzeitig eine Vorwärts- und eine Rückwärtsstrategie erlernt, wobei die Rückwärtsstrategie die Umgebung für einen nachfolgenden Versuch zurücksetzt.
Wir präsentieren Beweise dafür, dass LMs den gesunden Menschenverstand erfassen, mit Spitzenergebnissen sowohl bei der Winograd Schema Challenge als auch beim Commonsense Knowledge Mining.
Ein Online-Algorithmus für die kostenbewusste Erfassung und Vorhersage von Merkmalen
Wir argumentieren, dass Covolutional Networks als Standardausgangspunkt für Sequenzmodellierungsaufgaben betrachtet werden sollten.
Training von DNNs, um Black-Box-Funktionen mit Zwischenetiketten zu verbinden, unter Verwendung eines Schätzer-Teilnetzes, das nach dem Training durch die Black-Box ersetzt werden kann
Wir schlagen einen dualen akteurskritischen Algorithmus vor, der auf prinzipielle Weise aus der dualen Lagrangeschen Form der Bellman-Optimalitätsgleichung abgeleitet ist. Der Algorithmus erreicht in mehreren Benchmarks die beste Leistung.
Eine robuste Domänenanpassung durch den Einsatz eines aufgabenspezifischen Verlustes beim zyklischen adversen Lernen
Optimierung der Politik durch Verwendung vergangener guter Rollouts des Agenten; Lernen von geformten Belohnungen über Divergenzminimierung; SVPG mit JS-Kernel für populationsbasierte Exploration.
Wir untersuchen die Funktionsweise von Autoencodern in einer einfachen Umgebung und empfehlen neue Strategien für deren Regularisierung, um eine bessere Generalisierung mit latenter Interpolation für die Bildsynthese zu erreichen. 
Wir stellen eine einfache Idee vor, die es ermöglicht, einen Sprecher in einer bestimmten Sprache aufzunehmen und seine Stimme in anderen Sprachen zu synthetisieren, die er vielleicht nicht einmal kennt.
In dieser Arbeit haben wir einen modellfreien, richtlinienunabhängigen IL-Algorithmus für kontinuierliche Kontrolle vorgeschlagen. Experimentelle Ergebnisse zeigten, dass unser Algorithmus wettbewerbsfähige Ergebnisse mit GAIL erzielt und gleichzeitig die Interaktionen mit der Umgebung deutlich reduziert.
Ein System zum Umschreiben von Text, der von mehreren kontrollierbaren Attributen abhängt
Wir entwickeln einen neuen Optimierungsansatz für einfache ReLU-basierte RNN, der ein langes Kurzzeitgedächtnis und die Identifikation beliebiger nichtlinearer dynamischer Systeme mit sehr unterschiedlichen Zeitskalen ermöglicht.
Vorschlag eines verbesserten Rahmens für WGANs und Nachweis seiner besseren Leistung in Theorie und Praxis.
Operationen im latenten Raum des GAN können zu einer Ungleichverteilung im Vergleich zur Trainingsverteilung führen. 
Ein neuer Weg zum Erlernen der semantischen Programmeinbettung
Wir schlagen eine Erweiterung der Batch-Normalisierung vor, zeigen eine erstmalige Konvergenzanalyse für diese Erweiterung und zeigen in numerischen Experimenten, dass sie eine bessere Leistung hat als die ursprüngliche Batch-Normalisierung.
Wir schlagen einen Rahmen für die Modifizierung der latenten Raumoperationen vor, so dass die Verteilungsfehlanpassung zwischen den resultierenden Ausgaben und der vorherigen Verteilung, auf der das generative Modell trainiert wurde, vollständig beseitigt wird.
Dieses Papier bietet einen Multi-Stream-End-to-End-Ansatz, um einheitliche Einbettungen für Frage-Antwort-Paare in Dialogsystemen zu lernen, indem kontextuelle, syntaktische, semantische und externe Informationen zusammen genutzt werden.
Techniken zur Kombination verallgemeinerter Strategien mit Suchalgorithmen, um die Stärken und Schwächen beider Verfahren bei der Lösung probabilistischer Planungsprobleme zu nutzen
Wir erreichen den neuesten Stand der Technik in Bezug auf die Robustheit gegenüber Datenverschiebungen und behalten die Kalibrierung auch dann bei, wenn die Genauigkeit abnimmt.
Wir extrahieren automatisch Informationen über den Fingersatz aus Videos von Klavierspielen, um sie in automatischen Modellen zur Vorhersage des Fingersatzes zu verwenden.
SOTA auf unüberwachter Domänenanpassung durch Ausnutzung der Cluster-Annahme.
Generative Graphenmodelle auf der Grundlage der Verallgemeinerung der Nachrichtenübermittlung auf kontinuierliche Zeit unter Verwendung gewöhnlicher Differentialgleichungen 
Wir zeigen, dass mehrere Behauptungen der Theorie des Informationsengpasses beim Deep Learning im allgemeinen Fall nicht zutreffen.
Neuronale Netze haben von Haus aus große Gradienten; das macht sie anfällig für negative Einflüsse.
Wir stellen die Amortisierte Proximale Optimierung (APO) vor, eine Methode zur Online-Anpassung einer Vielzahl von Optimierungshyperparametern während des Trainings, einschließlich Lernraten, Dämpfungskoeffizienten und Varianzexponenten des Gradienten.
Wir zeigen, dass die hervorstechenden Dimensionen von Wortvektoren als semantische Merkmale interpretiert werden können, ohne dass irgendwelche Einschränkungen oder Nachbearbeitungen erforderlich sind. 
Strategie zur Reparatur beschädigter neuronaler Netze
Automatische Fragengenerierung aus Absätzen unter Verwendung hierarchischer Modelle
Die Abfrage eines neuronalen Black-Box-Netzes offenbart viele Informationen über dieses Netz; wir schlagen neuartige "Metamodelle" vor, um effektiv Informationen aus einer Black-Box zu extrahieren.
Verwendung eines überwachten latenten Variablenmodellierungsrahmens zur Bestimmung der Belohnung bei einer Aufgabe des inversen Reinforcement Learnings
Diese Arbeit kombiniert die Monte-Carlo-Baumsuche mit der lokalen 2-Opt-Suche in einem variablen Nachbarschaftsmodus, um das TSP effektiv zu lösen.
Anwendung der Programmsynthese auf die Aufgaben der Bildvervollständigung und -erzeugung innerhalb eines Deep Learning Frameworks
In diesem Beitrag wird ein Berechnungsmodell für die effiziente Anpassung der menschlichen Haltungskontrolle vorgestellt, das auf hierarchischen Erfassungsfunktionen mit bekannten Merkmalen basiert. 
Wir untersuchen das Problem der kontinuierlichen Kontrolle von Agenten in tiefen RL mit gegnerischen Angriffen und schlagen einen zweistufigen Algorithmus vor, der auf gelernter Modelldynamik basiert. 
Wir untersuchen die Spärlichkeit verursachende Verzerrung von tiefen Modellen, die durch ihre Lerndynamik verursacht wird.
Parametrische adversarische Divergenzen definieren implizit aussagekräftigere Aufgabenverluste für die generative Modellierung. Wir ziehen Parallelen zur strukturierten Vorhersage, um die Eigenschaften dieser Divergenzen und ihre Fähigkeit zur Codierung der interessierenden Aufgabe zu untersuchen.
Wir bieten einen rigorosen Vergleich verschiedener Graph Neural Networks für die Graphklassifizierung.
Automatisches Lernen von Datenerweiterungen unter Verwendung einer GAN-basierten Architektur zur Verbesserung eines Bildklassifizierers
Neue Klasse von Autoencodern mit pseudo-invertierbarer Architektur
Wir nutzen ein Inversionsschema für beliebige tiefe neuronale Netze, um einen neuen halbüberwachten Lernrahmen zu entwickeln, der auf viele Topologien anwendbar ist.
Vergleich von Siamesischen Neuronalen Netzen, GANs und VAT für das Few-Shot Learning. 
Wir schlagen eine leichtgewichtige Verbesserung der Aufmerksamkeit und eine neuronale Architektur, FusionNet, vor, um SotA auf SQuAD und adversarial SQuAD zu erreichen.
Adversarial trainiertes hierarchisches generatives Modell mit robuster und semantisch gelernter latenter Repräsentation.
Wir stellen fest, dass numerische PDE-Löser als Markov-Desicion-Prozesse betrachtet werden können, und schlagen vor, Reinforcement Learning zur Lösung skalarer 1D-Erhaltungsgesetze einzusetzen
Wir stellen eine neuronale Rendering-Architektur vor, die VAEs dabei hilft, entkoppelte latente Repräsentationen zu lernen.
Sensorische Defizite in frühen Trainingsphasen können sowohl in künstlichen als auch in neuronalen Netzen zu irreversiblen Leistungseinbußen führen, was auf Informationsphänomene als gemeinsame Ursache hindeutet und auf die Bedeutung des anfänglichen Einschwingens und Vergessens hinweist.
Wir schlagen eine Methode zum inkrementellen Lernen eines Einbettungsraums über die Domäne der Netzwerkarchitekturen vor, um eine sorgfältige Auswahl von Architekturen für die Evaluierung während der komprimierten Architektursuche zu ermöglichen.
Verbesserung der Leistung eines RL-Agenten in einem kontinuierlichen Handlungs- und Zustandsraum durch priorisierte Erfahrungswiedergabe und Parameterrauschen.
zeigen, dass mehrkanalige Aufmerksamkeitsgewichte semantische Merkmale enthalten, um die Aufgabe der Inferenz natürlicher Sprache zu lösen.
Wir approximieren Determinantal Point Processes mit neuronalen Netzen und begründen unser Modell theoretisch und empirisch.
In diesem Beitrag wird eine Netzwerkarchitektur zur Lösung des Structure-from-Motion (SfM)-Problems mittels Feature-Bundle-Anpassung (BA) vorgestellt.
Wir zeigen, dass das Hinzufügen einer Beschränkung zu TD-Updates das Lernen stabilisiert und Deep Q-learning ohne ein Zielnetzwerk ermöglicht
Wir schlagen DuoRC vor, einen neuartigen Datensatz für das Leseverständnis (RC), der 186.089 von Menschen erzeugte QA-Paare enthält, die aus einer Sammlung von 7680 Paaren paralleler Filmhandlungen erstellt wurden, und führen eine RC-Aufgabe ein, bei der eine Version der Handlung gelesen und Fragen beantwortet werden müssen, die aus der anderen Version erstellt wurden; dies erfordert komplexes logisches Denken und ein tieferes Sprachverständnis, um die geringe lexikalische Überschneidung zwischen der Handlung und der Frage zu überwinden.
Eine modellbasierte Planungskomponente verbessert RL-basiertes semantisches Parsing auf WikiTableQuestions.
Eine neue Art der Quantisierung der Aktivierung eines tiefen neuronalen Netzes durch parametrisiertes Clipping, das die Quantisierungsskala durch stochastischen Gradientenabstieg optimiert.
Wir zeigen, dass Pruning-Methoden, die eine größere Instabilität in den Verlust einbringen, auch zu einer verbesserten Generalisierung führen, und untersuchen die Mechanismen, die diesem Effekt zugrunde liegen.
Weniger biologisch unplausible tiefe neuronale Netze, die ohne Gewichtstransport trainiert werden, können schwerer zu täuschen sein.
Ein generatives Modell zur Reaktionsvorhersage, das die mechanistischen Elektronenschritte einer Reaktion direkt aus den Rohdaten der Reaktion lernt.
Die Möglichkeit, "Ich weiß es nicht" zu sagen, kann die Fairness eines Klassifizierers verbessern, ohne dass die Genauigkeit zu sehr darunter leidet, und diese Verbesserung wird noch effektiver, wenn der Klassifizierer Einblick in die nachgelagerte Entscheidungsfindung hat.
Ein Ansatz zur Durchführung der HTN-Planung unter Verwendung externer Prozeduren zur Auswertung von Prädikaten zur Laufzeit (semantische Anhänge).
Max-gepoolte Wortvektoren mit Fuzzy-Jacard-Set-Ähnlichkeit sind eine äußerst wettbewerbsfähige Grundlage für semantische Ähnlichkeit; wir schlagen eine einfache dynamische Variante vor, die noch besser abschneidet.
Das übergeordnete Ziel dieser Arbeit ist es, durch die Verwendung von f-Divergenzen eine stichprobeneffiziente Nachahmung von Expertendemonstrationen zu ermöglichen, sowohl mit als auch ohne die Bereitstellung von Experten-Handlungsbezeichnungen.
Mit einem kognitiven Brain-Machine-Interface zeigen wir einen direkten Zusammenhang zwischen Aufmerksamkeitseffekten auf die Wahrnehmungsgenauigkeit und der neuronalen Verstärkung der EEG-SSVEP-Leistung im menschlichen Gehirn.
Ein allgemeiner und einfach zu verwendender Rahmen, der die Widerstandsfähigkeit von tiefen Klassifikationsmodellen durch Einbettungsregularisierung verbessert.
Wir schlagen einen auf Matrix-Vervollständigung basierenden Aufgaben-Clustering-Algorithmus für tiefes Multitasking und "few-shot learning" in Umgebungen mit einer großen Anzahl unterschiedlicher Aufgaben vor.
Dieser Ansatz überwindet Skalierbarkeitsprobleme und impliziert neuartige mathematische Verbindungen zwischen Quantenvielteilchenphysik, Quanteninformationstheorie und maschinellem Lernen.
Wir trainieren eine Kombination von neuronalen Netzen, um optimale Flugbahnen für komplexe physikalische Systeme vorherzusagen.
Wir bieten eine PAC-Bayes-basierte Generalisierungsgarantie für unkomprimierte, deterministische tiefe Netzwerke, indem wir die Rauschresistenz des Netzwerks auf den Trainingsdaten auf die Testdaten verallgemeinern.
Wir beweisen, dass es für eine große Klasse von Funktionen f ein intervallzertifiziertes robustes Netzwerk gibt, das f bis zu einer beliebigen Genauigkeit approximiert.
Dies ist eine Arbeit, die darauf abzielt, alle bestehenden Pruning- und Mimicmethoden zu verbessern.
Wir führen ein zusätzliches datenabhängiges Gauß'sches Prioritätsziel ein, um das derzeitige MLE-Training zu ergänzen, das dazu dient, das Vorwissen in den "ground-truth"-Daten zu erfassen.
Wir schlagen einen interaktiven Ansatz für die Klassifizierung von natürlichsprachlichen Anfragen vor, indem wir den Benutzer um zusätzliche Informationen bitten,  unter der Nutzung des Informationgains und eines Reinforcement Learning Policy Controllers.
Convolutional Autoencoders verallgemeinert auf Mesh-Oberflächen für die Kodierung und Rekonstruktion von extremen 3D-Gesichtsausdrücken.
Jiffy ist ein Convolutional Ansatz zum Erlernen einer Abstandsmetrik für multivariate Zeitreihen, der bestehende Methoden in Bezug auf die Klassifizierungsgenauigkeit der nächsten Nachbarn übertrifft.
Eine erweiterbare modulare Architektur wird für die Entwicklung einer Vielzahl von Agentenverhaltensweisen im DQN vorgeschlagen.
Wir isolieren einen Faktor der RL-Verallgemeinerung, indem wir den Fall analysieren, in dem der Agent nur auf die Beobachtungen überpasst. Wir zeigen, dass architektonische implizite Regularisierungen in diesem Regime auftreten.
In diesem Papier schlagen wir ein neuartiges neuronales Sprachmodell vor, das so genannte Parsing-Reading-Predict Networks (PRPN), das gleichzeitig die syntaktische Struktur aus unannotierten Sätzen ableiten und die abgeleitete Struktur nutzen kann, um ein besseres Sprachmodell zu lernen.
Wir haben einen umfassenden Ansatz für unüberwachtes Einbettungslernen auf der Grundlage des AND-Algorithmus vorgeschlagen.
Wir schlagen eine schwach überwachte Lernmethode für die Klassifizierung und Lokalisierung von Krebserkrankungen in extrem hochauflösenden histopathologischen Ganzbildaufnahmen vor, die nur bildweite Markierungen verwendet.
 Wir schlagen eine neue Methode zur Verwendung von Ontologie-Informationen vor, um die Leistung bei massiven Mehr-Label-Prognosen/Klassifizierungsproblemen zu verbessern.
Zur Annäherung an die Wasserstein-Distanz wenden wir auf die projizierten Beispiele eine greedy Zuweisung an, anstatt zu sortieren
In diesem Beitrag werden die Wechselwirkungen zwischen den Modellen für schnelles Lernen und langsame Vorhersage untersucht und es wird gezeigt, wie solche Wechselwirkungen die Fähigkeit von Maschinen verbessern können, die Probleme des Joint Lifelong Lernens und des Few-Shot Lernens zu lösen.
Die erste prinzipielle Gewichtsinitialisierungsmethode für Hypernetze
Ein neuartiges Bayes'sches Deep-Learning-Framework, das hierarchische semantische und visuelle Konzepte erfasst und miteinander in Beziehung setzt und bei einer Vielzahl von Bild- und Textmodellierungs- und -generierungsaufgaben gute Ergebnisse erzielt.
Diese Arbeit führt partielles Grounding ein, um das Problem zu lösen, das auftritt, wenn der vollständige Grounding-Prozess, d. h. die Übersetzung einer PDDL-Eingabeaufgabe in eine geerdete Darstellung wie STRIPS, aufgrund von Speicher- oder Zeitbeschränkungen nicht durchführbar ist.
Einführung der Antwort-Charakterisierungsmethode zur Interpretation der Zelldynamik in gelernten Long Short-Term Memory Netzwerken (LSTM). 
Unseres Wissens ist dies die erste Studie, die zeigt, wie neuronale Repräsentationen des Raums, einschließlich gitterartiger Zellen und Grenzzellen, wie sie im Gehirn beobachtet werden, aus dem Training eines rekurrenten neuronalen Netzes zur Durchführung von Navigationsaufgaben hervorgehen können.
Wir vergleichen die Leistung eines Spektrogramm-basierten Modells mit einem Modell, das durchgängig im Wellenformbereich trainiert wurde.
Wir bieten eine skalierbare Lösung für die Multi-Agenten-Evaluierung mit linearer Zeit- und Speicherkomplexität in Bezug auf die Anzahl der Agenten
Wir schlagen einen neuartigen halb-überwachten Lernansatz mit SOTA-Leistung zur Bekämpfung des Lernens mit verrauschten Etiketten vor.
Wir entwickeln eine Trainingsmethode für Bayes'sche neuronale Netze, die eine viel stärkere Verteidigung gegen White-Box-Angriffe bietet.
Wirksame Modellvergiftungsangriffe auf das föderierte Lernen, die eine gezielte Fehlklassifizierung der gewünschten Eingaben mit hoher Zuverlässigkeit bewirken können
 In dieser Arbeit stellen wir die Hypothese auf, dass oberflächlich gestörte Datenpunkte nicht nur der gleichen Klasse zugeordnet werden sollten, sondern auch der gleichen Repräsentation zugeordnet werden sollten.
Harmonisch akustisches Modell
Den durch die Abhängigkeit der Klassen von den Domänen verursachten Kompromiss durch die Verbesserung der kontradiktorischen Netze der Domänen anzugehen
Wir schlagen FVD vor: eine neue Metrik für generative Videomodelle auf der Grundlage von FID. Eine groß angelegte Humanstudie bestätigt, dass FVD gut mit der qualitativen menschlichen Beurteilung von generierten Videos korreliert.
Eine dem menschlichen Gehirn nachempfundene duale Gedächtnisarchitektur zum Erlernen von sequentiell eingehenden Aufgaben bei gleichzeitiger Vermeidung von katastrophalem Vergessen.
Sprachmodellierung für lebenslanges Sprachenlernen.
Wir nutzen Ideen aus der Quanteninformatik, um Worteinbettungen vorzuschlagen, die mit viel weniger trainierbaren Parametern auskommen.
Wir trainieren neuronale Netzwerk-Agenten, um eine Sprache mit kompositorischen Eigenschaften aus rohen Pixel-Eingaben zu entwickeln.
Wir lernen eine Diversity-Sampling-Funktion mit DPPs, um einen vielfältigen Satz von Proben aus einem generativen Modell zu erhalten.
Repräsentationen von Sprachmodellen schneiden bei der Vorhersage von syntaktischen Hilfsmitteln durchweg besser ab als Übersetzungsencoder.
Wir schlagen ein auf Surrogate basierendes beschränktes Langevin-Sampling vor, das im Design von nano-porigen Materialkonfigurationen Anwendung findet.
Verbesserung hierarchischer Einbettungsmodelle durch Kernelglättung
Augmented-Bootstrapping-Ansatz, der Informationen aus einer Referenzmenge mit iterativen Verfeinerungen von Soft-Labels kombiniert, um die Erkennung von Namensbestandteilen aus biomedizinischer Literatur zu verbessern.
Wir erweitern die Quanten-SVMs auf eine halbüberwachte Umgebung, um mit dem wahrscheinlichen Problem vieler fehlender Klassenbezeichnungen in großen Datensätzen umzugehen.
Dieses Werk schlägt eine Brücke zwischen tiefen Netzwerkarchitekturen und numerischen (stochastischen) Differentialgleichungen. Diese neue Perspektive ermöglicht neue Designs von effektiveren tiefen neuronalen Netzen.
CNN und LSTM, um markup-ähnlichen Code zu erzeugen, der Bilder von grafischen Benutzeroberflächen beschreibt.
Wir zeigen, dass hyperbolische Einbettungen für anspruchsvolle Computer-Vision-Aufgaben nützlich sind, insbesondere für die Klassifizierung von wenigen Bildern.
Wir stellen eine Methode zum Erlernen interpretierbarer Repräsentationen von Zeitreihen vor, die auf Ideen von Variations-Autoencodern, selbstorganisierenden Karten und probabilistischen Modellen beruht.
Convolutional Architecture zum Lernen datenabhängiger Gewichte für die autoregressive Vorhersage von Zeitreihen.
Wir stellen eine neuartige Interpretation von MixUp vor, die einer Klasse angehört, die in hohem Maße mit dem gegnerischen Training vergleichbar ist, und auf dieser Grundlage führen wir eine einfache Verallgemeinerung ein, die MixUp übertrifft
Behandlung von Unsicherheiten in der visuellen Wahrnehmung für die Planerkennung
Differenzierbarer Multi-Hop-Zugriff auf eine textuelle Wissensbasis mit indizierten kontextuellen Darstellungen
Skalierbarer Allzweck-Faktorisierungsalgorithmus - hilft auch, das Kaltstartproblem zu umgehen.
Wir stellen ein System zur Erstellung von Tutorials mit gemischten Medien vor, das die Erstellung von Videos, Bildern, Text und dynamischen Anweisungen an Ort und Stelle rationalisiert.
Wir konnten nachweisen, dass die Verwendung von klinischen Notizen in Verbindung mit ICU-Instrumenten die Leistung bei ICU-Management-Benchmark-Aufgaben verbessert.
Wir schlagen einen ereignisbasierten Policy-Gradienten vor, um den Anführer zu trainieren, und einen Action-Abstraction-Policy-Gradienten, um die Follower im Anführer-Follower-Markov-Spiel zu trainieren.
Wir nutzen kulturelle Übertragung, um Kompositionalität in Sprachen zu fördern, die aus der Interaktion zwischen neuronalen Agenten entstehen.
Wir schlagen eine SVD-basierte Methode vor, um die lokale Dimension der Aktivierungsverteiler in tiefen neuronalen Netzen zu untersuchen.
Die Inferenz in großen Transformatoren ist aufgrund der Self-Attention in mehreren Schichten teuer. Wir zeigen, dass eine einfache Zerlegungstechnik zu einem schnelleren Modell mit geringem Speicherbedarf führen kann, das genauso genau ist wie die ursprünglichen Modelle.
Eine Deep Learning-Anpassung der Randomized Least Squares Value Iteration
Die Parameter eines trainierten neuronalen Netzwerks können vertauscht werden, um ein völlig separates Modell für eine andere Aufgabe zu erstellen, was die Einbettung von trojanischen Netwerken in ein anderes Netz ermöglicht.
Wir stellen ein alternatives GAN-Design vor, das auf zufälligen Routen im Generator basiert und als Werkzeug für die Interpretierbarkeit generativer Modelle dienen kann.
Wir präsentieren einen theoretischen und experimentellen Rahmen für die Definition, das Verständnis und das Erreichen von Generalisierung und damit von Robustheit beim Deep Learning, indem wir auf die algorithmische Informationstheorie und die Code-Theorie zurückgreifen.
Wir stellen die Entdeckung kausaler Strukturen als eine Bayes'sche Modellauswahl dar, die es uns ermöglicht, zwischen Markov-äquivalenten Graphen zu unterscheiden, um den einzigartigen kausalen Graphen zu identifizieren.
Untere Schranke für Compressed Sensing mit generativen Modellen, die den bekannten oberen Schranken entspricht
In diesem Beitrag wird eine empirische Analyse der Rolle verschiedener Arten von Bildrepräsentationen vorgestellt und die Eigenschaften dieser Repräsentationen für die Aufgabe der Bildbeschriftung untersucht.
Wir entwerfen inkrementelle Sequenz-zu-Aktion-Parser für Text-zu-SQL-Aufgaben und erzielen SOTA-Ergebnisse. Wir verbessern sie weiter, indem wir nicht-deterministische Orakel verwenden, um mehrere korrekte Aktionssequenzen zu ermöglichen. 
Ein Ansatz, der die Suche nach neuronalen Architekturen um das 10-fache beschleunigt und dabei 100-mal weniger Rechenressourcen benötigt.
Wir schlagen eine neue DNN-Architektur für Deep Learning auf tabellarischen Daten vor
Ein Rahmenwerk, das die Online-Verfeinerung von Pseudo-Etiketten mit einem neuartigen Softmax-Triplet-Verlust für die unüberwachte Domänenanpassung bei der Re-Identifizierung von Personen durchführt.
Wir stellen den ersten Ansatz vor, um die Robustheit neuronaler Netze gegen geräuschbasierte Störungen im Audiobereich zu zertifizieren.
In dieser Arbeit wird eine Methode zur Erzeugung und Verwendung von Ensembles vorgestellt, um verrauschte Beispiele in Gegenwart von Annotationsrauschen effektiv zu identifizieren. 
Ein datengesteuerter Lernalgorithmus, der auf der Abrollung der Alternierenden Minimierungsoptimierung für die Wiederherstellung spärlicher Graphen basiert.
Wir haben einen doppelten neuronalen Rahmen vorgeschlagen, um ein groß angelegtes unvollkommene Informationen Spiel zu lösen. 
Wir präsentieren den ersten Nachweis, dass ein neuronales Netz für Wahrnehmungsaufgaben für jede Eingabe von Interesse eine korrekte Ausgabe innerhalb einer bestimmten Toleranz erzeugt. 
Wir untersuchen Annäherungen an die Ratenverzerrung, um tiefe generative Modelle zu evaluieren, und zeigen, dass Ratenverzerrungskurven mehr Erkenntnisse über das Modell liefern als die Log-Likelihood allein, während sie ungefähr die gleichen Berechnungskosten erfordern.
Ein modellbasierter Meta-RL-Algorithmus, der es einem realen Roboter ermöglicht, sich in dynamischen Umgebungen online anzupassen
Das Papier analysiert den latenten Raum, der durch modellfreie Ansätze in einem Miniaturspiel mit unvollständiger Information erlernt wird, trainiert ein Vorwärtsmodell im latenten Raum und wendet es auf die Monte-Carlo-Baumsuche an, was zu einer positiven Leistung führt.
Wir analysieren die Ausdruckskraft der in DenseNets verwendeten Verbindungen über Tensorzerlegungen.
Wir nutzen implizites menschliches Feedback (über Fehlerpotentiale, EEG), um das Training eines DRL-Algorithmus auf praktische Weise zu beschleunigen und zu optimieren.
TCN für multimodales halb-überwachtes Lernen + Ablationsstudie seiner Mechanismen + Interpretationen latenter Repräsentationen
Anpassung von Adam, Amsgrad, Adagrad an Riemannsche Mannigfaltigkeiten. 
Verteidigung gegen physikalisch realisierbare Angriffe auf die Bildklassifizierung
Hebbsche plastische Gewichte können sich wie ein komprimierter episodischer Gedächtnisspeicher in neuronalen Netzwerken verhalten und in Kombination mit aufgabenspezifischer synaptischer Konsolidierung die Fähigkeit verbessern, katastrophales Vergessen beim kontinuierlichen Lernen zu lindern.
Diese Arbeit beweist, dass dünne neuronale Netze bestimmte Funktionen nicht annähern können, unabhängig davon, wie tief sie sind.
Wir stellen das Continuous Logic Network (CLN) vor, eine neuartige neuronale Architektur zum automatischen Lernen von Schleifeninvarianten und allgemeinen SMT-Formeln.
Unsere Ergebnisse werfen ein Licht auf die Verhinderung des Fortschreitens von Krebs
Wir erweitern autoregressive Flüsse und RealNVP auf diskrete Daten.
Eine geräuschresistente Deep-Learning-Architektur.
Wir lernen neuronale Einbettungen von Graphen im hyperbolischen statt im euklidischen Raum
Ideen für zukünftige ICKEPS
Wir generieren Wikipedia-Artikel abstrakt auf der Grundlage von Texten aus Quellendokumenten.
Ein Algorithmus zur Vereinheitlichung von SGD und Adam und empirische Untersuchung seiner Leistung
Lernen hierarchischer Strategien aus unsegmentierten Demonstrationen unter Verwendung gerichteter Informationen
Traditionelle Bildverarbeitungsalgorithmen werden mit Convolutional Neural Networks, einem neuen neuronalen Netz, kombiniert.
Wir haben eine spezielle Backpropagation-Methode über einen geeigneten spektralen Subgradienten vorgeschlagen, um den deterministischen Punktprozess in den Deep Learning-Rahmen zu integrieren.
Wir entwickeln einen neuartigen Cluster-to-Cluster-Rahmen für das NMT-Training, der die Vielfalt der Ausgangs- und Zielsprache besser verstehen kann.
Ein effizientes differenzierbares ILP-Modell, das logische Regeln erster Ordnung erlernt, die die Daten erklären können.
Wir stellen eine neue Methode zur Synthese von Gegenbeispielen vor, die in der physischen Welt robust sind, und verwenden sie zur Herstellung der ersten 3D-Gegenobjekte.
Lernen, wie man eine Menge permutiert und dann die permutierte Menge mit RNN kodiert, um eine Mengendarstellung zu erhalten.
Verwenden Sie Deep Reinforcement Learning, um die physischen Attribute eines Roboters zusammen mit einer Steuerungsstrategie zu entwerfen.
Wir schlagen einen Ansatz vor, der ein einziges Modell mit der Fähigkeit ausstattet, beide Extreme zu repräsentieren: gemeinsames Training und unabhängiges Training, was zu effektivem Multi-Task-Lernen führt.
Wir schlagen einen Regularisierungsterm vor, der, wenn er dem Ziel des Reinforcement Learnings hinzugefügt wird, es der Strategie ermöglicht, die Belohnung zu maximieren und gleichzeitig zu lernen, sich gegenüber irrelevanten Änderungen innerhalb der Eingabe nicht zu verändern.
In dieser Arbeit wurde eine universelle visuelle Darstellung für die neuronale maschinelle Übersetzung (NMT) vorgeschlagen, die Bilder mit ähnlichen Themen wie der Ausgangssatz verwendet und die Anwendbarkeit von Bildern in der NMT erweitert.
Durch die Kombination von Ideen aus dem traditionellen Algorithmenentwurf und dem Reinforcement Learning stellen wir einen neuartigen Rahmen für Lernalgorithmen vor, die online kombinatorische Optimierungsprobleme lösen.
Ein funktionaler Ansatz zeigt, dass eine flache Initialisierung, die durch den Gradientenabstieg erhalten bleibt, zu einer Generalisierungsfähigkeit führt.
Die Netzwerktiefe erhöht die Ausreißereigenwerte in der Hessian. Residuale Verbindungen mildern dies ab.
Eine experimentelle Arbeit, die beweist, wie viele redundante Gewichte erst ab der dritten Epoche eingefroren werden können, wobei die Genauigkeit nur sehr geringfügig abnimmt.
Ein neuartiger Ansatz für das Lernen von Lehrplänen durch inkrementelles Lernen von Beschriftungen und adaptive Glättung von Beschriftungen für falsch klassifizierte Proben, was die durchschnittliche Leistung erhöht und die Standardabweichung verringert.
Wir schlagen eine Methode vor, um mit seltenen Wörtern umzugehen, indem wir ihre Einbettung aus Definitionen berechnen.
Diese Arbeit schlägt einen tiefen generativen Klassifikator vor, der sowohl Beispiele außerhalb der Verteilung erkennen als auch Beispiele innerhalb der Verteilung klassifizieren kann, indem das Konzept der Gaußschen Diskriminanzanalyse in tiefe neuronale Netze integriert wird.
Aufzeigen, dass das Alter die Erkennung von kognitiven Beeinträchtigungen behindert + Lösen von Problemen mit dem Lernen von fairen Repräsentationen + Vorschlagen von Metriken und Modellen.
Wir stellen NetScore vor, eine neue Metrik, die eine quantitative Bewertung des Gleichgewichts zwischen Genauigkeit, Rechenaufwand und Komplexität der Netzwerkarchitektur eines tiefen neuronalen Netzwerks ermöglicht.
geordnete Top-k-Angriffe
Personalisierte Propagierung neuronaler Vorhersagen (PPNP) verbessert graphische neuronale Netze, indem es sie in Vorhersage und Propagierung mittels personalisiertem PageRank trennt.
Die Wahl der Zielsprache wirkt sich auf die Qualität der sprachübergreifenden Einbettungen aus, die nicht nur anhand englischsprachiger Wörterbücher bewertet werden sollten.
Das typische GAN-Training optimiert nicht Jensen-Shannon, sondern so etwas wie eine umgekehrte KL-Divergenz.
Wir zeigen, dass wir durch das Ziehen mehrerer Stichproben (Vorhersagen) pro Eingabe (Datenpunkt) mit weniger Daten lernen können, da wir eine REINFORCE-Basislinie erhalten.
Wir lösen Multitasking-Probleme effizient mit einem automatischen Algorithmus zur Erstellung von Lehrplänen auf der Grundlage eines generativen Modells, das die Leistung des Lernagenten verfolgt.
In dieser Arbeit werden Klassen von Problemen identifiziert, für die negative Beispiele unausweichlich sind, und es werden grundlegende Grenzen für die Anfälligkeit jedes Klassifizierers für negative Beispiele abgeleitet. 
Die Verringerung der Genauigkeit (auf 4 Bits, 2 Bits und sogar binär) und die Erweiterung der Filterbänke führt zu ebenso genauen Netzen wie die, die mit FP32-Gewichten und -Aktivierungen erhalten werden.
Wir betten eine CRF in eine VAE von Token und NER-Tags für halbüberwachtes Lernen ein und zeigen Verbesserungen in ressourcenarmen Umgebungen.
Ein prinzipieller Rahmen für die Modellquantisierung unter Verwendung der proximalen Gradientenmethode.
Eine neue generative Modellierungstechnik, die auf asymmetrischem adversarialem Training basiert, und ihre Anwendungen zur Erkennung von adversarialem Beispiel und robuster Klassifizierung
Das Meta-Lernen von neugierigen Algorithmen durch die Suche in einem reichhaltigen Raum von Programmen führt zu neuartigen Mechanismen, die sich über sehr unterschiedliche Bereiche des Reinforcement Learnings verallgemeinern lassen.
In diesem Beitrag wird ein einfaches Verfahren zur Bewertung der Kompositionsstruktur in gelernten Repräsentationen vorgeschlagen und das Verfahren verwendet, um die Rolle der Kompositionalität bei vier Lernproblemen zu untersuchen.
Ein DL-Modell für die Vorhersage von RNA-Sekundärstrukturen, das in der Architektur einen ausgerollten Algorithmus zur Durchsetzung von Beschränkungen verwendet.
Wir stellen eine Alternative zu BPTT vor, die mit experimentellen Daten zur synaptischen Plastizität kompatibel ist und mit BPTT in Benchmarks für maschinelles Lernen konkurriert.
Extrahieren einer endlichen Zustandsmaschine aus einem rekurrenten neuronalen Netz mittels Quantisierung zum Zwecke der Interpretierbarkeit mit Experimenten auf dem Atari.
Wir untersuchen die theoretischen und praktischen Beweise für die Verbesserung des On-Policy Reinforcement Learning durch die Wiederverwendung der Daten von mehreren aufeinanderfolgenden Policies.
Wir nutzen die nicht-euklidische Fourier-Transformation von Formen, die durch einen simpliziellen Komplex definiert sind, für Deep Learning und erzielen damit deutlich bessere Ergebnisse als mit punktbasierten Sampling-Techniken, die in der aktuellen Literatur zum 3D-Lernen verwendet werden.
Wir wenden Reinforcement Learning auf die Score-basierte Kausalerkennung an und erzielen vielversprechende Ergebnisse sowohl auf synthetischen als auch auf realen Datensätzen
Wir haben eine universelle Methode vorgeschlagen, die in der Vorverarbeitungsphase der Daten verwendet werden kann, um das aussagekräftigere Thema zu generieren, das das gegebene Dokument besser repräsentiert
Wir schlagen ein neuartiges Multitasking-Lernsystem vor, das automatisch Multi-View-Abhängigkeitsbeziehungen extrahiert und diese zur Steuerung des Wissenstransfers zwischen verschiedenen Aufgaben verwendet.
Prüfung der globalen Translationsinvarianz in Convolutional und Kapselnetzen
Wir entwickeln eine analytische Methode zur Untersuchung der Bayes'schen Inferenz von neuronalen Netzen mit endlicher Breite und stellen fest, dass sich das Bild des Renormierungsgruppenflusses auf natürliche Weise ergibt.
Theoretisches verstehen des Regularisierungseffekt der Destillation. Wir zeigen, dass ein frühes Anhalten in diesem Prozess essentiell ist. Aus dieser Perspektive haben wir eine Destillationsmethode für das Lernen mit korrupiertem Label und mit theoretischen Garantien entwickelt.
ein neuartiger Ansatz für lebenslanges Online-Lernen unter Verwendung von Output-Kerneln.
Wir modellieren ein Hausbau-Szenario in Minecraft in klassischer und HTN-Planung und vergleichen die Vor- und Nachteile beider Modellarten.
Wir stellen einen Rahmen für die Bewertung von Gegenbeispielen in der natürlichen Sprachverarbeitung vor und zeigen, dass generierte Gegenbeispiele oft nicht semantikerhaltend, syntaktisch korrekt oder unverdächtig sind.
Können wir der Erklärung eines neuronalen Netzes für seine Vorhersage vertrauen? Wir untersuchen die Robustheit verschiedener gängiger Vorstellungen von der Interpretierbarkeit neuronaler Netze, einschließlich herausragender Zuordnungen und Einflussfunktionen, und entwerfen Gegenbeispiele zu ihnen.
In dem Papier werden zwei Algorithmen für das stochastische AUC-Maximierungsproblem mit modernster Komplexität bei Verwendung eines tiefen neuronalen Netzes als Vorhersagemodell entwickelt, die auch durch empirische Studien verifiziert werden.
Wir lösen zielgerichtete Aufgaben, indem wir die Algorithmen des Einsichts-Erfahrungs-Wiederholens und des Nachahmungs-Lernens kombinieren, wobei wir eine schnellere Konvergenz als der erste und eine höhere Endleistung als der zweite Algorithmus zeigen.
Wir leiten einen neuen PAC-Bayesian Bound für unbeschränkte Verlustfunktionen (z.B. Negative Log-Likelihood) ab. 
Wir schlagen eine einfache Technik zur selbstüberwachten Datenerweiterung vor, die die Leistung vollständig überwachter Szenarien verbessert, einschließlich des Few-Shot Learnings und der unausgewogenen Klassifizierung.
Eine zweigliedrige LSTM-basierte Netzwerkarchitektur lernt die Repräsentation und Dynamik von 3D-Netzen aus numerischen Crash-Simulationen.
Merkmalsvektoren aus SoundNet können die Gehirnaktivität von Personen, die einen Film ansehen, in auditiven und sprachbezogenen Gehirnregionen vorhersagen.
Wir schlagen einen generativen Autoencoder vor, der aussagekräftige posteriore und bedingte Wahrscheinlichkeitsverteilungen unter Verwendung impliziter Verteilungen erlernen kann, und trainieren das Modell mit einer neuen Formulierung der ELBO.
Kinder nutzen die gegenseitige Ausschließlichkeit (Mutual Exclusivity, ME), um neue Wörter zu lernen, während herkömmliche neuronale Netze die gegenteilige Tendenz aufweisen, was das Lernen in naturalistischen Szenarien wie dem lebenslangen Lernen behindert.
Rekurrente neuronale Netzwerke mit Spikes, die eine Arbeitsgedächtnisaufgabe ausführen, verwenden lange heterogene Zeitskalen, die denen im präfrontalen Kortex verblüffend ähnlich sind.
 In dieser Arbeit befassen wir uns mit dem Problem des Lernens von Low-shot-Netzerweiterungen
Wir stellen ein Modell des menschlichen Fragenstellens vor, das neuronale Netze und symbolische Programme kombiniert, die lernen können, gute Fragen mit oder ohne überwachte Beispiele zu generieren.
Ein visueller Verständigungsmechanismus für eine besondere Umgebung
Besseres adversariales Training durch Lernen der Abbildung auf die Datenvielfalt mit Autoencodern in den verborgenen Zuständen.  
Wir zeigen, dass die Minimierung des Cross-Entropy-Loss mit Hilfe einer Gradientenmethode zu einer sehr schlechten Marge führen kann, wenn die Merkmale des Datensatzes in einem niedrigdimensionalen Unterraum liegen.
Ein neuartiges RNN-Modell, das die derzeitigen Modelle in einer Reihe von sequentiellen Aufgaben deutlich übertrifft.
Überraschend negative Ergebnisse bei modellbasierter + modelltiefer RL
Inspiriert von der Theorie des Informationsengpasses schlagen wir eine neue GAN-Architektur für das Erlernen einer entflochtenen Darstellung vor.
Erklärung der Verzerrungen bei MMD-GANs; MMD-GANs arbeiten mit kleineren kritischen Netzen als WGAN-GPs; neue GAN-Bewertungsmetrik.
Ein halbüberwachtes multimodales Klassifizierungssystem, TCN, das verschiedene Benchmarks übertrifft.
Eine iterative neuronale Methode zur Extraktion von Signalen, die nur gemischt mit anderen Signalen beobachtet werden
Einbettung physiologischer Signale für die Vorhersageleistung und die Krankenhausübertragung mit einer allgemeinen Shapley-Wert-Interpretationsmethode für gestapelte Modelle.
Wir stellen einen beweisbaren Algorithmus vor, der beide Faktoren des Wörterbuch-Lernmodells exakt wiederherstellt. 
Datenerweiterte Relationsextraktion mit GPT-2
Ein approximativer Inferenzalgorithmus für Deep Learning
Wir schlagen eine neuartige Regularisierungsstrategie vor, die auf adversarialem Training basiert und die Leistung von semi-supervised learning deutlich verbessern kann.
Eine neuartige Methode für überwachtes Lernen durch Unterteilung des Eingaberaums in Verbindung mit Funktionsannäherung.
Angriffe auf unüberwachte Knoteneinbettungen auf der Grundlage der Eigenwert-Störungstheorie.
Wir stellen einen neuartigen Algorithmus für die hierarchische Erkennung von Teilaufgaben vor, der den Rahmen des linearen Markov-Entscheidungsprozesses für mehrere Aufgaben nutzt.
Wir schlagen ein Gedächtnisnetzwerkmodell vor, um binäre LP-Instanzen zu lösen, bei denen die Gedächtnisinformationen für eine langfristige Verwendung aufbewahrt werden. 
Wir stellen eine neuartige Methode vor, um Seq2Seq-Modelle mit Sprachmodellen zu trainieren, die schneller konvergieren, besser verallgemeinern und mit weniger als 10 % der gelabelten Daten fast vollständig auf eine neue Domäne übertragen werden können.
Wir führen die Online-Meta-Learning-Problemstellung ein, um den Geist und die Praxis des kontinuierlichen lebenslangen Lernens besser zu erfassen.
Die Aufmerksamkeitsgewichte legen nicht vollständig offen, was BERT über die Syntax weiß.
Wir schlagen eine neuartige Methode zur Gesichtsauflösung vor, die explizit 3D-Gesichtsprioritäten einbezieht, die die scharfen Gesichtsstrukturen erfassen.
Selbsttraining mit verschiedenen Ansichten der Eingabe liefert hervorragende Ergebnisse für die halbüberwachte Bilderkennung, das Sequenz-Tagging und das Parsing von Abhängigkeiten.
Eine nicht umkehrbare Entscheidung über die Annahme oder Ablehnung kann von Vorteil sein.
Wir bieten einen neuen Rahmen für MAML in der ES/Blackbox-Umgebung und zeigen, dass er deterministische und lineare Strategien, bessere Exploration und nicht-differenzierbare Anpassungsoperatoren ermöglicht.
Wir stellen einen Software-Rahmen für die Transformation von Verteilungen vor und demonstrieren dessen Flexibilität bei der Lockerung von Mean-Field-Annahmen in der Variationsinferenz durch die Verwendung von Kopplungsflüssen zur Replikation von Strukturen aus dem generativen Zielmodell.
Adversarial Domain adaptation und Multi-domain learning: Ein neuer Loss um multi- und single-domain Klassen in halbüberwachten Umgebungen zu steuern.
Ein lernendes Netz, das den MLP-Rahmen verallgemeinert, um eine Regression von Verteilung zu Verteilung durchzuführen
Vorgeschlagene Methoden für zeitabhängige Ereignisdarstellung und Regularisierung für Sequenzvorhersage; Evaluierung dieser Methoden an fünf Datensätzen, die eine Reihe von Sequenzvorhersageaufgaben beinhalten.
Wir entwickeln eine Methode für stabiles Offline Reinforcement Learning aus aufgezeichneten Daten. Der Schlüssel dazu ist die Regularisierung der RL-Politik in Richtung eines gelernten "vorteilhaft gewichteten" Modells der Daten.
In diesem Beitrag wird ein Deep-Learning-Modell vorgestellt, das selbstorganisierende Zuordnungen und neuronale Convolutional Networks für das Repräsentationslernen von Multi-omics-Daten kombiniert.
Sparsifizierung als Feinabstimmung von Sprachmodellen
Verfahren zur Behandlung von Kovariatenverschiebungen beim Imitation Learning unter Verwendung von Ensemble-Unsicherheit
Wir verwenden eine überwachte Feinabstimmung der Merkmalsvektoren, um die Übertragung von der Simulation auf die reale Welt zu verbessern.
Wir führen die Fähigkeit ein, Informationen darüber zu nutzen, inwieweit ein beliebiges Ziel erreicht wurde, während ein anderes Ziel für die Regeln der Gradientenmethoden vorgesehen war.
Wir schlagen einen neuen Benchmark für das Verstehen von Videos vor, dessen Aufgaben im Gegensatz zu den meisten bestehenden Videodatensätzen von vornherein zeitliche Schlussfolgerungen erfordern.
Wir schlagen einen effizienten und robusten asynchronen Algorithmus für föderiertes Lernen vor, der die Existenz von Nachzüglern berücksichtigt.
Das Hinzufügen eines neuen Satzes von Gewichten zum LSTM, die den Zellspeicher drehen, verbessert die Leistung bei einigen bAbI-Aufgaben.
Neue Methodik zur variablen marginalen Inferenz von Permutationen auf der Grundlage des Sinkhorn-Algorithmus, angewandt auf die probabilistische Identifizierung von Neuronen
Wir schlagen die erste angriffsunabhängige Robustheitsmetrik, a.k.a. CLEVER, vor, die auf jeden Klassifikator eines neuronalen Netzes angewendet werden kann.
In diesem Beitrag wird ein Lernverfahren für spontane und selbstorganisierende Kommunikation (SSoC) für Multi-Agenten-RL-Aufgaben vorgeschlagen.
Über die Verwendung von BERT als Encoder für die sequentielle Vorhersage von Etiketten bei der Klassifizierung von Texten mit mehreren Etiketten
DNN und Encoder verbesserte FM mit bilinearer Aufmerksamkeit und Max-Pooling für CTR
Dropout-basierte Bayes'sche Inferenz wird erweitert, um mit Multimodalität umgehen zu können, und wird anhand von Aufgaben zur Szenenvorhersage evaluiert.
Wir stellen eine neue Art von bedingtem GAN vor, das darauf abzielt, die Struktur im Zielraum des Generators zu nutzen. Wir erweitern den Generator mit einem neuen, unbeaufsichtigten Pfad, um die Zielstruktur zu lernen. 
In diesem Papier schlagen wir ein neuartiges regularisiertes adversariales Trainingssystem ATLPA vor, nämlich Adversarial Tolerant Logit Pairing with Attention.
Wir befassen uns mit dem Problem der Generalisierung des Reinforcement Learnings auf ungesehenen Actionräumen.
Lernen in zeitlichen Punktprozessen durch Modellierung der bedingten Dichte, nicht der bedingten Intensität.
Ein tiefes Modell für die Themenmodellierung
Neuartiges adaptives, auf Instanznormalisierung basierendes GAN-Framework für nicht parallele Many-to-many- und Zero-Shot-VC. 
Diese Arbeit schlägt Sparse Transformer vor, um die Konzentration der Aufmerksamkeit auf den globalen Kontext durch eine explizite Auswahl der relevantesten Segmente für das Lernen von Sequenz zu Sequenz zu verbessern. 
Nicht überwachte Repräsentationen, die mit Contrastive Predictive Coding gelernt wurden, ermöglichen eine dateneffiziente Bildklassifizierung.
Die Neuverteilung und Vergrößerung der Gewichte entsprechend der Impulsgröße ermöglicht das Training von spärlichen Netzen aus zufälligen Initialisierungen, die dichte Leistungsniveaus mit 5 % bis 50 % Gewichten erreichen können, während das Training um das bis zu 5,6-fache beschleunigt wird.
Die Verlustfläche von neuronalen Netzen ist eine disjunkte Vereinigung von Regionen, bei der jedes lokale Minimum ein globales Minimum der entsprechenden Region ist.
Wir destillieren aus Sprachmodellen Repräsentationen für die Syntax durch unüberwachtes metrisches Lernen
Wir stellen eine neue Speicherarchitektur für die Navigation in bisher unbekannten Umgebungen vor, die von der Landmarken-basierten Navigation bei Tieren inspiriert ist.
Wir schlagen eine neuartige Architektur vor, die eine Bildpyramide von oben nach unten durchläuft und dabei nur die informativsten Regionen auf dem Weg besucht.
Wir verwenden ein Hypernetzwerk zur Vorhersage optimaler Gewichte bei gegebenen Hyperparametern und trainieren alles gemeinsam.
Wir schlagen eine neue Metrik zur Bewertung von bedingten GANs vor, die Bildqualität, bedingte Konsistenz und Vielfalt innerhalb der Konditionierung in einem einzigen Maß erfasst.
Wir stellen TreeQN und ATreeC vor, neue Architekturen für tiefes Reinforcement Learning in diskreten Handlungsdomänen, die differenzierbare Online-Baumplanung in die Handlungswertfunktion oder -politik integrieren.
Wir schlagen Message-Passing-Encoder-Decoder-Netzwerke für eine schnelle und genaue Modellierung von Label-Abhängigkeiten für Multi-Label-Klassifikation vor.
Wir schlagen eine Methode zur Durchführung von Few-Shot Regression vor, indem wir eine Reihe von Basisfunktionen zur Darstellung der Funktionsverteilung lernen.
Wir schlagen einen modellunabhängigen Weg vor, BERT für die Texterzeugung zu nutzen und erreichen Verbesserungen gegenüber Transformer bei 2 Aufgaben in 4 Datensätzen.
CNN-F erweitert CNN um ein generatives Netz mit Rückkopplung für robustes Sehen.
Es wird gezeigt, dass CNNs vom Typ ResNet ein universeller Approximator sind und dass ihre Ausdrucksfähigkeit nicht schlechter ist als die von vollständig verbundenen neuronalen Netzen (FNNs) mit einer \textit{block-sparse} Struktur, selbst wenn die Größe jeder Schicht im CNN festgelegt ist.
Ein neuartiges "few shot"-Lernverfahren zur Erzeugung abfragespezifischer Klassifikationsgewichte durch Informationsmaximierung.
Eine neuronale Methode zur Beantwortung von Gesprächsfragen mit einem Aufmerksamkeitsmechanismus und einer neuartigen Verwendung von BERT als kontextuellem Einbettungsmittel
Alternative zur Gradientenstrafe
automatische Suche nach Multi-Task-Architekturen, die die Nutzung von Funktionen pro Aufgabe reduzieren
Wir schlagen Nearest Neighbor Overlap vor, ein Verfahren, das die Ähnlichkeit zwischen Einbettungsprogrammen aufgabenunabhängig quantifiziert, und verwenden es, um 21 Satzeinbettungsprogramme zu vergleichen.
Wir schlagen ein neues Ziel für das Training hybrider VAE-GANs vor, das zu einer deutlichen Verbesserung der Modusabdeckung und der Qualität führt.
Eine neue Methode verwendet statistische Leverage-Score-Informationen, um die Wichtigkeit der Datenproben in jeder Aufgabe zu messen, und nimmt häufige Richtungen an, um eine lebenslange Lerneigenschaft zu ermöglichen.
Wir lernen Merkmalskarten, die invariant gegenüber Translation und äquivariant gegenüber Rotation und Skalierung sind.
Ein spezieller gradientenbasierter Meta-Lernalgorithmus, MAML, ist äquivalent zu einer Inferenzprozedur in einem hierarchischen Bayes'schen Modell. Wir nutzen diese Verbindung, um MAML durch Methoden der approximativen Inferenz und Krümmungsschätzung zu verbessern.
Automatisiertes maschinelles Lernsystem mit effizientem Suchalgorithmus und innovativer Struktur zur Bereitstellung besserer Modellgrundlagen.
Wir schlagen prinzipielle Batch-Bayesian-Versuchsplanungsstrategien und eine Methode zur Quantifizierung der Unsicherheit der Posterior-Zusammenfassungen in einem auf Gauß-Process-Surrogate basierenden Rahmen für approximative Bayesianische Berechnungen vor.
Wir liefern eine effiziente Konvergenzrate für den Gradientenabstieg beim Lernziel des vollständigen orthogonalen Wörterbuchs auf der Grundlage einer geometrischen Analyse.
Wir zeigen, dass kreativ gestaltete und trainierte RNN-Architekturen bekannte sequentielle Codes dekodieren können und nahezu optimale Leistungen erzielen.
Wir analysieren und lösen das Problem der Nicht-Konvergenz von Adam.
In diesem Papier schlagen wir eine generative Methode zur Anpassung von Multisource-Domänen vor, die auf der Zerlegung von Inhalts-, Stil- und Domänenfaktoren basiert.
Verwendung von geglättetem Wichtigkeits-Sampling für das Co-Generation Problem. 
Wir entwickeln eine neue Methode zur Schätzung von Parametern ohne Likelihood, die unter bestimmten Bedingungen der maximalen Likelihood gleichwertig ist
Unsere Methode ermittelt Einschränkungen bei der Aufgabenausführung, indem sie das Prinzip der maximalen Entropie nutzt, um zu quantifizieren, inwieweit sich Demonstrationen von dem erwarteten, nicht eingeschränkten Verhalten unterscheiden.
System zum Erlernen von Roboteraufgaben in der realen Welt mit Reinforcement Learning ohne Instrumentierung
Wir verwenden einen Variational Autoencoder, um Stil und Inhalt zu trennen, und erreichen die Sprachumwandlung durch Modifizierung der Stileinbettung und -dekodierung. Wir verwenden ein mehrsprachiges Sprachkorpus und untersuchen dessen Auswirkungen.
Diese Arbeit schlägt eine Methode vor, die CNNs dazu zwingt, räumliche Aufmerksamkeit zu nutzen, um objektzentriertere Darstellungen zu lernen, die in verschiedener Hinsicht besser sind.
Rekurrente neuronale Netze für Anwendungsfälle im Bereich der Cybersicherheit
Input-Strukturierung entlang des Chaos für Stabilität
Wir befassen uns mit dem Training von GANs mit diskreten Daten, indem wir einen Policy-Gradienten formulieren, der sich über f-Divergenzen hinweg verallgemeinern lässt
Handlungsabhängige Grundlinien können verzerrungsfrei sein und eine größere Varianzreduzierung bewirken als nur zustandsabhängige Grundlinien für politische Gradientenmethoden.
Wir schlagen einen aktiven Multitask-Lernalgorithmus vor, der einen Wissenstransfer zwischen den Aufgaben ermöglicht.
Wir lernen einen effizienten verlustbehafteten Bildcodec kennen, der so optimiert werden kann, dass er eine zuverlässige Erkennung von Fotomanipulationen zu einem Bruchteil der Kosten in Bezug auf Nutzlast/Qualität und sogar bei niedrigen Bitraten ermöglicht.
In diesem Papier wird ein effektives generisches Sequenzmodell vorgeschlagen, das die Stärken von RNNs und Multi-Head Attention nutzt.
Ein strukturierter latent-variabler Ansatz, der diskrete Kontrollzustände innerhalb eines standardmäßigen autoregressiven neuronalen Paradigmas hinzufügt, um eine willkürliche Grundlage für interne Modellentscheidungen zu schaffen, ohne die Darstellungsfähigkeit neuronaler Modelle zu beeinträchtigen.
Schätzung der Verteilung der Trainingsdaten aus dem trainierten Klassifikator unter Verwendung von GAN.
Wir stellen fest, dass die in (Bora et al., 2017) abgeleiteten Skalierungsgesetze optimal oder nahezu optimal sind, wenn keine weiteren Annahmen getroffen werden.
Meta-Lernen eines Lernalgorithmus, der in der Lage ist, kausale Schlussfolgerungen zu ziehen
Entwicklung eines korpusbasierten Algorithmus zur Erstellung eines amharischen Sentiment-Lexikons auf der Grundlage eines Korpus
Wir ergänzen die Q-Wert-Schätzungen mit einem zählbasierten Bonus, der für Optimismus bei der Aktionsauswahl und beim Bootstrapping sorgt, selbst wenn die Q-Wert-Schätzungen pessimistisch sind.
Wir schlagen Soft Actor-Critic vor, einen Deep-RL-Algorithmus, der auf dem Maximum-Entropy Reinforcement Learning Framework basiert.
Die Verteilungsanpassung durch Divergenzminimierung bietet eine gemeinsame Grundlage für den Vergleich von gegnerischen Maximum-Entropy Inverse Reinforcement Learning-Methoden mit Behaviour Cloning.
Wir schlagen einen generischen Rahmen vor, der es ermöglicht, die Low-Rank-Struktur sowohl beim Planungs- als auch beim Deep Reinforcement Learning zu nutzen.
Ein Benchmark zur Bewertung der neuronalen Einbettung von Bezeichnern im Quellcode.
Durch Umstellung der Terme in der maximalen mittleren Diskrepanz ergibt sich eine viel bessere Verlustfunktion für den Diskriminator generativer adversarischer Netze
In diesem Werk werden Algorithmen entwickelt, die direkt verlustfrei komprimierte Darstellungen von tiefen Feedforward-Netzen verwenden, um Inferenzen ohne vollständige Dekompression durchzuführen.
Wir gliedern GANs in den Rahmen der Variationsungleichung ein und importieren Techniken aus dieser Literatur, um GANs besser zu optimieren; wir geben algorithmische Erweiterungen an und testen empirisch ihre Leistung beim Training von GANs.
Lösung des Problems der Aufgabenheterogenität beim Meta-Lernen durch Einführung eines Meta-Wissensgraphen
 Es wird ein Deep-Boosting-Algorithmus entwickelt, um durch die nahtlose Kombination einer Reihe von Deep-CNNs einen diskriminanteren Ensemble-Klassifikator zu lernen.
Eine automatische Methode zur Umwandlung von Musik zwischen Instrumenten und Stilen
Wir schlagen mehrere neue Angriffe und eine Methodik zur Messung der Robustheit gegenüber unvorhergesehenen Angriffen vor.
Deep-Net: Deep Neural Network für Anwendungsfälle der Cybersicherheit
Wir lernen einen Raum motorischer Primitive aus unkommentierten Roboterdemonstrationen und zeigen, dass diese Primitive semantisch sinnvoll sind und für neue Roboteraufgaben zusammengestellt werden können.
Wir demonstrieren die Machbarkeit eines schwach überwachten Zeitreihenklassifizierungsansatzes für tragbare Sensordaten. 
Wir schlagen ein lokal-globales Alignment-Framework vor, um semantische Korrespondenzen aus verrauschten Daten-Text-Paaren mit schwacher Überwachung zu lernen.
Lernen, wie man Ziele von Grund auf erreicht, indem man Imitation Learning mit Datenumetikettierung einsetzt
In dieser Arbeit haben wir eine Ensemble-Methode namens InterBoost für das Training neuronaler Netze zur Klassifizierung kleiner Stichproben vorgeschlagen. Die Methode hat eine bessere Generalisierungsleistung als andere Ensemble-Methoden und reduziert die Varianz erheblich.
Wir erkennen statistische Interaktionen, die von einem mehrschichtigen neuronalen Netz mit Vorwärtskopplung erfasst werden, indem wir seine gelernten Gewichte direkt interpretieren.
Wir vergleichen das neuronale lineare Modell mit den UCI- und UCI-"Gap"-Datensätzen.
Wir haben AlphaZero auf der Google Cloud Platform reproduziert
Wir weisen globale Konvergenz zur Optimalität für IPM-basierte GANs nach, bei denen der Generator ein überparametrisiertes neuronales Netz ist. 
Wir entwickeln effiziente Verfahren zur approximativen Einbettung von Netzwerken mit mehreren Skalen und nachweisbaren Eigenschaften.
 Eine detaillierte empirische Studie über die Few-Shot Klassifizierung, die die Herausforderungen bei der Standardbewertung aufzeigt und eine neue Richtung angibt.
Wir stellen ein Bayes'sches Inferenzmodell vor, um kontrastive Erklärungen (als LTL-Spezifikationen) abzuleiten, die beschreiben, wie sich zwei Sätze von Planspuren unterscheiden.
Die tropische Geometrie kann genutzt werden, um die Entscheidungsgrenzen neuronaler Netze darzustellen und interessante Erkenntnisse zu gewinnen.
Eine neuartige Gram-Gauß-Newton-Methode zum Trainieren neuronaler Netze, inspiriert durch den neuronalen Tangens-Kernel und die Gauß-Newton-Methode, mit schneller Konvergenzgeschwindigkeit, sowohl theoretisch als auch experimentell.
Wir untersuchen das implizite syntaktische Wissen von Satzeinbettungen anhand eines neuen Analysesets von grammatikalisch annotierten Sätzen mit Akzeptanzurteilen.
Wir schlagen eine Erweiterung des Multi-Output-Lernens auf ein Kontinuum von Aufgaben unter Verwendung von Operator-bewerteten Kerneln vor.
Wir beweisen, dass für Aktivierungsfunktionen, die einige Bedingungen erfüllen, die Längen der Vektoren der versteckten Variablen zu einer Längenzuordnung konvergieren, wenn ein tiefes Netzwerk breit wird.
Wir schlagen einen Ansatz zur Datenerweiterung für das Meta-Lernen vor und beweisen, dass er zulässig ist.
Ein allgemeiner Rahmen für die Destillation von Bayes'schen posterioren Erwartungen für tiefe neuronale Netze.
In diesem Papier führen wir eine diskrete Hierarchie kategorialer latenter Variablen ein, die wir mit Hilfe der Concrete/Gumbel-Softmax-Relaxation trainieren, und wir leiten eine obere Schranke für die absolute Differenz zwischen dem unbiased und dem biased Ziel ab.
Wir schlagen eine neue Klasse von Optimierern für die beschleunigte nicht-konvexe Optimierung mittels einer nichtlinearen Gradiententransformation vor. 
Lösen Sie Aufgaben, die die visuell gesteuerte Fortbewegung von Humanoiden beinhalten, indem Sie das Fortbewegungsverhalten aus Bewegungserfassungsdaten wiederverwenden.
Wir schlagen Gated Linear Unit-Netzwerke vor - ein Modell, das bei realen Daten ähnliche Leistungen wie ReLU-Netzwerke erbringt, aber theoretisch viel einfacher zu analysieren ist.
Wir schlagen eine Architektur-Suchmethode vor, um eine Verteilung von Architekturen zu identifizieren und daraus ein Bayes'sches Ensemble zur Ausreißererkennung zu konstruieren.
Ein neuartiger Ansatz zur Erkennung von Ausreißern in Bilddaten unter Beibehaltung der Klassifizierungsgenauigkeit der Bildklassifizierung
In diesem Beitrag wird CloudLSTM vorgestellt, ein neuer Zweig rekurrenter neuronaler Modelle, der auf die Vorhersage von Datenströmen zugeschnitten ist, die von raumbezogenen Punkthäufungen erzeugt werden.
Wir schlagen TransINT vor, ein neuartiges und interpretierbares KG-Einbettungsverfahren, das die Implikationsordnung zwischen den Relationen im Einbettungsraum auf erklärbare, robuste und geometrisch kohärente Weise isomorph bewahrt.
Wir stellen eine neue, auf Gradientenablösung basierende, komplementär-objektive Trainingsstrategie zur domänenadaptiven Objekterkennung vor.
Wir schlagen einen neuen Ansatz für die Verbindung von aufgabenspezifischen Netzwerken in einer Multi-Task-Lernumgebung vor, der auf den jüngsten Fortschritten im Bereich der Residual Networks basiert.
Verwendung des Cross-Entropie-Verlustes für Zero Shot Learning mit Soft Labeling auf ungesehenen Klassen: eine einfache und effektive Lösung, die auf fünf ZSL-Benchmark-Datensätzen die beste Leistung erzielt.
Die schrittweise Vergrößerung des verfügbaren Aktionsraums ist ein großartiger Lehrplan für Lernagenten
Eine spieltheoretische Lösung für gegnerische Angriffe und Verteidigungsmaßnahmen.
Eine neuartige Methode zur Erstellung dichter Zeitdeskriptoren (Time Embeddings), um einfache Modelle zum Verständnis zeitlicher Strukturen zu erstellen
Wir schlagen eine neuartige Architektur für neuronale Graphen-Netzwerke vor, die auf der Non-Backtracking-Matrix basiert, die über die Kanten-Adjazenzen definiert ist, und demonstrieren ihre Effektivität bei der Erkennung von Gemeinschaften in Graphen.
Residuale Verbindungen führen wirklich iterative Inferenz durch
Wir verbessern die Rekonstruktionszeit und -qualität bei einem experimentellen maskenbasierten linsenlosen Imager mit einem End-to-End-Lernansatz, der das Wissen über das Bildgebungsmodell einbezieht.
Wir stellen ein neues Repräsentations-Lernmodell vor, nämlich das "Sample-Ensemble Genetic Evolutionary Network" (SEGEN), das als alternativer Ansatz für Deep-Learning-Modelle dienen kann.
Wir schlagen vor, Meta-Lernen für effizienteres Sprachenlernen zu nutzen, und zwar durch eine Art "Domänen-Randomisierung". 
MARTHE: eine neue Methode zur Anpassung aufgabenspezifischer Lernratenpläne aus der Perspektive der Hyperparameteroptimierung
Interaktive Bilderzeugung aus inkrementell wachsenden Szenegraphen in mehreren Schritten unter Verwendung von GANs unter Beibehaltung des Inhalts der in den vorherigen Schritten erzeugten Bilder
Wir untersuchen Klassifizierungsszenarien mit geringem und sehr geringem Signal-Rausch-Verhältnis, bei denen Objekte, die mit dem Klassenlabel korrelieren, einen winzigen Teil des gesamten Bildes einnehmen (z. B. medizinische oder hyperspektrale Bildgebung).
Eine Selbstaufmerksamkeitsschicht kann Convolution durchführen und lernt dies oft in der Praxis.
Lernbasierte Algorithmen können die Leistung klassischer Algorithmen für das Low-Rank-Approximationsproblem verbessern und gleichzeitig die Worst-Case-Garantie beibehalten.
Vorgeschlagene Architektur zur Lösung der Aufgabe der morphologischen Übereinstimmung
In diesem Beitrag wird die Verwendung von Spektralelementmethoden für ein schnelles und genaues Training von neuronalen gewöhnlichen Differentialgleichungen zur Systemidentifikation vorgeschlagen.
Ein neues intrinsisches Belohnungssignal auf der Grundlage von Nachfolgemerkmalen und eine neuartige Methode zur Kombination von extrinsischer und intrinsischer Belohnung.
Wir schlagen vor, eine separate Explorationsstrategie zu verwenden, um die Trajektorien vor der Adaption in MAML zu sammeln. Wir zeigen auch, dass die Verwendung eines selbstüberwachten Ziels in der inneren Schleife zu einem stabileren Training und einer viel besseren Leistung führt.
Verallgemeinerung der Rückwärtsausbreitung unter Verwendung formaler Methoden der Supersymmetrie.
Die Regulierung der Optimierungskurve mit den Fisher-Informationen der alten Aufgaben reduziert das katastrophale Vergessen erheblich
Wir geben eine Methode zur Generierung von typsicheren Programmen in einer Java-ähnlichen Sprache an, die eine kleine Menge an syntaktischen Informationen über den gewünschten Code enthält.
Wir zeigen, wie autoregressive Flüsse verwendet werden können, um sequentielle latente Variablenmodelle zu verbessern.
Widersprüchliche Beispiele können das Urheberrechtserkennungssystem von YouTube täuschen
Wir schlagen eine kontinuierliche Version der Gleichgewichtsausbreitung vor, bei der Neuronen- und Synapsendynamik während der gesamten zweiten Phase gleichzeitig auftreten, mit theoretischen Garantien und numerischen Simulationen.
Wir schlagen ein neues Meta-Modul-Netzwerk vor, um einige der Einschränkungen früherer neuronaler Modulnetzwerke zu beseitigen und eine starke Leistung auf realistischen visuellen Reasoning-Datensätzen zu erzielen.
Wir schlagen einen neuen Angriff vor, um die volle Kontrolle über neuronale Richtlinien in realistischen Umgebungen zu übernehmen.
Es kann effektive Hash-Codes für eine effiziente Kaltstart-Empfehlung generieren und gleichzeitig eine praktikable Marketingstrategie bieten.
Wir schlagen einen neuronalen Rahmen vor, der lernen kann, das Problem der Erfüllbarkeit von Schaltkreisen anhand von (unbeschrifteten) Schaltkreisinstanzen zu lösen.
Eine einheitliche Sichtweise verschiedener Lernalgorithmen für die Sequenzerzeugung, wie MLE, RL, RAML, Datenrauschen usw.
Wir führen ein "Resource by Collaborative Construction"-Schema ein, um KB, strukturierte Wikipedia zu erstellen 
Ein hochmodernes Modell auf der Grundlage globaler Schlussfolgerungen für die Superauflösung von Bildern
Vorschlag für einen Bewertungsrahmen zur Analyse und zum Erlernen von Graph Convolutional Filtern
Eine neue Pooling-Schicht für GNNs, die lernt, wie Knoten entsprechend ihrer Eigenschaften, der Graphenkonnektivität und dem Ziel der nachgelagerten Aufgabe zu poolen sind.
 Wir schlagen TuckER vor, ein relativ einfaches, aber leistungsfähiges lineares Modell zur Linkvorhersage in Wissensgraphen, das auf der Tucker-Dekomposition der binären Tensordarstellung von Wissensgraphen-Tripeln basiert. 
Ein Algorithmus zur Verringerung des Speicherbedarfs für das Training tiefer Netzwerke, der auf einer Annäherungsstrategie basiert.
Algorithmen für Datenströme können durch Deep Learning verbessert werden, ohne dass die Leistung beeinträchtigt wird.
Wir schlagen den Neural Hyperlink Predictor (NHP) vor. NHP adaptiert graphische Convolutional Networks für die Link-Vorhersage in Hypergraphen
Eine Methode, die separate Repräsentationen für die Bedeutung und die Form eines Satzes lernt
Wir haben untersucht, welche Visualisierungen chronische Patienten dabei unterstützen können, ihre Gesundheitsdaten bei Arztbesuchen zu präsentieren und zu überprüfen.
Wir schlagen einen stochastisch differenzierbaren Vorwärtsdynamik-Prädiktor vor, der in der Lage ist, mehrere physikalisch plausible Trajektorien unter demselben anfänglichen Eingangszustand abzutasten, und zeigen, dass er verwendet werden kann, um modellfreie Strategien effizienter zu trainieren.
Beyond-Worst-Case-Analyse der Repräsentationskraft von ReLU-Netzen und polynomialen Kerneln - insbesondere bei Vorhandensein einer spärlichen latenten Struktur.
Ein Modell zur Steuerung der Bilderzeugung mit GAN und beta-VAE in Bezug auf Maßstab und Position der Objekte
Wir schlagen eine differenzierbare Familie von "Kaleidoskop-Matrizen" vor, beweisen, dass alle strukturierten Matrizen in dieser Form dargestellt werden können, und verwenden sie, um handgefertigte lineare Zuordnungen in Deep-Learning-Modellen zu ersetzen.
Lernen der Label-Repräsentation für tiefe Netzwerke
Wir schlagen Choco-SGD--dezentralisiertes SGD mit komprimierter Kommunikation--für nicht-konvexe Ziele vor und zeigen seine starke Leistung in verschiedenen Deep-Learning-Anwendungen (On-Device-Lernen, Rechenzentrumsfall).
Wir zeigen, dass Entropie-SGD den Vorgänger einer PAC-Bayes-Schranke optimiert und damit gegen die Anforderung verstößt, dass der Vorgänger unabhängig von den Daten sein muss; wir verwenden Differential Privacy, um dieses Problem zu lösen und die Verallgemeinerung zu verbessern.
Wir zeigen experimentell, dass Transfer-Lernen spärliche Merkmale im Netzwerk erzeugt und dadurch ein komprimierbareres Netzwerk produziert. 
Wir erweitern die klassischen Label-Propation-Methoden zur gemeinsamen Modellierung von Graphen- und Merkmalsinformationen aus der Perspektive der Graphenfilterung und zeigen Verbindungen zu den Graph Convolutional Networks auf.
Wir bieten ein Softwarepaket an, das die Bewertung von Deep-Learning-Optimierern drastisch vereinfacht, automatisiert und verbessert.
Extrahieren von kontextuellen Einbettungen aus einem standardmäßigen überwachten Modell. Unterstützt nachgelagerte NLP-Modelle in ressourcenarmen Umgebungen
Praktische adaptive Algorithmen für gradientenbasiertes Meta-Lernen mit nachweisbaren Garantien.
Unser Ziel ist es, die Vielfalt der sprachlichen Strukturen für die Erstellung von Satzrepräsentationen zu nutzen.
Die unüberwachte Analyse von Daten, die vom peripheren Nervensystem aufgezeichnet wurden, entrauscht und kategorisiert die Signale.
Wir verbessern die bestehenden transformationsbasierten Abwehrmechanismen, indem wir einen Verteilungsklassifikator für die Verteilung von Softmax aus transformierten Bildern verwenden.
Wir schlagen einen Ansatz zum Erlernen dezentraler Strategien in Multi-Agenten-Umgebungen unter Verwendung aufmerksamkeitsbasierter Kritiker vor und zeigen vielversprechende Ergebnisse in Umgebungen mit komplexen Interaktionen.
Wir wenden RNN an, um das biologische Problem der Vorhersage von Chromatinfaltungsmustern aus epigenetischen Markierungen zu lösen, und zeigen zum ersten Mal, dass die Nutzung des Speichers für sequenzielle Zustände auf dem DNA-Molekül für die beste Leistung von Bedeutung ist.
Wir schlagen eine effiziente, nachweisbare und datenunabhängige Methode zur Netzwerkkompression durch neuronales Pruning unter Verwendung von Neuronen-Coresets vor - eine neuartige Konstruktion, die in dieser Arbeit vorgeschlagen wird.
Memory Augmented Network zur Planung in teilweise beobachtbaren Umgebungen. 
Ein Verfahren zur Destillation von kontextuellen Modellen in statische Einbettungen; wir wenden unsere Methode auf 9 populäre Modelle an und zeigen deutliche Verbesserungen der Darstellungsqualität im Vergleich zu Word2Vec/GloVe sowie ein verbessertes Analysepotenzial durch eine gründliche Untersuchung sozialer Verzerrungen.
Das Metalernen von unbeaufsichtigten Aktualisierungsregeln für neuronale Netze verbessert die Leistung und zeigt möglicherweise, wie Neuronen im Gehirn ohne Zugang zu globalen Bezeichnungen lernen.
Wir zeigen, dass das Entfernen von konstanten Termen aus CNN-Architekturen die Interpretierbarkeit der Entrauschungsmethode mittels linearer Algebra-Techniken ermöglicht und außerdem die Generalisierungsleistung über verschiedene Lärmpegel hinweg erhöht.
Wir liefern zum ersten Mal einen rigorosen Beweis dafür, dass orthogonale Initialisierung die Konvergenz für tiefe lineare Netze im Vergleich zur Gaußschen Initialisierung beschleunigt.
Eine erste differentially private Schätzung der Überlebensfunktion
CAML ist eine Instanz von MAML mit bedingten Klassenabhängigkeiten.
Wir untersuchen das Problem der Multiset-Vorhersage und schlagen eine neuartige Multiset-Verlustfunktion vor, die durch Analysen und empirische Belege ihre Wirksamkeit unter Beweis stellt.
In diesem Beitrag wird ein theoretischer Rahmen vorgestellt, der die Datenverteilung für tiefe und lokal verbundene ReLU-Netze explizit modelliert.
Wir stellen einen biologisch inspirierten modularen Evolutionsalgorithmus vor, in dem Deep-RL-Agenten lernen, in einem schwierigen sozialen Multi-Agenten-Spiel zu kooperieren, was helfen könnte, die Evolution des Altruismus zu erklären.
Wir stellen eine Art von neuronalem Netz vor, das strukturell resistent gegen feindliche Angriffe ist, selbst wenn es auf unaugmentierten Trainingssätzen trainiert wird.  Diese Widerstandsfähigkeit ist auf die Stabilität der Netzeinheiten gegenüber Eingabestörungen zurückzuführen.
Wir zeigen, dass die adversariale Robustheit zwar auf Kosten der normalen Klassifizierungsleistung geht, aber auch unerwartete Vorteile mit sich bringt.
Training auf konvexen Kombinationen zwischen zufälligen Trainingsbeispielen und ihren Bezeichnungen verbessert die Generalisierung in tiefen neuronalen Netzen
Wir stellen einen neuartigen Ansatz zur Spike-Sortierung vor, der den Neural Clustering Process (NCP) verwendet, eine kürzlich eingeführte neuronale Architektur, die skalierbare amortisierte approximative Bayes'sche Inferenz für effizientes probabilistisches Clustering durchführt.
Vorgeschlagene Methode zum Finden der am besten verallgemeinerbaren Lösung, die gegenüber Störungen der Trainingsdaten stabil ist.
Implementierung und Evaluierung des episodischen Speichers für RL.
Wir stellen eine Methode zur Anpassung von Hyperparametern probabilistischer Modelle durch optimalen Transport mit Anwendungen in der Robotik vor.
Ein Algorithmus zum Erlernen einer prädiktiven Zustandsrepräsentation mit allgemeinen Wertfunktionen und Off-Policy-Lernen wird auf das Problem der sichtbasierten Lenkung beim autonomen Fahren angewendet.
Wir befassen uns mit dem durchgängigen Lernen von energiebasierten Repräsentationen für Signal- und Bildbeobachtungsdaten mit unregelmäßigen Probemustern.
Ein neuartiger und theoretisch fundierter Algorithmus für Meta-Reinforcement Learning
Beim Side-Tuning wird ein bereits trainiertes Netz angepasst, indem ein leichtgewichtiges "Side"-Netz trainiert wird, das mit dem (unveränderten) bereits trainierten Netz durch einen einfachen additiven Prozess verschmolzen wird.
Trainieren von GANs mit differential privacy, um künstliche Datensätze zu erzeugen, die den Datenschutz wahren.
In diesem Beitrag werden Methoden zur Entflechtung und Interpretation von Kontexteffekten vorgestellt, die in einem tiefen neuronalen Netz kodiert sind.
Vorschlag einer neuen Methode, die auf der geführten Aufmerksamkeit basiert, um die Seltenheit in tiefen neuronalen Netzen zu verstärken.
Wir können die unterste Schicht eines tiefen neuronalen Netzes nachweislich wiederherstellen, wenn die unterste Schicht eine Aktivierung mit "hohem Schwellenwert" verwendet und das obige Netz ein "wohlerzogenes" Polynom ist.
Wir stellen FedProx vor, einen Rahmen zur Bewältigung statistischer Heterogenität in föderierten Umgebungen mit Konvergenzgarantien und verbesserter Robustheit und Stabilität.
Wir ermöglichen sowohl die kulturelle Evolution von Sprache als auch die genetische Evolution von Agenten in einem referentiellen Spiel, indem wir eine neue Sprachübertragungs Engine verwenden.
Wir haben eine neuartige, einfache und effiziente Methode zur Datenerweiterung eingeführt, die die Leistung bestehender GANs steigert, wenn die Trainingsdaten begrenzt und vielfältig sind.  
Wir entwickeln einen Simulator für das gesamte Konnektom und den Körper von C. elegans und demonstrieren eine gemeinsame Zustandsraum- und Parameterinferenz in diesem Simulator.
Inspiriert von CapsNet schlagen wir eine neue Architektur für Grapheneinbettungen auf der Basis von Knotenmerkmalen vor, die aus GNN extrahiert werden.
Gen-RKM: ein neuer Rahmen für generative Modelle unter Verwendung von Restricted Kernel Machines mit Multiview-Generierung und unkorreliertem Merkmalslernen.
Wir vergleichen viele Aufgaben und Aufgabenkombinationen zum Vortraining von BiLSTMs auf Satzebene für NLP-Aufgaben. Die Sprachmodellierung ist die beste einzelne Pretraining-Aufgabe, aber auch einfache Basisaufgaben schneiden gut ab.
Ein Einblick in den Grund der gegnerischen Verwundbarkeit, eine effektive Verteidigungsmethode gegen Adversarial Attacks.
Anwendung der Monte-Carlo-Baumsuche auf die Episodenerzeugung in Alpha Zero
Bei neuronalen Netzen mit Graphen kann die Aggregation in einem Graphen von einem kontinuierlichen Raum profitieren, der dem Graphen zugrunde liegt.
Wir verwenden einen einfachen Suchalgorithmus mit einem RNN und einer Prioritätswarteschlange, um Lösungen für Codierungsaufgaben zu finden.
Wir präsentieren das Graph Wavelet Neural Network (GWNN), ein neuartiges Graph-Convolutional Neural Network (CNN), das die Graph Wavelet Transformation nutzt, um die Unzulänglichkeit früherer spektraler Graph CNN Methoden zu beheben, die auf der Graph Fourier Transformation beruhen.
Wir liefern eine weitere neue Erklärung für den Verfall der Lernrate: Eine anfänglich hohe Lernrate verhindert, dass sich das Netzwerk verrauschte Daten merkt, während ein Verfall der Lernrate das Erlernen komplexer Muster verbessert.
Die Anpassung der UCB-Exploration an das Ensemble Q-learning verbessert frühere Methoden wie Double DQN, A3C+ im Atari-Benchmark
Neural Probabilistic Motor Primitives komprimieren die Richtlinien zur Bewegungserfassung in ein flexibles Modell, das one-shot Imitation und Wiederverwendung als Low-Level-Controller kann.
Wir stellen eine automatisierte adaptive Datenerweiterung vor, die für mehrere unterschiedliche Aufgaben funktioniert. 
Wir schlagen eine neue Regularisierungstechnik vor, die auf der Wissensdestillation basiert.
Effektive Regularisierungs- und Optimierungsstrategien für LSTM-basierte Sprachmodelle erreichen SOTA auf PTB und WT2. 
Auf der Suche nach Objektdetektoren unter Verwendung vieler verschiedener Selektivitätsmaße; CNNs sind leicht selektiv, aber nicht genug, um als Objektdetektoren bezeichnet zu werden.
Eine Methode zur Umwandlung von DNA-Sequenzen in 2D-Bilder unter Verwendung von raumfüllenden Hilbert-Kurven zur Verbesserung der Stärken von CNNs
Ein lernfähiges Clustering-Ziel zur Erleichterung des bereichs- und aufgabenübergreifenden Transfer Lernens
Gemeinsame Methode zum Erlernen von sprachenübergreifenden Einbettungen mit modernster Leistung für sprachenübergreifende Aufgaben und einsprachige Qualität
Generatives Modell zeitlicher Daten, das einen online belief state aufbaut, im latenten Raum operiert und sprunghafte Vorhersagen und Zustandsänderungen vornimmt.
Stellt ein informationstheoretisches Trainingsziel für Co-Training vor und demonstriert dessen Leistungsfähigkeit beim unüberwachten Lernen von Phonetik.
Unsere Modelle erzeugen Gesangsstimmen ohne Text und Partitur. Sie nehmen eine Begleitung als Eingabe und geben Gesangsstimmen aus.
Wir steigern die Effizienz von Dependenzparsern mit neuronalen Netzen durch Lehrer-Schüler-Destillation.
Adversarially Regularized Autoencoders lernen glatte Repräsentationen diskreter Strukturen und ermöglichen so interessante Ergebnisse bei der Texterzeugung, wie z.B. unaligned style transfer, semi-supervised learning und latent space interpolation and arithmetic.
Wir stellen eine Methode zur Schätzung von Sammlungen von Regressionsmodellen vor, bei der jedes Modell auf eine einzelne Stichprobe zugeschnitten ist.
Eine rekurrente neuronale Netzwerkzelle mit erweitertem Long Short Term Memory und ein Multi-Task-RNN-Modell für Sequence-in-Sequence-out Probleme
Wir trainieren in zufälligen Unterräumen des Parameterraums, um zu messen, wie viele Dimensionen tatsächlich erforderlich sind, um eine Lösung zu finden.
Ein primäres duales neuronales Netzmodell für halb-überwachtes Lernen
Wir beschreiben zwei End-to-End-Auto-Coding-Parser für semi-supervised graph-based dependency parsing.
Ein verbessertes Fast-Weight-Netzwerk, das bessere Ergebnisse bei einer allgemeinen Spielzeugaufgabe zeigt.
Einführung einer neuen Optimierungsmethode und ihre Anwendung auf Deep Learning.
Einführung einer neuen Klasse von neuronalen Quantennetzen zum Erlernen graphbasierter Darstellungen auf Quantencomputern.
In dieser Arbeit haben wir den Einfluss von Kommunikationsstrategien auf die mentalen Modelle von Nutzern einer Datenschutzverletzung untersucht.
Ein Zielerkennungsansatz, der auf einer Heuristik der Operatorenzählung basiert, um das Störungen im Datensatz zu berücksichtigen.
die Destillation von Einzelaufgabenmodellen in ein Multiaufgabenmodell verbessert die Leistung beim Verstehen natürlicher Sprache
Evaluierung von Methoden zur Erkennung von Verteilungsfehlern auf Pixelebene an zwei neuen realen Datensätzen mit PSPNet und DeeplabV3+.
Wir stellen eine neue adversarische Methode zur Anpassung neuronaler Repräsentationen vor, die auf einer Kritik basiert, die nicht-diskriminierende Merkmale erkennt.
Wir schlagen einen statistischen Rahmen und ein theoretisch konsistentes Verfahren zur Schätzung der Auffälligkeit vor.
Wir haben untersucht, wie eine neuartige Methode der kompositorischen Mengeneinbettung nicht nur eine einzelne Klasse, sondern eine ganze Menge von Klassen, die mit den Eingabedaten verbunden sind, wahrnehmen und darstellen kann.
Wir stellen einen Datensatz, Modelle und Trainings- und Evaluationsprotokolle für eine kollaborative Zeichenaufgabe vor, die es ermöglicht, zielgerichtete und wahrnehmungs- und handlungsbasierte Spracherzeugung und Sprachverständnis zu untersuchen. 
Wir schlagen eine Methode vor, die auf der adversen Trainingsstrategie basiert, um diskriminierende Merkmale zu erlernen, die unvoreingenommen und invariant gegenüber den Störfaktoren sind, indem wir eine Verlustfunktion einbauen, die eine verschwindende Korrelation zwischen der Verzerrung und den erlernten Merkmalen fördert.
Ein modularer Ansatz, der aus einem Satzselektionsmodul und einem anschließenden QS-Modell besteht, kann im Vergleich zu einem QS-Modell, das auf der Grundlage des gesamten Kontexts trainiert wurde, robuster gegenüber Angriffen durch Angreifer gemacht werden.
Multirelationale Grapheneinbettung mit Riemannschen Mannigfaltigkeiten und TransE-ähnlicher Verlustfunktion. 
Wir schlagen einen Meta-Lernalgorithmus für kontinuierliches Lernen vor, der das Problem des katastrophalen Vergessens wirksam verhindern und das Rückwärts-Transferlernen unterstützen kann.
Analyse von tiefen Convolutional Networks in Bezug auf die zugehörige Anordnung von Hyperebenen
Wir haben eine Bayes'sche Meta-Sampling-Methode zur Anpassung der Modellunsicherheit beim Meta-Lernen vorgeschlagen
Kontextadaptives Entropiemodell zur Verwendung in der durchgängig optimierten Bildkompression, das die Kompressionsleistung erheblich verbessert
Eine Methode zum Erlernen besserer Repräsentationen, die als Regularisierer fungiert und trotz ihres geringen zusätzlichen Rechenaufwands Verbesserungen im Vergleich zu starken Baselines bei überwachten und halb-überwachten Lernaufgaben erzielt.
Packen von Interessensgebieten (ROI) wie z. B. Krebsregionen, die in 3D-Volumendaten identifiziert wurden, Packen von Bereichen innerhalb des ROI, Drehen des ROI, Messen des Unterschieds in der gepackten Bereichen vor und nach der Drehung.
Seltenheit auf Filterebene entsteht implizit in CNNs, die mit adaptiven Gradientenabstiegsansätzen trainiert werden, aufgrund verschiedener Phänomene, und das Ausmaß der Seltenheit kann versehentlich durch verschiedene, scheinbar nicht zusammenhängende Hyperparameter beeinflusst werden.
Indem wir Parallelen zum menschlichen Lernen ziehen, schlagen wir einen einheitlichen Rahmen vor, um viele lebenslange Lernfähigkeiten in neuronalen Netzen zu zeigen, indem wir eine kleine Anzahl von Gewichtskonsolidierungsparametern verwenden.
Wir zeigen, dass robuste GAN priors besser funktionieren als GAN priors für die CT-Rekonstruktion mit begrenztem Winkel, die ein stark unterbestimmtes inverses Problem darstellt.
Eine neue Form der Aufmerksamkeit, die sich gut für die Überwachung aus der Ferne eignet, und ein Multitasking-Ansatz, um Anmerkungen auf Satzebene hinzuzufügen. 
Analyse des Aufmerksamkeitsmechanismus bei verschiedenen NLP-Aufgaben.
Die Darstellung von Programmen als Graphen inklusive Semantik hilft bei der Programmerstellung
Eine neue Methode zur Ableitung eines Modells, zur Schätzung der Entropierate und zur Vorhersage zeitkontinuierlicher, ereignisdiskreter Prozesse.
Ein vereinheitlichtes Modell zur Verbesserung der Robustheit des Modells bei mehreren Aufgaben
Wir haben eine progressive Lernmethode vorgeschlagen, um das Lernen und die Entflechtung latenter Repräsentationen auf verschiedenen Abstraktionsebenen zu verbessern.
Wir wenden die Copula-Transformation auf den Deep Information Bottleneck an, was zu wiederhergestellten Invarianzeigenschaften und einem entwirrten latenten Raum mit überlegenen Vorhersagefähigkeiten führt.
Benchmark und Methode zur Messung der kompositorischen Generalisierung durch Maximierung der Divergenz der Verbindungshäufigkeit bei geringer Divergenz der Atomhäufigkeit.
Unüberwachte Netze lernen von unten nach oben; Maschinen und Kleinkinder lernen visuelle Klassen in unterschiedlicher Reihenfolge
Für Klassifizierungsprobleme mit k Klassen zeigen wir, dass der Gradient dazu neigt, in einem winzigen, sich langsam entwickelnden Unterraum zu leben, der von den Eigenvektoren aufgespannt wird, die den k-größten Eigenwerten der Hessian entsprechen.
Ein graphenbasierter rekurrenter Retriever, der lernt, Argumentationspfade über den Wikipedia-Graphen abzurufen, übertrifft den aktuellen Stand der Technik bei HotpotQA um mehr als 14 Punkte.
Wir haben ein flaches Feature-Extraktionsnetzwerk mit einem großen rezeptiven Feld für Stereo-Matching-Aufgaben eingeführt, das eine einfache Struktur verwendet, um eine bessere Leistung zu erzielen.
Diese Arbeit schlägt eine neue Ausgabeschicht für tiefe Netzwerke vor, die die Verwendung von protokolliertem kontextuellem Bandit-Feedback für das Training ermöglicht. 
Klassifizierung von Proteinfamilien mit Deep Learning
Ziel dieses Beitrags ist es, eine empirische Antwort auf die Frage zu geben, ob ein gut trainiertes Dialogantwortmodell bösartige Antworten ausgeben kann.
das gesamte Problem der STOA VAEs theoretisch und qualitativ diagnostiziert
Wir schlagen einen neuen, spärlichen und strukturierten Aufmerksamkeitsmechanismus, TVmax, vor, der die Spärlichkeit fördert und dazu anregt, dass die Gewichtung verwandter benachbarter Orte gleich ist.
Gelernte Phonemeeinbettungen mehrsprachiger neuronaler Sprachsynthesenetzwerke könnten die Beziehungen der Phonemeaussprache zwischen den Sprachen darstellen.
GAN-Darstellungen werden im Detail untersucht, und es werden Sätze von Darstellungseinheiten gefunden, die die Erzeugung semantischer Konzepte im Output steuern.
Sparse MobileNets sind schneller als Dense Nets mit den entsprechenden Kerneln.
 Wir schlagen einen neuartigen Rahmen für das Erlernen von Grapheninferenzen vor, indem wir Strukturbeziehungen aufbauen, um unbekannte Knotenbeschriftungen aus den beschrifteten Knoten in einer durchgängigen Weise abzuleiten.
Da Sicherheit zu einem kritischen Begriff beim maschinellen Lernen wird, glauben wir, dass diese Arbeit als Grundlage für eine Reihe von Forschungsrichtungen dienen kann, wie z. B. für sicherheitsbewusste Lernalgorithmen.
Wir stellen zwei Ansätze für eine effiziente und skalierbare Inferenz in stochastischen Simulatoren vor, für die die Dichte nicht direkt ausgewertet werden kann, z.B. aufgrund von Rückweisungsschleifen.
Selbst wenn es bei unendlich vielen Daten keinen Kompromiss gibt, kann das adversarische Training selbst bei einem konvexen Problem eine schlechtere Standardgenauigkeit aufweisen.
Wir zeigen, dass Shortcut-Verbindungen in Mustern platziert werden sollten, die die Abstände zwischen den Schichten während der Backpropagation minimieren, und entwerfen Netzwerke, die log L Abstände mit L log(L) Verbindungen erreichen.
Graphenbasiertes Deep Q Network für die Webnavigation 
Wir haben einen theoretischen Rahmen für die schwach überwachte Entflechtung konstruiert und viele Experimente durchgeführt, um die Theorie zu untermauern.
Wir schlagen zum ersten Mal einen expansionsbasierten Ansatz für aufgabenfreies kontinuierliches Lernen vor. Unser Modell besteht aus einer Reihe von neuronalen Netz Experten und erweitert die Anzahl der Experten nach dem Bayes'schen nichtparametrischen Prinzip.
Amortisierung der Impulse von Nesterov für ein robusteres, leichteres und schnelleres Deep Learning Training.
Wir schlagen ein neues Modell vor, das mehrere dynamische Faktoren in sequentiellen Daten entflechten kann
Wir verifizieren deterministische und probabilistische Eigenschaften neuronaler Netze mit Hilfe nicht-konvexer Relaxationen über sichtbare Transformationen, die durch generative Modelle spezifiziert werden
Ein effizientes Multi-View-Video-Zusammenfassungsschema für die Aktivitätserkennung in IoT-Umgebungen.
Die Entzerrung in tiefen neuronalen Netzen führt natürlich dazu, dass sie eine invariante Darstellung bevorzugen.
Die Wave-U-Net-Architektur, die vor kurzem von Stoller et al. für die Trennung von Musikquellen eingeführt wurde, ist äußerst effektiv für die Sprachanhebung und übertrifft den Stand der Technik.
Wir stellen einen neuartigen Rahmen für das Lernen aus Demonstrationen vor, der kontinuierliches menschliches Feedback nutzt; wir evaluieren diesen Rahmen für die kontinuierliche Steuerung von autonomen Fahrzeugen.
skalieren und verbessern VQ-VAE mit leistungsstarken Prioren, um nahezu realistische Bilder zu erzeugen.
Ein Graph Neural Network Assisted Monte Carlo Tree Search Ansatz für das Problem des Handlungsreisenden
Direktionale Nachrichtenübermittlung beinhaltet räumliche Richtungsinformationen zur Verbesserung von neuronalen Graphennetzen.
Wir entwerfen eine einfache und effiziente modellfreie Off-Policy-Methode für bildbasiertes Reinforcement Learning, die den modernsten modellbasierten Methoden in Bezug auf die Effizienz der Beispiele entspricht.
Wir stellen einen neuartigen, einfachen Operator, Chopout, vor, mit dem neuronale Netze sogar in einem einzigen Trainingsprozess so trainiert werden, dass die abgeschnittenen Teilnetze die bestmögliche Leistung erbringen.
Multimodale Guassianische Verteilung des latenten Raums in GAN-Modellen verbessert die Leistung und ermöglicht einen Kompromiss zwischen Qualität und Vielfalt
SlowMo verbessert die Optimierungs- und Generalisierungsleistung kommunikationseffizienter dezentraler Algorithmen ohne Geschwindigkeitseinbußen.
Die syntaktische Struktur der Übersetzung mit Hilfe von Codes planen
 Wir identifizieren Memorisierung als induktive Verzerrung der Interpolation in überparametrisierten, vollständig vernetzten und faltbaren Autokodierern. 
Wir stellen die Spanne eines tiefen, mehrschichtigen neuronalen Netzes mit latenter Struktur nachweislich wieder her und wenden empirisch effiziente Algorithmen zur Wiederherstellung der Spanne an, um Netzwerke durch Verschleierung der Eingaben anzugreifen.
Wir untersuchen die implizite Verzerrung des Gradientenabstiegs und beweisen unter minimalen Annahmen, dass die Parameterrichtung von homogenen Modellen zu KKT-Punkten eines natürlichen Margenmaximierungsproblems konvergiert.
wir schlagen Convolutional Tensor-Train LSTM vor, die effizient Convolutional LSTM höherer Ordnung unter Verwendung von Convolutional Tensor-Train Dekomposition erlernen. 
Bei der vorgeschlagenen Methode handelt es sich um eine neuronale End-to-End-SVM, die für das Few-Shot Learning optimiert ist.
Eine neuartige 4D CNN-Struktur für das Lernen von Repräsentationen auf Videoebene, die die bisherigen 3D CNNs übertrifft.
Wir untersuchen das Problem des Lernens und Optimierens durch physikalische Simulationen mittels differenzierbarer Programmierung unter Verwendung der von uns vorgeschlagenen DiffSim-Programmiersprache und des Compilers.
Wir stellen eine Methode zur Interpretation von Black-Box-Modellen vor, bei der durch instanzielle Rückwärtsselektion minimale Teilmengen von Merkmalen ermittelt werden, die allein ausreichen, um eine bestimmte Entscheidung des Modells zu rechtfertigen.
Wir schlagen eine Methode vor, die die Unsicherheiten von Merkmalen in jeder Schicht von DNNs extrahiert und sie zur Erkennung von OOD-Proben bei der Lösung von Klassifizierungsaufgaben kombiniert.
Wir setzen deterministische Autoencoder als generative Modelle ein, indem wir Mischfunktionen vorschlagen, die verborgene Zustände aus Bildpaaren kombinieren. Diese Mischungen werden durch ein adversarial Framework realistisch dargestellt.
Wir stellen einen erweiterten robusten Merkmalsraum für Streaming-WiFi-Daten vor, der in der Lage ist, die Konzeptabweichung für die Lokalisierung in Innenräumen zu bewältigen.
wir stellen einen prinzipiellen Ansatz für das Problem der föderierten Domänenanpassung vor, der darauf abzielt, die zwischen den verschiedenen Knoten erlernten Repräsentationen an die Datenverteilung des Zielknotens anzupassen.
Kapselnetzwerke mit gelernten Pose-Matrizen und EM-Routing verbessern den Stand der Technik bei der Klassifizierung auf smallNORB, verbessern die Verallgemeinerbarkeit auf neue Blickwinkel und die White Box Adversarial Robustheit.  
Wir schlagen ein Meta-Lernverfahren vor, das eine übertragbare Strategie aus nur schwacher Überwachung lernt, um Syntheseaufgaben mit unterschiedlichen logischen Spezifikationen und Grammatiken zu lösen.
Machen Sie den Transformer streamfähig mit monotoner Aufmerksamkeit.
Erlernen einer optimalen Abbildung mit deepNN zwischen den Verteilungen zusammen mit theoretischen Garantien.
Ein neuartiger Ansatz zur Erstellung von unüberwachten Dokument-(Satz-)Einbettungen aus vorher trainierten Worteinbettungen
Wir führen einen statistischen Ansatz zur Bewertung der Robustheit neuronaler Netze ein, der einen informativen Begriff davon liefert, wie robust ein Netz ist, und nicht nur die herkömmliche binäre Aussage, ob eine Eigenschaft verletzt ist oder nicht.
Verbesserung der Robustheit von vortrainierten Transformationsmodellen gegen die lexikalische Überlappung durch Erweiterung der Eingabesätze der Trainingsdaten um die entsprechenden Prädikat-Argument-Strukturen 
Die Regression mit neuronalen Netzen sollte die Dirichlet-Ausgangsverteilung verwenden, wenn es sich bei den Zielen um Wahrscheinlichkeiten handelt, um die Unsicherheit der Vorhersagen zu quantifizieren.
Lernen für die Suche nach einem effizienten dichten Netz mit schichtweisem Pruning
Wir trainieren prädiktive Modelle auf der Grundlage propriozeptiver Informationen und zeigen, dass sie Eigenschaften von externen Objekten darstellen.
Eine GAN-basierte Methode zum Erlernen wichtiger topologischer Merkmale eines beliebigen Eingabegraphen.
Vorhersage des Auktionspreises von Kfz-Kennzeichen in Hongkong mit einem tiefen rekurrenten neuronalen Netz auf der Grundlage der Zeichen auf den Schildern.
Wir stellen ein neues tiefes latentes Modell natürlicher Bilder vor, das aus unbeschrifteten Datensätzen trainiert werden kann und zur Lösung verschiedener Bildwiederherstellungsaufgaben eingesetzt werden kann.
Automatische Bewertung von Aufsätzen auf spärlichen Daten durch Vergleich neuer Aufsätze mit bekannten Mustern mit Referee Network. 
Wir kombinieren das Matching Network Framework für das Lernen von wenigen Aufnahmen zu einem groß angelegten Multi-Label-Modell für die Klassifizierung genomischer Sequenzen.
Die Anwendung der Softmax-Funktion beim Training führt zu einer indirekten und unerwarteten Überwachung der Merkmale. Wir schlagen ein neues Trainingsziel vor, um explizit dichte Merkmalsregionen für lokal ausreichende Stichproben zu induzieren, um die adversarial Robustheit zu verbessern.
GAN-Darstellungen werden im Detail untersucht, und es werden Sätze von Darstellungseinheiten gefunden, die die Erzeugung semantischer Konzepte im Output steuern.
Pixelweise nächstgelegene Nachbarn zur Erzeugung mehrerer Bilder aus unvollständigen Voreinstellungen wie Bildern mit niedriger Auflösung, Oberflächennormalen, Kanten usw.
Wir bieten ein schnelles, prinzipielles adversariales Trainingsverfahren mit rechnerischen und statistischen Leistungsgarantien.
Wir zerlegen die Lücke zwischen der marginalen Log-Likelihood und der unteren Schranke der Evidenz und untersuchen die Auswirkungen des approximativen Posteriors auf die wahre Posterior-Verteilung in VAEs.
Verwendung von Ensembles und Pseudobeschriftungen für das unüberwachte Clustering 
Effizientes Lernen von Wörterbüchern durch L1-Minimierung mittels einer neuartigen Analyse der nicht-konvexen, nicht-glatten Geometrie.
Wir liefern die erste theoretische Analyse der garantierten Wiederherstellung von einschichtigen neuronalen Netzen unter Kreuzentropieverlust für Klassifizierungsprobleme.
Unser Codec für neuronale Netze (der auf Transformationskodierung und Clustering basiert) ermöglicht eine transparente Kompression neuronaler Netze mit geringer Komplexität und hoher Effizienz.
Wir beschleunigen die sichere DNN-Inferenz in vertrauenswürdigen Ausführungsumgebungen (um das 4- bis 20-fache), indem wir die Berechnung von linearen Schichten selektiv auf einen schnelleren, aber nicht vertrauenswürdigen Co-Prozessor auslagern.
In dieser Arbeit wird eine wirksame Methode zur Komprimierung neuronaler Netze vorgeschlagen, die auf jüngsten Ergebnissen der Informationstheorie beruht.
Ein allgemeiner Rahmen für die Erstellung kovarianter neuronaler Netze
In diesem Werk schlagen wir eine auf dreidimensionaler Regularisierung basierende Pruning Methode vor, um das 3D-CNN zu beschleunigen.
Ein praktischer Vorschlag für eine ethischere und verantwortungsvollere NLP-Technologie, die die Transparenz von Test- und Trainingsdaten operationalisiert
Entdeckung der Struktur funktionaler Kausalmodelle mit generativen neuronalen Netzen
Beim Pruning von Netzwerken führt die Feinabstimmung eines pruned Modells nur zu einer vergleichbaren oder schlechteren Leistung als das Training von Grund auf. Dies spricht für ein Überdenken der bestehenden Pruning-Algorithmen.
Diese Arbeit führt die Extremwerttheorie in k-means ein, um Ähnlichkeit zu messen, und schlägt einen neuen Algorithmus namens Extreme Value k-means für das Clustering vor.
Wir schlagen Tendenz-RL vor, um zielgerichtete Aufgaben mit großem Zustandsraum effizient zu lösen, indem wir automatisiertes Curriculum-Lernen und diskriminierende Formungsbelohnung verwenden, was das Potenzial hat, Robotermanipulationsaufgaben mit Wahrnehmung anzugehen.
Wir stellen Szenenprogramme vor, eine strukturierte Szenendarstellung, die sowohl das Erscheinungsbild von Objekten auf niedriger Ebene als auch die Regelmäßigkeit in der Szene auf hoher Ebene erfasst.
Komprimierung neuronaler Netze, die den Stand der Technik bei der Annäherung von Daten mit niedrigem Rang verbessert und die meisten anderen Komprimierungsverfahren ergänzt. 
Wir stellen ein Zuordnungsverfahren vor, das sparsamkeitsinduzierende Normen nutzt, um Interpretierbarkeit zu erreichen.
Wir verwenden Sparsamkeit, um die Rechenkomplexität von Varianzreduktionsmethoden zu verbessern.
Wir schlagen ein neuartiges VAE-basiertes Framework vor, das aus teilweise beobachteten Daten für Imputation und Generierung lernt. 
Einblicke in die Herausforderung der Domainanpassung bei der Vorhersage von Benutzerabsichten in Unternehmens-E-Mails.
Wir schlagen HiPPO vor, einen stabilen Algorithmus für hierarchisches Reinforcement Learning, der mehrere Ebenen der Hierarchie gleichzeitig trainieren kann und sowohl bei der Entdeckung von Fähigkeiten als auch bei der Anpassung gute Leistungen erbringt.
Wir gehen einen Schritt in Richtung der Messung der Schwierigkeit von Lernaufgaben und zeigen, dass die Leistung in der Praxis stark von der Übereinstimmung zwischen der Darstellung der Informationen und dem Modell, das sie interpretiert, abhängt.
Wir kombinieren Multi-Output-Gauß-Prozesse mit tiefen rekurrenten Q-Netzen, um optimale Behandlungen für Sepsis zu erlernen, und zeigen eine verbesserte Leistung im Vergleich zu standardmäßigen tiefen Verstärkungslernmethoden,
Wir entwickeln ein neues tiefes generatives Modell für halb-überwachtes Lernen und schlagen eine neue Max-Min-Kreuzentropie für das Training von CNNs vor.
Wir schlagen eine einfache Randomisierungstechnik vor, um die Generalisierung beim Deep Reinforcement Learning über Aufgaben mit verschiedenen ungesehenen visuellen Mustern zu verbessern.
Beschreibt eine Studie zur Untersuchung von Interferenz, Übertragung und Beibehaltung von mehreren Zuordnungen mit demselben Satz von Akkordtasten
Formale Überprüfung einer Spezifikation über die Untersensitivität der Vorhersage eines Modells mit Hilfe von Interval Bound Propagation
Wir schlagen ein neuartiges 8-Bit-Format vor, das eine Verlustskalierung, stochastische Rundung und andere Techniken mit geringer Genauigkeit überflüssig macht.
Neuartiger Algorithmus für inkrementelles Lernen von VAE mit fester Architektur
MAML ist großartig, aber es hat viele Probleme. Wir lösen viele dieser Probleme und als Ergebnis lernen wir die meisten Hyper-Parameter Ende-zu-Ende, beschleunigen das Training und die Inferenz und setzen eine neue SOTA in Few-Shot Learning
Erkennung von sich überschneidenden Gemeinschaften in Graphen mit Hilfe neuronaler Netze
Wir widerlegen empirisch eine grundlegende Hypothese der weit verbreiteten Strategie der getilten Nutzing von Gewichten bei der Suche in neuronalen Architekturen und erklären, warum die NAS-Algorithmen nach dem Stand der Technik eine ähnliche Leistung wie die Zufallssuche erbringen.
In diesem Beitrag verwenden wir die Sliced-Wasserstein-Distanz, um die latente Verteilung eines Autoencoders in eine beliebige, abtastbare Prioritätsverteilung zu bringen. 
Wir stellen eine Klasse generativer Modelle vor, die aus hochdimensionalen Beobachtungen zuverlässig die Hamilton-Dynamik lernen. Der gelernte Hamiltonian kann zur Sequenzmodellierung oder als normalisierender Fluss verwendet werden.
In diesem kurzen Beitrag stellen wir kurz die Vorteile des Einsatzes von KI-Planung in der Cloud-Migration, einen vorläufigen Prototyp sowie die Herausforderungen vor, die von der Planungs- und Scheduling-Gesellschaft zu beachten sind.
Eine allgemeine Obergrenze für das Risiko des Zielgebiets, die die Rolle der Komplexität der Einbettung widerspiegelt.
Um multivariate stationäre Zeitreihen zu prognostizieren, lernen wir Einbettungen mit kontextuellen Merkmalen innerhalb eines RNN; wir wenden das Framework auf Daten des öffentlichen Nahverkehrs an
Wir untersuchen ein Framework für die Entdeckung: die Kuratierung einer großen Sammlung von Vorhersagen, die verwendet werden, um die Repräsentation des Agenten in teilweise beobachtbaren Domänen zu konstruieren.
Eine globale Geolokalisierungsstrategie mit einer neuartigen Vernetzungsstrategie und der Einbeziehung zusätzlicher Informationen kann zur Verbesserung der Gesamtleistung eines Geolokalisierungsmodells verwendet werden.
Technik zum Lernen tiefer generativer Modelle mit gemeinsamen latenten Variablen, angewandt auf Omniglot mit einem PixelCNN-Decoder.
In diesem Beitrag stellen wir eine aufgabenagnostische Lesearchitektur für die dynamische Integration von explizitem Hintergrundwissen in neuronale NLU-Modelle vor. 
Dynamische Präzisionstechnik zum Trainieren tiefer neuronaler Netze
Wir führen die ISRLU-Aktivierungsfunktion ein, die kontinuierlich differenzierbar und schneller als ELU ist. Die zugehörige ISRU ersetzt tanh & sigmoid.
Wir untersuchen das Problem der kompositorischen Generalisierung und schlagen eine Methode vor, mit der neuronale Netzarchitekturen die Fähigkeit erhalten, sich selbst zu bilden, um diese Probleme zu lösen.
Wir erweitern die Informationsengpass-Methode auf die unüberwachte Multiview-Umgebung und zeigen Ergebnisse auf dem neuesten Stand der Technik für Standarddatensätze
Wir beschreiben einen biologisch plausiblen Lernalgorithmus für rekurrente Festpunktnetze ohne gebundene Gewichte
Wir trainieren ein generatives 3D-Modell von Formen aus natürlichen Bildern auf völlig unüberwachte Weise.
Wir zeigen, dass ein relativ einfaches Black-Box-Angriffsschema, das Bayes'sche Optimierung und Dimension Upsampling verwendet, den bestehenden Methoden vorzuziehen ist, wenn die Anzahl der verfügbaren Abfragen sehr gering ist.
Wir greifen die einfache Idee des Beschneidens von Verbindungen von DNNs durch $\ell_1$ Regularisierung wieder auf und erreichen so State-of-the-Art Ergebnisse auf mehreren Datensätzen mit theoretischen Garantien.
Ein nicht-parametrisches Verfahren zur Messung der Fehlermomente von Regressoren ohne Bodenwahrheit kann bei verzerrten Regressoren verwendet werden
Charakterisierung neuronaler Netze für die Erkennung und das Verständnis von Klassifizierern im Hintergrund.
Wir können hochdimensionale Beschränkungen aus Demonstrationen lernen, indem wir unsichere Trajektorien abtasten und eine bekannte Beschränkungsparametrisierung nutzen.
Parametrisches Manifold Learing mit neuronalen Netzen in einem geometrischen Rahmen 
Wir schlagen eine Verallgemeinerung von Besuchszählern vor, die den sich ausbreitenden Erkundungswert über Trajektorien auswerten und so eine effiziente Erkundung für modellfreie RL ermöglichen
Wir schlagen eine neue Kombination aus Evolutionsstrategie und Deep Reinforcement Learning vor, die das Beste aus beiden Welten vereint
Ein empirischer Vergleich von Bayesian Deep Networks für Thompson Sampling
Verstehen, wie Klassenbezeichnungen beim GAN-Training helfen. Vorschlag für eine neue Bewertungsmetrik für generative Modelle. 
Schneller iterativer Algorithmus zum Ausgleich der Energie eines Netzes unter Beibehaltung der gleichen funktionalen Äquivalenzklasse
Wir finden Umgebungsbedingungen, in denen SOTA-Agenten, die auf Navigationsaufgaben trainiert wurden, extreme Fehler zeigen, was auf Fehler bei der Generalisierung schließen lässt.
Robuste Bayes'sche Schätzung mittels maximaler mittlerer Diskrepanz
Wir entwickeln einen unverzerrten Schätzer für die logarithmische Wahrscheinlichkeit von Modellen mit latenten Variablen und erweitern damit den Anwendungsbereich solcher Modelle.
Kleinere Batch Größen können bei konstanten Schrittbudgets und richtig eingestellten Lernratenplänen sehr große Batch Größen auf der Testmenge übertreffen.
Besserer Deep Reinforcement Learning Algorithmus zur Annäherung an die kontrafaktische Regret Minimierung
Erzeugt nie gesehene Daten während des Trainings aus einer gewünschten Bedingung 
Wir analysieren den Gradientenabstieg für tiefe lineare neuronale Netze und bieten eine Garantie für die Konvergenz zum globalen Optimum mit einer linearen Rate.
Eine Schicht, die lokale zufällige Konnektome im Kortex in tiefen Netzen modelliert, die in der Lage sind, allgemeine nicht-parametrische Invarianten aus den Daten selbst zu lernen.
Wir stellen ein effektives, allgemeines Framework für die Einbeziehung von Konditionierungsinformationen in inferenzbasierte generative Modelle vor.
In diesem Beitrag werden drei Techniken beschrieben, die es einem nicht rückverfolgenden, rechenintensiven Scheduler ermöglichen, eine kleine Anzahl alternativer Aktivitäten auf der Grundlage der Ressourcenverfügbarkeit zu berücksichtigen.
Wir verwenden einfache und biologisch motivierte Modifikationen von Standard-Lerntechniken, um bei katastrophalen Vergessens-Benchmarks den neue Bestwerte zu erreichen.
Einsatz von Deep-Learning-Techniken bei Aufgaben im Zusammenhang mit Gesangsstimmen.
Spracherzeugung unter Verwendung von seq2seq-Modellen, die bei jedem Schritt Worteinbettungen anstelle einer Softmax-basierten Verteilung über das Vokabular erzeugen, was ein wesentlich schnelleres Training bei gleichbleibender Erzeugungsqualität ermöglicht
Ergebnisse in geschlossener Form für tiefes Lernen im Grenzbereich der Schichtenentkopplung, anwendbar auf Residualnetze
 In dieser Arbeit wird eine neue Methode vorgestellt, die wir Centered Initial Attack (CIA) nennen. Sie sorgt dafür, dass die maximale Störung kleiner ist als ein zuvor festgelegter Schwellenwert, ohne dass ein Clipping-Prozess stattfindet.
Beantwortung einer breiten Klasse von logischen Abfragen über Wissensgraphen mit Boxeinbettungen im Vektorraum
Wir befassen uns mit der vollständig parallelen Hyperparameter-Optimierung mit Determinanten-Punkt-Prozessen. 
Ein mehrstufiger spektraler Ansatz zur Verbesserung der Qualität und Skalierbarkeit der unüberwachten Grapheneinbettung.
Ein neues generatives Modell für diskrete strukturierte Daten. Das vorgeschlagene stochastische Lazy-Attribut wandelt die semantische Offline-Prüfung in eine Online-Anleitung für die stochastische Dekodierung um, die die syntaktischen und semantischen Beschränkungen wirksam berücksichtigt und außerdem eine bessere Leistung erzielt
die vorgeschlagenen Modelle mit externem Wissen verbessern weiter den Stand der Technik auf dem SNLI-Datensatz.
Ansatz zur Verbesserung der Vorhersagegenauigkeit durch das Erlernen von tiefen Merkmalen über benachbarte Szenenbilder bei der Analyse von Satellitenszenenbildern.
Wir schlagen physikbewusste Differenzgraphen-Netzwerke vor, die so konzipiert sind, dass sie räumliche Unterschiede zur Modellierung spärlich beobachteter Dynamik effektiv lernen.
Lernen funktional zerlegter Hierarchien für kontinuierliche Navigationsaufgaben
In dem Werk wird ein neuer Algorithmus beschrieben, mit dem Klangkorrespondenzmuster für mehrere Sprachen abgeleitet werden können.
Eine biologisch plausible Lernregel für das Training rekurrenter neuronaler Netze
Eine neue Methode zum unbeaufsichtigten Lernen von Repräsentationen auf Graphen, die auf der Maximierung der gegenseitigen Information zwischen lokalen und globalen Repräsentationen in einem Graphen beruht. Ergebnisse auf dem neuesten Stand der Technik, konkurrenzfähig mit überwachtem Lernen.
Ein Rückwärtsmodell des vorherigen Zustands (Zustand, Aktion) unter Berücksichtigung des nächsten Zustands, d.h. P(s_t, a_t | s_{t+1}), kann verwendet werden, um zusätzliche Trajektorien zu simulieren, die an interessanten Zuständen enden! Verbessert die Effizienz des RL-Lernens.
In dieser Arbeit geht es um eine tensorbasierte Methode zum Training der Präpositionsrepräsentation.
Wir verwenden eine kontinuierliche Zeitdynamik, um ein generatives Modell mit exakten Likelihoods und effizientem Sampling zu definieren, das durch uneingeschränkte neuronale Netze parametrisiert ist.
Ein neuer, nicht-adversarialer, auf Feature-Matching basierender Ansatz zum Trainieren generativer Modelle, der die besten Ergebnisse erzielt.
Eine neuartige Architektur für die Klassifizierung mit wenigen Aufnahmen, die mit Unsicherheiten umgehen kann.
Wir zeigen, dass CNNs und ResNets mit geeigneten Prioritäten auf den Parametern Gauß-Prozesse im Grenzfall von unendlich vielen Convolutional Filtern sind.
Wir stellen eine End-to-End-Design-Methode für den effizienten Einsatz von Deep Learning vor. 
Beschleunigen Sie die verteilte Optimierung durch Ausnutzung von Nachzüglern.
Mit Hilfe eines neuartigen differenzierbaren Renderers schlagen wir eine neue Metrik vor, die sich in der Praxis auf die Bewertung von Algorithmen für gegnerisches maschinelles Lernen auswirkt und den Mangel an Realismus der bestehenden Metrik auf der Grundlage von Pixelnormen behebt.
Wir zeigen, dass die Transformer-Architektur und die Neural GPU Turing-komplett sind.
Wir verwenden VAE, um die Formmerkmale für die automatische Segmentierungsbewertung zu erfassen.
Wir untersuchen die Eigenwerte der linearen Schichten in tiefen Netzwerken und zeigen, dass die Verteilungen während des Trainings ein Heavy-Tail-Verhalten entwickeln.
Wir schlagen ein neuartiges, durchgängig trainierbares Aufmerksamkeitsmodul vor, das durch die Verwendung von co-propagierenden RNN mit CNN einen globalen Gewichtsausgleich zwischen den Schichten herstellt.
Durch das bio-mimetische MothNet-Modell automatisch generierte Merkmale verbessern die Testgenauigkeit von Standard-ML-Methoden auf vektorisiertem MNIST signifikant. Die von MothNet generierten Merkmale übertreffen auch Standard-Merkmalsgeneratoren.
Durch eine stärkere Fokussierung auf die endgültigen Vorhersagen in Prädiktoren für beliebige Zeiträume (wie z. B. die jüngsten Multi-Scale-DenseNets) können wir erreichen, dass kleine Modelle für beliebige Zeiträume besser abschneiden als große Modelle, die keinen solchen Fokus haben. 
Wir schlagen ein speichereffizientes Lernverfahren vor, das die Reversibilität der Netzschichten ausnutzt, um ein datengesteuertes Design für die computergestützte Bildgebung in großem Maßstab zu ermöglichen.
Neuron as an Agent (NaaA) ermöglicht es uns, Multi-Agenten-Kommunikation ohne eine vertrauenswürdige dritte Partei zu trainieren.
Eine neue Pretraining-Methode, die bei den GLUE-, RACE- und SQuAD-Benchmarks mit weniger Parametern im Vergleich zu BERT-large neue Spitzenergebnisse erzielt. 
Deep Learning auf strukturierten tabellarischen Daten unter Verwendung von zweidimensionaler Worteinbettung mit fein abgestimmtem ImageNet vortrainiertem CNN-Modell.
Verbindungen zwischen prädiktiver Kodierung und VAEs + neue Grenzen
Neuartige Varianten von Optimierungsmethoden, die die Vorteile von adaptiven und nicht-adaptiven Methoden kombinieren.
In dieser Arbeit haben wir einen neuartigen Algorithmus, GenDICE, für die allgemeine stationäre Verteilungskorrekturschätzung vorgeschlagen, der sowohl diskontierte als auch durchschnittliche Off-Policy-Evaluierung auf mehreren verhaltensagnostischen Stichproben verarbeiten kann.
Eine intuitive empirische und visuelle Untersuchung der Generalisierungseigenschaften von tiefen neuronalen Netzen.
Wir verwenden Convolution, damit sich neuronale Netze eher wie symbolische Systeme verhalten.
Analytische Formulierung von Phänomenen der äquatorialen Stehenden Welle: Anwendung auf QBO und ENSO
Wir schlagen vor, für jede Klasse ein invertierbares neuronales Netz zu trainieren, um Klasse für Klasse ein kontinuierliches Lernen durchzuführen.
Ein neuartiges Ensemble aus abruf- und erzeugungsbasierten Systemen für Konversationssysteme in offenen Domänen.
Verstehen der Übertragbarkeit unter den Gesichtspunkten der verbesserten Verallgemeinerung, der Optimierung und der Durchführbarkeit der Übertragbarkeit.
Wir beweisen, dass es ReLU-Netze gibt, deren Parameter fast eindeutig durch die Funktion bestimmt sind, die sie implementieren.
Wir schlagen ein Top-Down-Modulationsnetzwerk für Multi-Task-Learning-Anwendungen vor, das mehrere Vorteile gegenüber den derzeitigen Systemen aufweist.    
Neuartiger Algorithmus zum Clustering von Zeitreihendaten auf der Grundlage dynamischer Systemmerkmale.
Eine Methode zur Förderung der axiomatischen Merkmalszuweisungen eines tiefen Modells, die der menschlichen Intuition entspricht.
Wir schlagen leistungsstarke LSTMs mit binären/ternären Gewichten vor, die die Implementierungskomplexität erheblich reduzieren können.
Wir befassen uns mit dem Problem der unbeaufsichtigten Objekterkennung mit wenigen Aufnahmen, bei der alle Trainingsbilder unbeschriftet sind und keine gemeinsamen Klassen mit den Testbildern haben.
Abfragebasierte Black-Box-Angriffe auf tiefe neuronale Netze mit Erfolgsraten, die denen von White-Box-Angriffen entsprechen
Wir konvertieren Subgraphen in strukturierte Bilder und klassifizieren sie mit 1. Deep Learning und 2. Transfer Learning (Caffe) und erzielen erstaunliche Ergebnisse.
Selbstorganisierender Algorithmus für visuelle Domänenanpassung, State of the Art-Ergebnisse, Gewinner der VisDA-2017 Image Classification Domain Adaptation Challenge.
Eine VAE-Variante, die verschiedene Bilder erzeugen kann, die neuartigen konkreten oder abstrakten "Konzepten" entsprechen, die durch Attributvektoren beschrieben werden.
Wir schlagen eine modellbasierte Methode namens "Search with Amortized Value Estimates" (SAVE) vor, die sowohl reale als auch geplante Erfahrungen nutzt, indem sie Q-Learning mit Monte-Carlo Tree Search kombiniert und so eine starke Leistung mit sehr kleinen Suchbudgets erzielt.
Ein kurzer Beweis für die Gleichwertigkeit von Soft-Q-Learning und Policy-Gradienten.
Wir schlagen einen Algorithmus zur Übertragung von Richtlinien vor, der große und schwierige Diskrepanzen in der Systemdynamik überwinden kann, wie z. B. Latenzzeiten, Modellierungsfehler der Aktoren usw.
Lernen Sie, wie man Sprachsignale quantisiert und Algorithmen, die diskrete Eingaben erfordern, auf Audiodaten anwendet, wie z. B. BERT.
Wir entwickeln Hierarchical Agent with Self-play (HASP), einen Lernansatz zur Erlangung hierarchisch strukturierter Strategien, die in wettbewerbsfähigen strategischen Echtzeitspielen eine höhere Leistung als herkömmliches Selbst-spiel Spiele erreichen können.
Einbettung von Wörtern mit variabler Kapazität und SOTA auf WikiText-103, Billion Word Benchmarks.
eine tiefe multivariate Mischung Gaußscher-Modell für Bounding-Box-Regression bei Okklusion
Wir ersetzen die Lp-Ball-Beschränkung durch die Voronoi-Zellen der Trainingsdaten, um robustere Modelle zu erzeugen. 
Wir schlagen vor, eine verallgemeinerte Strategie für natürlichsprachige Navigationsaufgaben durch umgebungsunabhängiges Multitasking zu erlernen.
wir schlagen vor, Wasserstein-Baryzentren für die Zusammenstellung semantischer Modelle zu verwenden
Label-effiziente Audio-Klassifizierung durch Multi-Task-Lernen und Selbst-Überwachung
Es werden erste Methoden zur exakten Optimierung der Softmax-Verteilung unter Verwendung des stochastischen Gradienten vorgeschlagen, wobei die Laufzeit unabhängig von der Anzahl der Klassen oder Datenpunkte ist.
Verwendung von Monte-Carlo-Baumsuche und Homoglyphen zur Erzeugung ununterscheidbarer adversarialer Muster in Textdaten
Lernen, wie man eine handgezeichnete Skizze in ein High-Level-Programm umwandelt
In diesem Beitrag werden Prinzipien aus dem Bereich der Kalibrierung des maschinellen Lernens auf die Logits eines neuronalen Netzes angewandt, um sich gegen feindliche Angriffe zu schützen.
Wir trainieren gemeinsam ein mehrsprachiges Skip-Gram-Modell und ein sprachübergreifendes Satzähnlichkeitsmodell, um qualitativ hochwertige mehrsprachige Texteinbettungen zu erlernen, die auch im Szenario mit geringen Ressourcen gut funktionieren.
Wir haben ein flexibles generatives Modell vorgeschlagen, das durch direkte Minimierung der exakten empirischen Wasserstein-Distanz stabil lernt.
Eine Studie darüber, wie verschiedene Komponenten in der NAS-Pipeline zur endgültigen Genauigkeit beitragen. Außerdem ein Benchmarking von 8 Methoden auf 5 Datensätzen.
Auffinden von Korrespondenzen zwischen Domänen durch Abgleich/Mapping-Iterationen
ein neuronales, sparsamkeitsverbessertes Themenmodell auf der Grundlage von VAE
Keras für unendliche neuronale Netze.
Wir stellen NLProlog vor, ein System, das regelbasierte Schlussfolgerungen auf natürlicher Sprache durch vortrainierte Satzeinbettungen und Feinabstimmung mit Evolutionsstrategien durchführt, und wenden es auf zwei Multi-Hop-Fragebeantwortungsaufgaben an.
Wir schlagen Fidelity-weighted Learning vor, einen halb-überwachten Lehrer-Schüler-Ansatz für das Training neuronaler Netze mit schwach markierten Daten.
Wir beweisen, dass der Gradientenabstieg trotz Überparametrisierung in einem Modell mit reichhaltigen Daten robust gegenüber Etikettenfehlern ist.
Wir stellen ein neues Gewichtskodierungsverfahren vor, das eine hohe Kompressionsrate und eine schnelle Umwandlung von dünner in dichte Matrix ermöglicht.
Verbesserung eines GAN-basierten Pixel-Inpainting-Netzwerk für die komprimierte seismische Bildwiederherstellung und Vorschlag einer ungleichmäßigen Umfrage Empfehlung, die leicht auf medizinische und andere Bereiche für die komprimierte Sensing-Technik angewendet werden kann.
Wir entwickeln Simplified Action Decoder, einen einfachen MARL-Algorithmus, der den bisherigen SOTA auf Hanabi bei Spielen mit 2 bis 5 Spielern deutlich übertrifft.
Durchgängig trainierbare optische Zeichenerkennung auf gedruckten Dokumenten; wir erzielen modernste Ergebnisse und übertreffen Tesseract4 in Benchmark-Datensätzen sowohl in Bezug auf die Genauigkeit als auch auf die Laufzeit, indem wir einen rein auf Computer Vision basierenden Ansatz verwenden.
NovoGrad - eine adaptive SGD-Methode mit schichtweiser Gradienten-Normalisierung und entkoppeltem Gewichtsverfall. 
Bessere Audiosynthese durch Kombination von interpretierbarem DSP mit Ende-zu-Ende-Lernen.
Ein neuartiger Ansatz zur Graphenklassifikation auf der Grundlage von spektralen Graph Convolutional Networks und seine Erweiterung auf Multigraphen mit lernbaren Beziehungen und hierarchischer Struktur. Wir zeigen Ergebnisse auf dem neuesten Stand der Technik für chemische, soziale und Bild-Datensätze.
Wir zeigen, wie man erfolgreich Backdoor-Angriffe durchführt, ohne die Trainingsetiketten zu ändern.
Wir stellen fest, dass tiefe Netzwerke, die schlecht verallgemeinern, stärker auf einzelne Richtungen angewiesen sind als solche, die gut verallgemeinern, und evaluieren die Auswirkungen von Dropout und Batch-Normalisierung sowie von Klassenselektivität auf die Abhängigkeit von einer Richtung.
Anstatt die Parameter eines grafischen Modells aus Daten zu lernen, lernen Sie ein Inferenznetz, das die gleichen probabilistischen Fragen beantworten kann.
Wir zeigen, wie Restblöcke als Gauß-Newton-Schritte betrachtet werden können; wir schlagen einen neuen Restblock vor, der Informationen zweiter Ordnung ausnutzt.
Wir stellen ein maschinelles Lernmodell vor, das bereichsunabhängige Merkmale verwendet, um die Kritikalität des aktuellen Zustands für einen bekannten unerwünschten Zustand zu schätzen.
Wir entwickeln einen Rahmen, um modulare interne Repräsentationen in generativen Modellen zu finden und dann zu manipulieren, um kontrafaktische Beispiele zu erzeugen.
Hebbsche plastische Gewichte können sich wie ein komprimierter episodischer Gedächtnisspeicher in neuronalen Netzen verhalten, was ihre Fähigkeit verbessert, katastrophales Vergessen beim kontinuierlichen Lernen zu lindern.
Wir entwickeln einen Ansatz zur Parzellierung einer versteckten Schicht in einem DNN in funktional verwandte Gruppen, indem wir spektrales Co-Clustering auf die Attributionswerte der versteckten Neuronen anwenden.
Einsatz von Anwendungen zur Textklassifizierung und Stimmungsanalyse für Englisch und Chinesisch auf einem 300mW CNN-Beschleunigerchip für On-Device-Anwendungsszenarien.
Wir entwickeln VAEs, bei denen der Encoder einen Modellparametervektor als Eingabe erhält, so dass wir für viele Modelle eine schnelle Inferenz durchführen können
Secret ist eine Transfermethode für RL, die auf der Anrechnung von Studienleistungen beruht.
Arbeit an generativen Wissensgraphenmodellen zur besseren Einschätzung der Vorhersageunsicherheit bei der Wissensinferenz. 
Ein auf Gradientenfluss basierendes dynamisches System für invertierbare generative Modellierung
Wir stellen eine effiziente Speicherschicht vor, die die Repräsentation und die Vergröberung von Eingabegraphen gleichzeitig erlernen kann, ohne sich auf die Weitergabe von Nachrichten zu verlassen.
Ein neuartiges Kodierungsschema, das {-1, +1} verwendet, um QNNs in binäre Netze mit mehreren Zweigen zu zerlegen, in denen wir bitweise Operationen (xnor und bitcount) verwenden, um eine Modellkompression, eine Beschleunigung der Berechnungen und eine Einsparung von Ressourcen zu erreichen. 
Wir schlagen einen neuen Weg vor, um bedingte Bildinformationen in den Diskriminator von GANs zu integrieren, indem wir eine Merkmalsfusion verwenden, die für strukturierte Vorhersageaufgaben verwendet werden kann.
Wir schlagen einen Algorithmus zum Erlernen nützlicher Fähigkeiten ohne Belohnungsfunktion vor und zeigen, wie diese Fähigkeiten zur Lösung nachgelagerter Aufgaben genutzt werden können.
Diese Arbeit löst ein lexikalisches Mehrdeutigkeitsproblem, das durch Homonyme in der neuronalen Übersetzung durch BERT verursacht wird.
In diesem Beitrag geht es um die synthetische Generierung von Daten zur menschlichen Mobilität in städtischen Gebieten mit Hilfe von GANs. 
In dieser Arbeit wird ein effektives Kodierungsschema für neuronale Netze vorgeschlagen, das einen zufälligen Satz von Gewichten aus einer Variationsverteilung kodiert.
Durch die Analyse eines Algorithmus, der einen nicht-konvexen Verlust minimiert, zeigen wir, dass bis auf einen kleinen Bruchteil das gesamte Rauschen aus einem Bild entfernt werden kann, indem ein auf einem tiefen neuronalen Netzwerk basierendes generatives Prior verwendet wird.
Die groß angelegte Multitasking-Architektur löst ImageNet und Übersetzung gemeinsam und zeigt Transfer-Lernen.
Kontinentalphilosophisch inspirierter Ansatz zum Lernen mit wenigen Daten.
Erlernen der Synthese von Audio-Rohwellenformen mit GANs
Ein neuartiges Verfahren zur Anpassung von Domänen, um Vielfältigkeiten aus Quell- und Zieldomänen mit Hilfe von Labelpropagation für eine bessere Genauigkeit abzugleichen.
Dieses Papier schlägt eine neue Methode für das Lernen von neuronalen Netzen in Online Bandit Umgebungen vor, indem es die letzte Schicht marginalisiert
Wir zeigen in Theorie und in Praxis, dass die Kombination mehrerer Erklärungsmethoden für DNN die Erklärung verbessert.
Lernen mit begrenzten Trainingsdaten durch Ausnutzung "hilfreicher" Instanzen aus einer umfangreichen Datenquelle.  
Wir erhöhen den Umfang der Spurüberwachung, die beim Training voll differenzierbarer neuronaler Maschinenarchitekturen genutzt werden kann.
Wir schlagen Bayes'sche quantisierte Netzwerke vor, für die wir eine Posterior-Verteilung über ihre quantisierten Parameter lernen.
Kaskadentraining und Ähnlichkeitslernen auf niedriger Ebene verbessern die Robustheit gegen White-Box- und Black-Box-Angriffe.
Wir zeigen, dass das Problem der explodierenden Gradienten entgegen der landläufigen Meinung noch nicht gelöst ist und dass es die Tiefe begrenzt, bis zu der MLPs effektiv trainiert werden können. Wir zeigen, warum Gradienten explodieren und wie ResNet mit ihnen umgeht.
Wir haben herausgefunden, dass adversariales Training nicht nur das GAN-Training beschleunigt, sondern auch die Bildqualität verbessert.
Eine Methode zur Binarisierung sowohl der Gewichte als auch der Aktivierungen eines tiefen neuronalen Netzes, die rechen- und speichereffizient ist und bessere Ergebnisse als der Stand der Technik liefert.
Wir stellen Good-Enough Model Spaces (GEMS) vor, einen Rahmen für das Lernen eines aggregierten Modells über verteilte Knoten innerhalb einer kleinen Anzahl von Kommunikationsrunden.
Wir schlagen einen neuen Auto-Encoder vor, der auf der Wasserstein-Distanz basiert und die Sampling-Eigenschaften der VAE verbessert.
Wahrung der differential privacy beim adversarial Lernen mit beweisbarer Robustheit gegenüber gegnerischen Beispielen
Wir konstruieren und untersuchen automatisch einen kleinen abstrakten Markov-Entscheidungsprozess, der es uns ermöglicht, bei Montezumas Rache, Pitfall! und Private Eye mit deutlichem Abstand die besten Ergebnisse zu erzielen.
Wir zeigen, dass die KL-Kontrolle durch einen vortrainierten Prior RL-Modelle in die Lage versetzt, aus einem statischen Stapel gesammelter Daten zu lernen, ohne die Möglichkeit, die Umgebung online zu erkunden.
Übertragung von Einzelmaßnahmen in einer Familie von Umgebungen mit verwandter Dynamik durch optimiertes Sondieren zur schnellen Ermittlung latenter Variablen und sofortige Ausführung einer universellen Maßnahme.
Wir schlagen ein Graph Convolutional Network basiertes Encoder-Decoder Modell mit sequentieller Aufmerksamkeit für zielorientierte Dialogsysteme vor.
Mechanismus zur Einbettung von Knotenfolgen, der sowohl Graph- als auch Texteigenschaften erfasst.
Dieses Papier zielt darauf ab, die guten Eigenschaften von robusten visuellen Merkmalen wie SIFT zu nutzen, um CNN-Architekturen im Hinblick auf bessere Genauigkeit und Robustheit zu renovieren.
Die Theorie sagt den Phasenübergang zwischen nicht erlernbaren und erlernbaren Werten von Beta für das Ziel des Informationsengpasses voraus
Erzeugen Sie verfälschte Trainingsbilder, die nicht wahrnehmbar sind, aber das CNN-Verhalten auf einem Ziel bei jedem neuen Training verändern.
Wir geben einen Algorithmus zum Lernen eines zweischichtigen neuronalen Netzes mit symmetrischer Eingangsverteilung an. 
Wir zeigen, dass das iterative Training eines Schüler- und Lehrernetzwerks, statt eines gemeinsamen, zu neuen, interpretierbaren Lehrstrategien führen kann.
Nicht-asymptotische Analyse von SGD und SVRG, die die Stärke der beiden Algorithmen in Bezug auf Konvergenzgeschwindigkeit und Rechenkosten sowohl in unter- als auch in überparametrisierten Einstellungen zeigt.
Wir untersuchen systematisch, warum die Wissensdestillation für das Training von nicht-autoregressiven Übersetzungsmodellen (NAT) entscheidend ist, und schlagen Methoden zur weiteren Verbesserung der destillierten Daten vor, um die Kapazität eines NAT-Modells bestmöglich zu erfüllen.
Es gelingt uns, Transformatoren für das Training in der RL-Umgebung zu stabilisieren und eine große Verbesserung gegenüber LSTMs auf DMLab-30 zu demonstrieren, was einer externen Speicherarchitektur entspricht.
Inspiriert von der Versuch zu Versuch Variabilität im Gehirn, die aus mehreren Rauschquellen resultieren kann, haben wir die Variabilität durch Rauschen in den Rahmen der Wissensdestillation eingeführt und ihre Auswirkungen auf die Generalisierung und Robustheit untersucht.
Wir stellen einen neuartigen Ende-zu-Ende Ansatz für das Lernen von Clustern in Abwesenheit von markierten Beispielen vor. Wir definieren eine differenzierbare Verlustfunktion, die den erwarteten normalisierten Schnitten entspricht.
Wir stellen verschiedene Datensätze für die kyrillische OCR und eine Methode zu deren Erkennung vor
Wir schlagen einen unüberwachten Weg vor, um mehrere Einbettungen für Sätze und Phrasen zu lernen 
Beispiel für effiziente Algorithmen zur Anpassung eines Text-to-Speech-Modells an einen neuen Sprachstil mit modernster Leistung.
In diesem Beitrag wird ein probabilistisches Verfahren zur Klassifizierung von k-shot-Bildern vorgestellt, das die besten Ergebnisse erzielt
Untersuchung der Kombination von rekurrenten neuronalen Netzen und Erfahrungswiedergabe, die zu einem hochmodernen Agenten auf dem Atari-57 und dem DMLab-30 mit einem einzigen Satz von Hyperparametern führt.
Unsere Kombination aus Multi-Task-Lernen und Self-Attention, bei der das Modell trainiert wird, die Eltern in einem syntaktischen Parse-Baum zu beachten, erzielt modernste CoNLL-2005- und CoNLL-2012-SRL-Ergebnisse für Modelle mit vorhergesagten Prädikaten.
Wir schlagen ein neues Modul vor, das alle ResNet-ähnlichen Architekturen verbessert, indem es den Faltungsschichten ein "kanalselektives" Verhalten aufzwingt.
Eine Theorie und ein algorithmischer Rahmen für die Vorhersage unter Verteilungsänderungen, einschließlich der Schätzung kausaler Effekte und der Anpassung von Bereichen
Wir untersuchen tiefe Ensembles durch die Linse der Verlustlandschaft und den Raum der Vorhersagen und zeigen, dass die Dekorrelationskraft von zufälligen Initialisierungen durch Unterraum-Sampling, das nur einen einzigen Modus erforscht, nicht erreicht wird.
Können wir unseren Deep-Learning-Modellen vertrauen? Ein Rahmen zur Messung und Verbesserung des Vertrauens eines Deep-Learning-Modells während des Trainings.
Ein modellbasierter RL-Ansatz, der eine differenzierbare Unsicherheitsstrafe verwendet, um Fahrstrategien aus reinen Beobachtungsdaten zu lernen.
Anpassung der Vorhersagen von Sequenzmodellen (wie LDS und RNN) über einen expliziten latenten Code.
Wir schlagen ein neuartiges Training mit Domänenanpassung vor, das die Verallgemeinerungsfähigkeit von Beispielen aus verschiedenen Angriffen deutlich verbessert.
In diesem Beitrag wird ein neuartiger, leichtgewichtiger Transformer für die Sprachmodellierung auf Zeichenebene vorgeschlagen, der gruppenweise Operationen verwendet.
Ein stabiler domänenadversarischer Trainingsansatz für robuste und umfassende Domänenanpassung
Wir verwenden maschinelles Lernen, um Synonyme für große Einkaufstaxonomien zu generieren.
MOHART verwendet einen Mechanismus der Selbstaufmerksamkeit, um relationale Schlussfolgerungen bei der Verfolgung von mehreren Objekten zu ziehen.
Wir untersuchen und schlagen Lösungen für zwei Herausforderungen beim Reinforcement Learning vor: (a) effizientes akteurskritisches Lernen mit Erfahrungswiederholung (b) Stabilität des Lernens von sehr unkonventionellen Strategien.
Wir schlagen einen Algorithmus zur Auswahl eines Gradientenschätzers vor, der die Effizienz der Optimierung verbessern soll.
Wir untersuchen das Problem der Instabilität im GAN-Trainingsverfahren durch ein neues Architekturdesign mit theoretischen Garantien.
Diese Arbeit beweist die Nicht-Beschleunigung von Nesterov SGD mit beliebigen Hyper-Parametern und schlägt einen neuen Algorithmus vor, der SGD in der überparametrisierten Umgebung nachweislich beschleunigt.
Eine Feedforward-Schicht zur Einbeziehung strukturierter Glätte in ein Deep-Learning-Modell
Deformieren Sie nicht Ihre Convolutions, sondern Ihre Kernels.
Wir stellen eine Methode zum Lernen von Proteinsequenz-Einbettungsmodellen vor, die strukturelle Informationen in Form von globaler struktureller Ähnlichkeit zwischen Proteinen und innerhalb von Protein Rest-Rest-Kontakten verwendet.
Wir betrachten den Optimierungsprozess neuronaler Netze als ein Modellauswahlproblem und stellen eine biologisch plausible Normalisierungsmethode vor, die statistische Regelmäßigkeiten unter dem MDL-Prinzip extrahiert, um das Problem unausgewogener und begrenzter Daten zu lösen.
Generative Modellierung ohne adversarial Training
Wir schlagen eine Methode des Imitation Learnings vor, um aus Demonstrationen unterschiedlicher Qualität zu lernen, die von Vorführern mit unterschiedlichem Wissensstand gesammelt wurden.
Beschreibung einer semantischen Heuristik, die auf einer OWL-S-Dienstbeschreibung aufbaut und Wort- und Satzdistanzmaße verwendet, um die Nützlichkeit von Diensten für ein bestimmtes Ziel zu bewerten. 
Topologie-basiertes Graph Convolutional Network (GCN)
Künstliche neuronale Netze entwickelten die gleichen Strukturen wie die Geruchssysteme von Fliegen und Mäusen, nachdem sie darauf trainiert wurden, Gerüche zu klassifizieren
Glatte Regularisierung über den Stichprobengraphen für die ungepaarte Bild-zu-Bild-Übersetzung führt zu einer deutlich verbesserten Konsistenz
Ein neuronales Faltungsnetzwerk für den Stereoabgleich mehrerer Ansichten, dessen Design von den besten Praktiken traditioneller geometriebasierter Ansätze inspiriert ist
Wir ergänzen das modellfreie Policy-Lernen mit einer Ersatz-Belohnungsfunktion auf Sequenz-Ebene und einem zählbasierten Besuchsbonus und demonstrieren die Effektivität bei der Entwicklung von DNA- und Proteinsequenzen in großen Mengen und mit wenigen Runden.
Wir verwenden Reinforcement Learning, um einen Agenten darauf zu trainieren, eine Reihe von visuellen Rechenaufgaben zu lösen, indem wir vortrainierte Wahrnehmungsmodule und Transformationen der von diesen Modulen erzeugten internen Repräsentationen verwenden.
Es wird ein neuer RL-Algorithmus mit der Bezeichnung "Interior Policy Differentiation" vorgeschlagen, um eine Sammlung verschiedener Strategien für eine gegebene primäre Aufgabe zu lernen.
Eine empirische Bewertung generativer adversarischer Netze
Vorhersage von numerischen Attributwerten, die mit Entitäten in Wissensdatenbanken verbunden sind.
Wir schlagen Verfahren zur Bewertung und Stärkung des kontextuellen Einbettungsabgleichs vor und zeigen, dass sie sowohl den Zero-Shot-XNLI-Transfer des mehrsprachigen BERT verbessern als auch nützliche Erkenntnisse über das Modell liefern.
Wir verwenden neuronale Netze, die für die Bildentrauschung trainiert wurden, als Plug-and-Play-Prioren in Energieminimierungsalgorithmen für Bildrekonstruktionsprobleme mit nachweisbarer Konvergenz.
Wir schlagen eine neue Methode zur Kalibrierung von Modellen zur Einbettung von Wissensgraphen vor, ohne dass Negativbeispiele benötigt werden.
Ein neuartiger adaptiver Curriculum-Lernverlust für tiefe Gesichtserkennung
Das adversarische Training mit einstufigen Methoden übertrifft die Anforderungen und bleibt anfällig für einfache Blackbox- und Whitebox-Angriffe. Wir zeigen, dass die Einbeziehung von adversarischen Beispielen aus mehreren Quellen hilft, Blackbox-Angriffe abzuwehren.
In diesem Beitrag wird ein neuer Ansatz für die Einbeziehung der gewünschten Invarianz in das Lernen von Darstellungen vorgeschlagen, der auf der Beobachtung beruht, dass der aktuelle Stand der Technik bei AFL praktische Probleme aufweist.
Die Verlustfläche ist *sehr* degeneriert, und es gibt keine Barrieren zwischen Lösungen für große und kleine Batches.
Wir schlagen CR-NAS vor, um engagierte Rechenressourcen in unterschiedlicher Auflösung und räumlicher Position neu zuzuweisen.
Eine Trainingsmethode, mit der Deep-Learning-Algorithmen auf neuromorphen Computerchips mit Unsicherheit besser funktionieren können
Eine CNN-Architektur, die die Unbekannten in den Testobjekten wirksam zurückweisen kann
Wir führen eine amortisierte Variationsinferenz auf einem latenten Gaußschen Prozessmodell durch, um eine bessere Imputationsleistung bei multivariaten Zeitreihen mit fehlenden Daten zu erzielen.
Wir führen umfangreiche experimentelle Studien durch, um die Beziehungen zwischen Jacobi-Normen, linearen Regionen und Generalisierung zu charakterisieren.
Das Rauschen im Parameterraum ermöglicht es den Algorithmen des Reinforcement Learnings, durch Störung von Parametern anstelle von Aktionen zu explorieren, was oft zu einer deutlich verbesserten Explorationsleistung führt.
Wir stellen neuartige Destillationstechniken vor, die es ermöglichen, Studentenmodelle mit unterschiedlichen Vokabularen zu trainieren und BERT um das 60-fache zu komprimieren, bei nur geringem Leistungsabfall.
Ende-zu-Ende Lernen von invarianten Repräsentationen mit Variablen über Beispiele hinweg, z. B. wenn jemand irgendwo hingegangen ist, ist er dort.
Wir lösen schlecht gestellte inverse Probleme mit wenigen Beispielen, indem wir ein Ensemble von zufälligen Projektionen des Modells anstelle des Modells selbst schätzen.
Eine Technik zur Beschleunigung der Auswahl neuronaler Architekturen durch Annäherung an die Gewichte der einzelnen Architekturkandidaten, anstatt sie einzeln zu trainieren.
Wir schlagen vier neue Methoden zur Erhebung von NLI-Daten vor. Einige helfen ein wenig als Pretraining-Daten, alle helfen, Annotationsartefakte zu reduzieren.
Stochastische variable Videovorhersage in realen Umgebungen.
Modellierung komplexer Multi-Agenten-Interaktionen innerhalb des Multi-Agenten Imitation Learning Frameworks mit expliziter Modellierung korrelierter Strategien durch Annäherung an die Strategien der Gegner. 
Ein System zur sprachübergreifenden (Englisch-Russisch) Plagiatserkennung
Wir haben eine Implementierung vorgeschlagen, um das parallele Training von DNN-Daten zu beschleunigen, indem der Bedarf an Kommunikationsbandbreite reduziert wird.
Störungen können zum Erlernen von Rückkopplungsgewichten in großen Fully-Connected und Convolutional Networks verwendet werden.
Wir schlagen vor, die ausreichende Anzahl von Bits für die Darstellung der Gewichte von DNNs und die optimale Bits sind konservativ bei der Lösung von realen Problemen.
Verfeinerung von Segmentierungsvorschlägen durch iterative Inferenz mit Conditional Denoising Autoencodern.
Wir schlagen eine auf Self-Attention basierende GAN-Architektur für die unbedingte Texterzeugung vor und verbessern die bisherigen Ergebnisse auf der Basis von adversarialem Code.
Wir schlagen ein neuartiges neuronales Wasserzeichen-Encoder-Decoder-Netzwerk vor. Sie führen ein kooperatives Spiel durch, um ihr eigenes Wasserzeichenverfahren zu definieren. Nutzer müssen keine Wasserzeichen-Methoden mehr entwerfen.
Wir leiten einen unverzerrten Gradientenschätzer mit geringer Varianz für Erwartungen über diskreten Zufallsvariablen ab, der auf Stichproben ohne Ersetzung basiert
Wir schlagen eine Methode vor, die es der CNN-Faltung ermöglicht, wiederkehrende Verbindungen zu schaffen
Die Gradienten clipping verleiht keine Robustheit gegenüber Etikettenrauschen, aber eine einfache verlustbasierte Variante schon.
zeigen experimenteller Beweise für die schwache Korrelation zwischen der Wahrscheinlichkeit von Datenströmen und der Semantik von Bildern.
Wir wenden eine modellagnostische Verteidigungsstrategie gegen gegnerische Beispiele an und erreichen eine White-Box-Genauigkeit von 60 % und eine Black-Box-Genauigkeit von 90 % gegen die wichtigsten Angriffsalgorithmen.
Wir beweisen den allerersten Konvergenzbeweis eines asynchronen beschleunigten Algorithmus, der einen Speedup erreicht.
Wir betten SG-MCMC-Sampler in eine Variationsapproximation ein
Wir argumentieren theoretisch, dass die einfache Annahme, dass die Gewichte eines ReLU-Netzes gaußverteilt sind (sogar ohne Bayes'schen Formalismus), dieses Problem lösen könnte; für eine kalibrierte Unsicherheit könnte eine einfache Bayes'sche Methode bereits ausreichen.
Wir verwenden Repräsentationen, die ohne parallele Daten trainiert wurden, um Wortausrichtungen zu erstellen.
Ein neuer Ansatz für das Lernen mit verrauschten Belohnungen beim Reinforcement Learning
Diese Arbeit konzentriert sich auf einen traditionell übersehenen Mechanismus - eine Architektur mit explizit entworfenen privaten und gemeinsam genutzten versteckten Einheiten, die darauf ausgelegt sind, den nachteiligen Einfluss des zusätzlichen unbeaufsichtigten Verlusts auf die überwachte Hauptaufgabe abzuschwächen.
Eine theoretische Studie zum Multi-Task-Lernen mit praktischen Implikationen für die Verbesserung von Multi-Task-Training und Transfer-Lernen
Neue Methode zur Bewertung der Qualität von Ähnlichkeitsauswertern und Aufzeigen des Potenzials von Transformer-basierten Sprachmodellen als Ersatz für BLEU und ROUGE.
Meta-Learning-Methoden, die für Vision verwendet werden und direkt auf NLP angewandt werden, schneiden bei neuen Klassen schlechter ab als die Nearest Neighbors; mit verteilten Signaturen können wir es besser machen.
Eine theoretische Analyse einer neuen Klasse von RNNs, die auf neurowissenschaftliche Aufgaben trainiert wurden, ermöglicht es uns, die Rolle der dynamischen Dimensionalität und der Zellklassen bei neuronalen Berechnungen zu identifizieren.
Wir schlagen den Fusionsdiskriminator vor, eine neuartige Architektur zur Einbeziehung bedingter Informationen in den Diskriminator von GANs für strukturierte Vorhersageaufgaben.
Wir definieren, erforschen und beginnen, das Problem der objektiven Fehlanpassung beim modellbasierten Reinforcement Learning anzugehen.
Wir geben eine detaillierte Erklärung der Trajektorien in der Informationsebene und untersuchen ihre Verwendung für das Design neuronaler Netze (Pruning)
Verwendung von Quantenverschränkungsmaßen zur Quantifizierung von Korrelationen beim Deep Learning und Nutzung der Verbindung zur Anpassung der Architektur des Deep Network an Korrelationen in den Daten.
Wir verwenden Reinforcement Learning über molekulare Graphen, um Begründungen für interpretierbare Vorhersagen molekularer Eigenschaften zu generieren.
Wir vereinen graphische Convolutional Networks als Co-Training und unitisierte Matrixfaktorisierung.
Ein flussbasiertes autoregressives Modell für die Erstellung von molekularen Graphen. Erzielung modernster Ergebnisse bei der Generierung von Molekülen und der Optimierung ihrer Eigenschaften.
Wir schlagen einen neuen Rahmen für das Lernen von Vorkonditionierern vor, leiten neue Formen von Vorkonditionierern und Lernmethoden ab und zeigen die Beziehung zu Methoden wie RMSProp, Adam, Adagrad, ESGD, KFAC, Batch-Normalisierung, etc. auf.
Einfache Techniken zur Texterweiterung können die Leistung bei Textklassifizierungsaufgaben erheblich steigern, insbesondere bei kleinen Datensätzen.
Wir schlagen ein Modell vor, das in der Lage ist, physikalische Parameter von Systemen aus Videos zu schätzen, bei denen die Differentialgleichungen, die die Dynamik der Szene steuern, bekannt sind, aber keine beschrifteten Zustände oder Objekte verfügbar sind.
ASAL ist eine poolbasierte aktive Lernmethode, die Muster mit hoher Entropie erzeugt und passende Muster aus dem Pool in sublinearer Zeit abruft.
Wir weisen auf wichtige Probleme mit der gängigen Praxis hin, die beste Leistung eines einzelnen Modells für den Vergleich von Deep-Learning-Architekturen zu verwenden, und schlagen eine Methode vor, die diese Schwächen korrigiert.
Wir untersuchen die Auswirkungen der Einbettungskomplexität beim Erlernen domäneninvarianter Repräsentationen und entwickeln eine Strategie, die die Empfindlichkeit gegenüber dieser Komplexität abschwächt.
Wir schlagen eine neue Architektur mit dem Namen Dual Adversarial Transfer Network (DATNet) für die ressourcenarme Named Entity Recognition (NER) vor und erreichen neue Spitzenleistungen bei CoNLL und Twitter NER.
Coulomb-GANs können eine Verteilung optimal erlernen, indem sie das Problem des Verteilungslernens als Optimierung eines Potentialfeldes darstellen
Wir schlagen ein konfidenzbasiertes Graph Convolutional Network für Semi-Supervised Learning vor.
Wir schlagen ImageNet-C zur Messung der Robustheit von Klassifizierungsfehlern und ImageNet-P zur Messung der Robustheit gegenüber Störungen vor.
Code mit seq2seq-Netzwerken verschlüsseln und mit dem verschlüsselten Code und dem Schlüsselpaar ausführen
GAN-basierte Methode für die gemeinsame Synthese von Bildern und Kommentaren pro Pixel
Wir stellen eine neue Aufgabe und einen neuen Datensatz zur sprachübergreifenden Navigation vor und schlagen einen allgemeinen sprachübergreifenden VLN-Rahmen für diese Aufgabe vor.
Unüberwachte Klassifizierung durch tiefe generative Modellierung mit kontrollierbarem Merkmalslernen, bewertet anhand einer schwierigen realen Aufgabe
Modelle Repräsentation Lernen über dynamischen Graphen als latenter verborgener Prozess, der zwei beobachtete Prozesse der topologischen Evolution von und Interaktionen auf dynamischen Graphen überbrückt.
Wir stellen eine Klasse von Spielen mit n Spielern vor, die sich für gradientenbasierte Methoden eignen.
Wir untersuchen die Klasse der formalen Sprachen, die von Echtzeit-Zählautomaten akzeptiert werden, ein Berechnungsmodell, das mit einigen Arten von rekurrenten neuronalen Netzen verwandt ist.
Ein Zusammenfassungsmodell, das eine neue Methode des Intra-Attention- und Reinforcement-Learnings kombiniert, um die ROUGE-Scores und die Qualität der Zusammenfassung für lange Sequenzen zu erhöhen.
Wir untersuchen, ob und wie die adaptive Datenerweiterung und die Wissensdestillation gleichzeitig auf synergetische Weise genutzt werden können, um die Ausbildung von Schülernetzwerken zu verbessern.
Wir stellen ein einfaches Verfahren vor, mit dem vortrainierte Transformer-basierte Sprachmodelle umfunktioniert werden können, um abstrakte Zusammenfassungen gut durchzuführen.
Wir stellen eine auf dem neuronalen Gedächtnis basierende Architektur für inkrementelle Domänenanpassung vor und liefern theoretische und empirische Ergebnisse.
Wir formulieren die Rückgewinnung spärlicher Signale als ein sequentielles Entscheidungsproblem und entwickeln eine auf RL und MCTS basierende Methode, die eine Strategie zur Entdeckung der Unterstützung des spärlichen Signals erlernt. 
Lernende Einbettung für Kontrolle mit hochdimensionalen Beobachtungen
Erste Ergebnisse im Schnittpunkt von Netzwerk-Neurowissenschaften und Deep Learning. C. Elegans und der visuelle Kortex einer Maus lernen, handgeschriebene Ziffern zu erkennen.
Eine unüberwachte Struktur-Lernmethode für parsimonisch tiefe Feed-Forward-Netze.
Verwendung von GANs als Priors für effiziente Bayes'sche Inferenz von komplexen Feldern.
Omniglot und miniImageNet sind zu einfach für das FewShot Lernen, weil wir sie ohne Verwendung von Labels während der Meta-Evaluierung lösen können, wie mit einer Methode namens Zentroid-Netzwerke gezeigt wurde
Identifizierung von Entscheidungszuständen (in denen der Agent wichtige Aktionen durchführen kann) ohne Belohnungsüberwachung, Verwendung für Transfer.
Ein Ansatz zur Kombination von Variationsinferenz und Bayes'scher Optimierung zur Lösung komplizierter inverser Probleme
Wir zeigen, dass es unter bestimmten Annahmen zur Fahrzeugdynamik und zur Unsicherheit der Umgebung möglich ist, automatisch Bewegungsprimitive zu synthetisieren, die im Laufe der Zeit keine Fehler akkumulieren.
Bestehende Deep Convolutional Networks für Bildklassifizierungsaufgaben sind empfindlich gegenüber Gabor-Rauschmustern, d.h. kleine strukturierte Änderungen am Input führen zu großen Änderungen am Output.
Die Untersuchung von Robustheit und Redundanz in tiefen neuronalen Netzen offenbart kapazitätsbeschränkende Merkmale, die dazu beitragen, die Nichtübereinstimmung zu erklären.
Ein skalierbarer Algorithmus, um robuste Ableitungen von tiefen Netzen in Bezug auf die Eingaben zu erstellen.
In diesem Beitrag untersuchen wir eine interne dichte und externe spärliche Netzwerkstruktur von tiefen neuronalen Netzen und analysieren ihre Schlüsseleigenschaften.
In dieser Arbeit versuchen wir, MCMC und VI durch eine neuartige hybride Methode zu verbessern, die auf der Idee basiert, die Simulationsverzerrung von MCMC-Ketten endlicher Länge durch gradientenbasierte Optimierung zu reduzieren.
Die Information darüber, ob die Ausgabe eines neuronalen Netzes richtig oder falsch sein wird, ist zum Teil in den Ausgaben der Zwischenschichten des Netzes enthalten.
Unüberwachter Algorithmus zur Selbstimitation, der aus einer einzigen Expertenvorführung Schlussfolgerungen ziehen kann.
Wir entwickeln einen Rahmen für die Generierung von menschenverständlichen Erklärungen dafür, warum in übermäßig eingeschränkten Instanzen einer Klasse von ressourcenbeschränkten Planungsproblemen Unausführbarkeit auftritt.
Wir zeigen, dass Gradienten nicht in der Lage sind, Verschiebungen in der Auffälligkeit aufgrund von Störungen durch Angreifer zu erfassen, und stellen eine alternative Verteidigung gegen Angreifer vor, die erlernte Auffälligkeitsmodelle verwendet und sowohl gegen Blackbox- als auch Whitebox-Angriffe wirksam ist.
Unzulänglichkeiten der Entflechtungsmetriken
Wir schlagen Mind-aware Multi-Agent Management Reinforcement Learning (M^3RL) für das Training eines Managers vor, um eigennützige Arbeiter zu motivieren, eine optimale Zusammenarbeit zu erreichen, indem ihnen geeignete Verträge zugewiesen werden.
Eine Methode für persistente latente Zustände in ResBlocks, die für die Superauflösung von alisierten Bildsequenzen demonstriert wird.
Wir schlagen Diversely Stale Parameters vor, um die Sperren des Backpropoagation-Algorithmus aufzubrechen und ein CNN parallel zu trainieren.
Eine zusätzliche Vorhersageaufgabe kann den Lernprozess in Sprachsystem Einstellungen beschleunigen.
TEB-Modul für IPC
Vollständig parallelisierbarer und gegen unerwünschtes Rauschen resistenter metrischer Lernalgorithmus mit theoretischen Garantien.
Wir entwickeln ansprechende, persönlichkeitsabhängige Bildbeschriftungsmodelle, die auch bei regulären Beschriftungsaufgaben den Stand der Technik repräsentieren.
Wir schlagen einen differential private Laplacian glättenden stochastischen Gradientenabstieg vor, um maschinelle Lernmodelle mit besserem Nutzen zu trainieren und differentielle Datenschutzgarantien zu erhalten.
Wir bieten eine statistische und rechnerische Analyse des One-Bit-Compressed-Sensing-Problems mit einem generativen Prior. 
Ein prinzipieller Ansatz für das Strukturlernen von tiefen neuronalen Netzen mit einer neuen Interpretation für Tiefe und Konnektivität zwischen den Schichten. 
Wir untersuchen, wie und warum die starke L1/L2-Regularisierung versagt und schlagen eine Methode vor, die eine starke Regularisierung erreichen kann.
Diese Arbeit zielt darauf ab, quantitative Antworten auf die relative Bedeutung von Konzepten von Interesse über Konzeptaktivierungsvektoren (CAV) zu geben. Dieser Rahmen ermöglicht es insbesondere Experten, die nicht aus dem Bereich des maschinellen Lernens kommen, Konzepte von Interesse zu formulieren und Hypothesen anhand von Beispielen zu testen (z. B. eine Reihe von Bildern, die das Konzept veranschaulichen). Wir zeigen, dass CAV mit einer relativ kleinen Menge von Beispielen erlernt werden kann. Hypothesentests mit CAV können die Frage beantworten, ob ein bestimmtes Konzept (z. B. das Geschlecht) bei der Vorhersage einer bestimmten Klasse (z. B. Arzt) wichtiger ist als andere Konzeptgruppen. Die Interpretation von Netzwerken mit CAV erfordert keine Umschulung oder Änderung des Netzwerks. 
Der Conditional Entropy Bottleneck ist eine informationstheoretische Zielfunktion für das Lernen optimaler Darstellungen.
Wir schlagen einen Ansatz vor, um spärliche hochdimensionale Repräsentationen zu lernen, die schnell zu durchsuchen sind, indem wir einen Ersatz der Anzahl der Operationen direkt in die Verlustfunktion einbeziehen.
Kombination von Hilfs- und Gegentraining, um das physische Verständnis zu erfragen und zu fördern.
Flussbasierte Modelle, aber nicht umkehrbar, um auch diskrete Variablen zu lernen
Wir beweisen konstruktiv, dass selbst die kleinsten nichtlinearen Aktivierungsfunktionen für allgemeine Datensätze und Aktivierungsfunktionen ungewollte lokale Minima einführen.
Kompositorische, attributbasierte Planung, die sich auf lange Testaufgaben verallgemeinern lässt, obwohl sie auf kurze und einfache Aufgaben trainiert wurde.
Wir identifizieren und formalisieren das Problem des Auswendiglernens beim Meta-Lernen und lösen dieses Problem mit einer neuartigen Meta-Regularisierungsmethode, die den Bereich, auf den Meta-Lernen anwendbar und effektiv ist, erheblich erweitert.
Wir schlagen einen neuartigen Rahmen für das Meta-Lernen einer gradientenbasierten Aktualisierungsregel vor, der über das Few-Shot Learning hinausgeht und auf jede Form des Lernens, einschließlich des kontinuierlichen Lernens, anwendbar ist.
Defense-GAN verwendet ein Generatives Adversariales Netzwerk zur Abwehr von White-Box- und Black-Box-Angriffen auf Klassifizierungsmodelle.
Wir entwickeln effiziente Methoden, um neuronale Einbettungsmodelle mit einer Punkt-Produkt-Struktur zu trainieren, indem wir die Zielfunktion in Form von verallgemeinerten Gram-Matrizen umformulieren und Schätzungen dieser Matrizen beibehalten.
Wir stellen eine Multi-Task-Benchmark- und Analyseplattform zur Bewertung der Generalisierung in Systemen zum Verstehen natürlicher Sprache vor.
Eine modulare Methode für vollständig kooperatives Multi-Ziel Multi-Agenten Reinforcement Learning, basierend auf Curriculum-Lernen für effiziente Exploration und Kreditvergabe für Aktion-Ziel-Interaktionen.
Wir befassen uns mit der Ineffizienz von Beispielen und der Verzerrung von Belohnungen in Algorithmen für adversarial Imitation Learning wie GAIL und AIRL.
Eine Variante von Kapselnetzen, die für paarweise Lernaufgaben verwendet werden kann. Die Ergebnisse zeigen, dass Siamesische Kapselnetzwerke in einer Few-Shot Learning Umgebung gut funktionieren.
In diesem Beitrag wird die Neuromodulation in künstlichen neuronalen Netzen vorgestellt.
Wir schlagen eine dynamische Convolution Methode vor, um die Inferenzzeit von CNNs erheblich zu beschleunigen und gleichzeitig die Genauigkeit zu erhalten.
In diesem Beitrag stellen wir eine Trainingsmethode vor, die so genannte Look-up-Table-Quantisierung (LUT-Q), die ein Wörterbuch erlernt und jedes Gewicht einem der Werte des Wörterbuchs zuordnet
Wir betrachten den Negativtransfer aus der Sicht der Domänenanpassung, um einen Algorithmus für adverses Lernen zu entwickeln.
Wir trainieren Quanten-Boltzmann-Maschinen mit einer Replika-Stacking-Methode und einem Quanten-Annealer, um eine Reinforcement Learning Aufgabe zu lösen.
Wir haben eine Nesterov Iterative Fast Gradient Sign-Methode (NI-FGSM) und eine Scale-Invariant Attack-Methode (SIM) vorgeschlagen, die die Übertragbarkeit von Gegenbeispielen für die Bildklassifikation erhöhen können.
Wir stellen eine stochastische Trainingsmethode für das Training binärer neuronaler Netze mit binären Gewichten und Aktivierungen vor.
Wir analysieren die Auswirkungen des latenten Raums der vollständig trainierten Generatoren, indem wir sie pseudo-invertieren.
Wir schlagen einen durchgängigen Modellbildungsprozess vor, der universell auf eine Vielzahl von Korpora zur Verifizierung der Autorenschaft anwendbar ist und den Stand der Technik mit wenigen bis gar keinen Änderungen oder Fine-Tuning übertrifft.
Wir betrachten die Lösung eines RL-Problems für einen einzelnen Agenten, indem wir es auf $n$ Lernende verteilen.
Ein hybrides Modell, das sowohl Roh-Audio- als auch Spektrogramm-Informationen für Sprachverbesserungsaufgaben nutzt.
Diese Arbeit vergleicht statistische Tests für RL-Vergleiche (falsch-positiv, statistische Macht), prüft die Robustheit gegenüber Annahmen unter Verwendung von simulierten Verteilungen und empirischen Verteilungen (SAC, TD3), bietet Richtlinien für RL-Studenten und Forscher.
Wir führen eine theoretische Analyse des Ziels "Informationsengpass" durch, um beobachtete Phasenübergänge beim Kompromiss zwischen Vorhersage und Kompression zu verstehen und vorherzusagen.
Vorschlag für lokal adaptive Aktivierungsfunktionen in tiefen und physikalisch informierten neuronalen Netzen für schnellere Konvergenz
Analyse der Bayesschen Hyperparameter-Inferenz in der Gaußschen Prozessregression 
Wir trainieren Variationsmodelle mit quantisierten Netzwerken für rechnerischen Determinismus. Dadurch können sie für die plattformübergreifende Datenkompression verwendet werden.
Eine neuronale Simulation der Universal Turing Machine
Die Verringerung der Lernrate und die Erhöhung der Losgröße während des Trainings sind gleichwertig.
Wir stellen das DCN+ mit Deep Residual Coattention und Mixed-Objective RL vor, das auf dem Stanford Question Answering Dataset den Stand der Technik erreicht.
Ein NAS-Benchmark, der auf fast alle NAS-Algorithmen anwendbar ist.
Wir schlagen eine null-zentrierte Gradientenstrafe zur Verbesserung der Generalisierung und Stabilität von GANs vor
Ein nüchterner Blick auf den aktuellen Stand der GANs aus praktischer Sicht
Wir schlagen einen modellunabhängigen Ansatz vor, um das Verhalten von Black-Box Deep-RL-Agenten zu erklären, die für das Spielen von Atari- und Brettspielen trainiert wurden, indem relevante Merkmale eines Eingabezustands hervorgehoben werden.
Wir untersuchen die tiefe Repräsentation von untrainierten CNN-DCN-Architekturen mit zufälligen Gewichten und zeigen deren Bildrekonstruktionsqualität und mögliche Anwendungen.
In diesem Beitrag wird ein neuer Ansatz zur dynamischen Merkmalsdarstellung vorgestellt, der eine effizientere Inferenz für tiefe neuronale Netze ermöglicht.
Wenn sie richtig initialisiert werden, können neuronale Netze die einfache Klasse der symmetrischen Funktionen erlernen; wenn sie zufällig initialisiert werden, versagen sie.  
Wir schlagen eine Methode zur effizienten Multi-Objective Neural Architecture Search vor, die auf Lamarck'scher Vererbung und evolutionären Algorithmen basiert.
Vereinheitlichtes neuronales Modell der Themen- und Sprachmodellierung zur Einführung von Sprachstrukturen in Themenmodelle für kontextualisierte Themenvektoren 
Wir schlagen eine neue Methode zur Komprimierung neuronaler Netze unter Verwendung probabilistischer Datenstrukturen vor.
Wir untersuchen die Auswirkungen der Verwendung verschiedener Arten von Teilworteinheiten auf die Qualität der resultierenden Darstellungen, wenn sie zur Modellierung von Syntax, Semantik und Morphologie verwendet werden.
Eine empirische Studie, die eine neue Perspektive auf das Few-Shot Learning bietet, in der eine Fine-Tuning Methode eine vergleichbare Genauigkeit wie komplexere State-of-the-Art-Methoden in mehreren Klassifikationsaufgaben zeigt.
Wir schlagen einen Ansatz zur Generierung realistischer und originalgetreuer Börsendaten auf der Grundlage generativer adversarialer Netzwerke vor.
Wir stellen einen vorzeichenbasierten Ansatz zur Gradientenschätzung vor, der die Gradientenschätzung von der kontinuierlichen zur binären Black-Box-Optimierung verlagert.
Wir analysieren die Gradientenausbreitung in tiefen RNNs und schlagen aufgrund unserer Analyse ein neues mehrschichtiges tiefes RNN vor.
Wir analysieren mathematisch die Auswirkung der Stapelnormalisierung auf ein einfaches Modell und erhalten wichtige neue Erkenntnisse, die für das allgemeine überwachte Lernen gelten.
Für komplexe Nebenbedingungen, bei denen es nicht einfach ist, den Gradienten zu schätzen, verwenden wir die diskontierte Strafe als Leitsignal. Wir beweisen, dass sie unter bestimmten Annahmen zu einer machbaren Lösung konvergiert.
Die Arbeit untersucht die Zielerfassung für handgehaltene virtuelle Tafeln in VR und zeigt, dass die Breite des Ziels, die Entfernung, die Richtung der Annäherung in Bezug auf die Schwerkraft und der Winkel der Annäherung die Leistung des Benutzers beeinflussen.
iSparse eliminiert irrelevante oder unbedeutende Netzkanten mit minimalen Auswirkungen auf die Netzleistung, indem es die Wichtigkeit der Kanten in Bezug auf die endgültige Netzausgabe bestimmt. 
Wir lernen Entitätsrepräsentationen, die Wikipedia-Kategorien mit nur wenigen Beispielen rekonstruieren können.
Unter Verwendung des q-deformierten Logarithmus leiten wir engere Grenzen als IWAE ab, um Variations-Autoencoder zu trainieren.
Das Bregman-Dilemma wird im Deep Learning aufgezeigt, dass die Verbesserung der Margen von überparametrisierten Modellen zu einer Überanpassung führen kann, und es werden Dynamiken von normalisierten Margenverteilungen vorgeschlagen, um Generalisierungsfehler vorherzusagen und ein solches Dilemma zu identifizieren. 
Wir haben ein Ende-zu-Ende Dialogsystem mit einem neuartigen mehrstufigen Dialogzustandsverfolger vorgeschlagen und erreichten eine konsistente Leistung auf MultiWOZ2.1 in Bezug auf Zustandsverfolgung, Aufgabenerfüllung und Antwortgenerierungsleistung.
Wir präsentieren eine skalierbare Annäherung an ein breites Spektrum von EBM-Zielen und Anwendungen in impliziten VAEs und WAEs
Dieses Papier entwickelt eine prinzipielle Methode für kontinuierliches Lernen in tiefen Modellen.
Ein Deep-RL-Algorithmus zur Lösung von POMDPs durch automatische Kodierung der zugrunde liegenden Zustände unter Verwendung eines rekurrenten Variationsmodells
Dieses Papier formalisiert das Problem der Online-Algorithmenauswahl im Kontext des Reinforcement Learning.
Wir schlagen ein neues latentes Variablenmodell vor, um latente Einbettungen für einige hochdimensionale Daten zu lernen. 
Wir schlagen Strategien für die gegnerische Verteidigung vor, die auf datenabhängigen Aktivierungsfunktionen, der Minimierung der Gesamtvariation und der Erweiterung der Trainingsdaten basieren.
Diese Arbeit stellt eine skalierbare Lösung für die kontinuierliche visuelle Spracherkennung vor.
Wir schlagen ein Modell von Variations-Autoencodern für die Textmodellierung vor, ohne den Decoder zu schwächen, was die Qualität der Texterzeugung und die Interpretierbarkeit der erworbenen Repräsentationen verbessert.
Wir zeigen, dass große, aber pruned Modelle (large-sparse) ihre kleineren, aber dichten (small-dense) Gegenstücke bei gleichem Speicherbedarf übertreffen.
Wir kontrollieren das Thema und die Stimmung der Texterstellung (fast) ohne Training. 
Die Lockerung der Einschränkung gemeinsamer Hierarchien ermöglicht ein effektiveres tiefes Multitasking.
Wir schlagen einen Rahmen zum Erlernen von vertrauenskalibrierten Netzwerken vor, indem wir eine neuartige Verlustfunktion entwerfen, die die durch stochastische Schlussfolgerungen geschätzte Vorhersageunsicherheit einbezieht.
Erlernen der Partikeldynamik mit dynamischen Interaktionsgraphen zur Simulation und Steuerung von starren Körpern, verformbaren Objekten und Flüssigkeiten. 
Wir stellen eine Theorie vor, um das Versagen von GANs bei komplexen Datensätzen zu erklären und schlagen eine Lösung vor, um dies zu beheben.
Datenerweiterung und kontradiktorisches Training sind sehr effektiv, um korrelierte Sprecher und Störgeräusche zu trennen und ermöglichen eine unabhängige Kontrolle jedes Attributs für die Text-to-Speech-Synthese.
LSTMs lernen weitreichende Abhängigkeiten kompositorisch, indem sie sie im Laufe des Trainings aus kürzeren Bestandteilen aufbauen.
Wir trainieren neuronale Netze, indem wir sie lokal linearisieren und bei jeder Iteration einen linearen SVM-Löser (Frank-Wolfe) verwenden.
Wir präsentieren eine verbesserte Version der auf universellen Nachfolgemerkmalen basierenden DRL-Methode, die das Transferlernen von Agenten verbessern kann.
Eine Methode zum Erlernen von Bildrepräsentationen, die sowohl für die Entflechtung von Variationsfaktoren als auch für die Erlangung getreuer Rekonstruktionen geeignet sind.
Wir verwenden den nicht-negativen Rang von ReLU-Aktivierungsmatrizen als Komplexitätsmaß und zeigen, dass er (negativ) mit guter Generalisierung korreliert.
Das Ziel dieser Arbeit ist es, die Wirkung von Netzwerken mit mehreren Lehrer-Netzwerken durch die Nutzung von stochastischen Blöcken und Sprungverbindungen zu erreichen.
Wir haben eine Android-Tastatur mit lexikalischen (wortbasierten) und semantischen (bedeutungsbasierten) Emoji-Vorschlägen entwickelt und ihre Wirkung in zwei verschiedenen Chat-Studien verglichen. 
Wir schlagen ein selbstadversarisches Lernparadigma (SAL) vor, das den Generator auf selbsttätige Weise verbessert, um die Leistung von GANs bei der Textgenerierung zu steigern.
In dieser Studie stellen wir eine neue Methode vor, die sich auf SVD stützt, um die Anzahl der latenten Dimensionen zu ermitteln.
Ein neuartiger Ansatz zur Erkennung von Widersprüchen, der Erklärungsmethoden verwendet, um Bilder zu identifizieren, deren Erklärungen nicht mit der vorhergesagten Klasse übereinstimmen.  
Ein durchgängig trainierbares Modellkompressionsverfahren, das die Genauigkeit zusammen mit der erwarteten Modellgröße optimiert.
Wir schlagen eine intelligente Batch-Auswahltechnik namens Ada-Boundary vor.
Wir stellen ontologiebasierte neuronale Netzarchitekturen für die Klassifizierung von Schallereignissen vor.
Unsere Arbeit zeigt, dass Positionsinformationen implizit in einem Netzwerk kodiert wurden. Diese Informationen sind wichtig für die Erkennung von positionsabhängigen Merkmalen, z. B. semantischen und auffälligen Merkmalen.
General-to-Detailed Neural Network (GDNN) mit Multi-Task Learning unter Einbeziehung von Cross-Domain Sketch (CDS) für semantisches Parsing
Wir zeigen, dass die Lernfähigkeit verschiedener neuronaler Architekturen direkt durch berechenbare Maße der Datenkomplexität charakterisiert werden kann.
Inverses Design mit genetischen Algorithmen und tiefen neuronalen Netzen. 
Wir untersuchen verborgene Zustandsaktivierungen von Transformer-Modellen bei Aufgaben zur Beantwortung von Fragen.
Reinforcement Learning und Imitation Learning kombinieren, um komplexe Robotermanipulationsaufgaben auf Pixeln zu lösen
Wir haben BatchEnsemble eingeführt, eine effiziente Methode für Ensembling und Lifelong Learning, die dazu verwendet werden kann, die Genauigkeit und Unsicherheit jedes neuronalen Netzes wie typische Ensemble-Methoden zu verbessern.
Schätzung der Belohnung anhand von Spielevideos
Wir schlagen eine Messung der Wichtigkeit von Phrasen und Algorithmen zur hierarchischen Erklärung von Vorhersagen des neuronalen Sequenzmodells vor
Ein höherer Impulsparameter $\beta$ hilft, Sattelpunkten schneller zu entkommen
Wir beschreiben, wie man ein generatives Bildmodell anhand eines langsam oder schwer auswertbaren Ziels, wie z. B. menschlichem Feedback, verbessern kann, was viele Anwendungen haben könnte, wie z. B. die Erstellung ästhetischerer Bilder.
Der von uns vorgeschlagene Algorithmus verwendet nicht alle unbeschrifteten Daten für das Training, sondern nutzt sie selektiv.
Ein neuer Ansatz zur bedingten Generierung durch Einschränkung des latenten Raums eines unbedingten generativen Modells.
Wir quantifizieren die Energiekosten in Form von Geld (Cloud-Credits) und den CO2-Fußabdruck des Trainings kürzlich erfolgreicher neuronaler Netzmodelle für NLP. Die Kosten sind hoch.
Die Pruning VAE wird vorgeschlagen, um nach entwirrten Variablen mit intrinsischer Dimension zu suchen.
Rekurrente Sprachmodelle auf Byte-Ebene lernen qualitativ hochwertige domänenspezifische Repräsentationen von Text.
Empirische Analyse und Erklärung von partikelbasierten Gradientenschätzern für approximative Inferenz mit tiefen generativen Modellen.
Skalierbares und genaues Deep Multi-Label-Learning mit Millionen von Labels.
Es wird gezeigt, dass GANs uns eine neue, effektive und robuste Mittelwertschätzung gegen agnostische Verunreinigungen liefern, die sowohl statistisch optimal als auch praktisch handhabbar ist.
Wir stellen State-Space-LSTM-Modelle vor, eine Kombination aus State-Space-Modellen und LSTMs, und schlagen einen Inferenzalgorithmus vor, der auf sequentiellem Monte Carlo basiert. 
Wir erweitern den Wake-Sleep-Algorithmus und verwenden ihn, um aus wenigen Beispielen strukturierte Modelle zu lernen, 
Proteine, Aminosäuresequenzen, maschinelles Lernen, tiefes Lernen, rekurrentes neuronales Netz (RNN), Long Short term Memory (LSTM), gated recurrent unit (GRU), tiefe neuronale Netze
Erkennung von gesprochenen Begriffen mit Hilfe von strukturierten Vorhersagen und tiefen Netzen, die eine neue Verlustfunktion implementieren, die sowohl die AUC maximiert als auch eine Rangfolge nach einem vordefinierten Schwellenwert erstellt.
Wir schlagen ein neuartiges Optimierungsziel vor, das Fairness in heterogenen föderierten Netzwerken fördert, und entwickeln eine skalierbare Methode zu seiner Lösung.
Wir schlagen ein neuartiges Autocodierungsmodell mit erweitertem adversarischem Rekonstruktionsverlust vor. Wir führen eine neue Metrik für die inhaltsbasierte Bewertung von Rekonstruktionen ein. 
Ein robuster Bayes'scher Deep-Learning-Algorithmus zur Ableitung komplexer Posterioritäten mit latenten Variablen
Wir haben die Raum-Zeit-Tensorzerlegung für einzelne Versuche auf der Grundlage der nicht-negativen Matrixfaktorisierung erweitert, um die Grundlinienaktivität vor dem Stimulus effizient zu berücksichtigen, was die Dekodierungsleistung bei Daten mit nicht vernachlässigbaren Grundlinien verbessert.
GAN-basierte Methode zur extremen Bildkomprimierung, die weniger als die Hälfte der Bits des SOTA-Codecs benötigt, ohne die visuelle Qualität zu beeinträchtigen
Einstufungslernen mit Hilfe der Transformer-Architektur.
Der Beitrag beschreibt einen strategischen, intrinsisch motivierten Lernalgorithmus, der das Lernen komplexer motorischer Strategien in Angriff nimmt.
Wir verwenden MCTS, um eine Bootstrapped Policy für kontinuierliche Aktionsräume unter den Bedingungen einer Policy-Iteration weiter zu optimieren.
Warum bisherige VAEs bei Texten keine kontrollierbare latente Repräsentation wie bei Bildern erlernen können, sowie eine Lösung, die den ersten Erfolg bei der kontrollierten Texterzeugung ohne Überwachung ermöglicht.
Ein neues hierarchisches kortikales Modell zur Kodierung des räumlich-zeitlichen Gedächtnisses und zur Videovorhersage
Vorschlag einer neuen kontrafaktischen Methode zur Bewertung von Hypothesen über das Verhalten von Deep-RL-Agenten, die auf der Grundlage von Salienzkarten erstellt wurden. 
Die meisten neuronalen Netze nähern sich der gleichen Klassifizierungsfunktion an, auch über verschiedene Architekturen hinweg, und zwar in allen Phasen des Lernens.
Stellen Sie jede Entität auf der Grundlage ihres Histogramms von Kontexten dar, und dann brauchen Sie nur noch Wasserstein!
Methoden für Empfehlungssysteme mit Nebeninformationen auf der Grundlage der Spurennorm-Regularisierung.
In dieser Arbeit wird ein auf Exploration und Imitation Learning basierender Agent vorgestellt, der beim Spielen textbasierter Computerspiele Spitzenleistungen erbringt. 
Beim Pruning von neuronalen Netzen ist es wichtig, die beschnittenen Gewichte auf Null zu setzen, das Zeichen der Initialisierung ist entscheidend, und die Maskierung kann als Training angesehen werden.
Wir haben SesameBERT vorgeschlagen, eine verallgemeinerte Feinabstimmungsmethode, die die Extraktion globaler Informationen zwischen allen Schichten durch Squeeze und Excitation ermöglicht und lokale Informationen durch die Erfassung benachbarter Kontexte mittels Gaußscher Unschärfe anreichert.
Opponent Shaping ist ein leistungsfähiger Ansatz für das Lernen von Multi-Agenten, kann aber Konvergenz verhindern; unser SOS-Algorithmus behebt dieses Problem mit starken Garantien in allen differenzierbaren Spielen.
Wir zeigen, dass das Frage-Antwort-Matching eine besonders gute Pre-Training-Aufgabe für Fragenähnlichkeit ist und veröffentlichen einen Datensatz für medizinische Fragenähnlichkeit
Eine Studie über den Nutzen der gemeinsamen Repräsentation beim Multi-Task Reinforcement Learning.
Tiefgehende Architekturen für 3D-Punktwolken, die äquivariant zu SO(3)-Drehungen sowie zu Translationen und Permutationen sind. 
Ein Vergleich und eine detaillierte Analyse verschiedener Modelle zur Satzeinbettung anhand der realen Aufgabe der automatischen Zusammenfassung.
Wir schlagen Value Propagation vor, einen neuartigen Ende-zu-Ende Planer, der lernen kann, 2D-Navigationsaufgaben durch Reinforcement Learning zu lösen, und der sich auf größere und dynamische Umgebungen verallgemeinern lässt.
Vorschlag für eine Methode zur Extraktion und Nutzung von Interpretationen der Wechselwirkungen zwischen Merkmalen
Wir zeigen, dass es möglich ist, die Parameter eines 1-schichtigen generativen ReLU-Modells aus der Betrachtung der von ihm erzeugten Beispiele zu ermitteln.
Lernen von dichten Vektordarstellungen beliebiger Arten von Merkmalen in beschrifteten und unbeschrifteten Datensätzen.
Endliche Automaten können linear aus spracherkennenden RNNs dekodiert werden, indem Abstraktionsfunktionen mit geringer Grobheit und hochgenaue Dekodierer verwendet werden. 
Wir schlagen HURRICANE vor, um die Herausforderung der Hardware-Vielfalt bei der Suche in einer neuronalen One-Shot-Architektur anzugehen.
Neuronale phrasenbasierte maschinelle Übersetzung mit linearer Dekodierzeit.
Wir schlagen ein AE-basiertes GAN vor, das den Mode-Kollaps in GANs mildert.
Reinforcement Learning und adaptives Sampling für optimierte Kompilierung von tiefen neuronalen Netzen.
Wir entwerfen eine Grammatik, die in einer kontradiktorischen Umgebung gelernt wird, und wenden sie auf die Zukunftsvorhersage in Videos an.
Wir schlagen Janossy-Pooling vor, eine Methode zum Erlernen tiefer permutationsinvarianter Funktionen, die entwickelt wurde, um Beziehungen innerhalb der Eingabesequenz und nachvollziehbare Inferenzstrategien wie ein stochastisches Optimierungsverfahren, das wir piSGD nennen, zu nutzen.
Ein neuartiges Meta-Lernmodell, das die Auswirkungen des Meta-Lernens und des aufgabenspezifischen Lernens sowie des klassenspezifischen Lernens innerhalb jeder Aufgabe adaptiv ausgleicht.
Wir greifen die Idee der Master-Slave-Architektur im Multi-Agenten Reinforcement Learning wieder auf und übertreffen den Stand der Technik.
Wir untersuchen die implizite Verzerrung von Gradientenmethoden bei der Lösung eines binären Klassifikationsproblems mit nichtlinearen ReLU-Modellen.
Eine Pruning-Methode für CNN-Modelle, die ISTA und einen Reskalierungstrick verwendet, um die Spärlichkeit der Skalierungsparameter bei der Stapelnormalisierung zu erzwingen.
Was können wir über das Training neuronaler Netze lernen, wenn wir jede Schicht als ein Gradientenverstärkungsproblem behandeln?
Antizipation verbessert die Konvergenz von Deep Reinforcement Learning.
Wir stellen fest, dass die Bewegung im Funktionsraum nicht proportional zur Bewegung im Parameterraum während der Optimierung ist. Wir schlagen einen neuen Optimierer im Stil eines natürlichen Gradienten vor, um dieses Problem zu lösen.
Wir führen eine allgemeine Familie von Lagranges ein, die es ermöglichen, die IB-Kurve in allen Szenarien zu erforschen. Wenn diese verwendet werden und die IB-Kurve bekannt ist, kann man direkt für ein Leistungs-/Komprimierungsniveau optimieren.
Wir schlagen die Neural Logic Machine (NLM) vor, eine neuronal-symbolische Architektur für induktives Lernen und logisches Denken.
Wir schlagen ein semantikbewusstes neuronales abstraktes Zusammenfassungsmodell und ein neuartiges automatisches Zusammenfassungsbewertungsschema vor, das misst, wie gut ein Modell themenfremde Informationen aus gegnerischen Stichproben identifiziert.
Drucken Sie den Eingabesatz und den aktuellen Antwortsatz auf ein Bild und verwenden Sie ein fine-tuned ImageNet CNN-Modell, um die nächste Antwort vorherzusagen.
Wir befähigen gewöhnliche CNNs zum Few-Shot Learning, indem wir visuelle Konzepte nutzen, die interpretierbare visuelle Hinweise sind, die innerhalb von CNNs gelernt werden.
Anwendung des Modells der gewöhnlichen Differentialgleichung auf graphstrukturierte Daten.
Vorgeschlagene neue Aufgabe, Datensätze und Baselines; 3D Conv CycleGAN bewahrt Objekteigenschaften über Frames hinweg; die Stapelstruktur in Methoden auf Frame-Ebene ist wichtig.
Ein neues skalierbares, gruppenäquivariantes Modell für Kapselnetzwerke, das die Kompositionalität bei Transformationen beibehält und empirisch stabiler ist als ältere Kapselnetzwerkmodelle.
In dem Werk wird eine einfache, aber effektive Basis für das Lernen mit verrauschten Labels vorgeschlagen.
Die Bewerter bevorzugen die Angemessenheit der menschlichen gegenüber der maschinellen Übersetzung, wenn sie ganze Dokumente bewerten, aber nicht, wenn sie einzelne Sätze bewerten.
Diese Arbeit erweitert das generative adversarial Imitation Learning für Multi-Agenten auf Markov-Spiele in extensiver Form.
Wir überdenken das Selbsttraining als halbüberwachte Lernmethode für das Problem der neuronalen Sequenzerzeugung und zeigen, dass das Selbsttraining mit eingespeistem Rauschen recht erfolgreich sein kann.
Ein generatives Speichermodell, das langsam lernende neuronale Netze und ein sich schnell anpassendes lineares Gauß-Modell als Speicher kombiniert.
Wir stellen einen neuen Ansatz, SNIP, vor, der einfach, vielseitig und interpretierbar ist. Er entfernt irrelevante Verbindungen für eine bestimmte Aufgabe auf einen Schlag vor dem Training und ist ohne Änderungen auf eine Vielzahl von neuronalen Netzwerkmodellen anwendbar.
Eine Technik zur automatischen Kennzeichnung großer unbeschrifteter Datensätze, damit diese Quellmodelle für das Transferlernen trainieren können, und ihre experimentelle Bewertung. 
In unserem Beitrag wird ein Aufmerksamkeitsmodul vorgeschlagen, das die Beziehungen zwischen den Kanälen erfasst und große Leistungsgewinne bietet.
Die Anwendbarkeit des inversen Reinforcement Learnings wird oft durch die Kosten für das Sammeln von Expertendemonstrationen behindert. In diesem Beitrag wird versucht, die Anwendbarkeit zu erweitern, indem vorherige Aufgabeninformationen durch Meta-Lernen einbezogen werden.
Eine neue neuronale Architektur, bei der die obersten dichten Schichten von Standard Convolutionalarchitekturen durch eine Annäherung an eine Kernel-Funktion ersetzt werden, indem man sich auf die Nyström-Näherung stützt.
Der ImageNet-Datensatz enthält nicht-einvernehmliche und pornografische Bilder.
In diesem Papier wird ein neuartiger Rahmen für das Lernen von Graphenähnlichkeit in einem induktiven und unbeaufsichtigten Szenario vorgeschlagen.
Wir zeigen, wie neuronale Kodierungsmodelle trainiert werden können, um sowohl die Signal- als auch die Höchststand Variabilität von neuronalen Populationsdaten mit Hilfe von GANs zu erfassen.
Ein auf schwach überwachtem Lernen basierender Clustering-Rahmen erbringt vergleichbare Leistungen wie vollständig überwachte Lernmodelle, indem er die eindeutige Klassenzahl ausnutzt.
Tiefes modellbasiertes RL, das gut funktioniert.
Wir analysieren und bestimmen die Präzisionsanforderungen für das Training neuronaler Netze, wenn alle Tensoren, einschließlich der back-propagated Signale und Gewichtsakkumulatoren, im Festkommaformat quantisiert sind.
Wir können beim maschinellen Lernen prototypische Beispiele und Ausreißer identifizieren, die sich quantitativ stark voneinander unterscheiden, und sie nutzen, um viele Aspekte neuronaler Netze zu verbessern.
Wir schlagen vor, das Deep Scattering Network zu verbessern, um die Kontrolle und Stabilität jeder beliebigen maschinellen Lernpipeline zu erhöhen, indem wir ein kontinuierliches Wavelet-Schwellwertverfahren vorschlagen.
Neuronales Clustering, ohne dass eine Anzahl von Clustern benötigt wird.
Die Modellabstimmung ist ein bewährter Rahmen für Planerklärungen, kann aber leicht für Lügen missbraucht werden.
Wir betrachten neue Varianten von Optimierungsalgorithmen für das Training tiefer Netze.
Wir beschleunigen die RNN-Inferenz, indem wir redundante Speicherzugriffe durch eine Mischung aus genauen und approximativen Modulen dynamisch reduzieren.
Wir initiieren einen Vorstoß zum Aufbau von ER-Systemen, die Tausende von Typen erkennen können, indem wir eine Methode zur automatischen Konstruktion geeigneter Datensätze auf der Grundlage der Typenhierarchie bereitstellen. 
In dieser Arbeit wird eine Theorie zur Klassifizierung von Methodenaufrufen nach verschiedenen Abstraktionsebenen vorgeschlagen und ein statistischer Ansatz zur Codevervollständigung vom Methodennamen bis zum Methodenaufruf durchgeführt.
Wir konzentrieren uns darauf, universelle Gegner zu schaffen, um Objektdetektoren zu täuschen und Objekte vor den Detektoren zu verbergen. 
Wasserstein-Autoencoder mit hyperbolischem latentem Raum.
Methode zur Komprimierung von Merkmalskarten, die quantisierte Aktivierungen in binäre Vektoren umwandelt, gefolgt von nichtlinearen Dimensionalitätsreduktionsschichten, die in ein DNN eingebettet sind.
Erzielung einer starken Robustheit gegenüber Gegnern, die mit gegnerischem Training vergleichbar ist, ohne Training an gegnerischen Beispielen.
Wir lernen einen unüberwachten Lernalgorithmus, der nützliche Repräsentationen aus einer Reihe von überwachten Aufgaben erzeugt. Zur Testzeit wenden wir diesen Algorithmus auf neue Aufgaben ohne Überwachung an und zeigen eine Leistung, die mit einer VAE vergleichbar ist.
Eine Überparametrisierung in der Breite scheint beim Deep Reinforcement Learning ebenso hilfreich zu sein wie beim überwachten Lernen.
Hierarchisches generatives Modell (Hybrid aus VAE und GAN), das eine entwirrte Darstellung der Daten lernt, ohne die generative Qualität zu beeinträchtigen.
Wir stellen eine leichtgewichtige Architektur für die Erkennung von Named Entities vor und führen ein inkrementelles aktives Lernen durch, das in der Lage ist, mit nur 25 % der ursprünglichen Trainingsdaten eine dem Stand der Technik entsprechende Leistung zu erzielen.
Wir trainieren genaue, vollständig quantisierte Netzwerke unter Verwendung einer Verlustfunktion, die die Genauigkeit des Modells mit voller Präzision maximiert und die Differenz zwischen dem Netzwerk mit voller Präzision und dem quantisierten Netzwerk minimiert.
In diesem Beitrag stellen wir einen Algorithmus vor, mit dem die Konnektivität von tiefen Mehrfachverzweigungsnetzen erlernt werden kann. Der Ansatz wird anhand von Bildkategorisierungen evaluiert, bei denen er durchgängig Genauigkeitsgewinne gegenüber modernen Modellen mit fester Konnektivität erzielt.
Wir zeigen, dass einzelne Einheiten in CNN-Repräsentationen, die in NLP-Aufgaben gelernt wurden, selektiv auf natürlichsprachliche Konzepte reagieren.
Wir schlagen ein neuartiges HRL-Framework vor, in dem wir das zeitliche Abstraktionsproblem als Lernen einer latenten Repräsentation einer Handlungssequenz formulieren.
Wir stellen MIST-RNNs vor, die a) im Vergleich zu LSTMs überragende Vanishing-Gradient-Eigenschaften aufweisen; b) die Leistung von LSTMs und Clockwork-RNNs bei Aufgaben, die sehr langfristige Abhängigkeiten erfordern, erheblich verbessern; und c) viel effizienter sind als die zuvor vorgeschlagenen NARX-RNNs, mit sogar weniger Parametern und Operationen als LSTMs.
Ein Modul zur Verflechtung von Merkmalen, um Merkmale aus einem genauen Satz zu nutzen, um das Lernen eines anderen, weniger zuverlässigen Satzes zu unterstützen.
Ein kontinuierlich lernender Rahmen, der lernt, seine Architektur auf der Grundlage eines vorgeschlagenen Variationsinferenzalgorithmus automatisch anzupassen. 
Wir schlagen Noisy-DR-L0-SSC (Noisy Dimension Reduction L0-Sparse Subspace Clustering) vor, um verrauschte Daten entsprechend ihrer zugrunde liegenden Unterraumstruktur effizient zu partitionieren.
Ein neuartiger Ansatz, der die Konnektivität von Modi in Verlustlandschaften nutzt, um nachteilige Auswirkungen abzuschwächen, manipulierte Modelle zu reparieren und die Widerstandsfähigkeit gegenüber nachteiligen Einflüssen zu bewerten.
Ein GAN-Rahmenwerk mit mehreren Generatoren und einem zusätzlichen Netz, das einen Prior über das Eingangsrauschen lernt.
BigGANs erfassen nicht die ImageNet-Datenverteilungen und sind nur mäßig erfolgreich bei der Datenerweiterung.
Wir präsentieren Leaf, ein modulares Benchmarking-Framework für das Lernen in föderierten Daten, mit Anwendungen für Lernparadigmen wie föderiertes Lernen, Meta-Lernen und Multi-Task-Lernen.
Wir führen einen neuen räumlich-zeitlichen Einbettungsverlust für Videos ein, der eine zeitlich konsistente Segmentierung von Videoinstanzen erzeugt, selbst bei Okklusion und verpassten Erkennungen, unter Verwendung von Aussehen, Geometrie und zeitlichem Kontext.
In dieser Arbeit wird eine fortschrittliche Methode zur Optimierung von Strategien mit rückblickender Erfahrung für spärliche Belohnungen beim Reinforcement Learning vorgeschlagen.
Wir erforschen, wie die Verwendung von Hintergrundwissen in Verbindung mit der Umformulierung von Abfragen dazu beitragen kann, bei der Beantwortung von wissenschaftlichen Multiple-Choice-Fragen bessere Belege zu finden.
Wir komprimieren tiefe CNNs, indem wir eine einzelne Convolution-Schicht iterativ wiederverwenden und so die Anzahl der Parameter um einen Faktor reduzieren, der proportional zu ihrer Tiefe ist, während ihre Genauigkeit weitgehend unberührt bleibt.
Spektralanalyse zum Verständnis, wie verschiedene Darstellungen die Optimierung und Verallgemeinerung verbessern können.
Wir bieten theoretische Unterstützung für Unsicherheitsschätzungen für Deep Learning, die durch die Anpassung von Zufallsprioritäten gewonnen werden.
Wir schlagen einen Rahmen für die Imputation beliebig konditionierter Daten vor, der auf variablen Autoencodern und normalisierenden Flüssen basiert.
Wir entwickeln einen Angriff auf die Privatsphäre, der die sensiblen Eingabedaten eines tiefen Netzes aus dessen Ausgabe wiederherstellen kann
Wir stellen eine neuartige Methode zur zweisprachigen Texterzeugung vor, die parallele, gleichzeitige Sätze in zwei Sprachen erzeugt.
Wir führen die Online-Erklärung ein, um die kognitiven Anforderungen des Menschen für das Verständnis der vom Agenten generierten Erklärung zu berücksichtigen.
Wir schlagen einen auf Reinforcement Learning basierenden Algorithmus zum Variablentausch und zur Neuberechnung vor, um die Speicherkosten zu reduzieren.
Iterative zeitliche Differenzierung mit fester zufälliger Rückkopplungsausrichtung unterstützt höchststand-zeitabhängige Plastizität in einfacher Backpropagation für Deep Learning.
In diesem Beitrag wird die Verwendung eines tiefen generativen akustischen Modells für die automatische Spracherkennung vorgeschlagen, das natürlich mit anderen tiefen Sequenz-zu-Sequenz-Modulen unter Verwendung der Bayes-Regel kombiniert wird.
Wir schlagen einen neuen Ansatz zur Generierung von Gegenbeispielen vor, der auf räumlicher Transformation basiert und im Vergleich zu bestehenden Angriffen wahrnehmungsrealistische Beispiele erzeugt. 
Ein hybrider DQN- und DDPG-Algorithmus wird vorgeschlagen, um den diskret-kontinuierlichen hybriden Aktionsraum zu behandeln.
Eine neue Methode zur Modellierung von Wissensgraphen auf der Grundlage von Distanzeinbettungen und neuronalen Netzen
Verbesserung der Trainingsstabilität von halbüberwachten generativen adversen Netzen durch kollaboratives Training
Wir zeigen, wie man mit tiefen Netzen Vorhersagen treffen kann, ohne tiefe Netze zu trainieren.
Graphon ist ein guter Suchraum für die Suche nach neuronalen Architekturen und erzeugt erfahrungsgemäß gute Netzwerke.
Instanzadaptives adversariales Training zur Verbesserung des Kompromisses zwischen Robustheit und Genauigkeit
Wir schlagen einen defensiven Ansatz zum Schutz der Unterscheidung vor und demonstrieren die starke Unterscheidbarkeit von gegnerischen Beispielen.
Wir stellen die NLC vor, eine Metrik, die im zufällig initialisierten Zustand des Netzwerks billig zu berechnen ist und eine hohe Vorhersagekraft für die Generalisierung hat, zumindest in vollständig verbundenen Netzwerken.
Die Lücke im Soft Computing zu schließen.
In diesem Papier wird MarginAttack vorgestellt, ein stärkerer und schnellerer zero-confidence adversarial Angriff.
Die unüberwachte Optimierung während der Inferenz gibt ein Top-Down-Feedback zur iterativen Anpassung der Feedforward-Vorhersage der Skalenvariation für eine äquivariantere Erkennung.
Wir untersuchen die Eigenschaften des kürzlich eingeführten Deep Image Prior (Ulyanov et al, 2017).
Wir schlagen eine Methode zur Erweiterung öffentlich verfügbarer Daten für Gerüchtestudien vor, die auf semantischer Verwandtschaft zwischen begrenzten beschrifteten und unbeschrifteten Daten basiert.
Dynamische leichtgewichtige Convolutions sind konkurrenzfähig zur Selbstaufmerksamkeit bei Sprachaufgaben.
Wir zeigen, wie die Einbeziehung eines zusätzlichen Gradientenschritts in GAN-Trainingsmethoden erster Ordnung die Stabilität verbessern und zu besseren Konvergenzergebnissen führen kann.
Die Batchnormalisierung verringert die Robustheit bei der Prüfung gegen häufige Verfälschungen und gegnerische Beispiele.
Wir lernen einen Optimierungsalgorithmus, der sich auf unbekannte Aufgaben generalisieren lässt.
Wir sagen den Generalisierungsfehler voraus und spezifizieren das Modell, das ihn über Modell/Datenskalen hinweg erreicht.
Unüberwachtes Reinforcement Learning-Verfahren zum Erlernen einer Strategie zur robusten Erreichung wahrnehmungsspezifischer Ziele.
Sequenzmodell, das den Umfang der Berechnungen für jede Eingabe dynamisch anpasst.
Wir charakterisieren problematische globale Optima des VAE-Ziels und stellen eine neuartige Inferenzmethode zur Vermeidung solcher Optima vor.
Wir schlagen eine neuartige unbeaufsichtigte Worteinbettung vor, die die Einschluss-Eigenschaft in der Kontext-Verteilung bewahrt, und erzielen modernste Ergebnisse bei der unbeaufsichtigten Erkennung von Hypernymen.
Ein auf Regularisierung basierender Ansatz für kontinuierliches Lernen unter Verwendung Bayes'scher neuronaler Netze zur Vorhersage der Bedeutung von Parametern.
In diesem Beitrag wird untersucht, wie tragbare Technologien zur Erweiterung der Sinneswahrnehmung dazu beitragen können, aus erster Hand zu erfahren, wie es ist, katzenartige Schnurrhaare zu haben.
Wir geben einige Verallgemeinerungsfehlergrenzen von verrauschten Gradientenmethoden wie SGLD, Langevin-Dynamik, verrauschter Impuls und so weiter an.
Beim sparse-reward Reinforcement Learning wird das Ensemble aus mehreren dynamischen Modellen verwendet, um eine intrinsische Belohnung zu generieren, die das Minimum der Überraschung darstellt.
Wir untersuchen Repräsentationen der Phonologie in neuronalen Netzwerkmodellen der gesprochenen Sprache mit verschiedenen Varianten von Analysetechniken.
Wir lösen das spezifische SR-Problem von JPG-Bildern geringer Qualität durch funktionale Teilmodelle.
Wir schlagen ein interpretierbares Modell zur Erkennung von benutzergewählten Weckwörtern vor, das aus den Beispielen des Benutzers lernt.
Wir schlagen ein Modell vor, das lernt, informative Bilder in einer zukünftigen Videosequenz zu entdecken und das Video über seine Schlüsselbilder darzustellen.
Wir lernen tiefe Repräsentation durch Maximierung der gegenseitigen Information, indem wir die Struktur des Ziels nutzen, und sind in der Lage, mit vollständig überwachten Klassifikatoren mit vergleichbaren Architekturen zu rechnen.
Wir schlagen zunächst einen vollautomatischen und zielgerichteten Schätzer für die atomare Bedeutung vor, der auf neuronalen Graphen-Netzwerken und einem neuen Konzept der umgekehrten Selbstaufmerksamkeit basiert.
Unüberwachtes spektrales Clustering mit tiefen neuronalen Netzen.
Wir stellen fest, dass tiefe Modelle für das Funktionieren von MAML entscheidend sind und schlagen eine Methode vor, die effektives Meta-Lernen in kleineren Modellen ermöglicht.
Das Pooling erfolgt mit Hilfe von Wavelets anstelle von traditionellen Nachbarschaftsansätzen (Maximum, Durchschnitt usw.).
Wir schlagen ein neuartiges dynamisches Ridesharing-System vor, das sowohl den operativen Wert für den Anbieter als auch den Nutzwert für die Fahrgäste optimiert, indem es die sozialen Präferenzen der Nutzer in den Entscheidungsprozess einbezieht.
Wir charakterisieren die dimensionalen Eigenschaften von gegnerischen Unterräumen in der Nachbarschaft von gegnerischen Beispielen durch die Verwendung von Local Intrinsic Dimensionality (LID).
Wir entwerfen und analysieren einen neuen stochastischen Optimierungsalgorithmus der nullten Ordnung, ZO-signSGD, und demonstrieren seine Verbindung und Anwendung auf Black-Box-Angriffe beim robusten Deep Learning
In diesem Beitrag wird eine einheitliche rekurrente neuronale Netzwerkarchitektur für kurzfristige Solarvorhersagen mit mehreren Zeithorizonten vorgeschlagen und die Leistungssteigerung der Vorhersage im Vergleich zu den bisher bekannten Methoden validiert.
Die Skip-Verbindung in ResNet und die Batch-Normalisierung verbessern die Fähigkeit zur Datentrennung und helfen beim Training eines tiefen neuronalen Netzes.
Wir schlagen ein generatives Modell vor, das nicht nur Daten mit den gewünschten Merkmalen aus dem vordefinierten latenten Raum erzeugt, sondern auch die Merkmale der Daten vollständig versteht, um Merkmale zu erzeugen, die nicht im Datensatz enthalten sind.
Ein Rahmen für die Untersuchung emergenter Kommunikation in einem vernetzten Multi-Agenten Reinforcement Learning System.
Wir erweitern tiefe Mengen auf funktionale Einbettungen und neuronale Prozesse, um übersetzungsäquivariante Mitglieder aufzunehmen.
Ein rotationsäquivariantes CNN-Modell von V1, das frühere Modelle übertrifft und funktionale Gruppierungen von V1-Neuronen empfiehlt.
Ableitung einer breiten Klasse von Inferenzverfahren aus einem Ziel des globalen Informationsengpass.
Das Training von Agenten mit adaptiven Berechnungen auf der Grundlage von Informationsengpässen kann die Generalisierung fördern. 
Wir haben eine Studie innerhalb von Probanden durchgeführt, um vier visuelle Darstellungen des Atemrhythmus zu bewerten, die in mobilen Apps üblich sind, um herauszufinden, welche am effektivsten bei der Anleitung zu Atemübungen ist.
Ein Weg zur Erstellung von Trainingskorpora für die neuronale Codesynthese unter Verwendung eines Diskriminators, der auf unmarkierten Daten trainiert wird
Wir demonstrieren die Anfälligkeit für Unterempfindlichkeitsangriffe in den neuronalen Leseverständnismodellen SQuAD2.0 und NewsQA, bei denen das Modell dieselbe Antwort mit erhöhtem Vertrauen auf gegnerisch gewählte Fragen vorhersagt, und vergleichen Abwehrstrategien.
Die Verwendung von Komprimierungstechniken zur Kodierung von Wörtern ist eine Möglichkeit für ein schnelleres Training von CNN und die Reduzierung der Dimensionalität der Darstellung.
Ein entropie-regulierter Formalismus zur Optimierung von Richtlinien fasst eine Reihe von Algorithmen zum Erlernen von Sequenzvorhersagen zusammen. Ein neuer Interpolationsalgorithmus mit verbesserten Ergebnissen bei der Texterzeugung und beim Imitation Learning von Spielen.
Wir schlagen ein rein Convolutional CNN-Modell mit einem Aufmerksamkeitsmechanismus vor, um räumlich-zeitliche Herkunfts-Ziel-Ströme vorherzusagen. 
Wir schlagen einen neuen Ansatz für adversariales Training mit dem Namen Robust Local Features for Adversarial Training (RLFAT) vor, der sowohl die adversarial robuste Generalisierung als auch die Standard-Generalisierung deutlich verbessert.
Warum und wie man die Verifikation von Planungsdomänenmodellen mit Planungszielen einschränkt, um unerreichbare Gegenbeispiele (falsch-positive Verifikationsergebnisse) zu vermeiden.
StrokeNet ist eine neuartige Architektur, bei der der Agent darauf trainiert wird, durch Striche auf einer differenzierbaren Simulation der Umgebung zu zeichnen, wodurch die Leistungsfähigkeit der Backpropagation effektiv genutzt werden kann.
Wir zeigen, wie maschinelles Lernen Experimente in der Quantenphysik modellieren kann.
Ein nichtlineares, unüberwachtes metrisches Lernsystem zur Steigerung der Leistung von Clustering-Algorithmen.
Bei komplexen Black-Box-Modellen ist die Verschlüsselung der Daten nicht so nützlich wie die Annäherung an das Modell und dessen Verwendung zur Preisfindung für eine potenzielle Transaktion.
Interaktives, stiftbasiertes, klangliches Schreibsystem.
Wir stellen einen Encoder-Decoder-Rahmen für die Übertragung von Sprachstilen vor, der die Verwendung von nicht parallelen Daten und Quelldaten mit verschiedenen unbekannten Sprachstilen ermöglicht.
Eine neue Verlustfunktion für PCA mit linearen Autoencodern, die nachweislich geordnete exakte Eigenvektoren liefert.
In diesem Beitrag wird ein Framework für die Integration von Benutzerfeedback unter Identitätsunsicherheit in Wissensdatenbanken entwickelt. 
In dieser Arbeit schlagen wir ein neuartiges generatives Modell vor, um systematische Vergiftungsangriffe mit Einschränkungen der Nachweisbarkeit gegen Klassifizierer des maschinellen Lernens, einschließlich tiefer Netzwerke, zu entwickeln. 
Es ist der Quantenalgorithmus für Erwartungsmaximierung. Er ist schnell: Die Laufzeit hängt nur polylogarithmisch von der Anzahl der Elemente im Datensatz ab. 
Ein spärlicher Optimierungsalgorithmus für tiefe CNN-Modelle.
Eine einfache und effektive Alternative zum adversarial Imitation Learning: Initialisieren des erfahrungswiederholungs Buffers mit Demonstrationen, setzen deren Belohnung auf +1, setzen der Belohnung für alle anderen Daten auf 0, ausführen von Q-Learning oder Soft-Actor-Critic zum Trainieren.
Wir stellen eine neue tiefe Architektur, VarPSOM, und ihre Erweiterung auf Zeitreihendaten, VarTPSOM, vor, die im Vergleich zu aktuellen tiefen Clustering-Methoden auf statischen und zeitlichen Daten eine überlegene Clustering-Leistung erzielen.
Wir analysieren, welche Aufgaben am besten gemeinsam in einem Netzwerk gelernt werden und welche am besten separat. 
Ein tiefes semantisches Gerüst für die Abfrage von Textdokumenten durch Suchmaschinen.
Wir schlagen einen neuen Algorithmus vor, der auf dem optimalen Transport basiert, um ein CNN in einer SSL-Art zu trainieren.
Die Batch Normalisierung verringert die Robustheit gegenüber Gegnern und in vielen Fällen auch die allgemeine Robustheit, insbesondere gegenüber Rauschstörungen.
Lernen von Domänen-Strukturen in HGNs für nicht-deterministische Planung.
ein einfacher RNN-basierter Meta-Learner, der SOTA-Leistungen bei gängigen Benchmarks erzielt.
LiTSE: Temporal Knowledge Graph Embedding Modell basierend auf linearen Zeitserien dekompositionen.
Wir stellen eine neue MuJoCo-Fußballumgebung für die Forschung im Bereich des kontinuierlichen Multi-Agenten Reinforcement Learning vor und zeigen, dass ein bevölkerungsbasiertes Training unabhängiger Reinforcement Learners kooperative Verhaltensweisen erlernen kann.
Verwendung der DSL-Grammatik und von Reinforcement Learning zur Verbesserung der Synthese von Programmen mit komplexem Kontrollfluss.
Wir schlagen ein neuartiges Zustandsraum-Zeitreihenmodell vor, das in der Lage ist, die Struktur von Veränderungspunkten und Anomaliepunkten zu erfassen, so dass es eine bessere Vorhersageleistung hat, wenn es Veränderungspunkte und Anomalien in der Zeitreihe gibt.
Ein multimodaler Transformer für multimodales sequentielles Lernen, mit überzeugenden empirischen Ergebnissen bei multimodalen Sprachmetriken wie multimodaler Stimmungsanalyse, Emotionserkennung und Erkennung von Persönlichkeitsmerkmalen. 
Wir verwenden einen adaptiven Koeffizienten zusätzlich zum regulären Impuls, der von der geodätischen Optimierung inspiriert ist und das Training sowohl bei konvexen als auch bei nicht-konvexen Funktionen erheblich beschleunigt.
Wir verwenden einen Transformer Encoder für die Übersetzung, indem wir ihn im Stil eines maskierten Übersetzungsmodells trainieren.
Wir stellen lokale Ensembles vor, eine Methode zur Erkennung von Extrapolation in trainierten Modellen, die die Varianz eines Ensembles anhand lokaler Informationen zweiter Ordnung annähert.
Der Schutz der Privatsphäre kann bei der Planung auf dieselbe Weise berücksichtigt werden wie andere Ressourcen.
Entwirrtes Repräsentationslernen.
Wir schlagen für das Problem der Generierung von Klärungsfragen einen kontradiktorischen Trainingsansatz vor, der die Antwort auf die Frage zur Modellierung der Belohnung verwendet. 
Die Verwendung der Partitionierung mit gesättigten Kosten zur Auswahl von Mustern ist allen bestehenden Algorithmen zur Musterauswahl vorzuziehen.
Die Verwendung von verzweigter Aufmerksamkeit mit erlernten Kombinationsgewichten übertrifft den Basis-Transformer für maschinelle Übersetzungsaufgaben.
Hybrides bildgesteuertes Imitation Learning und modellbasiertes Reinforcement Learning für Planung, Vorhersage und Kontrolle
Wir beleuchten die Probleme mit den gängigen Metriken für die Unsicherheit in einem Bereich und führen eine umfassende Studie über moderne Ensembling-Techniken durch.
Entwicklung eines allgemeinen Rahmens zur Feststellung der zertifizierten Robustheit von ML-Modellen gegenüber verschiedenen Klassen von Störungen durch ungünstige Einflüsse.
Wir schlagen ein generatives latentes Variablenmodell für die unüberwachte Zerlegung von Szenen vor, das eine faktorisierte Objektdarstellung pro Vordergrundobjekt bietet und gleichzeitig Hintergrundsegmente mit komplexer Morphologie zerlegt.
Wir schlagen eine Erweiterung des Conditional Variational Autoencoders vor, die es erlaubt, eine beliebige Teilmenge der Merkmale zu konditionieren und die verbleibenden Merkmale abzufragen.
In diesem Papier schlagen wir eine hierarchische Architekturdarstellung vor, bei der die zufällige oder evolutionäre Architektursuche zu äußerst wettbewerbsfähigen Ergebnissen führt und weniger Rechenressourcen benötigt als der Stand der Technik.
Wir schlagen ein Halluzinatives Topologisches Gedächtnis (HTM) vor, einen visuellen Planungsalgorithmus, der in neuen Umgebungen eine Zero-Shot-Planung mit langem Horizont durchführen kann. 
Wir schlagen vor, die Batch-Normalisierung (BN) zu beschleunigen, indem weniger korrelierte Daten für Reduktionsoperationen mit regelmäßigem Ausführungsmuster gesampelt werden, wodurch eine bis zu 2-fache bzw. 20-prozentige Beschleunigung für die BN selbst und das gesamte Training erreicht wird.
Kommunikationseffizientes föderiertes Lernen mit schichtweiser Anpassung.
Lernen mit begrenzten Trainingsdaten durch Ausnutzung "hilfreicher" Instanzen aus einer umfangreichen Datenquelle.  
Ein neuer ableitungsfreier Optimierungsalgorithmus, abgeleitet von Nesterovs beschleunigten Gradientenmethoden und der Hamilton-Dynamik.
Binarisierte Back-Propagation: Für ein vollständig binarisiertes Training muss lediglich die Größe des Netzes vergrößert werden.
Wie man die Initialisierungs- und Aktivierungsfunktion für tiefe neuronale Netze effektiv auswählt.
Wir stellen kontextuelle Dekompositionen vor, einen Interpretationsalgorithmus für LSTMs, der in der Lage ist, Wort-, Phrasen- und Interaktionsebenen-Bewertungen zu extrahieren.
In diesem Beitrag wird eine neuartige komplexe Maskierungsmethode zur Sprachverbesserung zusammen mit einer Verlustfunktion für eine effiziente Phasenschätzung vorgeschlagen.
Lernen von emergentem Verhalten durch Minimierung der Bayes'schen Überraschung mit RL in natürlichen Umgebungen mit Entropie.
Tiefe Repräsentationen in Kombination mit Gradientenabstieg können jeden Lernalgorithmus annähern.
Wir stellen eine offene Gehirn-Maschine-Schnittstelle vor, deren Leistung nicht an den traditionell verwendeten Bag-of-Words-Ansatz gebunden ist.
Wir stellen eine Einbettung von CNN in ein komplettes Netzwerk vor, die die Einbettung von einzelnen Schichten bei Transfer-Lernaufgaben übertrifft.
Wir stellen eine neuartige Methode zum Pruning von Netzwerken vor, die während des Trainingsprozesses die optimale spärliche Struktur mit einer trainierbaren Pruningschwelle finden kann.
Wir bewerten die biologische Plausibilität neuer ML-Lernalgorithmen abstrakt anhand der erforderlichen mathematischen Operationen.
Wir stellen eine Methode vor, um Modelle mit nachweisbarer Robustheit gegenüber allen $l_p$-Normen für $p\geq 1$ gleichzeitig zu trainieren.
Wir schlagen eine neuartige Methode vor, um mit Bildverschlechterungen verschiedener Stufen umzugehen, indem wir eine Diffusion Terminal Time lernen. Unser Modell kann auf ungesehene Degradationsstufen und unterschiedlichen Störstatistiken verallgemeinert werden.
Wir zeigen, dass es möglich ist, die Berechnung von Wasserstein-Distanzen schnell anzunähern, indem wir eine geeignete Einbettung finden, bei der die euklidische Distanz die Wasserstein-Distanz emuliert.
Wir stellen ein neuartiges Trainingsverfahren vor, mit dem sich effizient ordnungsbezogene Satzrepräsentationen erstellen lassen.
Rekursive Parametrisierung von rekurrenten Modellen verbessert die Leistung.
Eine vergleichende Studie über generative Modelle für Szenarien des kontinuierlichen Lernens.
Zunächst wird das Problem der Optimierung der Stichprobeneffizienz im nicht-parametrisierten Regelraum gestellt und gelöst, und dann wird ein überwachtes Regressionsproblem gelöst, um eine parametrisierte Regel zu finden, die nahe an der optimalen nicht-parametrisierten Regel liegt.
Ein zielorientierter Ansatz zur Modellierung von vier visuellen Arealen der Maus (V1, LM, AL, RL) auf der Grundlage von tiefen neuronalen Netzen, die auf statische Objekterkennung trainiert wurden, enthüllt im Gegensatz zu Primaten keine funktionelle Organisation des visuellen Kortex.
Ein verallgemeinertes transformationsbasiertes Gradientenmodell für die Variationsinferenz.
Wir stellen ein neuartiges Sprachmodell mit größerem Kontext vor, das gleichzeitig Syntax und Semantik erfasst und dadurch in der Lage ist, hochgradig interpretierbare Sätze und Absätze zu erzeugen.
Wir trainieren einen Maler-Agenten für natürliche Medien anhand eines Umgebungsmodells. Basierend auf unserem Maler-Agenten stellen wir einen neuen Ansatz vor, um einen eingeschränkten Maler-Agenten zu trainieren, der dem in der Beobachtung kodierten Befehl folgt.
Wir haben eine Such-Framework und eine Konsistenzstrafe entwickelt, um nicht reale Verzerrungen abzuschwächen.
Das vorgeschlagene System kann verhindern, dass Betrüger mit verstelltem Gesicht eine betrügerische Transaktion durchführen, indem es ein vorab trainiertes DCNN verwendet.
Theoretische Analyse des nichtlinearen breiten Autoencoders.
Wir stellen den ersten hierarchischen RL-Ansatz vor, der erfolgreich 3-Ebenen-Hierarchien parallel in Aufgaben mit kontinuierlichen Zustands- und Aktionsräumen lernt.
In dieser Arbeit wurde das PUbN-Klassifizierungsproblem untersucht, bei dem wir voreingenommene negative Daten (bN), d.h. negative Daten, die nicht vollständig repräsentativ für die wahre zugrundeliegende negative Verteilung sind, in das positiv-ungelabelte (PU) Lernen einbeziehen.
Verstärkungspraktiken für Leistungssteigerungen bei der maschinellen Übersetzung sind nicht unbedingt auf bessere Vorhersagen zurückzuführen.
Wir trainieren ein kleines, effizientes CNN mit der gleichen Leistung wie der OpenAI Transformer bei Textklassifizierungsaufgaben
Ensemble-Methode für das Reinforcement Learning, die Q-Funktionen auf der Grundlage der kumulierten TD-Fehler gewichtet.
Bsuite ist eine Sammlung von sorgfältig konzipierten Experimenten, die die Kernfähigkeiten von RL-Agenten untersuchen.
Verbessertes Vortraining und Analyse der Encoderleistung und der Aufmerksamkeit.
Eine Formulierung für eine Black-Box-Methode des Reinforcement Learnings, um den wahrscheinlichsten Ausfall eines Systems in komplexen Szenarien zu ermitteln.
Verwendung modellfreier Algorithmen wie DQN/TRPO zur iterativen Lösung von Problemen mit kurzem Horizont (modellfrei) in Form von Policy/Value Iteration.
Wir zeigen, dass selbst die stärksten adversarischen Trainingsmethoden nicht gegen adversarische Beispiele ankommen, die auf leicht skalierten und verschobenen Testbildern erstellt wurden.
Verbesserung der sättigenden Aktivierungen (sigmoid, tanh, htanh usw.) und binarisiertes neuronales Netz mit Bias-Initialisierung.
Verwendung von tiefen neuronalen Netzen und cleveren Algorithmen zur Erfassung menschlicher mentaler visueller Konzepte.
Wir stellen einen neuartigen gezielten Blackbox-Angriff vor, der in der Lage ist, den Stand der Technik bei der Umwandlung von Sprache in Text zu täuschen.
Synthese menschlicher Bewegungen bei interaktiven Aufgaben unter Verwendung von Mocap-Daten und hierarchischem RL.
Analyse der Konvergenz und des Moduszusammenbruchs durch Untersuchung des GAN-Trainingsprozesses als Regretminimierung.
Wir schlagen einen neues Framework vor, um die Interpretierbarkeit von neuronalen Netzen zu bewerten.
Ein Deep-RL-Agent, der hyperbolische (und andere nicht-exponentielle) Q-Werte und eine neue Multi-Horizont-Hilfsaufgabe lernt.
In diesem Beitrag wird die EEG-basierte Erkennung von Emotionen einer Person gegenüber einem Bildreiz und ihre Anwendbarkeit im Neuromarketing vorgestellt.
Schnelle Variationsannäherungen für die Annäherung an einen Benutzerzustand und das Lernen von Produkteinbettungen.
Wir stellen eine beweisbare und leicht zu berechnende Bewertungsfunktion vor, die die Leistung von übertragenen Repräsentationen von einer Lernaufgabe zu einer anderen beim Aufgabentransferlernen schätzt.
Wir schlagen ein zuverlässiges bedingtes adversariales Lernverfahren zusammen mit einem einfachen, generischen und dennoch effektiven Rahmen für UDA-Aufgaben vor.
Vom Abstand zum Kernel und Einbettungen über Zufallsmerkmale für strukturierte Eingaben.
Ein quanteninspirierter Kernel für ein Convolutional Network, das Interferenzphänomene aufweist, kann sehr nützlich sein (und mit dem realen Gegenstück verglichen werden).
Eine MMO-inspirierte Forschungsspielplattform zur Untersuchung des aufkommenden Verhaltens großer Populationen in einer komplexen Umgebung.
Diese Arbeit verbessert die bestehende stichprobenbasierte Bewertung für GANs und enthält einige aufschlussreiche Experimente.
Residuale binäre neuronale Netze verbessern die Konvergenzrate und die Inferenzgenauigkeit der binären neuronalen Netze erheblich.
Unüberwachtes Verfahren zur Erkennung gegnerischer Proben im Aktivierungs- und Rekonstruktionsfehlerraum des Autoencoders.
Wir schlagen vor, wissensbasierte Entitäts- und Beziehungsrepräsentationen von Bert für die Einbettung von Wissensgraphen zu lernen.
Ein skalierbares differenzierbares neuronales Modul, das Schlussfolgerungen auf symbolischen KBs implementiert.
DCEM lernt latente Bereiche für Optimierungsprobleme und hilft, die Lücke zwischen modellbasiertem und modellfreiem RL zu schließen --- wir erstellen einen differenzierbaren Controller und fine-tunen Teile davon mit PPO.
Wir verwenden dynamische Belohnungen, um Ereignisextraktoren zu trainieren.
GANs profitieren von der Hochskalierung.
Der gemeinsame Fehler ist für die unüberwachte Bereichsanpassung von Bedeutung, insbesondere wenn die Bereichsverschiebung groß ist.
"Knowledge Flow" trainiert ein tiefes Netz (Student), indem es Informationen von mehreren Netzen (Lehrern) einspeist. Der Student ist nach dem Training unabhängig und erbringt sehr gute Leistungen bei den gelernten Aufgaben, unabhängig von der Einstellung (Reinforcement oder supervised Learning).
Eine effiziente Schätzung des ersten Gaußschen Moments von DNNs als Regularisierer für das Training robuster Netzwerke.
Wir schlagen neuronale Kaskaden vor, einen einfachen und trivial parallelisierbaren Ansatz für das Leseverständnis, der nur aus Feed-Forward-Netzen und Aufmerksamkeit besteht und der auf dem TriviaQA-Datensatz die beste Leistung erzielt.
Wir bringen den Stand der Technik bei der Modellkompression voran, indem wir Atomic Compression Networks (ACNs) vorschlagen, eine neuartige Architektur, die durch rekursive Wiederholung einer kleinen Anzahl von Neuronen aufgebaut ist.
Wir zeigen, dass flussbasierte generative Modelle einen praktikablen und wettbewerbsfähigen Ansatz für die generative Modellierung von Videos bieten.
Wir bieten eine theoretische und empirische Analyse der Rolle des anisotropen Rauschens, das durch den stochastischen Gradienten eingeführt wird, bei der Überwindung von Minima.
Policy-Gradient durch Backpropagation über die Zeit unter Verwendung gelernter Modelle und Q-Funktionen. SOTA-Ergebnisse in Benchmark-Umgebungen mit Reinforcement Learning.
Meta-Lernen auf selbst vorgeschlagenen Aufgabenverteilungen zur Beschleunigung des Reinforcement Learnings ohne vom Menschen vorgegebene Aufgabenverteilungen.
Eine neuartige und praktisch wirksame Methode zur Anpassung vortrainierter neuronaler Netze an neue Aufgaben durch Neutraining einer minimalen (z.B. weniger als 2%) Anzahl von Parametern.
Eine neue theoretische Erklärung für die Existenz adversarialer Beispiele.
Elastic-InfoGAN ist eine Abwandlung von InfoGAN, die ohne jegliche Überwachung entwirrte Repräsentationen in klassenungleichgewichtigen Daten lernt.
Ein verteilter, auf einem latenten Raum basierender Rahmen für die gemeinsame Nutzung von Wissen für tiefes Multi-Task-Lernen.
Game Changer ist ein System, das sowohl Audiobeschreibungen als auch taktile Ergänzungen bietet, um den Zustand des Brettspiels für blinde und sehbehinderte Spieler zugänglich zu machen.
Gekoppelte Regel-Beispiel-Überwachung und ein Implikationsverlust helfen, gemeinsam zu lernen, Störungen aus Regeln zu entfernen und Labels zu implizieren.
Unsere Methode führt das empowerment-regularized maximum-entropy inverse reinforcement learning ein, um nahezu optimale Belohnungen und Strategien aus Expertendemonstrationen zu lernen.
RNNs implementieren implizit Tensor-Produkt-Darstellungen, eine prinzipielle und interpretierbare Methode zur Darstellung symbolischer Strukturen im kontinuierlichen Raum.
Die gelernte Datenerweiterung bewirkt, dass Algorithmen durch induktive Verzerrungen begünstigt werden, so dass RNNs Listenverarbeitungsalgorithmen aus weniger Beispielen lernen können.
Wir schlagen ein spezielles schwach überwachtes Multi-Label-Lernproblem zusammen mit einem neu zugeschnittenen Algorithmus vor, der den zugrundeliegenden Klassifikator lernt, indem er lernt, Pseudo-Labels zuzuweisen.
Ein auf Reinforcement Learning basierender konversationeller Suchassistent, der kontextbezogene Unterstützung bei der subjektiven Suche (z.B. nach digitalen Assets) bietet.
Convolutional Neural Networks verhalten sich wie kompositorische Nearest Neighbors!
Faktorisierung von LSTM-Zuständen und LSTM-Gewichtsmatrizen mit Nullen und Bindungen entsprechend den strukturellen Verzerrungen in der realen Welt, die durch Datalog-Programme ausgedrückt werden.
Wir schlagen einen neuartigen Ansatz zur Lösung von datengesteuerten, modellbasierten Optimierungsproblemen sowohl in passiven als auch in aktiven Umgebungen vor, der auf hochdimensionale Eingaberäume skaliert werden kann.
In dieser Arbeit schlagen wir ein neues Encoder-Decoder-Modell vor, das auf Tensor-Produkt-Darstellungen für die Generierung natürlicher und formaler Sprache basiert, genannt TP-N2F.
In dieser Arbeit wird ein Defogger vorgestellt, ein Modell, das lernt, zukünftige verborgene Informationen aus Teilbeobachtungen vorherzusagen, angewandt auf einen StarCraft-Datensatz.
Wir untersuchen die Generalisierung neuronaler Netze beim gradientenbasierten Meta-Lernen, indem wir verschiedene Eigenschaften der Ziellandschaft analysieren.
Wir stellen ein generatives Modell vor, das bei Graustufen- und Naturbildern den neuesten Stand der Technik beweist.
Wir schlagen ein neuartiges Netzwerk mit dem Namen Recurrent Identity Network (RIN) vor, das es einem einfachen rekurrenten Netzwerk ermöglicht, das Problem des verschwindenden Gradienten zu überwinden und gleichzeitig sehr tiefe Modelle ohne den Einsatz von Gates zu trainieren.
Diese Arbeit befasst sich mit der Fehlertoleranz bei zufälligen und gegnerischen Ausfällen.
Wir stellen eine neuartige, einheitliche Architektur vor, die Videobilder aus einem einzigen bewegungsunscharfen Bild durchgängig wiederherstellt.
Wir schlagen ein neuartiges strukturiertes, klassenblindes Pruning-Verfahren vor, um hochkomprimierte neuronale Netze zu erzeugen.
Wir verwenden Kronecker-Summen-Approximationen für Low-Rank-Training, um die Herausforderungen beim Training neuronaler Netze auf Edge-Geräten zu bewältigen, die neue Speichertechnologien nutzen.
Untersucht systematisch, wie gut wir die verborgenen Merkmale eines tiefen Netzes mit Hilfe logischer Regeln erklären können.
Verbesserte Likelihood-Schätzungen in variablen Autokodierern durch selbstüberwachtes Feauture Lernen.
Dieses Papier zeigt, dass modellfreie Policy-Gradienten-Methoden zur globalen optimalen Lösung für nicht-konvexe linearisierte Kontrollprobleme konvergieren können.
Wir nutzen die Theorie des Compressed Sensing, um zu beweisen, dass LSTMs bei der linearen Textklassifikation mindestens genauso gut abschneiden wie Bag-of-n-Grams.
Wir führen neue punktweise Convolution Layers ein, die mit extrem schnellen konventionellen Transformationen in tiefen neuronalen Netzen ausgestattet sind.
Wir schlagen die Verwendung von Gittern zur Darstellung von Objekten vor und beweisen ein grundlegendes Ergebnis, wie man Netze trainiert, die diese verwenden.
Wir versuchen, einen Klassifikator zu entwerfen und zu trainieren, dessen adversariale Robustheit mehr Ähnlichkeit mit der Robustheit des Menschen hat.
Wir schlagen ein diskretes Wasserstein-GAN-Modell (DWGAN) vor, das auf einer dualen Formulierung des Wasserstein-Abstands zwischen zwei diskreten Verteilungen basiert.
Dies ist ein Überblick über die Omega AGI-Architektur, die die Grundlage für ein Automatisierungssystem für die Datenwissenschaft bildet.
Wir schlagen den Flow-Mechanismus und eine Ende-zu-Ende Architektur, FlowQA, vor, die SotA auf zwei QA-Datensätzen für Konversation und eine Aufgabe zum Verstehen sequenzieller Anweisungen ermöglicht.
Wir stellen einen neuartigen Algorithmus zur Lösung von Reinforcement Learning und Bandit-strukturierten Vorhersageproblemen mit sehr spärlicher Verlustrückmeldung vor.
Wir stellen eine einfache Modifikation der alternierenden SGD-Methode vor, die als Vorhersageschritt bezeichnet wird und die Stabilität der adversarischen Netzwerke verbessert.
In diesem Beitrag werden domänenunabhängige Kompilationen von Benutzerfragen in Constraints für kontrastive Erklärungen vorgestellt.
 Wir betten die Knoten in einem Graphen als Gaußsche Verteilungen ein, die es uns ermöglichen, die Ungewissheit über ihre Darstellung zu erfassen.
Logit-Regularisierungsmethoden helfen bei der Erklärung und Verbesserung des Stands der Technik der gegnerischen Verteidigung.
Wir beschreiben eine modulare und komponierbare Sprache zur Beschreibung von ausdrucksstarken Suchräumen über Architekturen und einfachen Modellsuchalgorithmen, die auf diese Suchräume angewendet werden. 
Wir zeigen, dass Wissenstransfertechniken die Genauigkeit von Netzwerken mit geringer Genauigkeit verbessern können und eine neue Spitzengenauigkeit für ternäre und 4-Bit-Genauigkeit erreichen. 
Diese Arbeit stellt einen verbesserten Trainingsmechanismus vor, mit dem binäre Netze mit geringerem Genauigkeitsabfall erzeugt werden können, was dazu beiträgt, die Lücke zu ihrem Gegenstück mit voller Präzision zu schließen.
Vereinheitlichender Rahmen zur Durchführung von Clustering mit tiefen neuronalen Netzen
HYPE ist ein zuverlässiger Bewertungsmaßstab für die Bewertung generativer Modelle, beginnend mit der Erzeugung menschlicher Gesichter über 4 GANs.
Wir schlagen ein neues unüberwachtes maschinelles Übersetzungsmodell vor, das ohne die Verwendung paralleler Korpora lernen kann; experimentelle Ergebnisse zeigen eine beeindruckende Leistung bei mehreren Korpora und Sprachpaaren.
Wir belohnen Agenten dafür, dass sie einen kausalen Einfluss auf die Handlungen anderer Agenten haben, und zeigen, dass dies zu einer besseren Zusammenarbeit und sinnvolleren emergenten Kommunikationsprotokollen führt. 
Wir entwickeln einen Agenten, den wir Distributional Deterministic Deep Policy Gradient Algorithmus nennen, der bei einer Reihe von anspruchsvollen kontinuierlichen Steuerungsproblemen den Stand der Technik erreicht.
Wir schlagen eine neue Q-Wert-Funktion vor, die ein besseres Lernen von Gaußschen Strategien ermöglicht.
Wir stellen KG-A2C vor, einen Reinforcement Learning Agenten, der einen dynamischen Wissensgraphen aufbaut, während er natürliche Sprache unter Verwendung eines schablonenbasierten Aktionsraums erforscht und generiert - und dabei alle derzeitigen Agenten in einer Vielzahl textbasierter Spiele übertrifft.
Wir beweisen, dass tiefe neuronale Netze bei der Annäherung an spärliche multivariate Polynome exponentiell effizienter sind als flache Netze.
Wir schlagen ein CNN-Neuronen-Ranking mit zwei verschiedenen Methoden vor und zeigen deren Konsistenz bei der Erstellung des Ergebnisses, das es erlaubt, zu interpretieren, was das Netzwerk für wichtig hält, und das Netzwerk zu komprimieren, indem die wichtigsten Knoten beibehalten werden.
Ein modularer und hierarchischer Ansatz zum Erlernen von Strategien zur Erkundung von 3D-Umgebungen.
Eine unüberwachte Methode zur Anpassung von Simulationen an den realen Bereich für die semantische Segmentierung unter Verwendung privilegierter Informationen aus einem Simulator mit GAN-basierter Bildübersetzung.
Die erste Strengheitsdiagnose von groß angelegtem adversarialem Training auf ImageNet
Zuordnung der Bias-Terme tiefer neuronaler Netze zu Eingangsmerkmalen durch einen Backpropagation-Algorithmus; Generierung ergänzender und hochinterpretierbarer Erklärungen von DNNs zusätzlich zu gradientenbasierten Zuordnungen.
In diesem Beitrag wird eine Methode vorgestellt, mit der sich mehrere Periodizitäten in einem Signal unter Verwendung von FFT und ACF selbständig finden lassen, wobei drei neue Schritte (Clustering/Filterung/Detrending) hinzugefügt werden
Ein einfaches, aber effektives Imitation Learning Schema, das Anreize für die Erkundung einer Umgebung ohne äußere Belohnung oder menschliche Vorführung schafft.
Ein neues Framework, das den dualen Raum für die Erzeugung von Bildern nutzt, die bei einer großen Anzahl von Klassen mit mehreren Etiketten übereinstimmen.
Wir stellen ein neues Feed-Forward-Graph-ConvNet vor, das auf der Verallgemeinerung der Wavelet-Streuungstransformation von Mallat basiert, und demonstrieren seine Nützlichkeit bei Graphenklassifizierungs- und Datenexplorationsaufgaben.
Wir stellen einen groß angelegten Belegdatensatz für Post-OCR-Parsing-Aufgaben vor.
Beschleunigung des CNN-Trainings auf einer Pipeline von Beschleunigern mit veralteten Gewichten.
Wir zeigen, dass tiefe Netze nicht nur zu empfindlich auf aufgabenirrelevante Änderungen ihrer Eingaben reagieren, sondern auch zu invariant gegenüber einem breiten Spektrum aufgabenrelevanter Änderungen sind, wodurch große Regionen im Eingaberaum anfällig für Angriffe durch Angreifer werden.
Verbessertes Training aktueller strömungsbasierter generativer Modelle (Glow und RealNVP) anhand von Benchmarks zur Dichteschätzung.
Tiefe neuronale Netze, die mit Datenerweiterung trainiert werden, benötigen keine weitere explizite Regularisierung (wie z. B. Gewichtsabnahme und Dropout) und weisen eine größere Anpassungsfähigkeit an Änderungen der Architektur und der Menge der Trainingsdaten auf.
Diese Arbeit verbessert die Qualität des kürzlich vorgeschlagenen AFL-Ansatzes (Adversarial Feature Leaning) für die Einbeziehung expliziter Einschränkungen in Darstellungen, indem es das Konzept der Verwundbarkeit des Gegners einführt. 
Wir erweitern einen erfolgreichen rekurrenten Variations-Autoencoder für dynamische Systeme, um eine Instanz der dynamischen Systemhierarchie in den Neurowissenschaften mit Hilfe der Leitermethode zu modellieren.
Wir führen einen effizienten Quantisierungsprozess ein, der eine Leistungssteigerung auf einem spezialisierten Integer-Only-Beschleuniger für neuronale Netze ermöglicht.
Diese Arbeit schlägt ein neues CNN-Modell vor, das Energiekosten mit einer dynamischen Routing-Strategie kombiniert, um adaptive energieeffiziente Inferenzen zu ermöglichen.
Wir stellen LSH Softmax vor, eine Softmax-Approximationsschicht für sublineares Lernen und Inferenz mit starken theoretischen Garantien; wir zeigen sowohl seine Anwendbarkeit als auch seine Effizienz, indem wir es an einer realen Aufgabe evaluieren: Sprachmodellierung.
RL kann (stochastische) Multi-Roboter-/Planungsprobleme skalierbar und übertragbar mit Hilfe von Grapheneinbettung lösen.
Wir stellen einen neuen Algorithmus für das Lernen nach Lehrplan vor, der auf dem Begriff der Beherrschungsrate basiert und bisherige Algorithmen übertrifft.
Inspiration durch lokale dendritische Prozesse des neokortialen Lernens, um unüberwachtes Lernen wieder groß zu machen.
Speichernetzwerke mit schnellerer Inferenz.
Indem wir uns von der Linguistik inspirieren lassen, insbesondere von der Universalgrammatik-Hypothese, lernen wir sprachunabhängige, universelle Repräsentationen, die wir für das sprachenübergreifende Lernen nutzen können.
In diesem Beitrag wird gezeigt, dass das Wasserstein-Distanz-Ziel das Training von latenten Variablenmodellen mit diskreten Latenzen in einem Fall ermöglicht, in dem das Variations-Autoencoder-Ziel dies nicht schafft.
Ein neuronales Graphennetz, das in der Lage ist, automatisch eine dynamische interaktive Graphstruktur zu erlernen und zu nutzen.
Ein neuer Algorithmus für das Training neuronaler Netze, der im Vergleich zu gängigen adaptiven Methoden vorteilhaft ist.
Weist auf Probleme bei der Verlustfunktion hin, die in IRGAN, einem kürzlich vorgeschlagenen GAN-Rahmen für Information Retrieval, verwendet wird. Außerdem wird ein durch Co-Training motiviertes Modell vorgeschlagen, das eine bessere Leistung erzielt.
Wir schlagen Federated User Representation Learning (FURL) vor, eine einfache, skalierbare, datenschutzfreundliche und bandbreiteneffiziente Methode, um bestehende neuronale Personalisierungstechniken im Rahmen des Federated Learning (FL) zu nutzen.
Autoencoder für Text mit einer neuen Methode zur Verwendung eines diskreten latenten Raums.
Vielfältig strukturierter latenter Raum für generative Modelle.
LoopGAN erweitert die Zykluslänge in CycleGAN, um eine unausgerichtete sequentielle Transformation für mehr als zwei Zeitschritte zu ermöglichen.
Wir schlagen SWAP vor, einen verteilten Algorithmus für das Training von neuronalen Netzen in großen Mengen.
Ein neuer Trainingsalgorithmus für große Batchgrößen, der auf Layer-wise Adaptive Rate Scaling (LARS) basiert; mit LARS haben wir AlexNet und ResNet-50 auf einen Batch von 16K skaliert.
Lernen von kompositionellen Koopman-Operatoren für effiziente Systemidentifikation und modellbasierte Steuerung.
Wir stellen ein Verfahren zur Berechnung von Gradienten mit konstantem Speicher durch Lösungen von stochastischen Differentialgleichungen (SDEs) vor und wenden die Methode zum Lernen latenter SDE-Modelle an.
Lernen von datenschutzfreundlichen Transformationen aus Daten. Ein kollaborativer Ansatz.
Wir schlagen mehrere intrinsische Belohnungsfunktionen vor, um die koordinierte Erkundung in Multi-Agenten-Problemen zu fördern, und stellen einen Ansatz zur dynamischen Online-Auswahl der besten Erkundungsmethode für eine bestimmte Aufgabe vor.
Wir schlagen vor, synthetisierende "few-shot"-Klassifikatoren und "many-shot"-Klassifikatoren mit einer einzigen Zielfunktion für GFSL zu lernen.
Wir kombinieren A*-Suche mit Reinforcement Learning, um den Code für maschinelles Lernen zu beschleunigen.
Dateneffizientes tiefes Reinforcement Learning kann zum Erlernen präziser Stapelungsstrategien verwendet werden.
Regelparametrisierungen und unverzerrte Regelentropieschätzer für MDP mit großem mehrdimensionalen diskreten Aktionsraum.
Gewichte zerlegen, um weniger FLOPs mit SVD zu verwenden.
Asymptotische Konvergenz für stochastische Subgradien-Methode mit Momentum unter allgemeiner paralleler asynchroner Berechnung für allgemeine nichtkonvexe, nichtglatte Optimierung.
Wir präsentieren einen Low-Bias-Schätzer für boolesche stochastische Variablenmodelle mit vielen stochastischen Schichten.
Wir schlagen eine neuartige Methode vor, um gegebene Bilder mit Hilfe von natürlichsprachlichen Beschreibungen zu manipulieren.
Verwendung einer GAN-basierten Methode zur skalierbaren Lösung des optimalen Transports.
Wir haben einen probabilistischen Planungsalgorithmus vorgeschlagen, der die Kontrolle als Inferenz und die Sequential Monte Carlo-Methode nutzt.
Eine einfache GAN-Modifikation, die die Leistung bei vielen Verlusten, Architekturen, Regularisierungsverfahren und Datensätzen verbessert. 
Wie man den ursprünglichen Wahrscheinlichkeitsvektor für Millionen von Klassen aus Count-Min-Skizzenmessungen schätzt - ein theoretischer und praktischer Aufbau.
Sie können den Klassifikator in neuronalen Netzen korrigieren, ohne an Genauigkeit zu verlieren.
Erlernen der Erkennung von Objekten ohne Bildbeschriftung aus 3 Minuten Videomaterial.
Wir stellen ReClor vor, einen Datensatz zum Leseverständnis, der logisches Denken erfordert, und stellen fest, dass die aktuellen State-of-the-Art-Modelle mit echtem logischem Denken zu kämpfen haben und nur annähernd so gut abschneiden wie der Zufallsgenerator.
Nur Störungen eingeben, die Softmax-Ausgänge sammeln und die Gewichte stehlen.
Wir schlagen eine neue Form eines Autoencoding Modells vor, das die besten Eigenschaften von Variations-Autoencodern (VAE) und generativen adversarischen Netzen (GAN) vereint.
Wir nutzen die Verbindung zwischen gradientenbasiertem Meta-Lernen und hierarchischem Bayes, um eine Mischung von Meta-Lernern zu lernen, die für eine heterogene und sich entwickelnde Aufgabenverteilung geeignet ist.
Wir stellen eine neue Routing-Methode für Capsule-Netzwerke vor, die auf CIFAR-10/CIFAR-100 gleich gut abschneidet wie ResNet-18.
Wir stellen Doc2Dial vor, ein Ende-zu-Ende Framework zur Generierung von Konversationsdaten auf der Basis von Geschäftsdokumenten mittels Crowdsourcing für die Ausbildung von automatischen Dialogagenten.
Wir stellen ein autoregressives generatives Modell für Spektrogramme vor und zeigen Anwendungen für die Erzeugung von Sprache und Musik.
Moderne tiefe CNNs sind nicht invariant gegenüber Übersetzungen, Skalierungen und anderen realistischen Bildtransformationen, und dieser Mangel an Invarianz hängt mit der Unterabtastung und den in den Bilddatensätzen enthaltenen Verzerrungen zusammen.
Synthese komplexer und erweiterter menschlicher Bewegungen mit Hilfe eines automatisch konditionierten LSTM-Netzes.
Wir haben einen Mechanismus entwickelt, der als Wettbewerb zwischen den Pixeln bezeichnet wird und der es ermöglicht, dass (annähernd) vollständig Herausragende Methoden die Plausibilitätsprüfung bestehen.
Bildklassifizierung durch iterative Abfrage von Referenzbildern aus einer Kandidatenklasse mit einem RNN und Verwendung von CNN zum Vergleich mit dem Eingabebild.
Definieren wir zum ersten Mal das Pruning-Problem auf Filterebene für binäre neuronale Netze und schlagen eine Methode zu dessen Lösung vor.
Reduzierung der Rechen- und Speicherkomplexität von RNN-Modellen um das bis zu 100-fache mit Hilfe von spärlichen Low-Rank-Kompressionsmodulen, die durch Wissensdestillation trainiert werden.
Wir vergleichen Graph RNNs und Graph ConvNets und betrachten die allgemeinste Klasse von Graph ConvNets mit Residualität.
Vergleich von komplex- und reellwertigen mehrschichtigen Perzeptronen in Bezug auf die Anzahl der reellwertigen Parameter.
Ein spektrales neuronales Convolutional Neural Network mit spektralen Zoom-Eigenschaften.
Wir stellen ein Echtzeit-Segmentierungsmodell vor, das automatisch durch ein Multiskalen-NAS-Framework entdeckt wird und 30 % schneller ist als die neuesten Modelle.
Wir stellen R2D3 vor, einen Agenten, der effizient von Demonstrationen Gebrauch macht, um schwierige Explorationsprobleme in teilweise beobachtbaren Umgebungen mit hochvariablen Anfangsbedingungen zu lösen.
Wir untersuchen, wie ein rekurrentes neuronales Netz erfolgreich eine Aufgabe lernt, die Langzeitgedächtnis und sequenziellen Abruf kombiniert.
Wir schlagen die Idee vor, die Norm der Nachfolgerepräsentation als Explorationsbonus beim Reinforcement Learning zu verwenden. In harten Explorations-Atari-Spielen, entspricht die Leistung des deep RL Algorithmus der jüngsten Pseudo-Count-basierte Methode.
Datenabhängige Faktorisierung von Dimensionen in einer Mehrskalenarchitektur auf der Grundlage des Beitrags zur gesamten log-Wahrscheinlichkeit.
Die vier bestehenden Backpropagation-basierten Zuordnungsmethoden sind im Grunde genommen ähnlich. Wie sind sie zu bewerten?
SGD und Adam unter dem Single-Spiked-Modell für Tensor-PCA.
In diesem Beitrag wird eine Methode vorgestellt, mit der das in einem Convolutional Neural Network (CNN) kodierte Wissen quantitativ und semantisch erklärt werden kann.
Das Papier stellt eine dynamische Bewertungsmethode für die adaptive Sequenzmodellierung vor.
Wir schlagen einen neuartigen, projektionsbasierten Weg vor, um die bedingte Information in den Diskriminator von GANs zu integrieren, der die Rolle der bedingten Information in dem zugrunde liegenden probabilistischen Modell respektiert.
Die Frechet-Distanz zwischen Training- und Testverteilung korreliert mit der Veränderung der Leistung bei Funktionen, die nicht invariant gegenüber der Verschiebung sind.
Ein neues, sehr einfaches dynamisches System wird eingeführt, das schöne Muster erzeugt; Eigenschaften werden bewiesen und Möglichkeiten erforscht.
GMM-UNIT ist ein Bild-zu-Bild-Übersetzungsmodell, das ein Bild auf stochastische Weise auf mehrere Bereiche abbildet.
Wir stellen eine neuartige Architektur vor, die auf dynamischem Speicher, Aufmerksamkeit und Komposition für die Aufgabe des maschinellen Denkens basiert.
Um die im latenten Raum gespeicherten Informationen zu verstehen, trainieren wir einen GAN-ähnlichen Decoder, der darauf beschränkt ist, Bilder zu erzeugen, die der VAE-Codierer auf dieselbe Region des latenten Raums abbildet.
Zwei neuartige GANs werden konstruiert, um qualitativ hochwertige 3D-fMRI-Gehirnbilder und synthetische Gehirnbilder zu erzeugen, die die nachgelagerten Klassifizierungsaufgaben erheblich verbessern.
Zero-shot Cross-Lingual Transfer mit Hilfe von mehrsprachiger neuronaler maschineller Übersetzung 
Wir schlagen einen differenzierbaren Architektur-Suchalgorithmus sowohl für Convolutional als auch für Recurrent Networks vor, der mit dem Stand der Technik konkurrieren kann und dabei um Größenordnungen weniger Rechenressourcen benötigt.
Derzeitige Sprachgenerierungssysteme streben entweder eine hohe Wahrscheinlichkeit an und verfallen in eine generische Wiederholung oder sie sind in ihrer Stochastizität falsch kalibriert - wir liefern Beweise für beides und schlagen eine Lösung vor: Nukleus-Sampling.
Die erste Textverteidigungsmethode auf Wortebene und die verbesserte generische Angriffsmethode gegen Angriffe, die auf der Ersetzung von Synonymen basieren.
Effiziente Kreditvergabe in rekurrenten Netzen ohne Backpropagation durch Zeit.
Wir schlagen ein neuartiges adversariales Lernverfahren für strukturierte Vorhersagen vor, bei dem diskriminative Modelle zur Verfeinerung strukturierter Vorhersagemodelle in der Inferenzphase verwendet werden können. 
Eine neue Methode zur Erkennung von Neuheiten durch die Verwendung von Aktivierungswerten im verborgenen Raum, die von einem tiefen Autoencoder erhalten werden.
Lernen von Präferenzen über Planspuren durch aktives Lernen.
Wir erden Sprachbefehle in einer hochdimensionalen visuellen Umgebung, indem wir sprachbedingte Belohnungen durch inverses Reinforcement Learning erlernen.
Wir plädieren für zufällige Merkmale als Theorie biologischer neuronaler Netze und konzentrieren uns dabei auf spärlich verbundene Netze.
Wir schlagen die Prob2Vec-Methode zur Problemeinbettung vor, die in einem personalisierten E-Learning-Tool zusätzlich zu einer Klassifizierungsmethode auf Datenebene, dem so genannten negativen Pre-Training, eingesetzt wird, wenn der Trainingsdatensatz unausgewogen ist.
Wir stellen CrescendoNet vor, eine tiefe CNN-Architektur durch das Stapeln einfacher Bausteine ohne Residual Connections.
Wir kombinieren Splines mit neuronalen Netzen, um eine neuartige Verteilung über Funktionen zu erhalten und sie zur Modellierung von Intensitätsfunktionen von Punktprozessen zu verwenden.
Wir entwickeln einen aufgabenspezifisch komprimierten BERT, der 4,3x kleiner und 4,0x schneller als BERT-BASE ist und gleichzeitig eine konkurrenzfähige Leistung auf GLUE und SQuAD erreicht.
Wir zeigen, dass sich die meisten Varianten von importance-weighted Autoencodern auf prinzipiellere Weise als Spezialfälle von adaptiven importance-sampling Ansätzen wie dem reweighted-wake sleep Algorithmus ableiten lassen.
NUQSGD schließt die Lücke zwischen den theoretischen Garantien von QSGD und der empirischen Leistung von QSGDinf.
Neuronale Netze können so trainiert werden, dass sie ihre eigene Konnektivität ändern und so ihre Online-Lernleistung bei schwierigen Aufgaben verbessern.
Es wird ein geometrisches Verfahren auf Simplex-Basis vorgeschlagen, um Few-Shot Learning Probleme zu bewältigen.
Wir zeigen, dass ein Arbeitsgedächtnis-Input für ein Reservoir-Netzwerk eine lokale belohnungsmodulierte Hebb'sche Regel genauso gut funktionieren lässt wie die rekursive Least-Squares-Regel (auch bekannt als FORCE).
Wir kombinieren die rechnerischen Vorteile temporaler Convolutional Architekturen mit der Ausdruckskraft stochastischer latenter Variablen.
Es wird eine gradientenfreie Methode für ein nicht-konvexes Optimierungsproblem vorgeschlagen.
Wir schlagen eine neue Methode vor, die die Gradienten von differenzierbaren Simulatoren nutzt, um die Leistung von RL für die Robotersteuerung zu verbessern.
Wir schlagen Bayes'sche Hypernetze vor: einen Rahmen für approximative Bayes'sche Inferenz in neuronalen Netzen.
Deep Innovation Protection ermöglicht die Entwicklung komplexer Weltmodelle für 3D-Aufgaben von Anfang bis Ende.
Wir schlagen MACER vor: einen beweisbaren Verteidigungsalgorithmus, der robuste Modelle durch Maximierung des zertifizierten Radius trainiert. Er verwendet kein adversarial Training, ist aber besser als alle bestehenden beweisbaren l2-Verteidigungen.
Dieses Papier schlägt eine neuartige Akteur-Kritik-Methode vor, die Hessians einer Kritik verwendet, um einen Akteur zu aktualisieren.
Generative Klassifikatoren auf komplexen Datensätzen zu skalieren und ihre Effektivität bei der Zurückweisung illegaler Eingaben, einschließlich Proben außerhalb der Verteilung und gegnerischer Beispiele, zu bewerten.
Eine Theorie für die Initialisierung und Skalierung von Schichten eines neuronalen ReLU-Netzes.
Die Ausgaben moderner NLP-APIs auf unsinnigem Text liefern starke Signale über die Interna des Modells, was es Angreifern ermöglicht, die APIs zu stehlen.
Wir stellen SeaRNN vor, einen neuartigen Algorithmus für das RNN-Training, inspiriert durch den Ansatz des "Learning to Search" zur strukturierten Vorhersage, um die Einschränkungen des MLE-Trainings zu vermeiden.
Eine Methode des kontinuierlichen Lernens, die mit Hilfe von Destillation Expertenstrategien und Transferlernen kombiniert, um das Erlernen neuer Fähigkeiten zu beschleunigen.
Erklärbares Reinforcement-Learning-Modell unter Verwendung einer neuartigen Kombination von Expertenmischungen mit nicht-differenzierbaren Entscheidungsbaum-Experten.
Wir entwickeln und analysieren einen neuen ableitungsfreien Optimierungsalgorithmus mit Impuls- und Wichtigkeitsabtastung mit Anwendungen auf kontinuierliche Steuerung.
Wir schlagen eine neue Methode für das Training von Deep Hashing für die Bildsuche vor, bei der nur eine relationale Abstandsmetrik zwischen Proben verwendet wird.
Wir schlagen einen neuartigen Ansatz zur Verbesserung einer gegebenen flächenübergreifenden Abbildung durch lokale Verfeinerung mit einer neuen iterativen Methode zur Verformung des Netzes vor, um die Benutzeranforderungen zu erfüllen.
Einführung eines informationstheoretischen Blickwinkels auf das Verhalten von Optimierungsprozessen in tiefen Netzwerken und deren Generalisierungsfähigkeiten.
Die Darstellung der Netzarchitektur als eine Reihe von Syntaxbäumen und die Optimierung ihrer Struktur führt zu genauen und präzisen Regressionsmodellen. 
Empirische und theoretische Untersuchung der Auswirkungen von Staleness bei nicht-synchroner Ausführung auf Algorithmen des maschinellen Lernens.
In dieser Arbeit wird ein skalierbarer Algorithmus zur nichtlinearen Offline-Systemidentifikation aus Teilbeobachtungen vorgestellt.
Allgemeine Analyse von vorzeichenbasierten Methoden (z.B. signSGD) für nicht-konvexe Optimierung, aufbauend auf intuitiven Schranken für Erfolgswahrscheinlichkeiten.
Für das Off-Policy-Lernen mit Bandit-Rückkopplungen schlagen wir einen neuen varianzregulierten kontrafaktischen Lernalgorithmus vor, der sowohl theoretische Grundlagen als auch eine überlegene empirische Leistung aufweist.
Wir kombinieren harte handwerkliche Einschränkungen mit einer tiefen priorisierten schwachen Einschränkung, um seismische Bilder zu erstellen und Informationen über die "posteriore" Verteilung zu erhalten, indem wir die Multiplizität in den Daten nutzen.
Stand der Technik bei komplexem Text-zu-SQL-Parsing durch die Kombination von hartem und weichem relationalem Schließen in der Schema-/Fragenkodierung.
Wir stellen ein kontinuierliches Lernsystem vor, das auf der Sprachmodellierung basiert und bei dem kein explizites Aufgabensegmentierungssignal gegeben ist, und schlagen ein neuronales Netzmodell mit wachsendem Langzeitgedächtnis vor, um dieses Problem zu lösen.
Vorgeschlagener RNN-basierter Algorithmus zur Schätzung der wahrscheinlichen Verteilung bei ein- und mehrstufigen Prognosen in Zeitreihenvorhersageproblemen.
LSTMs können das Arbeitsgedächtnis effektiver modellieren, wenn sie durch Reinforcement Learning erlernt werden, ähnlich wie das Dopaminsystem, das das Gedächtnis im präfrontalen Kortex moduliert.
Reformulieren Sie die Nichtlinearitäten tiefer Netzwerke aus dem Bereich der Vektorquantisierung und verbinden Sie die meisten bekannten Nichtlinearitäten miteinander.
Wir lernen, Proteinsequenzen aus gegebenen Strukturen mit einem Modell zu generieren, das spärliche, weitreichende Abhängigkeiten erfasst.
Ein Framework, das tiefe Netzwerkschichten mit stochastischen Optimierungsalgorithmen verbindet; kann zur Verbesserung der Modellgenauigkeit und zur Information über das Netzwerkdesign verwendet werden.
Eine Methode zum Erlernen der Quantisierungskonfiguration für Netze mit geringer Genauigkeit, die den neuesten Stand der Technik für quantisierte Netze erreicht.
Wir schlagen mehrere allgemeine Debiasing-Strategien vor, um die in verschiedenen Datensätzen auftretenden Verzerrungen zu beseitigen und eine erheblich verbesserte Out-of-Domain-Leistung in allen Situationen zu erzielen.
Wir stellen einen auf CNN-Inferenz basierenden Rekonstruktionsalgorithmus vor, der für CT mit extrem wenigen Ansichten geeignet ist. 
Wir bauen kenntnisreiche Gesprächsagenten durch Konditionierung auf Wikipedia und eine neue überwachte Aufgabe.
Synthese von GCN- und LINUCB-Algorithmen für Online-Lernen mit fehlendem Feedback.
Ein guter Tagger vergibt ähnliche Tags für eine bestimmte Arbeit und die darin zitierten Arbeiten.
Wir schlagen eine auf Quantisierung basierende Methode vor, die die gelernten Repräsentationen eines CNN so reguliert, dass sie automatisch mit der trainierbaren Konzeptmatrix abgeglichen werden, um so effektiv negative Einflüsse herauszufiltern.
Die Arbeit bietet eine vollständige Charakterisierung von permutationsinvarianten und äquivarianten linearen Schichten für Graphdaten.
Kausal korrekte Teilmodelle müssen nicht die gesamte Beobachtung erzeugen, um in stochastischen Umgebungen kausal korrekt zu bleiben.
Ein effizienter Algorithmus für lebenslanges Lernen, der im Vergleich zu anderen Algorithmen einen besseren Kompromiss zwischen Genauigkeit und Zeit-/Speicherkomplexität bietet. 
Wir haben modernste Trainingsergebnisse mit 8-Bit-Gleitkommadarstellung für Resnet, GNMT und Transformer demonstriert.
Wir schlagen eine Instanz-Cross-Entropie (ICE) vor, die die Differenz zwischen einer geschätzten Instanz-Level-Matching-Verteilung und ihrer Grundwahrheit misst. 
Die Einbeziehung latenter Variablen in das Modell, die zukünftige Inhalte kodieren, verbessert die langfristige Vorhersagegenauigkeit, was für eine bessere Planung in modellbasierten RL entscheidend ist.
Integrative Tensor-based Anomaly Detection (ITAD) für ein Satellitensystem.
Die Begrenzung der Zustandsinformationen für die Standardregeln kann die Leistung in einem KL-regulierten RL-Framework verbessern, in dem sowohl der Agent als auch die Standardregeln gemeinsam optimiert werden.
Wir berechnen die Auffälligkeit mit Hilfe eines starken generativen Modells, um effizient über plausible alternative Eingaben zu marginalisieren und so konzentrierte Pixelbereiche zu erkennen, die Labelinformationen enthalten.
Das Variationsnetzwerk ist ein generatives Modell, das in der Lage ist, ohne Überwachung hochrangige Attribute zu erlernen, die dann für eine kontrollierte Eingabemanipulation verwendet werden können.
Die Menschen in der Schleife überarbeiten die Dokumente, damit sie mit den kontrafaktischen Bezeichnungen übereinstimmen, was dazu beiträgt, die Abhängigkeit von falschen Assoziationen zu verringern.
Wir schlagen ein neues objektives Maß für die Bewertung von Erklärungen vor, das auf dem Begriff der Widerstandsfähigkeit gegenüber Widrigkeiten beruht. Die Bewertungskriterien ermöglichen es uns, neue Erklärungen abzuleiten, die die relevanten Merkmale qualitativ und quantitativ erfassen.
In dieser Arbeit wird ein GAN-basierter Rahmen für das Lernen der Verteilung aus hochdimensionalen unvollständigen Daten vorgestellt.
Wir schlagen eine neue Kompressionsmethode, Inter-Layer Weight Prediction (ILWP) und eine Quantisierungsmethode vor, die die vorhergesagten Residuen zwischen den Gewichten in den Convolutional Layers quantisiert.
Wir erweitern eine moderne Technik, um FLOPs direkt als Teil des Optimierungsziels einzubeziehen, und wir zeigen, dass bei einer gewünschten FLOPs-Anforderung verschiedene neuronale Netze erfolgreich trainiert werden.
Granularitätsgesteuertes multidomänen und multimodales Bild-zu-Bild-Übersetzungsverfahren.
Analyse der Ausdruckskraft und Allgemeinheit von rekurrenten neuronalen Netzen mit ReLu-Nichtlinearitäten unter Verwendung der Tensor-Train-Zerlegung.
Wir schlagen einen neuen, effizienten Algorithmus zur Konstruktion von adversarialen Beispielen vor, der auf Deformationen statt auf additiven Störungen beruht.
Regularisierendes adversariales Lernen mit einem Informationsengpass, angewandt auf Imitation Learning, inverses Reinforcement Learning und generative adversariale Netzwerke.
Die einfache Augmentierungsmethode überwindet den in der Literatur beobachteten Kompromiss zwischen Robustheit und Genauigkeit und wirft Fragen zu den Auswirkungen der Trainingsverteilung auf die Generalisierung außerhalb der Verteilung auf.
Wir verwenden Mixed-Density-Netzwerke, um eine vollständige bedingte Dichteschätzung für die räumliche Offset-Regression durchzuführen, und wenden sie auf die Aufgabe der menschlichen Posenschätzung an.
Visualisierung der Unterschiede zwischen regulärer und relativer Aufmerksamkeit für Music Transformer.
Drei Faktoren (Chargengröße, Lernrate, Gradientenrauschen) verändern in vorhersehbarer Weise die Eigenschaften (z. B. Schärfe) der mit SGD gefundenen Minima.
Einfacher generativer Ansatz zur Lösung des Wortanalogieproblems, der Einblicke in die Wortbeziehungen und die Probleme bei deren Schätzung liefert.
In diesem Papier werden mehrere gängige Praktiken zur Festlegung von Hyperparametern für das Fine-Tuning erneut untersucht.
Verbesserung des tiefen Transferlernens mit Regularisierung unter Verwendung aufmerksamkeitsbasierter Merkmalszuordnungen.
Neuronales Modell zur Vorhersage von Gefühlen in mehreren Aspekten und zur gleichzeitigen Erstellung einer probabilistischen mehrdimensionalen Maske. Das Modell übertrifft starke Grundlinien und generiert Masken, die starke Merkmalsvorhersager, aussagekräftig und interpretierbar sind.
Vorschlag einer neuen Zielfunktion für die Erzeugung neuronaler Sequenzen, die ML- und RL-basierte Zielfunktionen integriert.
Ein paarweise gelerntes Kapselnetzwerk, das bei begrenzten Datenmengen gute Leistungen bei der Gesichtsverifikation erbringt.
Reactor kombiniert mehrere algorithmische und architektonische Beiträge, um einen Agenten mit höherer Stichproben-Effizienz als Prioritized Dueling DQN zu erzeugen und gleichzeitig eine bessere Laufzeitleistung als A3C zu erzielen.
Die Arbeit beschreibt Methoden zur Verifizierung und Erkennung von HTN-Plänen durch Parsing von Attributgrammatiken.
Wir untersuchen die neuronale Architektursuche für Sprachaufgaben. Die Suche nach rekurrenten Zellen ist eine Herausforderung für NMT, aber die Suche nach Aufmerksamkeitsmechanismen funktioniert. Das Ergebnis der Aufmerksamkeitssuche bei der Übersetzung ist auf das Leseverstehen übertragbar.
Wir schlagen einen Regularisierer vor, der Interpolations- und Autoencoder verbessert und zeigen, dass er auch die gelernte Repräsentation für nachgelagerte Aufgaben verbessert.
In diesem Beitrag wird eine Methode zur stochastischen Erzeugung von Zwischenbildern aus gegebenen Schlüsselbildern unter Verwendung direkter 3D-Faltungen vorgestellt.
In diesem Beitrag wird der schwach überwachte Wissensgraphenabgleich mit adversarial Trainingskonzepten untersucht.
Lernen mit mehreren Ansichten verbessert das unüberwachte Lernen von Satzrepräsentationen.
Wir schlagen einen Meta-Lernansatz vor, um visuelle Segmentierungsaufgaben aus unterschiedlichen Mengen an Überwachung zu steuern.
Die latente Optimierung verbessert die Dynamik des gegnerischen Trainings. Wir präsentieren sowohl eine theoretische Analyse als auch eine hochmoderne Bilderzeugung mit ImageNet 128x128.
In diesem Beitrag werden die theoretischen Eigenschaften des optimalen Punktes erster Ordnung eines zweischichtigen neuronalen Netzes im überparametrisierten Fall behandelt.
Eine Architektur, die ein CNN ermöglicht, das auf schnell konvergierenden Videosequenzen trainiert wird. 
Wir stellen eine neue Methode vor, die transferbasierte und erzielte Blackbox-Angriffe kombiniert und die Erfolgsrate und Abfrageeffizienz von Blackbox-Angriffen in verschiedenen Netzwerkarchitekturen verbessert.
Beschreibung einer Neuro-AI-Schnittstellentechnik zur Bewertung generativer adversarischer Netzwerke.
Mit Hilfe von adaptiven Stichprobenverfahren zur Beschleunigung der Wahrscheinlichkeitsberechnung für seltene Ereignisse schätzen wir die Wahrscheinlichkeit eines Unfalls unter einer Basisverteilung, die das normale Verkehrsverhalten bestimmt. 
Ein dynamischer Bagging-Methoden-Ansatz zur Vermeidung negativer Übertragungen in neuronalen Netzen mit Few-Shot Transfer Learning.
Ein neuartiger, auf Matrixvervollständigung basierender Algorithmus zur Modellierung des Krankheitsverlaufs mit Ereignissen.
Einfache Ähnlichkeitsbeschränkungen auf der Grundlage der mehrsprachigen NMT ermöglichen zum ersten Mal eine qualitativ hochwertige Übersetzung zwischen ungesehenen Sprachpaaren.
Der neuronale Tangens-Kernel in einem zufällig initialisierten ReLU-Netz unterliegt nicht-trivialen Fluktuationen, solange die Tiefe und Breite vergleichbar sind. 
Wir schlagen neue Tensorzerlegungen und zugehörige Regularisierer vor, um den Stand der Technik bei der Vervollständigung temporaler Wissensbasen zu erreichen.
Wir haben eine Modellstruktur vom Typ CVAE verwendet, um zu lernen, wie man direkt Slates/Ganzseiten für Empfehlungssysteme erzeugt.
Durch die Kombination von neuronalen Graphen-Netzwerken und dem generativen RNN-Graphenmodell schlagen wir eine neuartige Architektur vor, die in der Lage ist, aus einer Sequenz von sich entwickelnden Graphen zu lernen und die Entwicklung der Graphen-Topologie für zukünftige Zeitschritte vorherzusagen.
Wir schlagen einen "Learning-to-Decompose"-Agenten vor, der den Beantwortern einfacher Fragen hilft, zusammengesetzte Fragen über einen Wissensgraphen zu beantworten.
Gelerntes energiebasiertes Modell mit Score-Matching.
Vorschlag für ein allgemeines tensorbasiertes RBM-Modell, das das Modell stark komprimieren kann und gleichzeitig eine starke Ausdrucksfähigkeit des Modells behält.
Ein akteurskritischer Reinforcement Learning Ansatz mit mehrstufigen Rückgaben, angewandt auf autonomes Fahren mit dem Carla-Simulator.
Wir schlagen neue Methoden zur Bewertung und Quantifizierung der Qualität von synthetischen GAN-Verteilungen aus der Perspektive von Klassifizierungsaufgaben vor.
Das Ziel des Überlebensclustering ist die Zuordnung von Personen zu Clustern. Ohne Lebensende-Signale ist dies eine schwierige Aufgabe. Um diese Aufgabe zu bewältigen, schlagen wir eine neue Verlustfunktion vor, indem wir die Kuiper-Statistik modifizieren.
Wir zeigen, wie die Verwendung semiparametrischer Prioritätsschätzungen die HPO über alle Datensätze und Metriken hinweg erheblich beschleunigen kann.
Routingverfahren sind für CapsNets nicht erforderlich.
Indem man die Breite oder die Initialisierungsvarianz jeder Schicht unterschiedlich einstellt, kann man das Problem der Gradientenexplosion in residualen Netzen (mit vollständig verbundenen Schichten und ohne Batchnorm) tatsächlich unterdrücken. Es wurde eine mathematische Theorie entwickelt, die nicht nur erklärt, wie man das macht, sondern die überraschenderweise auch vorhersagen kann, wie schnell das Netz nach Anwendung solcher Tricks trainiert, um eine bestimmte Leistung in einem Test zu erreichen. Das ist schwarze Magie, und sie heißt "Deep Mean Field Theory".
Gezielte Kommunikation bei kooperativem Reinforcement Learning mit mehreren Agenten.
Wir haben ein System zur Unterstützung der Etching Latte Art entwickelt, das den Herstellungsprozess direkt auf einen Cappuccino projiziert, um Anfängern zu helfen, eine ausgewogene Etching Latte Art herzustellen.
Wir schlagen temporale Selbstüberwachungen für das Lernen stabiler temporaler Funktionen mit GANs vor.
Halbüberwachte sprachenübergreifende Dokumentenklassifikation.
Sind HMMs ein Spezialfall von RNNs? Wir untersuchen eine Reihe von architektonischen Transformationen zwischen HMMs und RNNs, sowohl durch theoretische Ableitungen als auch durch empirische Hybridisierung und liefern neue Erkenntnisse.
Wir bieten eine informationstheoretische und experimentelle Analyse der modernsten Variations-Autoeencoder.
Wir entwickeln theoretische Grundlagen für die Ausdruckskraft von GNNs und entwerfen ein nachweislich leistungsfähigstes GNN.
Ein neuer Algorithmus für Online-Multi-Task-Lernen, der ohne Neustarts an den Aufgabengrenzen lernt.
Sprachübergreifende Fähigkeiten von mehrsprachigen BERT: Eine empirische Studie.
Ein theoretischer Rahmen für tiefe ReLU-Netzwerke, der mehrere rätselhafte Phänomene wie Überparametrisierung, implizite Regularisierung, Lotterielose usw. erklären kann. 
Angleichung von Sprachen ohne Rosetta Stone: Ohne parallele Daten konstruieren wir zweisprachige Wörterbücher mit Hilfe von adversarialem Training, bereichsübergreifender lokaler Skalierung und einem genauen Proxy-Kriterium für die Kreuzvalidierung.
Wir führen die Zählung für die Beantwortung visueller Fragen durch; unser Modell erzeugt interpretierbare Ergebnisse, indem es direkt die erkannten Objekte zählt.
Wir untersuchen das Problem der Graphengenerierung und schlagen ein leistungsfähiges tiefes generatives Modell vor, das in der Lage ist, beliebige Graphen zu erzeugen.
Repräsentieren von Sätzen, indem sie mit Tree-LSTMs nach automatisch induzierten Parse-Bäumen zusammengesetzt werden.
Drei neue Algorithmen mit Ablationsstudien zur Optimierung des neuronalen Netzes im Hinblick auf die Länge der Verdrahtung im Gegensatz zur Anzahl der verbleibenden Gewichte.
Schwach überwachte textbasierte Suche nach Videomomenten.
Wie kann man gestapelte Generalisierung nutzen, um die Leistung bestehender Transfer-Learning-Algorithmen zu verbessern, wenn nur begrenzte beschriftete Daten zur Verfügung stehen?
In diesem Beitrag wird ein Physik-Prior für Deep Learning eingeführt und die daraus resultierende Netzwerktopologie für die modellbasierte Steuerung eingesetzt.
Wir verbessern generative Modelle, indem wir einen Meta-Algorithmus vorschlagen, der neue Trainingsdaten aus den Ausgaben des Modells filtert.
Wir verwenden einen abgerollten Simulator als differenzierbares Ende-zu-Ende Modell der Proteinstruktur und zeigen, dass er (manchmal) hierarchisch auf unbekannte Faltentopologien verallgemeinert werden kann.
Automatisiertes Training von Mäusen für die Neurowissenschaft mit iterativer Online-Strategie für die Verhaltensvorhersage
Wir analysieren rekurrente Netzwerke, die für die Klassifizierung von Gefühlen trainiert wurden, und stellen fest, dass sie alle eine annähernde Linienattraktor-Dynamik zeigen, wenn sie diese Aufgabe lösen.
Wir haben eine physikalische Simulation eines Nagetiers erstellt, es trainiert, eine Reihe von Aufgaben zu lösen, und die resultierenden Netzwerke analysiert.
Eine Erweiterung von GANs, die optimalen Transport in primärer Form mit einer Energiedistanz kombiniert, die in einem gegnerisch gelernten Merkmalsraum definiert ist.
Training eines Agenten in einer virtuellen 2D-Welt für den Erwerb und die Verallgemeinerung von Grundsprache.
Ein RL-Algorithmus, der lernt, robust gegenüber Veränderungen in der Dynamik zu sein.
Diese Arbeit bietet ein spielbasiertes Abstraktionsschema zur Berechnung von beweisbar soliden Strategien für POMDPs.
Ein Spielzeugdatensatz, der auf kritischer Perkolation in einem planaren Graphen basiert, bietet ein analytisches Fenster zur Trainingsdynamik von tiefen neuronalen Netzen  
Wir fassen das Generierungsproblem als ein Problem der Bearbeitung bestehender Punkte auf und können daher besser extrapolieren als herkömmliche GANs.
Ein neuer Ansatz, der eine Repräsentation zur Beschreibung von Übergangsmodellen in komplexen Unsicherheitsdomänen mit Hilfe relationaler Regeln erlernt. 
Wir schlagen ein differenzierbares Ende-zu-Ende Planungsnetz für Graphen vor. Dies kann auf viele Probleme der Bewegungsplanung angewendet werden
Wir erlernen eine qualitativ hochwertige Störungsunterdrückung, indem wir nur einzelne Instanzen von beschädigten Bildern als Trainingsdaten verwenden.
Wir lösen das spärliche Belohnungsproblem bei Web-UI-Aufgaben durch demonstrationsgeleitete Exploration.
Eingebettete Architektur für Deep Learning auf optimierten Geräten zur Gesichts- und Emotionserkennung.
Wir zeigen, dass ein Tensor-Dekompositions-Algorithmus spärliche kovariaten-spezifische Einbettungen und natürlich trennbare Themen gemeinsam und dateneffizient erlernt, wenn dieselbe Einbettung für alle Kovariaten verwendet wird.
Mit Hilfe der linearen Programmierung zeigen wir, dass die Rechenkomplexität des Trainings von Deep Neural Networks für verschiedene Architekturen polynomiell von der Datengröße abhängt.
Wir vereinheitlichen den erweiterten Kalman-Filter (EKF) und den Zustandsraum-Ansatz zur Leistungserwartungsausbreitung (PEP), indem wir die schwer lösbaren Moment-Matching-Integrale in PEP durch Linearisierung lösen. Dies führt zu einer global iterierten Erweiterung des EKF.
Erforschung der Lernfähigkeit von erlernten neuronalen Netzen.
Wir schlagen eine verallgemeinerte Bewertungsmethodik vor, um Modellverzerrungen, Datensatzverzerrungen und ihre Korrelation zu interpretieren.
Soziale Agenten lernen, in natürlicher Sprache miteinander zu sprechen, um ein Ziel zu erreichen.
Wir zeigen, dass der Posterior-Kollaps bei linearen VAEs ausschließlich durch die marginale Log-Likelihood (nicht ELBO) verursacht wird. Experimente mit tiefen VAEs legen nahe, dass ein ähnliches Phänomen im Spiel ist.
In diesem Werk wird ein neues Modell vorgeschlagen, das Informationen auf mehreren Ebenen für das Lernen von Sequenzen kombiniert.
Wir schlagen ein neues zertifiziertes adversariales Trainingsverfahren, CROWN-IBP, vor, das den neuesten Stand der Technik in Bezug auf L_inf norm adversarial perturbations erreicht.
Beim strukturierten Pruning von Netzwerken führt das Fine-Tuning eines pruned Modells nur zu einer vergleichbaren Leistung wie das Training des Modells von Grund auf.
Interaktive Technik zur Verbesserung des Brushing in dichten Trajektoriendatensätzen durch Berücksichtigung der Form des Brush.
Wir entwickeln einen neuartigen Ansatz zur Modellierung der Objektkompositionalität in Bildern im Rahmen eines GAN.
Adversarial-Audio-Diskriminierung mit Verwendung zeitlicher Abhängigkeit.
Wir schlagen ein neuartiges GAN-Trainingsverfahren vor, bei dem bestimmte gefälschte Proben als real angesehen werden, um den Kollaps des Modells zu verringern und den Trainingsprozess zu stabilisieren.
Wir präsentieren ein visuelles Werkzeug zur interaktiven Erkundung des latenten Raums eines Auto-Encoders für Peptidsequenzen und deren Attribute.
Wir berichten über Experimente, die eindeutig belegen, dass sich ein Neuron während des Trainings und der Tests wie ein binärer Klassifikator verhält.
Ein Verfahren zur Anreicherung und Kombination von Merkmalen zur Verbesserung der Klassifizierungsgenauigkeit.
Erweiterung der GAN-Architektur, um die Kontrolle über Standorte und Identitäten mehrerer Objekte in den erzeugten Bildern zu erhalten.
Wir schlagen ein neuartiges End-to-End-Modell (SPNet) vor, um semantische Gerüste zur Verbesserung der abstrakten Dialogzusammenfassung einzubeziehen.
Wir stellen einen RL-Agenten MINERVA vor, der lernt, einen Wissensgraphen abzuschreiten und Anfragen zu beantworten.
Eine Annäherung des ventralen Stroms von Primaten als Convolutional Network schneidet bei der Objekterkennung schlecht ab, und mehrere architektonische Merkmale tragen dazu bei. 
Wir betrachten das Problem des Lernens optimaler Strategien in zeitlich begrenzten und zeitlich nicht begrenzten Bereichen unter Verwendung zeitlich begrenzter Interaktionen.
Eines der theoretischen Probleme beim Deep Learning.
Wir analysieren und entwickeln eine rechnerisch effiziente Implementierung der Jacobischen Regularisierung, die die Klassifizierungsmargen neuronaler Netze erhöht.
Wir sind die ersten auf diesem Gebiet, die zeigen, wie man einen effektiven Sparse-Kernel unter drei Aspekten entwickelt: Zusammensetzung, Leistung und Effizienz.
Ein neuartiges marginalisiertes durchschnittliches Aufmerksamkeitsnetz für schwach überwachte zeitliche Handlungslokalisierung.
Wir schlagen einen neuen Autocodierer vor, der mit einer Multiway-Delay-Embedding-Transformation ausgestattet ist, um ein tiefes Bild zu interpretieren.
Sicherstellung, dass die im Verbund erlernten Modelle die Beteiligung eines Kunden nicht offenbaren.
Wir zeigen, wie das Vortraining eines untrainierten neuronalen Netzes mit nur 5-25 Beispielen die Rekonstruktionsergebnisse bei komprimiertem Abfühlen und semantischen Wiederherstellungsproblemen wie der Kolorierung verbessern kann.
Wir haben Cooperative Training vorgeschlagen, einen neuartigen Trainingsalgorithmus für die generative Modellierung diskreter Daten.
Wir stellen eine Methode zur Berechnung einer intrinsischen Belohnung für Neugier vor, bei der Metriken verwendet werden, die aus dem Sampling eines latenten Variablenmodells abgeleitet werden, das zur Schätzung der Dynamik verwendet wird.
Wir stellen ein generatives Modell für kompositionelle Worteinbettungen vor, das syntaktische Beziehungen erfasst, und bieten eine empirische Überprüfung und Bewertung.
Wir schlagen einen hybriden modellbasierten und modellfreien Ansatz vor, der semantische Informationen nutzt, um die DRL-Verallgemeinerung in künstlichen Umgebungen zu verbessern.
Wir verwenden Deep Learning-Techniken, um das Problem der spärlichen Signaldarstellung und -wiederherstellung zu lösen.
Wir stellen Dreamer vor, einen Agenten, der Verhaltensweisen mit langem Zeithorizont rein durch latente Vorstellungskraft unter Verwendung analytischer Wertgradienten erlernt.
Wir schlagen MULTIPOLAR vor, eine Transfer-RL-Methode, die eine Reihe von Quellrichtlinien, die unter unbekannten, unterschiedlichen Umweltdynamiken gesammelt wurden, nutzt, um effizient eine Zielrichtlinie in einer anderen Dynamik zu lernen.
Ein Agent, der nur mit Neugier und ohne extrinsische Belohnung trainiert wird, schneidet in 54 beliebten Umgebungen erstaunlich gut ab, darunter die Atari-Spiele, Mario usw.
Für räumliche Transformationen robuster Minimierer minimiert auch die Standardgenauigkeit; invarianzinduzierende Regularisierung führt zu besserer Robustheit als spezialisierte Architekturen.
Der Begriff des Ordnungslernens wird vorgeschlagen und auf Regressionsprobleme in der Computer Vision angewendet.
Wir zeigen, dass neuronale Netze funktionieren, indem sie die Topologie eines Datensatzes verändern, und untersuchen, wie die Wahl der Architektur diese Veränderung bewirkt.
Wir verwenden die empirischen Instrumente der Moduskonnektivität und der SVCCA, um die Trainingsheuristiken für neuronale Netze zu untersuchen, d.h. die Wiederaufnahme der Lernrate, das Aufwärmen und die Wissensdestillation.
Variationsinferenz zur Ableitung einer diskreten Verteilung, aus der ein neuronales Netz mit geringer Genauigkeit abgeleitet wird.
Wir schlagen eine neue tensorbasierte Methode für Graph Convolutional Networks auf dynamischen Graphen vor.
Generierung neuer chemischer Materialien mit Hilfe neuartiger domänenübergreifender GANs.
Wir stellen einen Schätzer und einen Schätzalgorithmus für eine Klasse von Multi-Task-Regressionsproblemen zur Verfügung und bieten statistische und rechnerische Analysen.
Nutzung von Deep Reinforcement Learning, um Agenten die Koordination von Mitfahrzentralen beizubringen.
Stabilität von Streutransformationsdarstellungen von Graphdaten gegenüber Verformungen des zugrunde liegenden Graphträgers.
Wir schlagen eine neuartige Deep-Network-Architektur vor, die dynamisch über ihre Netzwerkkapazität entscheiden kann, während sie in einem Szenario des lifelong Learnings trainiert.
In diesem Beitrag werden verschiedene Methoden der Verknüpfung von VO mit Deep Learning erörtert und eine gleichzeitige Vorhersage von Korrekturen und Unsicherheiten vorgeschlagen.
Wir haben das Deep Density Network eingeführt, ein einheitliches DNN-Modell zur Abschätzung der Unsicherheit bei der Erkundung/Ausnutzung in Empfehlungssystemen.
Wir trainieren RNNs mit berühmten Twitter-Nutzern, um festzustellen, ob die allgemeine Twitter-Bevölkerung nach einer Naturkatastrophe eher an den Klimawandel glaubt.
Unter Verwendung einer neuartigen Darstellung symmetrischer linearer dynamischer Systeme mit einem latenten Zustand formulieren wir die optimale Steuerung als konvexes Programm und liefern den ersten Polynomialzeit-Algorithmus, der die optimale Steuerung mit einer nur im Zeithorizont polylogarithmischen Stichprobenkomplexität löst.
Wir modellieren den Datengenerator (im GAN) durch ein Polynom hoher Ordnung, das durch Tensoren hoher Ordnung dargestellt wird.
Wir zeigen, dass tiefe neuronale Netze in der Lage sind, aus Daten zu lernen, die durch eine beliebige Menge an Störungen verwässert wurden.
Wir schlagen einen Meta-Lernansatz für neuronale maschinelle Übersetzung mit geringen Ressourcen vor, der schnell lernen kann, eine neue Sprache zu übersetzen.
Eine Methode zur aktiven Erkennung von Anomalien. Wir stellen eine neue Schicht vor, die an jedes Deep-Learning-Modell, das für die unbeaufsichtigte Erkennung von Anomalien entwickelt wurde, angehängt werden kann, um es in eine aktive Methode zu verwandeln.
Wir generieren Beispiele, um eine Klassifikatorentscheidung durch Interpolationen im latenten Raum zu erklären. Die Variationskosten des Autoencoders werden um ein Funktional des Klassifikators über den generierten Beispielpfad im Datenraum erweitert.
Einführung eines Ansatzes, der es Agenten ermöglicht, PPDDL-Aktionsmodelle inkrementell über mehrere Planungsprobleme im Rahmen des Reinforcement Learnings zu lernen.
Wir schlagen einen neuen DRL-Off-Policy-Algorithmus vor, der die beste Leistung erzielt. 
Wir schlagen eine Methode vor, die die Informationen aus mehreren Passagen für die Qualitätssicherung in einem offenen Bereich nutzen kann.
Algorithmus zur Dimensionalitätsreduzierung zur Visualisierung von Text mit Netzwerkinformationen, z. B. ein E-Mail-Korpus oder Ko-Autorenschaften.
Wir stellen einen Rahmen vor, der hochrealistische Computersimulationen nutzt, um Verzerrungen in ML-Klassifikatoren zu untersuchen und zu diagnostizieren. 
Wir präsentieren und bewerten Sampling-basierte Point Cloud Decoder, die den MLP-Ansatz durch eine bessere Anpassung der Semantik von Point Clouds übertreffen.
Wir verwenden Deep RL, um eine Strategie zu erlernen, die die Suche eines genetischen Algorithmus lenkt, um die Ausführungskosten von Berechnungsgraphen besser zu optimieren, und zeigen verbesserte Ergebnisse auf realen TensorFlow-Graphen.
Wir zeigen, dass mit dem richtigen Verlust und der richtigen Architektur die 3D-Objekterkennung durch vorausschauendes Lernen verbessert wird.
Ein generatives adversariales Netzwerk für die Stilmodellierung in einem Text-to-Speech-System.
Wir zeigen in einer vereinfachten Lernaufgabe, dass Überparametrisierung die Generalisierung eines Convnets verbessert, das mit Gradientenabstieg trainiert wird.
Bayessches Meta-Lernen unter Verwendung des PAC-Bayes-Rahmens und impliziter Prioritätsverteilungen
Positionspapier mit Vorschlägen für rebellische und trügerische Erklärungen für Agenten.
Wir untersuchen eine Variante von Variations-Autoencodern, bei der eine Überstruktur von diskreten latenten Variablen über den latenten Merkmalen liegt.
Wir zeigen, dass der Schlüssel zur Erzielung einer guten Leistung mit IDMs im Erlernen latenter Repräsentationen liegt, um die zwischen gleichwertigen Erfahrungen geteilten Informationen zu kodieren, so dass sie auf ungesehene Szenarien verallgemeinert werden können.
Wir identifizieren die Winkelverzerrung, die das Problem des verschwindenden Gradienten in tiefen Netzen verursacht, und schlagen eine effiziente Methode zur Reduzierung der Verzerrung vor.
Wir verwenden graphische neuronale Netze im Rahmen von Variational EM für effiziente Inferenz und Lernen von Markov Logic Networks.
Ein Meta-Reinforcement-Learning-Ansatz, in den ein neuronales Netz eingebettet ist, wird auf das autonome Fahren mit dem Carla-Simulator angewendet.
Ableitung eines Rahmens für Informationsengpässe beim Reinforcement Learning und einiger einfacher relevanter Theorien und Werkzeuge.
Wir schlagen einen Ansatz für neuronale Module zum kontinuierlichen Lernen vor, der eine einheitliche visuelle Umgebung mit einem großen Aktionsraum verwendet.
Wir schlagen eine Methode zur Verwendung von GANs vor, um qualitativ hochwertige visuelle Begründungen zur Erklärung von Modellvorhersagen zu erzeugen. 
Wir schlagen einen neuen Algorithmus vor, der die Ausdruckskraft von Generativen Neuronalen Netzen nutzt, um die Algorithmen der Evolutionsstrategien zu verbessern.
Wir komprimieren und beschleunigen Spracherkennungsmodelle auf eingebetteten Geräten durch eine Spurnormregularisierungstechnik und optimierte Kernel.
Wir liefern eine theoretische Begründung für das Konzept des Straight-Through-Schätzers.
Mit einer Reihe von Modifikationen unter 10 LOC an A2C erhält man einen Off-Policy-Actor-Critic, der A2C übertrifft und ähnlich gut abschneidet wie ACER. Die Modifikationen sind große Batchsizes, aggressives Clamping und Policy-Forcing mit Gumbel Noise.
Generierung von Text unter Verwendung von Satzeinbettungen aus Skip-Thought Vektoren mit Hilfe von Generative Adversarial Networks.
Neuer Out-of-Order Decoder für neuronale maschinelle Übersetzungen.
In diesem Papier untersuchen wir ein neues Graphenlernproblem: Das Zählen von Untergraphen-Isomorphismen.
Wir stellen einen Agenten vor, der eine Beta-Vae verwendet, um visuelle Merkmale zu extrahieren und einen Aufmerksamkeitsmechanismus, um irrelevante Merkmale aus visuellen Beobachtungen zu ignorieren, um einen robusten Transfer zwischen visuellen Domänen zu ermöglichen.
Wir schlagen den ersten Algorithmus zur Überprüfung der Robustheit von Transformern vor.
Im Gegensatz zu früheren Annahmen ist die Trainingsleistung von tiefen Netzwerken, wenn sie angemessen gemessen wird, prädiktiv für die Testleistung, was der klassischen Theorie des maschinellen Lernens entspricht.
Wir schlagen einen Rahmen vor, der die Planung für effizientes Erkunden und Lernen in komplexen Umgebungen beinhaltet.
Unser Beitrag analysiert die enorme Repräsentationskraft von Netzwerken, insbesondere mit "Skip Connections", die als Methode für eine bessere Generalisierung genutzt werden können.
In dieser Arbeit wird eine neue Zielfunktion vorgeschlagen, die den Begriff KL durch eine Funktion ersetzt, die das Ziel der maximalen mittleren Diskrepanz (MMD) nachbildet. 
Wir stellen fest, dass die Wahrscheinlichkeiten generativer Modelle übermäßig von der Komplexität der Eingaben beeinflusst werden, und schlagen einen Weg vor, dies zu kompensieren, wenn wir Eingaben außerhalb der Verteilung erkennen.
Wir untersuchen die Konvergenz populärer Optimierungsalgorithmen wie Adam, RMSProp und schlagen neue Varianten dieser Methoden vor, die in konvexen Umgebungen nachweislich zur optimalen Lösung konvergieren. 
Wir präsentieren wirksame Abwehrmaßnahmen gegen Clean-Label-Poisoning-Angriffe. 
MXGNet ist eine mehrschichtige, auf Multiplex-Graphen basierende Architektur, die gute Leistungen bei verschiedenen Diagramm-Aufgaben erzielt.
Wir schlagen ein neuartiges Multitasking-System vor, das die Erkennung von Tabellen, semantischen Komponenten und die Klassifizierung von Zelltypen für Tabellenkalkulationen mit vielversprechenden Ergebnissen erlernt.
Wir schlagen automatische Metriken zur ganzheitlichen Bewertung der Erzeugung offener Dialoge vor, die stark mit der menschlichen Bewertung korrelieren.
Wir entwickeln eine neuartige tiefenselektierbare Graphen Convolution (Depthwise Separable Graph Convolution, DSGC) für generische räumliche Daten, die in hohem Maße mit der tiefenselektierbaren Convolution kompatibel ist.
Wir trainieren eine Reihe von Modellen, die in der Lage sind, Audiowellenformen mit kohärenter musikalischer Struktur zu transkribieren, zu komponieren und zu synthetisieren, was durch den neuen MAESTRO-Datensatz ermöglicht wird.
Eine empirische Untersuchung der Variationsinferenz auf der Grundlage der Minimierung der Chi-Quadrat-Divergenz, die zeigt, dass die Minimierung des CUBO schwieriger ist als die Maximierung des ELBO.
Wir nutzen die globale Linearität der durch die Verwechslung trainierten Modelle bei der Inferenz, um die Lokalität der gegnerischen Störungen zu brechen.
Die Feinabstimmung von BERT auf juristische Korpora bietet marginale, aber wertvolle Verbesserungen bei NLP-Aufgaben im juristischen Bereich.
Wir formulieren ein probabilistisches latentes Sequenzmodell, um den unüberwachten Textstiltransfer zu bewältigen, und zeigen seine Wirksamkeit in einer Reihe von unüberwachten Textstiltransferaufgaben. 
Schlägt ein analytisch nachvollziehbares Modell und Inferenzverfahren vor (misparametrisierte spärliche Regression, abgeleitet mit L_1 Strafe und untersucht in der Daten-Interpolationsgrenze), um Deep-Net-verwandte Phänomene im Kontext von inversen Problemen zu untersuchen. 
Wir schlagen einen neuen variationalen Hashing-basierten kollaborativen Filteransatz vor, der für eine neuartige Selbstmasken-Variante der Hamming-Distanz optimiert ist und den Stand der Technik bei NDCG um bis zu 12 % übertrifft.
Ein Optimierungsalgorithmus, der verschiedene Chargengrößen auf der Grundlage der Wahrscheinlichkeit untersucht und automatisch die erfolgreichste Chargengröße nutzt, die den Validierungsverlust minimiert.
Wir haben einen einheitlichen Rahmen für Generative Adversarial Networks (GAN) vorgeschlagen, um eine geräuschbewusste Wissensgrapheneinbettung zu erlernen.
Eine Rest-EBM für Text, deren Formulierung der Unterscheidung zwischen menschlichem und maschinell generiertem Text entspricht. Wir untersuchen sein Generalisierungsverhalten.
Pseudo-Labeling hat sich als schwache Alternative für semi-supervised learning erwiesen. Umgekehrt zeigen wir, dass Pseudo-Labeling ein geeigneter Ansatz ist, um Bestätigungsfehler mit verschiedenen Regularisierungen zu beseitigen.
Wir zeigen, dass eine spezielle, mit modellfreien Methoden trainierte Zielzustandswertfunktion im Rahmen der modellbasierten Steuerung verwendet werden kann, was zu einer wesentlich besseren Stichprobeneffizienz und Leistung führt.
Eine neuartige neuronale Architektur zur effizienten Amortisierung von Schlussfolgerungen über latente Permutationen.
Wir schlagen eine neuartige Zwei-Turm-Shared-Bottom-Modellarchitektur vor, um Wissen aus reichhaltigen impliziten Rückmeldungen zu übertragen, um Relevanz für groß angelegte Retrievalsysteme vorherzusagen.
Wir befassen uns mit der Aufgabe der autonomen Exploration und Navigation unter Verwendung von räumlichen Affordanzzuordnungen, die auf selbstüberwachte Weise erlernt werden können. Diese übertreffen die klassischen geometrischen Grundlinien und sind gleichzeitig effizienter als zeitgenössische RL-Algorithmen.
