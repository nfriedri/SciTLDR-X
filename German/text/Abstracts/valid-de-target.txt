Wir entwickeln eine adaptive Verlustskalierung, um das Training mit gemischter Präzision zu verbessern, das die Ergebnisse des Stands der Technik übertrifft.
Vorschlag für eine adaptive Verlustskalierungsmethode während der Backpropagation für das Training mit gemischter Präzision, bei der die Skalierungsrate automatisch festgelegt wird, um die Rückflussrate zu reduzieren.
Die Autoren schlagen eine Methode zum Trainieren von Modellen mit FP16-Präzision vor, die eine ausgefeiltere Methode zur gleichzeitigen und automatischen Minimierung der Rückflussrate in jeder Schicht verwendet.
Wir präsentieren einen neuartigen Ansatz für das Erlernen der Vorhersage von Mengen mit unbekannter Permutation und Kardinalität unter Verwendung von Feed-Forward Deep Neural Networks.
Eine Formulierung zum Erlernen der Verteilung über unbeobachtbare Permutationsvariablen auf der Grundlage von tiefen Netzwerken für das Problem der Mengenvorhersage.
Wir vergleichen die Leistung der Objekterkennung bei Bildern, die einheitlich und mit drei verschiedenen Foveationsschemen abgetastet wurden.
Wir entwickeln Methoden, um tiefe neuronale Modelle zu trainieren, die sowohl robust gegenüber negativen Störungen sind als auch deren Robustheit wesentlich einfacher zu verifizieren ist.
Diese Arbeit stellt verschiedene Möglichkeiten vor, einfache ReLU-Netze zu regularisieren, um die Robustheit gegenüber Angreifern, die nachweisbare Robustheit gegenüber Angreifern und die Verifikationsgeschwindigkeit zu optimieren.
In diesem Beitrag werden Methoden zur Ausbildung robuster neuronaler Netze vorgeschlagen, die schneller überprüft werden können. Dabei werden Pruning Methoden zur Förderung der Gewichtsdistanz und Regularisierungen zur Förderung der ReLU-Stabilität eingesetzt.
Untersuchung, wie BatchNorm eine Sicherheitslücke verursacht und wie man sie vermeiden kann. 
Dieses Werk befasst sich mit der Anfälligkeit von BatchNorm für unerwünschte Störungen und schlägt eine Alternative namens RobustNorm vor, die anstelle der Normalisierung eine Min-Max-Reskalierung verwendet.
Dieses Papier untersucht den Grund für die Anfälligkeit von BatchNorm und schlägt Robust Normalization vor, eine Normalisierungsmethode, die unter einer Vielzahl von Angriffsmethoden deutlich bessere Ergebnisse erzielt.
Unser variational-rekurrentes Imputationsnetzwerk (V-RIN) berücksichtigt die korrelierten Merkmale, die zeitliche Dynamik und nutzt die Unsicherheit, um das Risiko einer verzerrten Schätzung fehlender Werte zu verringern.
Ein Netzwerk zur Imputation fehlender Daten, das Korrelationen, zeitliche Beziehungen und Datenunsicherheiten für das Problem der spärlichen Daten in EHRs berücksichtigt und eine höhere AUC bei der Klassifizierung von Mortalitätsraten erzielt.
In dem Papier wird eine Methode vorgestellt, die VAE und GRU mit Unsicherheitsfaktoren für die sequentielle Imputation fehlender Daten und die Ergebnisvorhersage kombiniert.
Eine adaptive Methode für die Festkomma-Quantisierung neuronaler Netze, die eher auf theoretischer Analyse als auf Heuristiken beruht. 
Schlägt eine Methode zur Quantisierung neuronaler Netze vor, die es ermöglicht, Gewichte je nach ihrer Wichtigkeit mit unterschiedlicher Genauigkeit zu quantisieren, wobei der Verlust berücksichtigt wird.
In dieser Arbeit wird eine Technik zur Quantisierung der Gewichte eines neuronalen Netzes vorgeschlagen, bei der die Bittiefe/Präzision für jeden Parameter unterschiedlich ist.
Auf der Grundlage der Fuzzy-Mengen-Theorie schlagen wir ein Modell vor, das nur die Größen der symmetrischen Unterschiede zwischen Paaren von Multisets angibt und Repräsentationen solcher Multisets und ihrer Elemente lernt.
Dieses Papier schlägt eine neue Aufgabe des Mengenlernens vor, die Vorhersage der Größe der symmetrischen Differenz zwischen mehreren Mengen, und gibt eine Methode zur Lösung der Aufgabe auf der Grundlage der Fuzzy-Mengentheorie.
In diesem Werk wird eine Deep-Learning-gestützte Methode vorgeschlagen, um glaubwürdige Proben von eigennützigen Agenten zu erhalten. 
Die Autoren schlagen einen Rahmen für die Auswahl von Stichproben für das Problem der Auswahl von glaubwürdigen Stichproben von Agenten für komplexe Verteilungen vor, schlagen vor, dass tiefe neuronale Rahmen in diesem Rahmen angewendet werden können, und verbinden Stichprobenauswahl und f-GAN.
In diesem Papier wird das Problem der Stichprobenauswahl untersucht und ein Ansatz des Deep Learning vorgeschlagen, der sich auf den dualen Ausdruck der f-Divergenz stützt, die sich als Maximum über eine Menge von Funktionen t schreibt.
Graph to Sequence Learning mit aufmerksamkeitsbasierten neuronalen Netzen
Eine graph2seq-Architektur, die einen Graphen-Encoder, der GGNN- und GCN-Komponenten mit einem Aufmerksamkeits-Sequenz-Encoder kombiniert, und die Verbesserungen gegenüber den Basislinien zeigt.
In dieser Arbeit wird ein Ende-zu-Ende Graph Encoder zu Sequenz Decoder Modell mit einem dazwischen liegenden Aufmerksamkeitsmechanismus vorgeschlagen.
Ein Zero-Shot-Segmentierungsrahmen für die Segmentierung von 3D-Objektteilen. Modellierung der Segmentierung als Entscheidungsprozess und Lösung als kontextuelles Bandit-Problem.
Eine Methode zur Segmentierung von 3D-Punktansammlungen von Objekten in Einzelteilen, die sich auf die Verallgemeinerung von Teilgruppierungen auf neue Objektkategorien konzentriert, die während des Trainings nicht gesehen wurden, und die eine starke Leistung im Vergleich zu den Basislinien zeigt.
In dieser Arbeit wird eine Methode zur Segmentierung von Teilen in Objektpunktansammlungen vorgeschlagen.
Eine neue Perspektive für die Erfassung der Korrelation zwischen Knoten auf der Grundlage von Diffusionseigenschaften.
Eine neue Diffusionsoperation für neuronale Graphennetze, die keine Eigenwertberechnung erfordert und sich im Vergleich zu herkömmlichen neuronalen Graphennetzen exponentiell schneller ausbreiten kann.
In dieser Arbeit wird vorgeschlagen, das Problem der Diffusionsgeschwindigkeit durch die Einführung eines ballistischen Laufs zu lösen.
Wir schlagen eine Trainings-Pipeline mit schwacher Überwachung vor, die auf dem Datenprogrammierungs-Framework für Ranking-Aufgaben basiert, in dem wir ein BERT-basiertes Ranking-Modell trainieren und neue SOTA-Ergebnisse erzielen.
Die Autoren schlagen eine Kombination aus BERT und dem Framework für schwache Überwachung vor, um das Problem des Passagen-Rankings anzugehen, und erzielen damit bessere Ergebnisse als der vollständig überwachte Stand der Technik.
Bei realen Problemen haben wir festgestellt, dass DNNs während des Trainingsprozesses häufig Zielfunktionen von niedrigen zu hohen Frequenzen anpassen.
In dieser Arbeit wird der Verlust von neuronalen Netzen im Fourier-Bereich analysiert und festgestellt, dass DNNs dazu neigen, niederfrequente Komponenten vor hochfrequenten zu lernen.
Das Werk untersucht den Trainingsprozess von NNs durch Fourier-Analyse und kommt zu dem Schluss, dass NNs niederfrequente Komponenten vor hochfrequenten Komponenten lernen.
Wir schlagen einen hierarchisch gekoppelten Encoder-Decoder mit mehreren Auflösungen für die Übersetzung von Graphen in Graphen vor.
Ein hierarchisches Graph-zu-Graph-Übersetzungsmodell zur Erzeugung molekularer Graphen unter Verwendung chemischer Substrukturen als Bausteine, das vollständig autoregressiv ist und kohärente Mehrfachauflösungsrepräsentationen erlernt, wodurch es frühere Modelle übertrifft.
Die Autoren stellen eine hierarchische Graph-zu-Graph-Übersetzungsmethode zur Erzeugung neuartiger organischer Moleküle vor.
Wir nutzen die Aufmerksamkeit, um äquivariante neuronale Netze auf die Menge oder die gemeinsam auftretenden Transformationen in den Daten zu beschränken. 
In dieser Arbeit wird Aufmerksamkeit mit Gruppenäquivarianz kombiniert, wobei insbesondere die p4m-Gruppe der Rotationen, Translationen und Spiegelungen betrachtet wird, und es wird eine Form der Selbstaufmerksamkeit abgeleitet, die die Äquivarianzeigenschaft nicht zerstört.
Die Autoren schlagen einen Selbstbeobachtungsmechanismus für rotationsäquivariante neuronale Netze vor, der die Klassifizierungsleistung gegenüber regulären rotationsäquivarianten Netzen verbessert.
Wir trainieren ein GAN zur Generierung und Wiederherstellung von Protein-Rückgraten mit vollständigen Atomen und zeigen, dass wir in ausgewählten Fällen die generierten Proteine nach Sequenzdesign und in von Anfang an Vorwärtsfaltung wiederherstellen können.
Ein generatives Modell für das Proteinrückgrat, das ein GAN, ein Autoencoder-ähnliches Netzwerk und einen Verfeinerungsprozess verwendet, sowie eine Reihe von qualitativen Bewertungen, die auf positive Ergebnisse hindeuten.
In diesem Beitrag wird ein Ende-zu-Ende Ansatz für die Generierung von Protein-Rückgraten unter Verwendung generativer adversarialer Netzwerke vorgestellt.
Beim Meta Learning für Few Shot Learning wird davon ausgegangen, dass Trainings- und Testaufgaben aus der gleichen Verteilung gezogen werden. Was macht man, wenn das nicht der Fall ist? Meta-Learning mit Bereichsanpassung auf Aufgabenebene.
Diese Arbeit schlägt ein Modell vor, das unüberwachte adversarische Domänenanpassung mit prototypischen Netzwerken kombiniert, die besser abschneiden als die Basissysteme für Few-Shot Learning Aufgaben mit Domänenverschiebung.
Die Autoren schlugen eine Meta-Domänenanpassung vor, um das Szenario der Domänenverschiebung in einer Meta-Lernumgebung anzugehen, und konnten in mehreren Experimenten Leistungsverbesserungen nachweisen.
Divide, Conquer, and Combine ist ein neues Inferenzschema, das auf probabilistischen Programmen mit stochastischer Unterstützung angewendet werden kann, d.h. die Existenz der Variablen selbst ist stochastisch.
Ein gemeinschaftserhaltender Knoteneinbettungsalgorithmus, der zu einer effektiveren Erkennung von Gemeinschaften mit einer Clusterbildung im eingebetteten Raum führt.
Ein autoregressives Deep-Learning-Modell zur Erzeugung verschiedener Punkt Clouds.
Ein Ansatz zur Erzeugung von 3D-Formen als Punkt Clouds, der die lexikografische Ordnung von Punkten nach Koordinaten berücksichtigt und ein Modell zur Vorhersage von Punkten in dieser Reihenfolge trainiert.
In diesem Beitrag wird ein generatives Modell für Punkt Clouds vorgestellt, das ein Pixel-RNN-ähnliches autoregressives Modell und ein Aufmerksamkeitsmodell zur Behandlung von Interaktionen über größere Entfernungen verwendet.
Beschreibt eine Reihe von Erklärungsmethoden, die auf einen einfachen neuronalen Netzcontroller für die Navigation angewendet werden.
Dieser Beitrag bietet Einblicke und Erklärungen für das Problem, Erklärungen für ein mehrschichtiges Perzeptron zu liefern, das als inverser Controller für die Rover-Bewegung verwendet wird, sowie Ideen, wie man ein Black-Box-Modell erklären kann.
Wir schlagen einen selbstüberwachenden Agenten für die Seh- und Sprachnavigationsaufgabe vor.
Eine Methode für die Navigation in Bild und Sprache, die den Fortschritt im Unterricht mit Hilfe eines Fortschrittsmonitors und eines visuell-textuellen Co-Grounding-Moduls verfolgt und bei Standard-Benchmarks gut abschneidet.
In diesem Beitrag wird ein Modell für die Navigation durch Sehen und Sprache mit panoramischer visueller Aufmerksamkeit und einem zusätzlichen Verlust bei der Fortschrittskontrolle beschrieben, das Ergebnisse auf dem neuesten Stand der Technik liefert.
Ereigniserkennung zur Darstellung der Historie für den Agenten in RL
Die Autoren untersuchen das RL-Problem unter teilweise beobachteten Bedingungen und schlagen eine Lösung vor, die ein FFNN verwendet, aber eine Geschichtsdarstellung bietet und PPO übertrifft.
In diesem Beitrag wird eine neue Methode zur Darstellung der Vergangenheit als Eingabe für einen RL-Agenten vorgeschlagen, die sich als besser erweist als PPO und eine RNN-Variante von PPO.
Wir zeigen, dass autoregressive Modelle sehr realitätsnahe Bilder erzeugen können. 
Eine Architektur, die Decoder einsetzt, Decoder zur Größenanpassung und Decoder zur Tiefenanpassung, um das Problem des Erlernens von weitreichenden Abhängigkeiten in Bildern zu lösen, um Bilder mit hoher Wiedergabetreue zu erhalten.
Dieser Beitrag befasst sich mit dem Problem der Generierung von Bildern mit hoher Genauigkeit und zeigt erfolgreich überzeugende Imagenet-Beispiele mit einer Auflösung von 128x128 für ein Likelihood-Density-Modell.
Ein tiefes hierarchisches Zustandsraummodell, in dem die Zustandsübergänge korrelierter Objekte durch graphische neuronale Netze koordiniert werden.
Ein hierarchisches latentes Variablenmodell für sequenzielle dynamische Prozesse mehrerer Objekte, wenn jedes Objekt eine signifikante Stochastizität aufweist.
Diese Arbeit stellt ein relationales Zustandsraummodell vor, das die gemeinsamen Zustandsübergänge korrelierter Objekte simuliert, die in einer Graphenstruktur hierarchisch koordiniert sind.
Wir führen eine neue induktive Verzerrung ein, die Baumstrukturen in rekurrente neuronale Netze integriert.
In diesem Papier wird ON-LSTM vorgeschlagen, eine neue RNN-Einheit, die die latente Baumstruktur in rekurrente Modelle integriert und gute Ergebnisse bei der Sprachmodellierung, dem unbeaufsichtigten Parsing, der gezielten syntaktischen Auswertung und der logischen Inferenz erzielt.
Degenerierte Mannigfaltigkeiten, die sich aus der Nicht-Identifizierbarkeit des Modells ergeben, verlangsamen das Lernen in tiefen Netzen; Skip-Verbindungen helfen, indem sie Degenerationen aufbrechen.
Die Autoren zeigen, dass Eliminations- und Überlappungssingularitäten das Lernen in tiefen neuronalen Netzen behindern, und demonstrieren, dass Skip-Verbindungen die Häufigkeit dieser Singularitäten reduzieren und das Lernen beschleunigen können.
Das Werk untersucht die Verwendung von Skip-Verbindungen in tiefen Netzwerken als eine Möglichkeit, Singularitäten in der Hessian-Matrix während des Trainings zu mildern.
Lernen von Zustandsdarstellungen, die die für die Kontrolle notwendigen Faktoren erfassen.
Ein Ansatz für das Repräsentationslernen im Rahmen des Reinforcement Learnings, der zwei Stufen funktional in Bezug auf die Aktionen unterscheidet, die erforderlich sind, um sie zu erreichen.
In dem Werk wird eine Methode zum Erlernen von Darstellungen vorgestellt, bei denen die Nähe im euklidischen Abstand Zustände darstellt, die durch ähnliche Strategien erreicht werden.
Wir untersuchen das Verhalten eines CNN bei der Bewältigung neuer Aufgaben unter Beibehaltung der Beherrschung bereits gelernter Aufgaben
Morty überarbeitet vordefinierte Worteinbettungen, um entweder: (a) Verbesserung der gesamten Einbettungsleistung (für Multi-Task-Einstellungen) oder Verbesserung der Single-Task-Leistung, wobei nur minimaler Aufwand erforderlich ist.
Das selektive Erweitern von schwer zu klassifizierenden Punkten führt zu einem effizienten Training.
Die Autoren untersuchen das Problem der Identifizierung von Subsampling-Strategien für die Datenerweiterung und schlagen Strategien vor, die auf Modelleinfluss und -verlust sowie auf einem empirischen Benchmarking der vorgeschlagenen Methoden basieren.
Die Autoren schlagen vor, Einfluss- oder verlustbasierte Methoden zu verwenden, um eine Teilmenge von Punkten auszuwählen, die zur Erweiterung von Datensätzen für Trainingsmodelle verwendet werden, wobei der Verlust additiv über die Datenpunkte ist.
Ein tiefes generatives Modell für organische Moleküle, das zunächst Reaktionsbausteine erzeugt und diese dann mithilfe eines Reaktionsvorhersageprogramms kombiniert.
Ein molekulares generatives Modell, das Moleküle in einem zweistufigen Prozess erzeugt und Synthesewege für die erzeugten Moleküle bereitstellt, so dass die Benutzer die synthetische Zugänglichkeit der erzeugten Verbindungen untersuchen können.
Verbesserung der Robustheit und Energieeffizienz eines tiefen neuronalen Netzes unter Verwendung der versteckten Darstellungen.
Dieser Artikel zielt darauf ab, die Fehlklassifikationen von tiefen neuronalen Netzen auf energieeffiziente Weise zu reduzieren, indem relevante merkmalsbasierte Hilfszellen nach einer oder mehreren versteckten Schichten hinzugefügt werden, um zu entscheiden, ob die Klassifizierung vorzeitig beendet werden soll.
Verständnis der Struktur von Wissensgraphen unter Verwendung von Erkenntnissen aus Worteinbettungen.
In diesem Beitrag wird versucht, die latente Struktur zu verstehen, die den Methoden zur Einbettung von Wissensgraphen zugrunde liegt, und es wird gezeigt, dass die Fähigkeit eines Modells, einen Beziehungstyp darzustellen, von den Beschränkungen der Modellarchitektur in Bezug auf die Beziehungsbedingungen abhängt.
Diese Arbeit schlägt eine detaillierte Studie über die Erklärbarkeit von Link-Prädiktionsmodellen (LP) vor, indem es eine neue Interpretation von Worteinbettungen verwendet, um ein besseres Verständnis der Modellleistung von LPs zu ermöglichen.
Ein neuartiger Mechanismus zur Selbstbeobachtung für die Imputation multivariater, geogetaggter Zeitreihen.
In diesem Beitrag wird das Problem der Anwendung des Transformatorennetzwerks auf räumlich-zeitliche Daten in einer rechnerisch effizienten Art und Weise behandelt und es werden Möglichkeiten zur Implementierung von 3D-Attention untersucht.
In diesem Beitrag wird die Wirksamkeit von Transformationsmodellen für die Imputation von Zeitreihendaten über verschiedene Dimensionen der Eingabe empirisch untersucht.
Wir haben ein REDNET (ResNet Encoder-Decoder) mit 8 Skip-Verbindungen entwickelt und getestet, um Störungen aus Dokumenten zu entfernen, einschließlich Unschärfe und Wasserzeichen, was zu einem leistungsstarken tiefen Netzwerk für die Bereinigung von Dokumentenbildern führt. 
Wir identifizieren eine Familie von Schutztechniken und zeigen, dass sowohl eine deterministische verlustbehaftete Kompression als auch zufällige Störungen der Eingabe zu einer ähnlichen Verbesserung der Robustheit führen.
In diesem Beitrag wird erörtert, wie ein bestimmter gegnerischer Angriff destabilisiert werden kann, was gegnerische Bilder nicht robust macht und ob es für Angreifer möglich ist, ein universelles Modell von Störungen zu verwenden, um ihre gegnerischen Beispiele robust gegen solche Störungen zu machen.
Das Papier untersucht die Robustheit von gegnerischen Angriffen gegenüber Transformationen ihrer Eingaben.
Wir bieten eine Methode zum Benchmarking von Optimierern an, die den Prozess der Hyperparameterabstimmung berücksichtigt.
Einführung einer neuartigen Metrik zur Erfassung der Abstimmbarkeit eines Optimierers und ein umfassender empirischer Vergleich von Deep-Learning-Optimierern bei unterschiedlichem Umfang der Hyperparameter-Abstimmung. 
In diesem Beitrag wird ein einfaches Maß für die Abstimmbarkeit eingeführt, das den Vergleich von Optimierern unter Ressourcenbeschränkungen ermöglicht. Es zeigt sich, dass die Abstimmung der Lernrate von Adam-Optimierern am einfachsten ist, um gut funktionierende Hyperparameter-Konfigurationen zu finden.
Wir führen ein halbüberwachtes tiefes neuronales Netzwerk ein, um die Lösung des Phasenproblems in der Elektronenmikroskopie zu approximieren
Word2net ist eine neuartige Methode zum Erlernen neuronaler Netzwerkrepräsentationen von Wörtern, die syntaktische Informationen nutzen kann, um bessere semantische Merkmale zu lernen.
Dieser Artikel erweitert SGNS mit einem architektonischen Wechsel von einem Bag-of-Words-Modell zu einem Feedforward-Modell und trägt eine neue Form der Regularisierung bei, indem es eine Teilmenge von Schichten zwischen verschiedenen assoziierten Netzwerken bindet.
Eine Methode zur Verwendung nichtlinearer Kombination von Kontextvektoren zum Erlernen der Vektordarstellung von Wörtern, wobei die Hauptidee darin besteht, jede Worteinbettung durch ein neuronales Netz zu ersetzen.
Unter Verwendung eines 10-Sekunden-Fensters von fMRI-Signalen identifizierte unser GCN-Modell 21 verschiedene Aufgabenbedingungen aus dem HCP-Datensatz mit einer Testgenauigkeit von 89 %.
Effiziente Induktion von tiefen neuronalen Netzen mit niedrigem Rang durch SVD-Training mit spärlichen Singulärwerten und orthogonalen Singulärvektoren.
In diesem Artikel wird ein Ansatz zur Netzwerkkompression vorgestellt, bei dem die Gewichtsmatrix in jeder Schicht einen niedrigen Rang hat und die Gewichtsmatrizen explizit in eine SVD-ähnliche Faktorisierung zur Behandlung als neue Parameter faktorisiert werden.
Vorschlag, jede Schicht eines tiefen neuronalen Netzes vor dem Training mit einer Low-Rank-Matrixzerlegung zu parametrisieren, die Convolutions durch zwei aufeinanderfolgende Convolutions zu ersetzen und dann die zerlegte Methode zu trainieren.
Wir schlagen ein Few-Shot Learning Modell vor, das speziell auf Regressionsaufgaben zugeschnitten ist.
In dieser Arbeit wird ein neues Shot-Learning-Verfahren für Regressionsprobleme mit kleinen Stichproben vorgeschlagen.
Eine Methode, die mit wenigen Beispielen ein Regressionsmodell erlernt und andere Methoden übertrifft.
Wir präsentieren einen neuen Ansatz zur Erkennung von Pixeln, die nicht in der Verteilung liegen, bei der semantischen Segmentierung.
Diese Arbeit befasst sich mit der Erkennung von Verteilungsfehlern, um den Segmentierungsprozess zu unterstützen, und schlägt einen Ansatz für das Training eines binären Klassifizierers vor, der Bildfelder aus einer bekannten Gruppe von Klassen von denen einer unbekannten unterscheidet.
Diese Arbeit zielt auf die Erkennung von Pixeln außerhalb der Verteilung für die semantische Segmentierung ab, und diese Arbeit nutzt Daten aus anderen Bereichen, um unbestimmte Klassen zu erkennen und die Unsicherheit besser zu modellieren.
Genaue, schnelle und automatisierte Quantisierung neuronaler Netze nach dem Kernelprinzip mit gemischter Präzision unter Verwendung von hierarchischem Deep Reinforcement Learning
Eine Methode zur Quantisierung von Gewichten und Aktivierungen neuronaler Netze, die tiefes Reinforcement Learning verwendet, um die Bitbreite für einzelne Kernel in einer Schicht auszuwählen, und die eine bessere Leistung bzw. Latenz als frühere Ansätze erzielt.
In diesem Artikel wird vorgeschlagen, automatisch nach Quantisierungsschemata für jeden Kernel im neuronalen Netz zu suchen, wobei hierarchisches RL zur Steuerung der Suche verwendet wird. 
Gaggle, ein interaktives visuelles Analysesystem, das den Benutzern hilft, sich interaktiv im Modellraum für Klassifizierungs- und Rankingaufgaben zu bewegen.
Ein neues visuelles Analysesystem, das es auch unerfahrenen Nutzern ermöglichen soll, interaktiv in einem Modellraum zu navigieren, indem es einen demonstrationsbasierten Ansatz verwendet.
Ein visuelles Analysesystem, das unerfahrenen Analysten hilft, sich im Modellraum zurechtzufinden, um Klassifizierungs- und Rangordnungsaufgaben durchzuführen.
Wir schlagen ein neuartiges Aufmerksamkeitsnetz mit dem Hybird-Encoder vor, um das Problem der Textdarstellung bei der Klassifizierung chinesischer Texte zu lösen, insbesondere die sprachlichen Phänomene der Aussprache wie Polyphone und Homophone.
Dieses Papier schlägt ein aufmerksamkeitsbasiertes Modell vor, das aus einem Wort-Encoder und einem Pinyin-Encoder für die Klassifizierung chinesischer Texte besteht, und erweitert die Architektur um den Pinyin-Zeichen-Encoder.
Vorschlag für ein Aufmerksamkeitsnetz, bei dem sowohl Wort als auch Pinyin für die chinesische Repräsentation berücksichtigt werden, mit verbesserten Ergebnissen, die in mehreren Datensätzen für die Textklassifizierung gezeigt wurden.
Multimodales Imitation Learning aus unstrukturierten Demonstrationen unter Verwendung stochastischer neuronaler Netzmodelle. 
Ein neuer, auf Stichproben basierender Ansatz für die Inferenz in latenten Variablenmodellen, der auf multimodales Imitation Learning anwendbar ist und besser funktioniert als deterministische neuronale Netze und stochastische neuronale Netze für eine reale visuelle Robotikaufgabe.
Dieser Artikel zeigt, wie man mehrere Modalitäten durch Imitation Learning aus visuellen Daten mit Hilfe stochastischer neuronaler Netze erlernen kann, und eine Methode zum Lernen aus Demonstrationen, bei denen mehrere Modalitäten derselben Aufgabe gegeben sind.
Ein neuartiger Ansatz zur Konstruktion hierarchischer Erklärungen für die Textklassifizierung durch Erkennung von Merkmalsinteraktionen.
Eine neuartige Methode zur Bereitstellung von Erklärungen für Vorhersagen, die von Textklassifizierern getroffen wurden, die bei der Bewertung der Wichtigkeit auf Wortebene besser abschneidet, sowie eine neue Metrik, der Kohäsionsverlust, zur Bewertung der Wichtigkeit auf Bereichsebene.
Eine Interpretationsmethode auf der Grundlage von Merkmalsinteraktionen und Merkmalswichtigkeitspunkten im Vergleich zu unabhängigen Merkmalsbeiträgen.
Wir sorgen dafür, dass Convolutional Layers schneller laufen, indem wir Kanäle bei der Merkmalsberechnung dynamisch verstärken und unterdrücken.
Eine Methode zur Verstärkung und Unterdrückung von Merkmalen für dynamisches Channel Pruning, die die Bedeutung jedes Kanals vorhersagt und dann eine affine Funktion zur Verstärkung/Unterdrückung der Kanalbedeutung verwendet.
Vorschlag für eine Channel Pruning Methode zur dynamischen Auswahl von Kanälen während der Prüfung.
Neuronale Netze können so vordefiniert werden, dass sie ohne Leistungseinbußen eine spärliche Konnektivität aufweisen.
Diese Arbeit untersucht spärliche Verbindungsmuster in den oberen Schichten von Convolutional Bildklassifikations Netzwerken und stellt Heuristiken zur Verteilung von Verbindungen zwischen Fenstern/Gruppen und ein Maß namens Streuung zur Konstruktion von Konnektivitätsmasken vor.
Vorschlag zur Verringerung der Anzahl von Parametern, die von einem tiefen Netz gelernt werden, durch die Einrichtung von spärlichen Verbindungsgewichten in Klassifizierungsschichten und der Einführung eines Konzepts der "Streuung".
Wir bieten ein umfassendes, strenges und kohärentes Benchmarking zur Bewertung der adversarial Robustheit von Deep-Learning-Modellen.
In diesem Artikel werden verschiedene Arten von Klassifizierungsmodellen unter verschiedenen Angriffsmethoden bewertet.
Eine groß angelegte empirische Studie, in der verschiedene Angriffs- und Verteidigungstechniken miteinander verglichen werden, sowie die Verwendung von Kurven für die Genauigkeit im Vergleich zum Störungsbudget und die Genauigkeit im Vergleich zur Angriffsstärke zur Bewertung von Angriffen und Verteidigungen.
Wir schlagen eine Modifikation traditioneller Künstlicher Neuronaler Netze vor, die durch die Biologie der Neuronen motiviert ist und es ermöglicht, die Form der Aktivierungsfunktion kontextabhängig zu gestalten.
Eine Methode zur Skalierung der Aktivierungen einer Schicht von Neuronen in einem ANN in Abhängigkeit von den Eingaben in diese Schicht, die Verbesserungen gegenüber den Grundlinien aufweist.
Einführung einer architektonischen Änderung für Basisneuronen in einem neuronalen Netz und die Idee, die Ausgabe der Linearkombinationen der Neuronen mit einem Modulator zu multiplizieren, bevor sie in die Aktivierungsfunktion eingespeist wird.
Wir identifizieren das Problem des Vergessens beim Fine-Tuning von vortrainierten NLG-Modellen und schlagen eine Mix-Review-Strategie vor, um dieses Problem zu lösen.
In dieser Arbeit wird das Problem des Vergessens im Rahmen des Pretraining Fine-Tunings aus der Perspektive der Kontextsensitivität und des Wissenstransfers analysiert und eine Fine-Tuningstrategie vorgeschlagen, die die Methode des Gewichtsabfalls übertrifft.
Untersuchung des Vergessensproblems im Pretrain-Finetuning Framework, insbesondere bei Aufgaben zur Erzeugung von Dialogantworten, und Vorschlag einer Mix-Review-Strategie, um das Vergessensproblem zu mildern.
Verbesserte Modellierung komplexer Systeme unter Verwendung hybrider neuronaler/domänenbezogener Modellkompositionen, neuer Dekorrelationsverlustfunktionen und extrapolativer Testsätze.
In diesem Beitrag werden Experimente durchgeführt, um die extrapolativen Vorhersagen verschiedener hybrider Modelle, die physikalische Modelle, neuronale Netze und stochastische Modelle umfassen, zu vergleichen und die Herausforderung der nicht modellierten Dynamik als Engpass zu bewältigen.
In diesem Beitrag werden Ansätze zur Kombination von neuronalen Netzen mit Nicht-NN-Modellen vorgestellt, um das Verhalten komplexer physikalischer Systeme vorherzusagen.
Wir lernen dichte Werte und ein dynamisches Modell als Priors aus Explorationsdaten und verwenden sie, um eine gute Strategie für neue Aufgaben in Zero-Shot Bedingungen zu entwickeln.
In diesem Beitrag wird die Verallgemeinerung von Zero Shots auf neue Umgebungen diskutiert und ein Ansatz mit Ergebnissen zu Grid-World, Super Mario Bros und 3D Robotics vorgeschlagen.
Eine Methode, die darauf abzielt, aufgabenagnostische Prioritäten für die Zero-Shot-Generalisierung zu erlernen, mit der Idee, einen Modellierungsansatz zusätzlich zum modellbasierten RL-Framework einzusetzen.
Analyse der zugrundeliegenden Mechanismen des Varianzkollapses von SVGD in hohen Dimensionen.
Neudenken der Verallgemeinerung erfordert ein Überdenken alter Ideen: Ansätze der statistischen Mechanik und komplexes Lernverhalten
Die Autoren schlagen vor, dass Ideen aus der statistischen Mechanik zum Verständnis der Generalisierungseigenschaften von tiefen neuronalen Netzen beitragen, und geben einen Ansatz vor, der starke qualitative Beschreibungen empirischer Ergebnisse in Bezug auf tiefe neuronale Netze und Lernalgorithmen liefert.
Eine Reihe von Ideen zum theoretischen Verständnis der Verallgemeinerungseigenschaften von mehrschichtigen neuronalen Netzen und eine qualitative Analogie zwischen dem Verhalten beim Deep Learning und den Ergebnissen der quantitativen statistischen Physikanalyse von ein- und zweischichtigen neuronalen Netzen.
Wir präsentieren doppelt spärlichen Softmax, die spärliche Mischung aus spärlichen Experten, um die Effizienz der Softmax-Inferenz durch Ausnutzung der zweistufigen überlappenden Hierarchie zu verbessern. 
In diesem Beitrag wird eine schnelle Annäherung an die Softmax-Berechnung vorgeschlagen, wenn die Anzahl der Klassen sehr groß ist.
In diesem Artikel wird eine spärliche Mischung aus spärlichen Experten vorgeschlagen, die eine zweistufige Klassenhierarchie für eine effiziente Softmax-Inferenz erlernt.
Wir untersuchen die Verwendung von passiv gesammelten Eye-Tracking-Daten, um die Menge an markierten Daten, die während des Trainings benötigt werden, zu reduzieren.
Eine Methode zur Nutzung von Blickinformationen, um die Komplexität der Stichprobe eines Modells und den erforderlichen Beschriftungsaufwand zu reduzieren, um eine Zielleistung zu erreichen, mit verbesserten Ergebnissen bei mittelgroßen Stichproben und schwierigeren Aufgaben.
Eine Methode zur Einbeziehung von Blicksignalen in Standard-CNNs für die Bildklassifizierung durch Hinzufügen eines Verlustfunktionsterms, der auf der Differenz zwischen der Klassenaktivierungskarte des Modells und der aus Blickverfolgungsinformationen konstruierten Zuordnungen basiert.
Wir wenden eine Verlustkorrektur auf graphische neuronale Netze an, um ein Modell zu trainieren, das robuster gegen Rauschen ist.
Diese Arbeit führt eine Verlustkorrektur für Graph Neural Networks ein, um mit symmetrischem Graph Label Störungen umzugehen, und konzentriert sich auf eine Graph-Klassifikationsaufgabe.
In diesem Beitrag wird die Verwendung eines Rauschkorrekturverlustes im Zusammenhang mit neuronalen Graphennetzen vorgeschlagen, um mit verrauschten Etiketten umzugehen.
Wir verwenden Graph Co-Attention in einem gepaarten Graph-Trainingssystem für Graph-Klassifikation und -Regression.
In diesem Artikel wird ein Multi-Head-Co-Attention-Mechanismus in GCN eingeführt, der es einem Medikament ermöglicht auf ein anderes zu folgen, während der Vorhersage von Nebenwirkungen eines Medikaments.
Eine Methode zur Erweiterung des graphenbasierten Lernens mit einer co-attentionalen Schicht, die bei einer paarweisen Graphenklassifizierungsaufgabe andere frühere Methoden übertrifft.
Bildbeschriftung als bedingtes GAN-Training mit neuartigen Architekturen, untersucht auch zwei diskrete GAN-Trainingsmethoden. 
Ein verbessertes GAN-Modell für Bildbeschriftungen, das einen kontextabhängigen LSTM-Beschrifter vorschlägt, einen stärkeren co-attentiven Diskriminator mit besserer Leistung einführt und SCST für das GAN-Training verwendet.
Ausnutzung der Krümmung, damit MCMC-Methoden schneller konvergieren als der Stand der Technik.
Keras für unendliche neuronale Netze.
Wir schlagen eine neuartige Verlustfunktion vor, die sowohl bei Bild- als auch bei Textklassifizierungsaufgaben modernste Ergebnisse in der Out-of-Distribution-Erkennung mit Outlier Exposure erzielt.
In diesem Beitrag werden die Probleme der Erkennung von Ausreißern und der Modellkalibrierung durch die Anpassung der Verlustfunktion der Ausreißer-Expositions-Technik angegangen. Die Ergebnisse zeigen eine höhere Leistung als OE bei Bildverarbeitungs- und Textbenchmarks sowie eine verbesserte Modellkalibrierung.
Vorschlag für eine neue Verlustfunktion zum Trainieren des Netzes mit Outlier Exposure, die im Vergleich zu einfachen Verlustfunktionen mit KL-Divergenz zu einer besseren OOD-Erkennung führt.
Aufgabenunabhängiges Vortraining kann die Attraktor Landschaft von RNN formen und verschiedene induktive Vorlieben für verschiedene Navigationsaufgaben ausbilden.
In dieser Arbeit werden die internen Repräsentationen von rekurrenten neuronalen Netzen untersucht, die für Navigationsaufgaben trainiert wurden, und es wird festgestellt, dass RNNs, die für die Pfadintegration trainiert wurden, kontinuierliche 2D-Attraktoren enthalten, während RNNs, die für den Landmark-Speicher trainiert wurden, diskrete Attraktoren enthalten.
In diesem Beitrag wird untersucht, wie das Pre-Training von rekurrenten Netzen auf verschiedene Navigationsziele unterschiedliche Vorteile für die Lösung nachgelagerter Aufgaben mit sich bringt, und es wird gezeigt, wie sich das unterschiedliche Pre-Training in unterschiedlichen dynamischen Strukturen in den Netzen nach dem Pre-Training manifestiert.
Verifizierung neuronaler Netze für zeitliche Eigenschaften und Modelle zur Erzeugung von Sequenzen.
Dieses Papier erweitert die Intervall-Bound-Propagation auf rekurrente Berechnungen und autoregressive Modelle, führt die Signal Temporal Logic ein und erweitert sie, um zeitliche Einschränkungen zu spezifizieren, und liefert den Beweis, dass STL mit Bound-Propagation sicherstellen kann, dass neuronale Modelle mit der zeitlichen Spezifikation übereinstimmen.
Eine Möglichkeit, Zeitreihenregressoren nachweislich in Bezug auf eine Reihe von Regeln zu trainieren, die durch die zeitliche Logik von Signalen definiert sind, und Arbeit bei der Ableitung von Regeln für die gebundene Fortpflanzung in der STL-Sprache.
Wir schlagen eine universelle Lösung für neuronale Netze vor, um effektive NN-Architekturen für tabellarische Daten automatisch abzuleiten.
Ein neues Trainingsverfahren für neuronale Netze, das für tabellarische Daten entwickelt wurde und darauf abzielt, aus GBDTs extrahierte Merkmalscluster zu nutzen.
Vorschlag für einen hybriden Algorithmus für maschinelles Lernen unter Verwendung von Gradient Boosted Decision Trees und Deep Neural Networks, mit beabsichtigter Forschungsrichtung auf tabellarischen Daten.
Probabilistisches Regel-Lernsystem mit gehobener Inferenz.
Ein Modell für probabilistisches Regellernen zur Automatisierung der Vervollständigung probabilistischer Datenbanken, das AMIE+ und gehobene Inferenz zur Steigerung der Recheneffizienz verwendet.
Wir schlagen das erste nicht-autoregressive neuronale Modell für Dialogue State Tracking (DST) vor, das die SOTA-Genauigkeit (49,04%) beim MultiWOZ2.1 Benchmark erreicht und die Inferenzlatenz um eine Größenordnung reduziert.
Ein neues Modell für die DST-Aufgabe, das die Komplexität der Inferenzzeit mit einem nicht-autoregressiven Decoder reduziert, eine wettbewerbsfähige DST-Genauigkeit erzielt und Verbesserungen gegenüber anderen Grundmodellen aufweist.
Vorschlag für ein Modell, das in der Lage ist, Dialogzustände auf nicht-rekursive Weise zu verfolgen.
Eine neuartige Netzwerkarchitektur zur Durchführung von Deep 3D Zoom oder Nahaufnahmen.
Ein Verfahren zur Erstellung eines "gezoomten Bildes" für ein gegebenes Eingangsbild und ein neuartiger Rückprojektions-Rekonstruktionsverlust, der es dem Netzwerk ermöglicht, die zugrunde liegende 3D-Struktur zu erlernen und ein natürliches Erscheinungsbild beizubehalten.
Ein Algorithmus für die Synthese von 3D-Zoom-Verhalten, wenn sich die Kamera vorwärts bewegt, eine Netzwerkstruktur, die Disparitätsschätzung in einem GAN-Framework zur Synthese neuartiger Ansichten beinhaltet, und eine vorgeschlagene neue Computer Vision Aufgabe.
Eine quantitative Verfeinerung des universellen Angleichungstheorems durch einen algebraischen Ansatz.
Die Autoren leiten die Beweise für die universelle Approximationseigenschaft algebraisch ab und versichern, dass die Ergebnisse allgemein für andere Arten von neuronalen Netzen und ähnlichen Lernern gelten.
Ein neuer Beweis von Leshnos Version der universellen Approximationseigenschaft für neuronale Netze und neue Erkenntnisse über die universelle Approximationseigenschaft.
Modulares Framework für die Dokumentenklassifizierung und Datenaggregationstechnik, um den Rahmen robust gegenüber verschiedenen Verzerrungen und Störungen zu machen und sich nur auf die wichtigen Wörter zu konzentrieren. 
Die Autoren betrachten das Training einer RNN-basierten Textklassifikation, bei der es eine Ressourcenbeschränkung für die Testzeitvorhersage gibt, und stellen einen Ansatz vor, bei dem ein Maskierungsmechanismus verwendet wird, um die in der Vorhersage verwendeten Wörter/Phrasen/Sätze zu reduzieren, gefolgt von einem Klassifikator, der diese Komponenten verarbeitet.
Zulassen von Teilkanalverbindungen in Supernetzen zur Regulierung und Beschleunigung der Suche nach differenzierbaren Architekturen
Eine Erweiterung des neuronalen Architektur-Suchverfahrens DARTS, die dessen Manko der immensen Speicherkosten durch Verwendung einer zufälligen Teilmenge von Kanälen und einer Methode zur Normalisierung von Kanten behebt.
Diese Arbeit schlägt vor, DARTS in Bezug auf die Trainingseffizienz zu verbessern, von großen Speicher-und Rechen-Overheads, und schlägt eine teilweise verbunden DARTS mit partiellen Kanal-Verbindung und Rand Normalisierung.
Agenten interagieren (sprechen, handeln) und können in einer reichhaltigen Welt mit vielfältiger Sprache Ziele erreichen, indem sie die Kluft zwischen Plauderei und zielorientiertem Dialog überbrücken.
Dieses Papier untersucht eine Multi-Agenten-Dialogaufgabe, bei der der lernende Agent darauf abzielt, natürlichsprachliche Handlungen zu generieren, die dem anderen Agenten eine bestimmte Handlung entlocken, und zeigt, dass RL-Agenten einen höheren Grad an Aufgabenerfüllung erreichen können als die Basisprogramme des Imitationslernens.
Diese Arbeit untersucht die zielorientierte Dialoggestaltung mit Reinforcement Learning in einem Fantasy Text Adventure Game und stellt fest, dass die RL-Ansätze die überwachten Lernmodelle übertreffen.
Eine neue, teilweise policy-agnostische Methode zur Evaluierung von Off-Policy-Policies mit unendlichem Horizont und mehreren bekannten oder unbekannten Verhaltensrichtlinien.
Eine geschätzte gemischte Stategie, die Ideen von Schätzern für unendliche Horizonte und Regressions-Bedeutsamkeitsstichproben für die Wichtigkeitsgewichtung aufgreift und sie auf viele Strategien und unbekannte Regeln ausweitet.
Ein Algorithmus zur Bewertung von Strategien mit unendlichem Horizont und mehreren Verhaltensstrategien durch Schätzung einer gemischten Strategie unter Regression sowie der theoretische Beweis, dass ein geschätztes Strategieverhältnis die Varianz reduzieren kann.
Wir stellen eine effizientere neuronale Architektur für amortisierte Inferenz vor, die kontinuierliche und bedingte Normalisierungsflüsse mit einer prinzipiellen Wahl der Seltenheitsstruktur kombiniert.
Wir zeigen, dass ENAS mit ES-Optimierung in RL hochgradig skalierbar ist, und verwenden es, um die Richtlinien neuronaler Netze durch Gewichtsteilung zu verdichten.
Die Autoren konstruieren Reinforcement-Learning-Strategien mit sehr wenigen Parametern, indem sie ein Feed-Forward-Neuronalnetz komprimieren, es zwingen, Gewichte zu teilen, und eine Reinforcement-Learning-Methode verwenden, um die Zuordnung der geteilten Gewichte zu lernen.
In diesem Beitrag werden Ideen aus ENAS- und ES-Methoden zur Optimierung kombiniert und die chromatische Netzarchitektur eingeführt, die die Gewichte des RL-Netzes in gebundene Untergruppen unterteilt.
Wir stellen Deep SAD vor, eine Deep-Methode für die allgemeine semi-supervised Anomalieerkennung, die insbesondere die Vorteile von gelabelten Anomalien nutzt.
Eine neue Methode, um anomale Daten zu finden, wenn einige gekennzeichnete Anomalien gegeben sind, die den von der Informationstheorie abgeleiteten Verlust anwendet, der darauf basiert, dass normale Daten normalerweise eine geringere Entropie haben als abnormale Daten.
Vorschlag für ein Rahmenwerk zur Erkennung von Anomalien unter Bedingungen, in denen unbeschriftete Daten, beschriftete positive Daten und beschriftete negative Daten verfügbar sind, und Vorschlag zur Annäherung an semi-supervised AD aus einer informationstheoretischen Perspektive.
In diesem Artikel werden die Trainingsdynamik und die kritischen Punkte beim Training eines tiefen ReLU-Netzwerks mittels SGD in einer Lehrer-Schüler-Umgebung analysiert. 
Untersuchung der Überparametrisierung in mehrschichtigen Schüler-Lehrer ReLU-Netzwerken, ein theoretischer Teil über kritische SGD-Punkte für die Lehrer-Schüler-Umgebung und ein heuristischer und empirischer Teil über die Dynamik des SDG-Algorithmus in Abhängigkeit von Lehrernetzwerken.
Unter bestimmten Bedingungen für die linearen Eingangs- und Ausgangstransformationen können sowohl GD als auch SGD globale Konvergenz für das Training tiefer linearer ResNets erreichen.
Die Autoren untersuchen die Konvergenz des Gradientenabstiegs bei der Ausbildung von tiefen linearen Residual Networks und stellen eine globale Konvergenz von GD/SGD und lineare Konvergenzraten von SG/SGD fest.
Untersuchung der Konvergenzeigenschaften von GD und SGD auf tiefen linearen ResNets und Nachweis, dass GD und SGD unter bestimmten Bedingungen für die Eingangs- und Ausgangstransformationen und mit Null-Initialisierung zu globalen Minima konvergieren.
Wir analysieren den Trainingsprozess für Deep Networks und zeigen, dass sie mit dem schnellen Lernen von flachen, klassifizierbaren Beispielen beginnen und langsam auf härtere Datenpunkte verallgemeinern.
Lernen von tiefen latenten Variablen-MRFs mit einem Sattelpunkt-Ziel, das von der Bethe-Partitionsfunktion-Approximation abgeleitet ist.
Eine Methode zum Erlernen tiefer latent-variabler MRF mit einem Optimierungsziel, das die freie Bethe-Energie nutzt, die auch die zugrundeliegenden Beschränkungen der Optimierungen der freien Bethe-Energie löst.
Eine Zielsetzung für das Lernen von MRFs mit latenten Variablen auf der Grundlage der freien Bethe-Energie und der amortisierten Inferenz, die sich von der Optimierung der Standard-ELBO unterscheidet.
Ein allgemeines Framework für die Erstellung von Erklärungen mit Hilfe der Logik.
Dieses Papier untersucht die Generierung von Erklärungen aus der Sicht der KR und führt Experimente durch, in denen die Größe der Erklärungen und die Laufzeit mit Zufallsformeln und Formeln aus einer Blocksworld Instanz gemessen werden.
Dieses Papier bietet eine Perspektive auf Erklärungen zwischen zwei Wissensbasen und läuft parallel zu Arbeiten über Modellabgleich in der Planungsliteratur.
Tiefe und enge neuronale Netze konvergieren je nach Verlust mit hoher Wahrscheinlichkeit zu fehlerhaften Mittel- oder Medianwerten der Zielfunktion.
In diesem Beitrag werden die Fehlermöglichkeiten von tiefen und engen Netzen untersucht, wobei der Schwerpunkt auf möglichst kleinen Modellen liegt, bei denen das unerwünschte Verhalten auftritt.
In diesem Beitrag wird gezeigt, dass das Training tiefer neuronaler ReLU-Netze mit hoher Wahrscheinlichkeit zu einem konstanten Klassifikator konvergiert, wenn die Breite der versteckten Schichten zu klein ist.
Wir schlagen MMA Training zur direkten Maximierung des Eingaberaumrands vor, um die adversarial Robustheit des Systems vor allem dadurch zu verbessern, dass die Vorgabe einer festen Verzerrungsgrenze entfällt.
Ein adaptiver, auf Rand basierender Ansatz zum Training von robusten DNNs durch Maximierung der kürzesten Marge der Eingaben zur Entscheidungsgrenze, der ein adversariales Training mit großen Störungen möglich macht.
Es wird eine Methode für robustes Lernen gegen gegnerische Angriffe vorgestellt, bei dem der Rand des Eingaberaums direkt maximiert wird und eine Softmax-Variante der Max-Margin eingeführt wird.
Wir schlagen eine Methode zur Erkennung von Anomalien mit GANs vor, indem wir den latenten Raum des Generators nach guten Musterdarstellungen durchsuchen.
Die Autoren schlagen vor, GAN für die Erkennung von Anomalien zu verwenden, eine auf Gradientenabstieg basierende Methode zur iterativen Aktualisierung latenter Repräsentationen und eine neuartige Parameteraktualisierung für die Generatoren.
Ein GAN-basierter Ansatz zur Erkennung von Anomalien bei Bilddaten, bei dem der latente Raum des Generators erforscht wird, um eine Darstellung für ein Testbild zu finden.
Das transiente Verhalten von gradientenbasierten MCMC- und Variationsinferenz-Algorithmen ist ähnlicher als man denkt, was die Behauptung in Frage stellt, dass Variationsinferenz schneller ist als MCMC.
Ein kompositionsbasiertes Graph Convolutional Framework für multirelationale Graphen.
Die Autoren entwickeln GCN für multirelationale Graphen und schlagen CompGCN vor, das Erkenntnisse aus der Einbettung von Wissensgraphen nutzt und Knoten- und Beziehungsrepräsentationen lernt, um das Problem der Überparametrisierung zu mildern.
Dieses Papier stellt ein GCN-Framework für multirelationale Graphen vor und verallgemeinert mehrere bestehende Ansätze zur Einbettung von Wissensgraphen in ein Framework.
Wir quantisieren den Transformer vollständig auf 8 Bit und verbessern die Übersetzungsqualität im Vergleich zum Modell mit voller Präzision.
Eine 8-Bit-Quantisierungsmethode zur Quantisierung des maschinellen Übersetzungsmodell Transformers, die vorschlägt, eine einheitliche Min-Max-Quantisierung während der Inferenz und Bucketing-Gewichte vor der Quantisierung zu verwenden, um Quantisierungsfehler zu reduzieren.
Eine Methode zur Verringerung des benötigten Speicherplatzes durch eine Quantisierungstechnik, die sich auf die Verringerung des Speicherplatzes für die Transformer Architektur konzentriert.
Latent Embedding Optimization (LEO) ist ein neuartiges gradientenbasiertes Meta-Lernverfahren, das bei den anspruchsvollen 5-Wege-1-Shot und 5-Shot-miniImageNet und tieredImageNet Klassifizierungsaufgaben eine hervorragende Leistung zeigt.
Ein neues Meta-Lernsystem, das einen datenabhängigen latenten Raum erlernt, eine schnelle Anpassung im latenten Raum durchführt, effektiv für das Few-Shot Learning ist, eine aufgabenabhängige Initialisierung für die Anpassung hat und gut für multimodale Aufgabenverteilung funktioniert.
Diese Arbeit schlägt eine latente Einbettungsoptimierungsmethode für Meta-Learning vor und behauptet, dass der Beitrag darin besteht, optimierungsbasierte Meta-Learning-Techniken vom hochdimensionalen Raum der Modellparameter zu entkoppeln.
Relationale induktive Verzerrungen verbessern die Verallgemeinerungsfähigkeit außerhalb der Verteilung in modellfreien Agenten mit Reinforcement Learning.
Eine gemeinsame relationale Netzwerkarchitektur zur Parametrisierung des Akteurs- und Kritiknetzwerks, die sich auf verteilte vorteilhafte Akteur-Kritik Algorithmen konzentriert und modellfreie tiefe Verstärkungstechniken mit relationalem Wissen über die Umgebung erweitert, so dass Agenten interpretierbare Zustandsdarstellungen lernen können.
Eine quantitative und qualitative Analyse und Bewertung des Selbstaufmerksamkeits Mechanismus in Kombination mit dem Beziehungsnetz im Kontext des modellfreien RL.
Wir schlagen ein einfaches generatives Modell zur unbeaufsichtigten Bildübersetzung und Erkennung von Auffälligkeiten vor.
Wir definieren ein Konzept schichtweiser modellparalleler tiefer neuronaler Netze, bei denen die Schichten parallel arbeiten, und stellen eine Toolbox zur Verfügung, um diese Netze zu entwerfen, zu trainieren, zu bewerten und online mit ihnen zu interagieren.
Eine GPU-beschleunigte Toolbox für die parallele Aktualisierung von Neuronen, geschrieben in Theano, die verschiedene Aktualisierungsreihenfolgen in rekurrenten Netzen und Netzen mit Verbindungen, die Schichten überspringen, unterstützt. 
Eine neue Toolbox für das Lernen und Bewerten von tiefen neuronalen Netzen und ein Vorschlag für einen Paradigmenwechsel von schichtweise-sequentiellen Netzen zu schichtweise-parallelen Netzen.
Eine adversarial Verteidigungsmethode, die die Robustheit von tiefen neuronalen Netzen mit der Lyapunov-Stabilität verbindet
Die Autoren formulieren das Training von NNs als Suche nach einem optimalen Regler für ein diskretes dynamisches System, was es ihnen ermöglicht, die Methode der sukzessiven Annäherung zu verwenden, um ein NN so zu trainieren, dass es robuster gegen gegnerische Angriffe ist.
In diesem Beitrag wird die theoretische Sichtweise eines neuronalen Netzes als diskretisierte ODE verwendet, um eine Theorie der robusten Steuerung zu entwickeln, die darauf abzielt, das Netz zu trainieren und gleichzeitig die Robustheit zu verstärken.
Wir schlagen ein einfaches, aber effektives Neugewichtungsschema für GCNs vor, das theoretisch durch die Theorie des Mittelwert Feldes unterstützt wird.
Eine Methode, bekannt als DrGCN, zur Neugewichtung der verschiedenen Dimensionen der Knotendarstellungen in Graph Convolutional Networks durch Reduzierung der Varianz zwischen den Dimensionen.
Unser Ansatz ist der erste Versuch, ein sequentielles latentes Variablenmodell für die Wissensauswahl in einem wissensbasierten Dialog mit mehreren Runden zu nutzen. Er erreicht eine neue Spitzenleistung beim Wizard of Wikipedia Benchmark.
Ein sequentielles latentes Variablenmodell für die Wissensselektion bei der Dialoggenerierung, das das posteriore Aufmerksamkeitsmodell auf das Problem der latenten Wissensselektion ausweitet und eine höhere Leistung als bisherige State-of-the-Art-Modelle erzielt.
Eine neuartige Architektur für die Auswahl von wissensbasierten Multi-Turn-Dialogen, die in relevanten Benchmark-Datensätzen den Stand der Technik erreicht und bei menschlichen Bewertungen besser abschneidet.
Wir schlagen eine Meta-Learning-Methode vor, die hierarchische Variationsinferenz über Trainingsepisoden hinweg effizient amortisiert.
Eine Anpassung an MAML-Modelle, die die posteriore Unsicherheit in aufgabenspezifischen latenten Variablen berücksichtigt, indem sie Variationsinferenz für aufgabenspezifische Parameter in einer hierarchischen Bayes'schen Sichtweise von MAML einsetzt.
Die Autoren ziehen Meta-Lernen in Betracht, um eine Priorität über die Gewichte des neuronalen Netzes zu erlernen, was mittels amortisierter Variationsinferenz geschieht.
Repräsentation/Wissensdestillation durch Maximierung der gegenseitigen Information zwischen Lehrer und Schüler.
Diese Arbeit kombiniert ein kontrastives Ziel, das die gegenseitige Information zwischen den Repräsentationen misst, die von Lehrer- und Schülernetzwerken für die Modelldestillation erlernt wurden, und schlägt ein Modell vor, das Verbesserungen gegenüber bestehenden Alternativen bei Destillationsaufgaben aufweist.
Netzwerke, die mit rückgekoppelten Verbindungen und lokalen Plastizitätsregeln lernen, können durch Metalernen optimiert werden.
CNNs mit biologisch inspirierten lateralen Verbindungen, die auf unüberwachte Weise gelernt werden, sind robuster gegenüber störhaften Eingaben. 
In dieser Arbeit soll ein Ansatz zur Darstellung von Tensorprodukten für Deep-Learning-basierte Anwendungen zur Verarbeitung natürlicher Sprache entwickelt werden.
Wir untersuchen die zertifizierte Robustheit für Top-k Vorhersagen durch randomisierte Glättung unter Gaußschem Störrungen und leiten eine enge Robustheitsgrenze in der L_2 Norm ab.
Dieser Artikel erweitert die Arbeit zur Ableitung eines zertifizierten Radius durch randomisierte Glättung und zeigt den Radius, bei dem ein geglätteter Klassifikator unter Gaußschen Störungen für die besten k Vorhersagen zertifiziert ist.
Dieser Beitrag baut auf der Technik der Zufallsglättung für die Top-1 Vorhersage auf und zielt darauf ab, eine Zertifizierung für Top-k Vorhersagen zu liefern.
Wir stellen strukturierte Priors für das unüberwachte Lernen von entwirrten Repräsentationen in VAEs vor, die den Kompromiss zwischen Entwirrung und Rekonstruktionsverlust deutlich abmildern.
Ein allgemeiner Rahmen für die Verwendung der Familie der L^p-verschachtelten Verteilungen als Prior für den Code-Vektor der VAE, der eine höhere MIG demonstriert.
Die Autoren weisen auf Probleme in aktuellen VAE-Ansätzen hin und bieten eine neue Perspektive auf den Kompromiss zwischen Rekonstruktion und Orthogonalisierung für VAE, beta-VAE und beta-TCVAE.
Wir verallgemeinern Residualblöcke zu Tandemblöcken, die beliebige lineare Zuordnungen anstelle von Verknüpfungen verwenden, und verbessern die Leistung gegenüber ResNets.
Diese Arbeit führt eine Analyse der Verknüpfungen in ResNet-ähnlichen Architekturen durch und schlägt vor, die Identitätsverknüpfungen durch eine alternative Convolutional Verknüpfung zu ersetzen, der als Tandemblock bezeichnet wird.
Dieser Artikel untersucht die Auswirkungen des Ersetzens von Identitäts-Sprungverbindungen durch trainierbare Convolutional Sprungverbindungen in ResNet und stellt fest, dass sich die Leistung verbessert.
Wir stellen einen neuen Rahmen für die Anpassung von Methoden des Typs Adam vor, nämlich AdamT, um die Trend Informationen bei der Aktualisierung der Parameter mit der adaptiven Schrittgröße und den Gradienten einzubeziehen.
Eine neue Art von Adam-Variante, die die lineare Methode von Holt zur Berechnung des geglätteten Impulses erster und zweiter Ordnung verwendet, anstatt den exponentiell gewichteten Durchschnitt zu verwenden.
Eine Methode zur Erläuterung eines Klassifizierers durch Erzeugung einer visuellen Störung eines Bildes, indem die semantischen Merkmale, die der Klassifizierer mit einer Zielbezeichnung assoziiert, übertrieben oder vermindert werden.
Ein Modell, das, wenn eine Abfrage in eine Blackbox eingegeben wird, versucht, das Ergebnis zu erklären, indem es plausible und progressive Variationen der Abfrage liefert, die zu einer Änderung der Ausgabe führen können.
Eine Methode zur Erklärung des Ergebnisses einer Black-Box-Klassifizierung von Bildern, die eine allmähliche Störung der Ergebnisse als Reaktion auf allmählich gestörte Eingangsabfragen erzeugt.
Wir stellen einen einflussgesteuerten Ansatz vor, um Erklärungen für das Verhalten von tiefen Convolutional Netzwerken zu finden, und zeigen, wie dieser Ansatz verwendet werden kann, um eine breite Palette von Fragen zu beantworten, die mit früheren Arbeiten nicht beantwortet werden konnten.
Eine Methode zur Messung des Einflusses, die bestimmte Axiome erfüllt, und ein Begriff des Einflusses, der verwendet werden kann, um festzustellen, welcher Eingabeteil den größten Einfluss auf die Ausgabe eines Neurons in einem tiefen neuronalen Netz hat.
In diesem Beitrag wird vorgeschlagen, den Einfluss einzelner Neuronen in Bezug auf eine interessierende Größe zu messen, die von einem anderen Neuron repräsentiert wird.
Wir stellen eine Technik vor, mit der Systeme zur Verarbeitung natürlicher Sprache ein neues Wort aus dem Kontext lernen können, wodurch sie wesentlich flexibler werden.
Eine Technik zur Nutzung von Vorwissen, um Einbettungsrepräsentationen für neue Wörter mit minimalen Daten zu lernen.
Eine Speicherarchitektur zur Unterstützung des schlussfolgernden Denkens.
Dieses Papier schlägt Änderungen an der Ende-zu-Ende Speicher-Netzwerk Architektur vor, stellt eine neue Paired-Associative-Inference Aufgabe vor, die die meisten bestehenden Modelle nur schwierig lösen können, und zeigt, dass die vorgeschlagene Architektur die Aufgabe besser löst.
Eine neue Aufgabe (Paired Associate Inference) aus der kognitiven Psychologie und ein Vorschlag für eine neue Speicherarchitektur mit Eigenschaften, die eine bessere Leistung bei der Paired Associate-Aufgabe ermöglichen.
In der Tiefe trennbare Convolutions verbessern die neuronale maschinelle Übersetzung: je trennbarer, desto besser.
In dieser Arbeit wird vorgeschlagen, in einem vollständig Convolutional neuronalen maschinellen Übersetzungsmodell tiefenweise trennbare Convolutional Layers zu verwenden, und es wird eine neue super-trennbare Convolutional Layer eingeführt, die die Rechenkosten weiter reduziert.
Nicht sättigendes GAN-Training minimiert effektiv eine umgekehrte KL-ähnliche f-Divergenz.
In diesem Beitrag wird ein nützlicher Ausdruck für die Klasse der f-Divergenzen vorgeschlagen, die theoretischen Eigenschaften der beliebten f-Divergenzen anhand neu entwickelter Werkzeuge untersucht und GANs mit dem nicht-sättigenden Trainingsschema untersucht.
Wir stellen eine neuartige Textdarstellungsmethode vor, die es ermöglicht, Bildklassifikatoren auf Textklassifizierungsprobleme anzuwenden, und wenden die Methode auf die Disambiguierung von Erfindernamen für Patente an.
Eine Methode zur Abbildung eines Paares von Textinformationen in ein 2D-RGB-Bild, das in 2D Convolutional neuronale Netze (Bildklassifizierer) eingespeist werden kann.
Die Autoren befassen sich mit dem Problem der Disambiguierung von Erfindernamen für Patente und schlagen vor, eine Bildseiten-Darstellung der beiden zu vergleichenden Namensstränge zu erstellen und einen Bildklassifikator anzuwenden.
Wir haben das Modell "Difference-Seeking Generative Adversarial Network" (DSGAN) vorgeschlagen, um die Zielverteilung zu erlernen, für die es schwierig ist, Trainingsdaten zu sammeln.
In diesem Papier wird DS-GAN vorgestellt, das darauf abzielt, den Unterschied zwischen zwei beliebigen Verteilungen zu erlernen, deren Stichproben schwer oder gar nicht zu erheben sind, und das seine Effektivität bei halbüberwachten Lern- und gegnerischen Trainingsaufgaben zeigt.
In diesem Papier wird das Problem des Lernens eines GAN zur Erfassung einer Zielverteilung mit nur sehr wenigen verfügbaren Trainingsstichproben aus dieser Verteilung betrachtet.
Eine allgemeine Methode zur Verbesserung der Bildübersetzungsleistung des GAN-Rahmens durch Verwendung eines in die Aufmerksamkeit eingebetteten Diskriminators.
Ein Feedback-Mechanismus im GAN-Rahmen, der die Qualität der erzeugten Bilder bei der Bild-zu-Bild-Übersetzung verbessert und dessen Diskriminator eine Karte ausgibt, die angibt, worauf sich der Generator konzentrieren sollte, um seine Ergebnisse überzeugender zu machen.
Vorschlag für ein GAN mit einem aufmerksamkeitsbasierten Diskriminator für die I2I-Übersetzung, der die Wahrscheinlichkeit von echt/falsch und eine Aufmerksamkeitszuordnung liefert, die die Auffälligkeit für die Bilderzeugung widerspiegelt.
Wir schlagen einen neuen Datensatz vor, um das Entailment-Problem unter halbstrukturierten Tabellen als Prämisse zu untersuchen.
In diesem Papier wird ein neuer Datensatz für die tabellenbasierte Faktenüberprüfung vorgeschlagen und es werden Methoden für diese Aufgabe vorgestellt.
Die Autoren stellen das Problem der Faktenüberprüfung mit halbstrukturierten Datenquellen wie Tabellen vor, erstellen einen neuen Datensatz und evaluieren Basismodelle mit Variationen.
Wir entwickeln eine Deep Graph Matching-Architektur, die anfängliche Korrespondenzen verfeinert, um einen nachbarschaftlichen Konsens zu erreichen.
Ein Rahmen für die Beantwortung von Fragen zum Graphenabgleich, bestehend aus lokalen Knoteneinbettungen mit einem Verfeinerungsschritt durch Nachrichtenübermittlung.
Eine zweistufige GNN-basierte Architektur zur Herstellung von Korrespondenzen zwischen zwei Graphen, die sich bei realen Aufgaben des Bildabgleichs und des Abgleichs von Wissensgraphen gut bewährt.
Dieser Beitrag erweitert den Nachweis der Dichte von neuronalen Netzen im Raum der kontinuierlichen (oder sogar messbaren) Funktionen auf euklidischen Räumen auf Funktionen über kompakten Mengen von Wahrscheinlichkeitsmaßen. 
In diesem Beitrag werden die Approximationseigenschaften einer Familie neuronaler Netze untersucht, die für die Bewältigung von Lernproblemen mit mehreren Instanzen entwickelt wurden, und es wird gezeigt, dass die Ergebnisse für standardmäßige einschichtige Architekturen auch für diese Modelle gelten.
Diese Arbeit verallgemeinert den universellen Approximationssatz auf reelle Funktionen im Raum der Maße.
Ein neuer Rahmen für kontextabhängige und kontextfreie Erklärungen von Vorhersagen
Die Autoren erweitern die lineare lokale Attributionsmethode LIME zur Interpretation von Black-Box-Modellen und schlagen eine Methode zur Unterscheidung zwischen kontextabhängigen und kontextfreien Interaktionen vor.
Eine Methode, die hierarchische Erklärungen für ein Modell liefern kann, einschließlich kontextabhängiger und kontextfreier Erklärungen durch einen lokalen Interpretationsalgorithmus.
Das Fine-Tuning nach der Quantisierung entspricht oder übertrifft den Stand der Technik in Bezug auf Netzwerke mit voller Genauigkeit sowohl bei 8- als auch bei 4-Bit Quantisierung.
In diesem Beitrag wird vorgeschlagen, die Leistung von Modellen mit geringer Genauigkeit zu verbessern, indem die Quantisierung an vortrainierten Modellen durchgeführt wird, große Batchgrößen verwendet werden und eine geeignete Lernrate beim auskühlen mit längerer Trainingszeit verwendet wird.
Eine Methode für niedrige Bit-Quantisierung, um Inferenz auf effizienter Hardware zu ermöglichen, die volle Genauigkeit auf ResNet50 mit 4-Bit-Gewichten und -Aktivierungen erreicht, basierend auf der Beobachtung, dass Fine-Tuning bei niedriger Präzision Störungen im Gradienten einführt.
Zwei Methoden, die auf der Representational Similarity Analysis (RSA) und Tree Kernels (TK) basieren und direkt quantifizieren, wie stark die in neuronalen Aktivierungsmustern kodierte Information mit der durch symbolische Strukturen repräsentierten Information übereinstimmt.
In diesem Artikel wird ein Rahmen für dateneffizientes Repräsentationslernen durch adaptives Sampling im latenten Raum vorgestellt.
Eine Methode zur sequentiellen und adaptiven Auswahl von Trainingsbeispielen, die dem Trainingsalgorithmus vorgelegt werden, wobei die Auswahl im latenten Raum auf der Grundlage der Auswahl von Beispielen in Richtung des Gradienten des Verlustes erfolgt.
Eine Methode zur effizienten Auswahl harter Proben während des Trainings neuronaler Netze, die durch einen variationalen Autoencoder erreicht wird, der Proben in einem latenten Raum kodiert.
Eine auf adversarialem Training basierende Methode zur Unterscheidung von zwei komplementären Variationsgruppen in einem Datensatz, von denen nur eine gekennzeichnet ist, getestet an Stil und Inhalt von Anime-Illustrationen.
Eine Methode zur Bilderzeugung, die bedingte GANs und bedingte VAEs kombiniert, um originalgetreue Anime-Bilder mit verschiedenen Stilen von verschiedenen Künstlern zu erzeugen. 
Vorschlag für eine Methode zum Erlernen von entkoppelten Stil- (Künstler-) und Inhaltsdarstellungen in Anime.
Wir führen eine Glättungsregularisierung für Convolutional Kernels von CNN ein, die dazu beitragen kann, die adversariale Robustheit zu verbessern und zu wahrnehmungsgerechten Gradienten zu führen
In diesem Beitrag wird ein neues Regularisierungsschema vorgeschlagen, das die Convolutional Kernel glättet. Es wird argumentiert, dass eine geringere Abhängigkeit des neuronalen Netzes von hochfrequenten Komponenten die Robustheit gegenüber feindlichen Beispielen erhöht. 
Die Autoren schlagen eine Methode zum Erlernen glatterer Convolutional Kernels vor, insbesondere einen Regularisierer, der große Änderungen zwischen aufeinanderfolgenden Pixeln des Kernels bestraft, mit der Intuition, die Verwendung hochfrequenter Eingangskomponenten zu bestrafen.
Wir untersuchen das Verhalten der Q-Wert-Schätzungen bei großen Stichproben und schlagen eine effiziente Erkundungsstrategie vor, die sich auf die Schätzung der relativen Diskrepanzen zwischen den Q-Schätzungen stützt. 
Wir trainieren Worteinbettungen auf der Grundlage von Entailment anstelle von Ähnlichkeit und sagen erfolgreich lexikalisches Entailment voraus.
In diesem Beitrag wird ein Worteinbettungsalgorithmus für lexikalisches Entailment vorgestellt, der sich an die Arbeit von Henderson und Popa (ACL, 2016) anlehnt.
Unüberwachtes Lernen für Reinforcement Learning unter Verwendung eines automatischen Lehrplans für das Selbstspiel.
Eine neue Formulierung für die unbeaufsichtigte Erkundung der Umgebung, um später bei einer bestimmten Aufgabe zu helfen, wobei ein Agent immer schwierigere Aufgaben vorschlägt und der lernende Agent versucht, sie zu erfüllen.
Ein Selbstspielmodell, bei dem ein Agent lernt, Aufgaben vorzuschlagen, die für ihn leicht, für einen Gegner aber schwierig sind, wodurch ein sich bewegendes Ziel von Selbstspielzielen und Lernplänen entsteht. 
Nutzung reichhaltiger struktureller Details in graphenstrukturierten Daten durch adaptive struktuelle Fingerabdrücke.
Eine auf Graphenstrukturen basierende Methodik zur Erweiterung des Aufmerksamkeitsmechanismus von graphischen neuronalen Netzen, mit der Hauptidee, Interaktionen zwischen verschiedenen Arten von Knoten in der lokalen Nachbarschaft eines Wurzelknotens zu untersuchen.
Diese Arbeit erweitert die Idee der Selbstaufmerksamkeit in Graphen-NNs, die typischerweise auf der Ähnlichkeit von Merkmalen zwischen Knoten basiert, um strukturelle Ähnlichkeit miteinzubeziehen.
Wir schlagen einen skalierbaren Bayes'schen Reinforcement Learning Algorithmus vor, der eine Bayes'sche Korrektur über ein Ensemble von hellsichtigen Experten erlernt, um Probleme mit komplexen latenten Belohnungen und Dynamiken zu lösen.
Diese Arbeit betrachtet das Bayesian Reinforcement Learning Problem über latente Markov Decision Processes (MDPs), indem Entscheidungen mit Experten getroffen werden.
In diesem Beitrag motivieren die Autoren einen Lernalgorithmus, genannt Bayesian Residual Policy Optimization (BRPO), für Bayesian Reinforcement Learning Probleme und schlagen ihn vor.
Wir beweisen, dass der Gradientenabstieg bei überparametrisierten neuronalen Netzen einen Trainingsverlust von Null mit einer linearen Rate erreicht.
Diese Arbeit befasst sich mit der Optimierung eines zweischichtigen überparametrisierten ReLU-Netzes mit quadratischen Verlust und einem Datensatz mit willkürlichen Labels.
In diesem Papier werden neuronale Netze mit einer versteckten Schicht und quadratischem Verlust untersucht. Es wird gezeigt, dass in einer überparametrisierten Umgebung eine zufällige Initialisierung und ein Gradientenabstieg zu einem Verlust von Null führt.
Analyse von inversen Problemen mit invertierbaren neuronalen Netzen.
Der Autor schlägt vor, invertierbare Netzwerke zu verwenden, um mehrdeutige inverse Probleme zu lösen, und schlägt vor, nicht nur das Forward Modell, sondern auch das inverse Modell mit einem MMD-Kritiker zu trainieren.
Die Forschungsarbeit schlägt ein invertierbares Netzwerk mit Beobachtungen für die posteriore Wahrscheinlichkeit von komplexen Eingangsverteilungen mit einem theoretisch gültigen bidirektionalen Trainingsschema vor.
Leistungskennzahlen sind unvollständige Angaben; der Zweck heiligt nicht immer die Mittel.
Die Autoren zeigen, wie Meta-Lernen die versteckten Anreize für Verteilungsverschiebungen aufdeckt, und schlagen einen Ansatz vor, der auf dem Austausch von Lernenden zwischen Umgebungen basiert, um selbst verursachte Verteilungsverschiebungen zu reduzieren.
Der Artikel verallgemeinert den inhärenten Anreiz für den Lernenden zu gewinnen, indem es die Aufgabe beim Meta-Lernen auf eine größere Klasse von Problemen ausweitet.
Wir schlagen einen Ansatz zur Erkennung von Anomalien vor, bei dem die Modellierung der Vordergrundklasse über mehrere lokale Dichten mit einem adversarialem Training kombiniert wird.
Der Beitrag schlägt eine Technik vor, um generative Modelle robuster zu machen, indem man sie mit der lokalen Dichte in Einklang bringt.
Wir schlagen eine GAN-Variante vor, die lernt, Punktwolken zu erzeugen. Es wurden verschiedene Studien durchgeführt, darunter eine engere Wasserstein-Abstandsschätzung, bedingte Erzeugung, Verallgemeinerung auf ungesehene Punktwolken und Bild zu Punktwolken.
In diesem Artikel wird vorgeschlagen, GAN zur Erzeugung von 3D-Punktwolken zu verwenden und ein Sandwiching-Ziel einzuführen, das die obere und untere Grenze des Wasserstein-Abstands zwischen den Verteilungen mittelt.
Dieser Artikel schlägt ein neues generatives Modell für ungeordnete Daten vor, mit einer besonderen Anwendung auf Punktwolken, das eine Inferenzmethode und eine neuartige Zielfunktion beinhaltet. 
In diesem Beitrag wird ein neuartiger Ansatz für Aufmerksamkeitsmechanismen vorgestellt, der für eine Reihe von Aufgaben wie maschinelle Übersetzung und Bildbeschriftung von Nutzen sein kann.
In diesem Beitrag werden die derzeitigen Aufmerksamkeitsmodelle von der Wortebene auf die Kombination benachbarter Wörter ausgedehnt, indem die Modelle auf Elemente angewendet werden, die aus zusammengefügten benachbarten Wörtern bestehen.
Wir identifizieren ein Phänomen, die neuronale Gehirnwäsche, und führen einen statistisch begründeten Gewichts Plastizitätverlust durch, um dies zu überwinden.
In diesem Beitrag wird das Phänomen der neuralen Gehirnwäsche erörtert, das sich darauf bezieht, dass die Leistung eines Modells durch ein anderes Modell beeinflusst wird, das Parameter des Modells teilt.
In diesem Beitrag wird Morpho-MNIST vorgestellt, eine Sammlung von Formmetriken und Störungen, die einen Schritt zur quantitativen Evaluierung des Repräsentationslernens darstellt.
In diesem Beitrag wird das Problem der Bewertung und Diagnose der mit einem generativen Modell erlernten Darstellungen erörtert.
Die Autoren stellen eine Reihe von Kriterien zur Kategorisierung von MNISt-Digisten und eine Reihe interessanter Störungen zur Modifizierung des MNIST-Datensatzes vor.
Strukturierte Exploration im tiefen Reinforcement Learning durch unüberwachte visuelle Abstraktionsentdeckung und -kontrolle
In dem Artikel werden visuelle Abstraktionen vorgestellt, die für das Reinforcement Learning verwendet werden, bei dem ein Algorithmus lernt, jede Abstraktion zu "kontrollieren" und die Optionen auszuwählen, um die Gesamtaufgabe zu erfüllen.
Ein neuer Strategie Gradient Algorithmus, der für die Lösung von kombinatorischen Black-Box-Optimierungsproblemen entwickelt wurde. Der Algorithmus stützt sich nur auf Funktionsbewertungen und liefert mit hoher Wahrscheinlichkeit lokal optimale Lösungen.
Die Arbeit schlägt einen Ansatz zur Konstruktion von Ersatzzielen für die Anwendung von Policy-Gradienten-Methoden in der kombinatorischen Optimierung vor, um die Notwendigkeit der Abstimmung von Hyperparametern zu verringern.
In dem Artikel wird vorgeschlagen, den Belohnungsbegriff im Policy-Gradienten-Algorithmus durch seine zentrierte empirische kumulative Verteilung zu ersetzen. 
Schnelle, kalibrierte Unsicherheitsabschätzung für neuronale Netze ohne Stichproben
In diesem Beitrag wird ein neuartiger Ansatz zur Schätzung der Zuverlässigkeit von Vorhersagen in einer Regressionsumgebung vorgeschlagen, der die Tür zu Online-Anwendungen mit vollständig integrierten Unsicherheitsschätzungen öffnet.
In diesem Artikel wird die tiefe evidenzbasierte Regression vorgeschlagen, eine Methode zum Trainieren neuronaler Netze, die nicht nur die Ausgabe, sondern auch die damit verbundenen Beweise zur Unterstützung dieser Ausgabe schätzt.
Wir schlagen einen neuen Algorithmus vor, der mittels neuronaler Netze schnell Lotterie Gewinner-Lose findet.
In diesem Artikel wird eine neuartige Zielfunktion vorgeschlagen, die zur gemeinsamen Optimierung eines Klassifizierungsziels verwendet werden kann und gleichzeitig die Sparsifikation in einem Netz fördert, das eine hohe Genauigkeit aufweist.
In dieser Arbeit wird eine neue iterative Pruning Methode mit dem Namen Continuous Sparsification vorgeschlagen, die das aktuelle Gewicht kontinuierlich pruned, bis das Zielverhältnis erreicht ist.
Einführung einer formalen Einstellung für das budgetierte Training und Vorschlag für einen budgetgerechten linearen Lernratenplan.
In dieser Arbeit wird eine Technik zur Abstimmung der Lernrate für das Training neuronaler Netze bei einer festen Anzahl von Epochen vorgestellt.
In diesem Beitrag wurde untersucht, welcher Lernratenplan verwendet werden sollte, wenn die Anzahl der Iterationen begrenzt ist, wobei ein neues Konzept, der BAS (Budget-Aware Schedule), verwendet wurde.
Wir führen die Erkundung mit Hilfe von intrinsischen Belohnungen durch, die auf einem gewichteten Abstand der nächsten Nachbarn im Repräsentationsraum beruhen.
Diese Arbeit schlägt eine Methode zur effizienten Exploration in tabellarischen MDPs sowie eine einfache Kontrollumgebung vor, die deterministische Encoder zum Erlernen einer niedrigdimensionalen Darstellung der Umgebungsdynamik verwendet.
In diesem Beitrag wird eine Methode zur stichprobeneffizienten Exploration für RL-Agenten vorgeschlagen, die eine Kombination aus modellbasierten und modellfreien Ansätzen mit einer Neuheitsmetrik verwendet.
Die Robustheit trainierter PGD-Modelle reagiert empfindlich auf semantikerhaltende Transformationen von Bilddatensätzen, was bedeutet, dass die Bewertung robuster Lernalgorithmen in der Praxis heikel ist.
Wir schlagen einen Rangordnungsregel-Gradienten vor, der die optimale Rangordnung von Aktionen erlernt, um den Ertrag zu maximieren. Wir schlagen einen allgemeinen Off-Policy Lern Framework mit den Eigenschaften der Optimalitätserhaltung, Varianzreduktion und Stichproben-Effizienz vor.
In diesem Artikel wird vorgeschlagen, die Politik durch eine Form der Rangfolge neu zu parametrisieren, um das RL-Problem in ein überwachtes Lernproblem umzuwandeln.
In diesem Papier wird eine neue Sichtweise auf Regel Gradienten Methoden aus der Perspektive des Rankings vorgestellt. 
Durch die Kombination von Klassifizierung und Bildabfrage in einer neuronalen Netzwerkarchitektur erzielen wir eine Verbesserung für beide Aufgaben.
In diesem Beitrag wird eine einheitliche Einbettung für die Bildklassifizierung und den Instanzenabruf vorgeschlagen, um die Leistung für beide Aufgaben zu verbessern.
In der Arbeit wird vorgeschlagen, ein tiefes neuronales Netz für die Bildklassifizierung, Instanz- und Kopiererkennung gemeinsam zu trainieren.
Wir untersuchen die Abbildung der Hyponymie-Relation von Wortordnungen auf Merkmalsvektoren
In dieser Arbeit wird untersucht, wie Hyponymie zwischen Wörtern auf Merkmalsrepräsentationen abgebildet werden kann.
In diesem Beitrag wird der Begriff der Hyponymie in Wortvektordarstellungen untersucht und eine Methode beschrieben, mit der WordNet-Beziehungen in einer Baumstruktur organisiert werden, um Hyponymie zu definieren.
Wir entwickeln einen leistungsfähigeren Generator für natürliche Sprache, indem wir diskriminierende Bewertungsfunktionen trainieren, die die Kandidatengenerationen in Bezug auf verschiedene Eigenschaften guten Schreibens einstufen.
In diesem Werk wird vorgeschlagen, mehrere induktive Verzerrungen zusammenzuführen, die Inkonsistenzen bei der Sequenzdekodierung korrigieren sollen, und die Parameter einer vordefinierten Kombination verschiedener Teilziele zu optimieren. 
Diese Arbeit kombiniert das RNN-Sprachmodell mit mehreren diskriminativ trainierten Modellen, um die Spracherzeugung zu verbessern.
In diesem Beitrag wird vorgeschlagen, die Generierung von RNN-Sprachmodellen mit Hilfe erweiterter Zielsetzungen zu verbessern, die sich an Grice' Kommunikationsmaximen orientieren.
Skalierbare und kommunikationsarme Lastausgleichslösung für heterogene Server-Multi-Dispatcher-Systeme mit starken theoretischen Garantien und vielversprechenden empirischen Ergebnissen. 
Ein quantitatives Maß zur Vorhersage der Leistung von tiefen neuronalen Netzmodellen.
Diese Arbeit schlägt eine neuartige Größe vor, die die Anzahl der Pfade im neuronalen Netz zählt, die die Leistung neuronaler Netze mit der gleichen Anzahl von Parametern vorhersagt.
Diese Arbeit stellt eine Methode zum Zählen von Pfaden in tiefen neuronalen Netzen vor, die wohl zur Messung der Leistung des Netzes verwendet werden kann.
In dieser Arbeit wird eine rigorose Studie darüber vorgelegt, warum praktisch verwendete Lernratenschemata (für ein gegebenes Rechenbudget) erhebliche Vorteile bieten, obwohl diese Schemata von der klassischen Theorie der stochastischen Approximation nicht befürwortet werden.
In diesem Beitrag wird eine theoretische Untersuchung verschiedener Lernratenpläne vorgestellt, die zu statistischen Minimax-Untergrenzen sowohl für Polynom- als auch für Constant-and-Cut-Schemata führte.
Der Beitrag untersucht die Auswirkungen der Wahl der Lernrate bei stochastischer Optimierung, wobei der Schwerpunkt auf Least-Mean-Squares mit abnehmender Schrittweite liegt.
Wir stellen Planer vor, die auf Convnets basieren, die stichprobeneffizient sind und sich auf größere Instanzen von Navigations- und Wegfindungsproblemen verallgemeinern lassen.
Es werden Methoden vorgeschlagen, die als Abwandlungen von Value Iteration Networks (VIN) betrachtet werden können, mit einigen Verbesserungen, die auf die Verbesserung der Stichprobeneffizienz und die Verallgemeinerung auf große Umgebungsgrößen abzielen.
Das Papier stellt eine Erweiterung der originalen Werteiterations Netzwerke (VIN) durch die Berücksichtigung einer zustandsabhängigen Übergangsfunktion.
Erlernen einer besseren Einbettung von Bereichen durch lebenslanges Lernen und Meta Lernen
Stellt eine Methode des lebenslangen Lernens zum Erlernen von Worteinbettungen vor.
Diese Arbeit schlägt einen Ansatz zum Erlernen von Einbettungen in neuen Domänen vor und übertrifft die Baseline in einer Aspekt-Extraktionsaufgabe deutlich. 
Wir schlagen eine neue regularisierungsbasierte Pruning Methode (IncReg genannt) vor, um verschiedene Regularisierungsfaktoren schrittweise verschiedenen Gewichtsgruppen auf der Grundlage ihrer relativen Bedeutung zuzuordnen.
In diesem Artikel wird eine auf Regularisierung basierende Pruning Methode vorgeschlagen, um verschiedene Regularisierungsfaktoren schrittweise verschiedenen Gewichtsgruppen auf der Grundlage ihrer relativen Bedeutung zuzuordnen.
Bestehende Momentum-/Beschleunigungsverfahren wie die Heavy-Ball-Methode und die Nesterov-Beschleunigung, die mit stochastischen Gradienten eingesetzt werden, bringen keine Verbesserung gegenüber dem einfachen stochastischen Gradientenabstieg, insbesondere wenn sie mit kleinen Batchgrößen eingesetzt werden.
Wir zeigen, dass Überbelegungs-Planungsaufgaben mit A* gelöst werden können, und stellen neuartige beschränkungssensitive Heuristiken für Überbelegungs-Planungsaufgaben vor.
Es wird ein Ansatz zur optimalen Lösung von Überbelegungsplanungsaufgaben (OSP) vorgestellt, der eine Übersetzung der klassischen Planung mit mehreren Kostenfunktionen verwendet.
In der Arbeit werden Änderungen an zulässigen Heuristiken vorgeschlagen, um sie in einem Multi-Kriterien Umfeld besser zu informieren.
Wir entwickeln Meta-Lernmethoden für adversarial robustes Few-Shot Learning.
In diesem Beitrag wird eine Methode vorgestellt, die die Robustheit des Few-Shot Lernens durch die Einführung eines Angriffs auf die Abfragedaten in der Fine-Tuning Phase eines Meta-Lernalgorithmus verbessert.
Die Autoren dieser Arbeit schlagen einen neuen Ansatz für das Training eines robusten Few-Shot Modells vor. 
Wir stellen fest, dass das Pooling allein nicht für die Deformationsstabilität von CNNs ausschlaggebend ist und dass die Filter Glätte eine wichtige Rolle für die Stabilität spielt.
Wir schlagen ein Self-Ensemble Framework vor, um robustere Deep Learning Modelle unter störhaften, gelabelten Datensätzen zu trainieren.
In dieser Arbeit wird eine "Self-Ensemble-Label-Filterung" für das Lernen mit störhaften Labels vorgeschlagen, bei der das Label stören instanzunabhängig ist, was eine genauere Identifizierung inkonsistenter Vorhersagen ermöglicht. 
In diesem Artikel wird ein Algorithmus für das Lernen aus Daten mit verrauschten Etiketten vorgeschlagen, der abwechselnd das Modell aktualisiert und Beispiele entfernt, die aussehen, als hätten sie störende Labels.
Wir untersuchen das Pruning von DNNs vor dem Training und geben eine Antwort auf die Frage, welche Topologie für das Training von a priori spärlichen Netzwerken verwendet werden sollte.
Die Autoren schlagen vor, dichte Schichten durch spärlich verbundene lineare Schichten zu ersetzen und einen Ansatz zu finden, um die beste Topologie zu finden, indem gemessen wird, wie gut die spärlichen Schichten die zufälligen Gewichte ihrer dichten Gegenstücke approximieren.
Die Arbeit schlägt eine dünn besetzte Kaskadenarchitektur vor, die eine Multiplikation mehrerer dünn besetzter Matrizen und ein spezielles Konnektivitätsmuster ist, das andere Überlegungen übertrifft.
Wir präsentieren Multitask Neural Model Search, einen Meta-Learner, der Modelle für mehrere Aufgaben gleichzeitig entwerfen und das Lernen auf unbekannte Aufgaben übertragen kann.
In dieser Arbeit wird die neuronale Architektursuche auf das Problem des Multitasking-Lernens ausgedehnt, bei dem ein aufgabenabhängiger Controller für die Modellsuche gelernt wird, um mehrere Aufgaben gleichzeitig zu bewältigen.
In diesem Beitrag fassen die Autoren ihre Arbeit an ein Framework zusammen, das als Multitask Neural Model Search Controller bezeichnet wird und die automatische Konstruktion neuronaler Netze für mehrere Aufgaben gleichzeitig ermöglicht.
Wir modellieren nicht-lineare visuelle Prozesse als autoregressive Störungen mittels generativem Deep Learning.
Vorschlagen einer neuen Methode, die nichtlineare visuelle Prozesse mit einer tiefen Version eines linearen Prozesses (Markov-Prozess) modelliert.
In diesem Artikel wird ein neues tiefes generatives Modell für Sequenzen, insbesondere Bildsequenzen und Videos, vorgeschlagen, das in einem Teil des Modells eine lineare Struktur verwendet.
In diesem Artikel wird ein neues Feed-Forward-Netzwerk, das so genannte PDE-Net, vorgeschlagen, um PDEs aus Daten zu lernen. 
Diese Arbeit erläutert die Verwendung von Deep Learning Maschinen zum Zweck der Identifizierung dynamischer Systeme, die durch PDEs spezifiziert sind.
Der Artikel schlägt einen auf einem neuronalen Netz basierenden Algorithmus für das Lernen aus Daten vor, die sich aus dynamischen Systemen mit Gleichungen ergeben, die als partielle Differentialgleichungen geschrieben werden können.
Diese Arbeit befasst sich mit der Modellierung komplexer dynamischer Systeme durch nichtparametrische partielle Differentialgleichungen unter Verwendung neuronaler Architekturen, wobei die wichtigste Idee des Papiers (PDE-Netz) darin besteht, sowohl Differentialoperatoren als auch die Funktion, die die PDE regelt, zu lernen.
Wir geben ein schnelles normalisierungsflussähnliches Stichprobenverfahren für diskrete latente Variablenmodelle an.
In dieser Arbeit wird eine autoregressive Filter-Variationsapproximation zur Parameterschätzung in diskreten dynamischen Systemen unter Verwendung von Fixpunkt-Iterationen verwendet.
Die Autoren setzen eine allgemeine autoregressive Posterior-Familie voraus für diskrete Variablen oder deren kontinuierliche Entlastungen. 
Dieser Artikel hat zwei Hauptbeiträge: Es erweitert Normalisierungsflüsse auf diskrete Einstellungen und stellt eine ungefähre Festpunkt-Aktualisierungsregel für autoregressive Zeitreihen vor, die die GPU-Parallelität nutzen kann. 
Wir schlagen einen Rahmen vor, der lernt, Wissen symbolisch zu kodieren und Programme zu generieren, um über das kodierte Wissen nachzudenken.
Die Autoren schlagen die N-Gram-Maschine vor, um Fragen über lange Dokumente zu beantworten.
In dieser Arbeit wird die n-Gramm-Maschine vorgestellt, ein Modell, das Sätze in einfache symbolische Darstellungen kodiert, die effizient abgefragt werden können.
In diesem Artikel wird ein Meta-Lernziel vorgeschlagen, das auf der Geschwindigkeit der Anpassung an Transferverteilungen basiert, um eine modulare Dekomposition und kausale Variablen zu entdecken.
Diese Arbeit zeigt, dass sich ein Modell mit der richtigen Grundstruktur schneller an eine kausale Intervention anpasst als ein Modell mit der falschen Struktur.
In dieser Arbeit schlugen die Autoren einen allgemeinen und systematischen Rahmen für ein Meta-Transfer Ziel vor, das das Lernen der kausalen Struktur bei unbekannten Interventionen einschließt.
Eine andere Perspektive des katastrophalen Vergessens.
In diesem Beitrag wird ein Framework zur Bekämpfung des katastrophalen Vergessens vorgestellt, der auf der Änderung des Verlustterms beruht, um die Änderungen der Klassifizierungswahrscheinlichkeit zu minimieren, die durch eine Taylor-Reihen-Approximation erzielt wird.
Diese Arbeit versucht, das Problem des kontinuierlichen Lernens zu lösen, indem es sich auf Regularisierungsansätze konzentriert und eine L_1-Strategie zur Entschärfung des Problems vorschlägt.
Wir schlagen einen Ansatz zur Konstruktion realistischer veränderbarer 3D-Gesichtsmodelle (3DMM) vor, der einen intuitiven Arbeitsablauf zur Bearbeitung von Gesichtsattributen durch die Auswahl der besten Sätze von Eigenvektoren und anthropometrischen Messungen ermöglicht.
Schlägt ein stückweise veränderbares Modell für menschliche Gesichtsnetze vor und schlägt auch eine Zuordnung zwischen anthropometrischen Messungen des Gesichts und den Parametern des Modells vor, um Gesichter mit gewünschten Attributen zu synthetisieren und zu bearbeiten. 
In diesem Beitrag wird eine Methode für ein teilbasiertes veränderbares Gesichtsmodell beschrieben, das eine lokalisierte Benutzersteuerung ermöglicht.
Zwei Algorithmen schnitten bei einem EEG-basierten BCI-Experiment besser ab als acht andere.
Wir bringen Agenten bei, nur mit Hilfe von Reinforcement Learning zu verhandeln; egoistische Agenten können dies tun, aber nur über einen vertrauenswürdigen Kommunikationskanal, und prosoziale Agenten können mit billigem Gerede verhandeln.
Die Autoren beschreiben eine Variante des Verhandlungsspiels mit der Berücksichtigung eines zweiten Kommunikationskanals für billiges Sprechen und stellen fest, dass der zweite Kanal die Verhandlungsergebnisse verbessert.
In diesem Beitrag wird untersucht, wie Agenten lernen können, zu kommunizieren, um eine Verhandlungsaufgabe zu lösen, und es wird festgestellt, dass prosoziale Agenten in der Lage sind, mit Hilfe von RL zu lernen, Symbole zu erden, eigennützige Agenten jedoch nicht.
Untersucht die Frage, wie Agenten die Kommunikation nutzen können, um ihre Gewinne in einem einfachen Verhandlungsspiel zu maximieren.
Wir schlagen ein neuartiges Meta-Lern Framework für transduktive Inferenz vor, das die gesamte Testmenge auf einmal klassifiziert, um das Problem der geringen Datenmenge zu lindern.
Diese Arbeit schlägt vor, das Few-Shot Lernen auf eine transduktive Art und Weise anzugehen, indem es ein Label Propagations Modell in einer Ende-zu-Ende Weise lernt. Es ist das erste System, das Label Propagation für transduktives Few-Shot Lernen lernt und effektive empirische Ergebnisse produziert. 
In dieser Arbeit wird ein Meta-Lernsystem vorgeschlagen, das unbeschriftete Daten durch das Erlernen der graphbasierten Label-Propogation in einer Ende-zu-Ende Weise nutzt.
Studien zum Few-Shot Lernen in einer transduktiven Umgebung: Verwendung von Meta-Lernen, um zu lernen, wie man Labels von Trainingsbeispielen auf Testbeispiele überträgt. 
Wir beschreiben den Einsatz eines automatisierten Planungssystems für die Entwicklung von Beobachtungsstrategien und die Planung des Betriebs der ECOSTRESS-Mission der NASA.
In diesem Beitrag wird eine Anpassung eines automatischen Planungssystems, CLASP, vorgestellt, um ein EO-Experiment (ECOSTRESS) auf der ISS zu planen. 
Die hybride Speicherung und Darstellung von gelerntem Wissen kann ein Grund für adversarial Beispiele sein.
Neue Experimente und Theorie zum Adam basierten Q-Learning.
Diese Arbeit liefert ein Konvergenzergebnis für traditionelles Q-Lernen mit linearer Funktionsannäherung bei Verwendung einer Adam-ähnlichen Aktualisierung. 
In diesem Beitrag wird eine Methode zur Verbesserung des AltQ-Algorithmus beschrieben, bei der eine Kombination aus einem Adam-Optimierer und einem regelmäßigen Neustart der internen Parameter des Adam-Optimierers verwendet wird.
Ein neues Kapselnetz, das bei unseren Benchmark-Experimenten im Gesundheitswesen schneller konvergiert.
Stellt eine Variante von Kapselnetzwerken vor, die anstelle von EM-Routing einen linearen Unterraum verwendet, der durch den dominanten Eigenvektor der gewichteten Abstimmungsmatrix der vorherigen Kapsel aufgespannt wird.
Der Artikel schlägt eine verbesserte Routing-Methode, die Werkzeuge der Eigendekomposition verwendet, um Kapsel Aktivierung und Stellung zu finden.
Wir schlagen eine Methode zum verteilten Fine-Tuning von Sprachmodellen auf Benutzergeräten ohne Erhebung privater Daten vor.
Diese Arbeit befasst sich mit der Verbesserung von Sprachmodellen auf mobilen Geräten, die auf kleinen Textabschnitten basieren, die der Benutzer eingegeben hat, indem eine linear interpolierte Zielsetzung zwischen benutzerspezifischem Text und allgemeinem Englisch verwendet wird. 
Dieses Papier nutzt die Analyse von Lipschitz-Verlusten auf einem begrenzten Hypothesenraum, um neue ERM-Algorithmen mit starken Leistungsgarantien abzuleiten, die auf das nicht-konjugierte Sparse GP Modell angewendet werden können.
Wir schlagen eine Regularisierungsmethode für neuronale Netze und eine Methode zur Störungsanalyse vor
In diesem Beitrag wird eine neue Regularisierungsmethode vorgeschlagen, um das Problem der Überanpassung von tiefen neuronalen Netzen durch die Rotation von Merkmalen mit einer zufälligen Rotationsmatrix zu entschärfen, um die Co-Adaptation zu reduzieren.
In diesem Beitrag wird eine neuartige Regularisierungsmethode für das Training neuronaler Netze vorgeschlagen, bei der Neuronen mit Rauschen in einer unabhängigen Weise hinzugefügt werden.
Ein probabilistischer Rahmen für Multi-Agenten Reinforcement Learning.
Diese Arbeit schlägt einen neuen Algorithmus namens Multi-Agent Soft Actor-Critic (MA-SAC) vor, der auf dem Off-Policy Maximum-Entropy Actor-Critic Algorithmus Soft Actor-Critic (SAC) basiert.
Wir bieten eine kontinuierliche Entspannung des Sortieroperators, die eine durchgängige, gradientenbasierte stochastische Optimierung ermöglicht.
In diesem Beitrag wird untersucht, wie eine Reihe von Elementen sortiert werden kann, ohne dass deren tatsächliche Bedeutungen oder Werte explizit bekannt sind, und es wird eine Methode zur Durchführung der Optimierung mittels einer kontinuierlichen Entlastung vorgeschlagen.
Diese Arbeit baut auf einer sum(top k)-Identität auf, um einen pfadweise differenzierbaren Prüfer von 'unimodalen zeilenstochastischen' Matrizen abzuleiten.
Es wird eine kontinuierliche Entlastung des Sortieroperators eingeführt, um eine durchgängige gradientenbasierte Optimierung zu konstruieren, und es wird eine stochastische Erweiterung der Methode unter Verwendung von Placket-Luce-Verteilungen und Monte Carlo eingeführt.
Wir führen effizientes und flexibles Transferlernen im Framework der Bayes'schen Optimierung durch meta-gelernte neuronale Erfassungsfunktionen durch.
Die Autoren stellen MetaBO vor, das Reinforcement Learning zum Meta-Lernen der Erfassungsfunktion für Bayes'sche Optimierung einsetzt und eine zunehmende Effizienz der Stichprobe bei neuen Aufgaben zeigt.
Die Autoren schlagen eine auf Meta-Learning basierende Alternative zu Standard-Erfassungsfunktionen (AFs) vor, bei der ein vortrainiertes neuronales Netz Erfassungswerte in Abhängigkeit von handverlesenen Merkmalen ausgibt.
Deterministische tiefe neuronale Netze verwerfen keine Informationen, aber sie clustern ihre Eingaben.
Dieses Papier bietet einen prinzipiellen Weg, um die Kompressionsphrase in tiefen neuronalen Netzen zu untersuchen, indem es einen theoretisch fundierten Entropieschätzer zur Schätzung der gegenseitigen Information bereitstellt. 
Wir schlagen Regularisierungsziele für Multi-Agenten-RL-Algorithmen vor, die die Koordination bei kooperativen Aufgaben fördern.
In dieser Arebit werden zwei Methoden vorgeschlagen, um Agenten dazu zu bringen, koordinierte Verhaltensweisen zu erlernen, und beide werden in Multi-Agenten-Domänen von angemessener Komplexität rigoros evaluiert.
In diesem Artikel werden zwei Methoden vorgeschlagen, die auf MADDPG aufbauen, um die Zusammenarbeit zwischen dezentralen MARL-Agenten zu fördern.
Wir stellen ein Modell vor, das robuste gemeinsame Repräsentationen erlernt, indem es hierarchische zyklische Übersetzungen zwischen mehreren Modalitäten durchführt.
In diesem Beitrag wird das Multimodal Cyclic Translation Network (MCTN) vorgestellt und für die multimodale Sentiment Analyse evaluiert.
Das Verständnis der Hessian Eigenwerte des neuronalen Netzes unter der datenerzeugenden Verteilung.
Diese Arbeit analysiert das Spektrum der Hessian Matrix großer neuronaler Netze, mit einer Analyse der Max/Min-Eigenwerte und einer Visualisierung der Spektren unter Verwendung eines Lanczos-Quadratur-Ansatzes.
In dieser Arbeit wird die Theorie der Zufallsmatrix verwendet, um die Spektrumverteilung der empirischen Hessian und der wahren Hessian für Deep Learning zu untersuchen, und es wird eine effiziente Methode zur Visualisierung des Spektrums vorgeschlagen.
Ein einfacher Trick zur Verbesserung von Sequenzmodellen: Kombinieren Sie sie mit einem Graphenmodell
In diesem Beitrag wird ein strukturelles Zusammenfassungsmodell mit einem graphenbasierten Encoder vorgestellt, der auf RNN basiert.
Diese Arbeit kombiniert Graph Neuronale Netze mit einem sequentiellen Ansatz zur abstrakten Zusammenfassung, der im Vergleich zu externen Baselines über alle Datensätze hinweg effektiv ist.
Ein spärlicher Klassifikator auf der Grundlage eines diskriminativen Gaußschen Mischmodells, das auch in ein neuronales Netz eingebettet werden kann.
Diese Arbeit stellt ein Gaußsches Mischungsmodell vor, das mit Hilfe von Gradientenabstiegsargumenten trainiert wird, die es ermöglichen, Spärlichkeit zu induzieren und die trainierbaren Modellschichtparameter zu reduzieren.
In dieser Arbeit wird ein Klassifikator, genannt SDGM, vorgeschlagen, der auf einer diskriminativen Gaußschen Mischung und ihrer spärlichen Parameterschätzung basiert.
Initialisierung von Gewichten mit handelsüblichen Grassmann'schen Codebüchern, schnelleres Training und bessere Genauigkeit
Ein unüberwachter Ansatz zur Bereichsanpassung, der sich sowohl auf der Pixel- als auch auf der Merkmalsebene anpasst
Dieser Artikel schlägt einen Ansatz zur Domänenanpassung vor, indem es den CycleGAN um aufgabenspezifische Verlustfunktionen erweitert und den Verlust sowohl über Pixel als auch über Merkmale auferlegt. 
In dieser Arbeit wird die Verwendung von CycleGANs für die Bereichsanpassung vorgeschlagen
Diese Arbeit macht eine neuartige Erweiterung der bisherigen Arbeit auf CycleGAN durch die Kopplung mit adversarial Anpassungsansätzen, einschließlich einer neuen Funktion und semantischem Verlust in das übergeordnete Ziel der CycleGAN, mit klaren Vorteilen.
Amharic Light Stemmer wurde entwickelt, um die Leistung der Amharic Sentiment Classification zu verbessern.
In diesem Beitrag wird das Stemming für morphologisch reichhaltige Sprachen mit einem leichten Stemmer untersucht, der Affixe nur so weit entfernt, dass die ursprüngliche semantische Information im Wort erhalten bleibt.
In diesem Beitrag wird eine Technik zur Amharic Light Stemming vorgeschlagen, die eine Kaskade von Transformationen verwendet, die die Form standardisieren und Suffixe, Präfixe und Infixe entfernen.
Wir untersuchten, ob einfache tiefe Netze über gitterzellenartige künstliche Neuronen verfügen, während der Gedächtnisabruf im erlernten Konzeptraum erfolgt.
Vor- und Nachteile der Sakkaden-basierten Computer Vision unter der Perspektive des voraussagenden Codens.
Stellt einen rechnerischen Rahmen für das Problem des aktiven Sehens vor und erklärt, wie die Kontrollpolitik erlernt werden kann, um die Entropie der nachträglichen Überzeugung zu reduzieren.
Wir untersuchen theoretisch die Konsistenz des Laplacian-Spektrums und verwenden es als Ganzgrapheneinbettung
In diesem Beitrag wird das Laplacian-Spektrum eines Graphen als Mittel zur Erstellung einer Darstellung verwendet, die zum Vergleich und zur Klassifizierung von Graphen eingesetzt werden kann.
In dieser Arbeit wird vorgeschlagen, das Graph-Laplacian-Spektrum zum Erlernen der Graphdarstellung zu verwenden.
Das FGSM-basierte adversarial Training mit Randomisierung funktioniert genauso gut wie das PGD-basierte adversarial Training: Wir können damit einen robusten Klassifikator auf einem einzigen Rechner in 6 Minuten auf CIFAR10 und in 12 Stunden auf ImageNet trainieren.
In diesem Artikel wird die Random+FGSM Methode überarbeitet, um robuste Modelle gegen starke PID-Umgehungsangriffe schneller als frühere Methoden zu trainieren.
Die Hauptbehauptung dieser Arbeit ist, dass eine einfache Strategie der Randomisierung plus Fast-Gradient-Sign-Methode (FGSM) adversariales Training zu robusten neuronalen Netzen führt.
Wir schlagen fast überall differenzierbare und skaleninvariante Regularisierer für das DNN Pruning vor, die zu Supremum Sparsity durch Standard SGD Training führen können.
Diese Arbeit schlägt einen skaleninvarianten Regularisierer (DeepHoyer) vor, der durch das Hoyer-Maß inspiriert ist, um Spärlichkeit in neuronalen Netzen zu erzwingen. 
Wir zeigen, dass zusätzliche unbeschriftete Daten für selbstüberwachte Hilfsaufgaben nicht erforderlich sind, um für die Klassifizierung von Zeitreihen nützlich zu sein, und stellen neue und effektive Hilfsaufgaben vor.
In diesem Beitrag wird eine selbstüberwachte Methode für das Lernen aus Zeitreihendaten im Gesundheitswesen vorgeschlagen, bei der Hilfsaufgaben auf der Grundlage der internen Struktur der Daten entworfen werden, um mehr beschriftete Hilfsaufgaben für das Training zu erstellen.
In diesem Artikel wird ein Ansatz für selbstüberwachtes Lernen auf Zeitreihen vorgeschlagen.
Eigenwerte von Conjugate (auch bekannt als NNGP) und Neural Tangent Kernel können in geschlossener Form über den Booleschen Würfel berechnet werden und zeigen die Auswirkungen von Hyperparametern auf die induktive Verzerrung, das Training und die Generalisierung neuronaler Netze.
In diesem Beitrag wird eine Spektralanalyse des konjugierten Kernels neuronaler Netze und des neuronalen Tangentenkernels auf booleschen Würfeln durchgeführt, um zu klären, warum tiefe Netze auf einfache Funktionen ausgerichtet sind.
Alle funktionellen Gehirnparzellierungen sind falsch, aber einige sind nützlich.
Nachahmung aus Pixeln, mit spärlicher oder keiner Belohnung, unter Verwendung von Off-Policy-RL und einer winzigen, adversarisch erlernten Belohnungsfunktion.
Das Papier schlägt die Verwendung eines "minimalen Gegners" beim generativen adversarial Imitationslernen in hochdimensionalen visuellen Räumen vor.
Dieses Papier zielt darauf ab, das Problem der Schätzung von spärlichen Belohnungen in einer hochdimensionalen Eingabeumgebung zu lösen.
Wir zeigen Strategien zur einfachen Identifizierung gefälschter Proben, die mit dem Generative Adversarial Network Framework erzeugt wurden.
Es wird gezeigt, dass gefälschte Beispiele, die mit gängigen GAN-Implementierungen (Generative Adversarial Network) erstellt wurden, mit verschiedenen statistischen Verfahren leicht identifiziert werden können. 
Diese Arbeit schlägt Statistiken vor, um gefälschte Daten, die mit GANs generiert wurden, auf der Grundlage einfacher marginaler Statistiken oder formaler Spezifikationen, die automatisch aus realen Daten generiert wurden, zu identifizieren.
Wir stellen einen analytischen Rahmen vor, um die Anforderungen an die Akkumulations-Bitbreite in allen drei Deep Learning Trainings GEMMs zu bestimmen, und verifizieren die Gültigkeit und Dichtigkeit unserer Methode durch Benchmark Experimente.
Die Autoren schlagen eine analytische Methode zur Vorhersage der Anzahl der Mantissenbits vor, die für partielle Summierungen für Convolutional und Fully-Connected Layers benötigt werden.
Die Autoren führen eine gründliche Analyse der numerischen Präzision durch, die für die Akkumulationsoperationen beim Training neuronaler Netze erforderlich ist, und zeigen die theoretischen Auswirkungen einer Verringerung der Anzahl der Bits im Fließkomma-Akkumulator.
Eine neue Theorie der unüberwachten Domänenanpassung für metrisches Distanzlernen und ihre Anwendung auf die Gesichtserkennung bei verschiedenen ethnischen Variationen.
Schlägt ein neuartiges Merkmalstransfernetzwerk vor, das den Verlust der gegnerischen Domäne und den Verlust der Domänentrennung optimiert.
Wir schlagen einen konvergenten stochastischen Gradientenabstiegsalgorithmus vom proximalen Typ für eingeschränkte, nicht-glatte, nicht-konvexe Optimierungsprobleme vor.
Dieses Papier schlägt Prox-SGD vor, einen theoretischen Rahmen für stochastische Optimierungsalgorithmen, die asymptotisch zur Stationarität konvergieren für glatte nicht-konvexe Verluste + konvexe Constraints/Regulierer.
Diese Arbeit schlägt einen neuen gradientenbasierten stochastischen Optimierungsalgorithmus mit Gradientenmittelung vor, indem die Theorie für proximale Algorithmen an die nicht-konvexe Umgebung angepasst wird.
Wir geben eine Schranke für NNs für den Ausgabefehler bei zufälligen Gewichtsausfällen an, indem wir eine Taylor-Erweiterung für den kontinuierlichen Grenzwert verwenden, bei dem benachbarte Neuronen ähnlich sind
In diesem Beitrag wird das Problem des Herausfallens von Neuronen aus einem neuronalen Netz betrachtet. Es wird gezeigt, dass es ausreicht, mit Dropout zu trainieren, wenn das Ziel darin besteht, gegenüber zufällig herausfallenden Neuronen während der Auswertung robust zu werden.
Dieser Beitrag untersucht die Auswirkung von Löschungen zufälliger Neuronen auf die Vorhersagegenauigkeit einer trainierten Architektur, mit der Anwendung auf die Fehleranalyse und den spezifischen Kontext neuromorpher Hardware.
Wir erforschen und untersuchen die Synergien zwischen Klang und Aktion.
In diesem Beitrag werden die Verbindungen zwischen Aktion und Ton durch die Erstellung eines Ton-Aktions-Sicht-Datensatzes mit einem Neigungsroboter untersucht.
In diesem Beitrag wird die Rolle von Audio bei der Wahrnehmung von Objekten und Handlungen untersucht, und es wird gezeigt, wie auditive Informationen beim Erlernen von Modellen der Vorwärts- und Rückwärtsdynamik helfen können.
Wir schlagen Hierarchical Complement Objective Training vor, ein neuartiges Trainingsparadigma zur effektiven Nutzung der Kategorienhierarchie im Beschriftungsraum sowohl bei der Bildklassifikation als auch bei der semantischen Segmentierung.
Eine Methode, die die Entropie der posterioren Verteilung über die Klassen reguliert, was für Bildklassifizierungs- und Segmentierungsaufgaben nützlich sein kann
Diese Arbeit identifiziert das Problem der bestehenden Gewichtsteilung bei der Suche nach neuronalen Architekturen und schlägt eine praktische Methode vor, die gute Ergebnisse erzielt.
Der Autor identifiziert ein Problem mit NAS, das als Posterior Fading bezeichnet wird, und führt Posterior Convergent NAS ein, um diesen Effekt abzuschwächen.
Wir schlagen einen neuartigen zweistufigen Trainingsansatz vor, der auf einem "frühen Stoppen" für robustes Training auf störhaften Etiketten basiert.
In dem Papier wird untersucht, wie frühes Stoppen bei der Optimierung hilft, sichere Beispiele zu finden.
In diesem Papier wird eine zweistufige Trainingsmethode für das Lernen mit störhaften Labels vorgeschlagen.
Wir stellen IC3Net vor, ein einzelnes Netzwerk, das zum Trainieren von Agenten in kooperativen, kompetitiven und gemischten Szenarien verwendet werden kann. Wir zeigen auch, dass Agenten mit unserem Modell lernen können, wann sie kommunizieren sollen.
Der Autor schlägt eine neue Architektur für Multi-Agenten Reinforcement Learning vor, die mehrere LSTM Controller mit verbundenen Gewichten verwendet, die sich gegenseitig einen kontinuierlichen Vektor übermitteln.
Die Autoren schlagen ein interessantes Gating-Schema vor, das es den Agenten erlaubt, in einer Multi-Agenten RL Umgebung zu kommunizieren. 
Wir stellen das erste neuronale abstrakte Zusammenfassungsmodell vor, das in der Lage ist, die generierten Zusammenfassungen individuell anzupassen.
Wir schlagen ein Software-Framework vor, das auf den Ideen des Lern-Kompressions-Algorithmus basiert, der es erlaubt, jedes neuronale Netzwerk durch verschiedene Kompressionsmechanismen (Pruning, Quantisierung, Low-Rank, etc.) zu komprimieren.
In diesem Beitrag wird der Entwurf einer Softwarebibliothek vorgestellt, die es dem Benutzer erleichtert, seine Netzwerke zu komprimieren, indem sie die Details der Kompressionsmethoden verbirgt.
In diesem Beitrag wird eine Methode zur durchgängigen multimodalen Generierung von menschlichen Gesichtern aus Sprache vorgeschlagen, die auf einem selbstüberwachten Lernsystem basiert.
In diesem Artikel wird ein multimodales Learning Framework vorgestellt, das die Inferenzphase und die Generierungsphase miteinander verbindet, auf der Suche nach der Möglichkeit, das menschliche Gesicht ausschließlich aus der Stimme zu generieren.
Diese Arbeit zielt darauf ab, einen bedingten Rahmen für die Erzeugung von Gesichtsbildern aus Audiosignalen zu schaffen. 
Es wird ein Top-Down-Ansatz zur rekursiven Darstellung von Satzformeln durch neuronale Netze vorgestellt.
Diese Arbeit bietet ein neues neuronales Netzmodell für logische Formeln, das Informationen über eine gegebene Formel sammelt, indem es ihren Parse-Baum von oben nach unten durchläuft.
Der Beitrag verfolgt den Weg eines baumstrukturierten Netzwerks, das isomorph zum Parse-Baum einer propositionalen Kalkülformel ist, aber Informationen von oben nach unten und nicht von unten nach oben weitergibt.
Ape-X DQfD = Distributed (many actors + one learner + prioritized replay) DQN mit Demonstrationen zur Optimierung der unclipped 0,999-discounted return auf Atari.
Diese Arbeit schlägt drei Erweiterungen (Bellman-Update, temporaler Konsistenzverlust und Expertendemonstration) für DQN vor, um die Lernleistung bei Atari-Spielen zu verbessern und übertrifft damit die State-of-the-Art-Ergebnisse für Atari-Spiele. 
In dieser Arbeit wird ein transformierter Bellman-Operator vorgeschlagen, der darauf abzielt, die Sensitivität gegenüber nicht abgegrenzter Belohnung, die Robustheit gegenüber dem Wert des Diskontierungsfaktors und das Explorationsproblem zu lösen.
Trainingsmethode zur Erzwingung strenger Beschränkungen für gelernte Einbettungen während des überwachten Trainings. Angewandt auf visuelles Fragen beantworten.
Die Autoren schlagen ein Framework vor, um zusätzliches semantisches Vorwissen in das traditionelle Training von Deep-Learning-Modellen einzubeziehen, um den Einbettungsraum anstelle des Parameterraums zu regularisieren.
Der Beitrag plädiert für die Kodierung von externem Wissen in der linguistischen Einbettungsschicht eines multimodalen neuronalen Netzes in Form einer Reihe von harten Einschränkungen.
Lösung inverser Probleme durch Verwendung glatter Approximationen der Forward Algorithmen zum Trainieren der inversen Modelle.
Eine Deep Learning Methode für schwach überwachte punktuelle Lokalisierung, die nur auf der Bildebene lernt. Es stützt sich auf die bedingte Entropie zur Lokalisierung relevanter und irrelevanter Regionen mit dem Ziel, falsch positive Regionen zu minimieren.
In dieser Arbeit wird das Problem der WSL mit Hilfe eines neuartigen Designs von Regularisierungsbegriffen und eines rekursiven Löschalgorithmus untersucht.
In diesem Beitrag wird ein neuer, schwach überwachter Ansatz zum Erlernen der Objektsegmentierung mit Klassenbezeichnungen auf Bildebene vorgestellt.
Erfassen von Zuständen aus dem Hochfrequenzbereich für die Suchkontrolle in Dyna.
Die Autoren schlagen vor, die Probenentnahme im Hochfrequenzbereich durchzuführen, um die Effizienz der Probenentnahme zu erhöhen.
In dieser Arbeit wird ein neuer Weg vorgeschlagen, um Zustände auszuwählen, von denen aus Übergänge im Dyna-Algorithmus durchgeführt werden.
Wir stellen ein datengesteuertes Distributed Source Coding Framework vor, das auf Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC) basiert.
In dem Artikel wird ein verteilter rekurrenter Autoencoder für die Bildkomprimierung vorgeschlagen, der ein ConvLSTM verwendet, um binäre Codes zu lernen, die nach und nach aus den Residuals der zuvor kodierten Informationen aufgebaut werden
Die Autoren schlagen eine Methode zum Trainieren von Bildkompressionsmodellen auf mehreren Quellen vor, mit einem separaten Encoder für jede Quelle und einem gemeinsamen Decoder. 
Gates übernehmen in LSTMs die ganze schwere Arbeit, indem sie elementweise gewichtete Summen berechnen, und das Entfernen des internen einfachen RNN verschlechtert die Leistung des Modells nicht.
In diesem Beitrag wird eine vereinfachte LSTM Variante vorgeschlagen, bei der die Nichtlinearität von Inhaltselement und Ausgangs Gate entfernt wird
Diese Arbeit präsentiert eine Analyse von LSTMS, die zeigt, dass sie eine Form haben, in der der Inhalt der Speicherzelle bei jedem Schritt eine gewichtete Kombination der Inhaltsaktualisierten Werte ist, die bei jedem Zeitschritt berechnet werden, und bietet eine Vereinfachung von LSTMs, die den Wert berechnen, mit dem die Speicherzelle bei jedem Zeitschritt in Bezug auf eine deterministische Funktion der Eingabe anstatt einer Funktion der Eingabe und des aktuellen Kontexts.
Diese Arbeit schlägt eine neue Einsicht in LSTM vor, bei der der Kern eine elementweise gewichtete Summe ist, und argumentiert, dass LSTM redundant ist, indem nur Eingabe- und Vergessensgatter zur Berechnung der Gewichte beibehalten werden.
Bei der Analyse von mehr als 300 Beiträgen auf aktuellen Konferenzen zum maschinellen Lernen haben wir festgestellt, dass Anwendungen des maschinellen Lernens für die Gesundheit (ML4H) in Bezug auf die Reproduzierbarkeit hinter anderen Bereichen des maschinellen Lernens zurückbleiben.
In diesem Beitrag wird der Stand der Reproduzierbarkeit für ML-Anwendungen im Gesundheitswesen quantitativ und qualitativ untersucht und es werden Empfehlungen für eine bessere Reproduzierbarkeit der Forschung gegeben.
Wir trainieren viele kleine Netze, jedes für eine bestimmte Operation, die dann kombiniert werden, um komplexe Operationen durchzuführen.
In diesem Artikel wird vorgeschlagen, neuronale Netze zur Auswertung mathematischer Ausdrücke zu verwenden, indem 8 kleine Bausteine für 8 grundlegende Operationen, z. B. Addition, Subtraktion usw., entworfen werden und dann mehrstellige Multiplikationen und Divisionen mit diesen kleinen Bausteinen entworfen werden.
In dem Papier wird eine Methode zur Entwicklung eines NN-basierten Bewertungssystems für mathematische Ausdrücke vorgeschlagen.
Verbesserung der Qualität und Stabilität von GANs unter Verwendung eines relativistischen Diskriminators; IPM-GANs (wie WGAN-GP) sind ein Spezialfall.
Diese Arbeit schlägt einen relativistischen Diskriminator vor, der in einigen Situationen hilft, obwohl er etwas empfindlich auf Hyperparameter, Architekturen und Datensätze reagiert.
In dieser Arbeit betrachten die Autoren eine Variante des GAN, indem sie gleichzeitig die Wahrscheinlichkeit verringern, dass die Daten für den Generator real sind.
Eine auf Zustandswertfunktionen basierende Version von MPO, die gute Ergebnisse bei einer Vielzahl von Aufgaben in der diskreten und kontinuierlichen Steuerung erzielt.
In diesem Beitrag wird ein Algorithmus für On-Policy Reinforcement Learning vorgestellt, der sowohl kontinuierliche/diskrete Steuerung als auch Single-/Multi-Task-Lernen ermöglicht und sowohl niedrigdimensionale Zustände als auch Pixel verwendet.
Das Papier schlägt eine Online-Variante von MPO, V-MPO, vor, die die V-Funktion erlernt und die nicht-parametrische Verteilung in Richtung der Vorteile aktualisiert.
Wir schlagen neuronale Ausführungs Engines (NEEs) vor, die eine gelernte Maske und überwachte Ausführungsspuren nutzen, um die Funktionalität von Unterprogrammen zu imitieren und eine starke Generalisierung zu zeigen.
In diesem Beitrag wird das Problem des Aufbaus einer Programmausführungs Engine mit neuronalen Netzen untersucht und ein Transformer-basiertes Modell zum Erlernen grundlegender Unterprogramme vorgeschlagen, das in mehreren Standardalgorithmen angewendet wird.
Diese Arbeit befasst sich mit dem Problem des Entwurfs neuronaler Netzwerkarchitekturen, die allgemeine Programme lernen und implementieren können.
Die Bayes'sche Changepoint-Erkennung ermöglicht Meta-Learning direkt aus Zeitreihendaten.
Diese Arbeit betrachtet Meta-Lernen in einer Aufgabe unsegmentierten Umgebung und wendet Bayesian Online Änderungspunkt Erkennung mit Meta-Lernen an.
In diesem Beitrag wird das Meta-Lernen auf nicht segmentierte Aufgaben ausgedehnt, wobei das MOCA Framework ein Bayessches Changepoint Schätzschema zur Erkennung von Aufgabenänderungen verwendet.
Ein auf tiefem Lernen basierender Ansatz zur Erkennung von fricative Phoneme ohne Verzögerung.
In diesem Beitrag werden Methoden des überwachten tiefen Lernens angewandt, um die exakte Dauer eines fricative Phoneme zu erkennen und so den Algorithmus zur praktischen Frequenzabsenkung zu verbessern.
Ein Online- und Linearzeit-Aufmerksamkeitsmechanismus, der schwache Aufmerksamkeit über adaptiv platzierte Teile der Eingabesequenz ausübt.
In diesem Beitrag wird eine kleine Modifikation der monotonen Aufmerksamkeit in [1] vorgeschlagen, indem dem durch die monotone Aufmerksamkeit vorhergesagten Segment eine schwache Aufmerksamkeit hinzugefügt wird.
Die Arbeit schlägt eine Erweiterung eines früheren monotonen Aufmerksamkeitsmodells (Raffel et al. 2017) vor, um ein Fenster fester Größe bis zur Ausrichtungsposition zu beachten.
Entwicklung neuer Techniken, die sich auf die Neuordnung von Bereichen stützen, um eine detaillierte Analyse der Beziehung zwischen Datensatz und Trainings- und Generalisierungsleistung zu ermöglichen.
Wir produzieren Reinforcement Learning Agenten, die sich mit Hilfe einer neuartigen Regularisierungstechnik gut auf eine Vielzahl von Umgebungen verallgemeinern lassen.
Diese Arbeit stellt die Herausforderung der hohen Varianzregelungen bei der Domänenrandomisierung für Reinforcement Learning vor und konzentriert sich hauptsächlich auf das Problem der visuellen Randomisierung, bei der sich die verschiedenen randomisierten Domänen nur im Zustandsraum unterscheiden und die zugrunde liegenden Belohnungen und Dynamiken gleich sind.
Um die Generalisierungsfähigkeit von Deep-RL-Agenten bei Aufgaben mit unterschiedlichen visuellen Mustern zu verbessern, wurde in diesem Beitrag eine einfache Regularisierungstechnik für die Domänenrandomisierung vorgeschlagen.
Wir erforschen den Schnittpunkt von Netzwerk Neurowissenschaften und Deep Learning. 
In diesem Beitrag wird ein System für die unüberwachte, hochpräzise Konstruktion von Knowledge Bases vorgestellt, das ein probabilistisches Programm verwendet, um einen Prozess der Umwandlung von Fakten aus der Knowledge Base in unstrukturierten Text zu definieren.
Überblick über die bestehende Knowledge Base, die mit einem probabilistischen Modell erstellt wird, wobei der Ansatz zur Erstellung der Knowledge Base im Vergleich zu anderen Knowledge Base Ansätzen wie YAGO2, NELL, Knowledge Vault und DeepDive bewertet wird.
Diese Arbeit verwendet ein probabilistisches Programm, das den Prozess beschreibt, durch den Fakten, die Entitäten beschreiben, in Texten und einer großen Anzahl von Webseiten realisiert werden können, um zu lernen, wie man Fakten über Personen anhand eines einzigen Seed-Fakts extrahiert.
Neue Methode zur Signalextraktion im Fourier-Bereich
Verbesserung der Skalierbarkeit graphischer neuronaler Netze beim Imitation Learning und der Vorhersage von Schwarmbewegungen
In der Arbeit wird ein neues Zeitreihenmodell für das Lernen einer Folge von Graphen vorgeschlagen.
Diese Arbeit befasst sich mit Problemen der Sequenzvorhersage in einem Multiagentensystem.
Wir schlagen ein differenzierbares Produktquantisierungsverfahren vor, das die Größe der Einbettungsschicht in einem Ende-zu-Ende Training ohne Leistungseinbußen reduzieren kann.
Diese Arbeit befasst sich mit Methoden zur Komprimierung von Einbettungsschichten für Inferenzen mit geringem Speicherbedarf, wobei komprimierte Einbettungen zusammen mit den aufgabenspezifischen Modellen in einer differenzierbaren Ende-zu-Ende Methode gelernt werden.
Wir stellen einen einfachen und neuartigen modalen Regressionsalgorithmus vor, der sich leicht auf große Probleme übertragen lässt. 
Diese Arbeit schlägt einen impliziten Funktionsansatz zum Erlernen der Modi der multimodalen Regression vor.
In der vorliegenden Arbeit wird ein parametrischer Ansatz zur Schätzung des bedingten Modus unter Verwendung des Impliziten Funktionssatzes für multimodale Verteilungen vorgeschlagen. 
Effiziente Meta-RL durch die Kombination von Variationsschlussfolgerung von probabilistischen Aufgabenvariablen mit Off-Policy RL.
In diesem Beitrag wird vorgeschlagen, während der Meta-Trainingszeit Off-Policy RL einzusetzen, um die Stichprobeneffizienz von Meta-RL Methoden erheblich zu verbessern.
Dieser Artikel konzentriert sich auf die Identifizierung von qualitativ hochwertigen Webquellen für industrielle Knowledge Base Augmentation Pipelines.
Wir untersuchen die Vorzüge des Einsatzes neuronaler Netze bei dem Problem der Vorhersage von Übereinstimmungen, bei dem es darum geht, die Wahrscheinlichkeit abzuschätzen, dass eine Gruppe von M Gegenständen gegenüber einer anderen bevorzugt wird, und zwar auf der Grundlage partieller Gruppenvergleichsdaten.
In diesem Beitrag wird eine Lösung für das Problem der Rangfolge von Mengen durch ein tiefes neuronales Netz vorgeschlagen und eine Architektur für diese Aufgabe entworfen, die sich an früheren, manuell entwickelten Algorithmen orientiert.
In diesem Beitrag wird eine Technik zur Lösung des Problems der Treffervorhersage mithilfe einer Deep-Learning-Architektur vorgestellt.
Ein neuartiger Ansatz zur Erhaltung orthogonaler rekurrenter Gewichtsmatrizen in einem RNN.
Stellt ein Schema zum Lernen der rekurrenten Parametermatrix in einem neuronalen Netz vor, das die Cayley-Transformation und eine skalierende Gewichtsmatrix verwendet. 
Diese Arbeit schlägt eine RNN-Reparametrisierung der rekurrenten Gewichte mit einer schiefsymmetrischen Matrix unter Verwendung der Cayley-Transformation vor, um die rekurrente Gewichtsmatrix orthogonal zu halten.
Die neuartige Parametrisierung von RNNs ermöglicht die relativ einfache Darstellung orthogonaler Gewichtsmatrizen.
Wir verwenden ein einziges Modell, um eine Vielzahl von Aufgaben zur Analyse natürlicher Sprache zu lösen, indem wir sie in einem einheitlichen Span-Relation-Format formulieren.
Diese Arbeit verallgemeinert eine breite Palette von Aufgaben zur Verarbeitung natürlicher Sprache in einem einzigen, auf Spans basierenden Rahmen und schlägt eine allgemeine Architektur zur Lösung all dieser Probleme vor.
In dieser Arbeit wird eine einheitliche Formulierung für verschiedene NLP-Aufgaben auf Phrasen- und Token-Ebene vorgestellt.
Wir stellen eine untere Variationsschranke für GP-Modelle vor, die ohne die Berechnung teurer Matrixoperationen wie Inversen optimiert werden kann und dabei die gleichen Garantien wie bestehende Variationsannäherungen bietet.
Variationale Autoencoder mit latenten Räumen, die als Produkte von Riemannschen Mannigfaltigkeiten mit konstanter Krümmung modelliert sind, verbessern die Bildrekonstruktion gegenüber Varianten mit nur einer Mannigfaltigkeit.
In diesem Artikel wird eine allgemeine Formulierung des Begriffs einer VAE mit einem latenten Raum, der aus einer gekrümmten Mannigfaltigkeit besteht, vorgestellt.
In diese geht Arbeit es um die Entwicklung von VAEs in nicht-euklidischen Räumen.
Wir stellen einen Black-Box-Algorithmus für die wiederholte Optimierung von Verbindungen unter Verwendung eines Übersetzungs Frameworks vor.
Die Autoren stellen die Moleküloptimierung als ein equence-to-sequence Problem dar und erweitern bestehende Methoden zur Verbesserung von Molekülen. Sie zeigen, dass dies für die Optimierung von logP, nicht aber von QED von Vorteil ist.
Die Arbeit baut auf bestehenden Übersetzungsmodellen auf, die für die molekulare Optimierung entwickelt wurden, und verwendet iterative sequence-to-sequence oder Graph-zu-Graph Übersetzungsmodelle.
Vorschlag des ersten Wasserzeichen Frameworks für die Einbettung und Extraktion von Multibit-Signaturen unter Verwendung der DNN-Ausgänge. 
Es wird eine Methode für das Multi-Bit-Wasserzeichen von neuronalen Netzen in einer Black-Box Umgebung vorgeschlagen und gezeigt, dass die Vorhersagen bestehender Modelle eine Multi-Bit-Zeichenkette tragen können, die später zur Überprüfung des Eigentums verwendet werden kann.
Die Arbeit schlägt einen Ansatz für Modell-Wasserzeichen vor, bei dem das Wasserzeichen eine in das Modell eingebettete Bitfolge ist, die Teil eines Feinabstimmungsprozesses ist.
Sie wissen nicht, wie man optimiert? Dann lernen Sie einfach zu optimieren!
In diesem Artikel wird ein Weg vorgeschlagen, Bildklassifizierungsmodelle so zu trainieren, dass sie resistent gegen L-infinity perturbation Attacken sind.
In diesem Beitrag wird vorgeschlagen, einen Angreifer mit Hilfe des Learning-to-Learn-Konzepts zu erlernen.
Eine einfache und leicht zu trainierende Methode für multimodale Vorhersagen in Zeitreihen. 
In diesem Beitrag wird ein Modell zur Vorhersage von Zeitserien vorgestellt, das eine deterministische Zuordnung erlernt und ein weiteres Netz trainiert, um künftige Bilder anhand der Eingabe und des residual Fehlers des ersten Netzes vorherzusagen.
In dem Beitrag wird ein Modell für die Vorhersage unter Unsicherheit vorgeschlagen, bei dem zwischen deterministischer Komponentenvorhersage und unsicherer Komponentenvorhersage unterschieden wird.
Dieser Artikel stellt simple_rl vor, eine neue Open-Source-Bibliothek zur Durchführung von Reinforcement Learning Experimenten in Python 2 und 3 mit dem Schwerpunkt auf Einfachheit.
Diese Arbeit beschäftigt sich mit der Stabilität einer einfachen Gradientenbestrafung,  $\mu$-WGAN-Optimierung genannt, durch Einführung eines Konzepts der maßstäblichen Differenzierung.
WGAN mit einem quadratischen, nullpunktzentrierten Gradientenbestrafungs Term für ein allgemeines Maß wird untersucht.
Charakterisiert die Konvergenz des gradientenbestraften Wasserstein-GAN.
Modernste Trainingsmethode für binäre und ternäre Gewichtsnetze auf der Grundlage der alternierenden Optimierung von zufällig entspannten Gewichtspartitionen.
In der Arbeit wird ein neues Trainingsschema zur Optimierung eines ternären neuronalen Netzes vorgeschlagen.
Die Autoren schlagen RPR vor, eine Methode zur zufälligen Partitionierung und Quantisierung von Gewichten und zum Trainieren der verbleibenden Parameter, gefolgt von Entspannung in abwechselnden Zyklen, um quantisierte Modelle zu trainieren.
Wir ersetzen einige Gradientenpfade in hierarchischen RNNs durch einen Hilfsverlust. Wir zeigen, dass dies die Speicherkosten reduzieren kann, während die Leistung erhalten bleibt.
In dem Artikel wird eine hierarchische RNN-Architektur vorgestellt, die speichereffizienter trainiert werden kann.
Die vorgeschlagene Arbeit schlägt vor, die verschiedenen Schichten der Hierarchie in RNN mit Hilfe von Hilfsverlusten zu entkoppeln.
Neuronale Netze, die eine gute Klassifizierung vornehmen, projizieren Punkte in sphärische Formen, bevor sie in geringere Dimensionen komprimiert werden.
Wir schlagen eine neuartige Lernmethode für tiefe Tonerkennung vor, das sogenannte BC-Lernen.
Die Autoren definierten eine neue Lernaufgabe, bei der ein DNN das Mischungsverhältnis zwischen Geräuschen aus zwei verschiedenen Klassen vorhersagen muss, um die Unterscheidungskraft des schließlich gelernten Netzwerks zu erhöhen.
Es wird eine Methode zur Verbesserung der Leistung einer generischen Lernmethode durch die Erzeugung von Trainingsmustern "zwischen den Klassen" vorgeschlagen und die grundlegende Intuition und Notwendigkeit der vorgeschlagenen Technik vorgestellt.
Wir schlagen eine Methode vor, die das zeitlich variierende Qualitätsniveau der Daten für räumlich-zeitliche Prognosen ohne explizit zugewiesene Labels ableitet.
Einführung einer neuen Definition von Datenqualität, die sich auf den Begriff der lokalen Variation stützt, der in (Zhou und Scholkopf) definiert wurde, und ihn auf mehrere heterogene Datenquellen ausweitet.
In dieser Arbeit wurde eine neue Methode zur Bewertung der Qualität verschiedener Datenquellen mit dem zeitvariablen Graphenmodell vorgeschlagen, wobei das Qualitätsniveau als Regularisierungsterm in der Zielfunktion verwendet wird
Wir schlagen 3D-Form Programme vor, eine strukturierte, kompositorische Formdarstellung. Unser Modell lernt, Form Programme abzuleiten und auszuführen, um 3D Formen zu erklären.
Ein Ansatz zur Ableitung von Formprogrammen aus 3D Modellen. Die Architektur besteht aus einem rekurrenten Netzwerk, das eine 3D Form kodiert und Anweisungen ausgibt, und einem zweiten Modul, das das Programm in 3D rendert.
In diesem Beitrag wird eine semantische Beschreibung auf hoher Ebene für 3D Formen vorgestellt, die durch das ShapeProgram gegeben ist.
Wir zeigen, dass konventionelle Regularisierungsmethoden (z.B. $L_2$, Dropout), die in RL Methoden weitgehend ignoriert wurden, bei der Optimierung von Richtlinien sehr effektiv sein können.
Die Autoren untersuchen eine Reihe bestehender Methoden zur direkten Optimierung von Strategien im Bereich des Reinforcement Learnings und bieten eine detaillierte Untersuchung der Auswirkungen von Vorschriften auf die Leistung und das Verhalten von Agenten, die diesen Methoden folgen.
Diese Arbeit bietet eine Studie über die Auswirkung der Regularisierung auf die Leistung in Trainingsumgebungen in Regel-Optimierungsmethoden in mehreren kontinuierlichen Steuerungsaufgaben.
Wir präsentieren einen Frage-Antwort-Datensatz, FigureQA, als einen ersten Schritt zur Entwicklung von Modellen, die intuitiv Muster aus visuellen Darstellungen von Daten erkennen können.
In diesem Beitrag wird ein Datensatz zur Beantwortung von Beispiel Fragen zu Abbildungen vorgestellt, der Schlussfolgerungen zu Abbildungselementen enthält.
In diesem Beitrag wird ein neuer Datensatz für visuelles Reasoning namens Figure-QA vorgestellt, der aus 140.000 Abbildungen und 1,55 Mio. QA-Paaren besteht und bei der Entwicklung von Modellen helfen kann, die nützliche Informationen aus visuellen Datendarstellungen extrahieren können.
In diesem Positionspapier werden verschiedene Arten von Selbsterklärungen analysiert, die in Planungs- und verwandten Systemen auftreten können. 
Erörtert verschiedene Aspekte von Erklärungen, insbesondere im Zusammenhang mit sequenzieller Entscheidungsfindung. 
Der erste Deep-Learning-Ansatz für MFSR, der Registrierung, Fusion und Up-Sampling durchgängig löst.
In diesem Beitrag wird ein durchgängiger Algorithmus für die Superauflösung mehrerer Bilder vorgeschlagen, der auf paarweisen Co-Registrierungen und Fusionsblöcken (Convolutional Residual Blocks) beruht, die in einem Encoder-Decoder Netzwerk "HighRes-Net" eingebettet sind, das das Superauflösungsbild schätzt.
Diese Arbeit schlägt einen Rahmen vor, der eine rekursive Fusion mit Co-Registrierungs Verlusten beinhaltet, um das Problem zu lösen, dass die Ergebnisse der Superauflösung und die hochauflösenden Labels nicht pixelgenau ausgerichtet sind.
Verwenden von Gossip-basierten approximativen verteilten Durchschnittsberechnungen für verteiltes Training über Netze mit hoher Latenz, anstelle von exakten verteilten Durchschnittsberechnungen wie AllReduce.
Die Autoren schlagen vor, Gossip-Algorithmen als allgemeine Methode zur Berechnung des ungefähren Durchschnitts über eine Gruppe von Arbeitnehmern zu verwenden.
Die Arbeit beweist die Konvergenz der SGP für nicht-konvexe glatte Funktionen und zeigt, dass die SGP eine signifikante Beschleunigung in der Low-Latency-Umgebung erreichen kann, ohne zu viel Vorhersageleistung zu opfern. 
In diesem Beitrag wird ein adversarial Learning Framework für neuronale Konversationsmodelle mit Personas entwickelt.
Diese Arbeit schlägt eine Erweiterung von hredGAN vor, um gleichzeitig eine Reihe von Attributeinbettungen zu lernen, die die Persona jedes Sprechers repräsentieren und Persona-basierte Antworten generieren kann.
Biologisch inspirierte künstliche neuronale Netze, die aus in einem zweidimensionalen Raum angeordneten Neuronen bestehen, sind in der Lage, unabhängige Gruppen für die Ausführung verschiedener Aufgaben zu bilden.
Diskreter Transformer, der mit harter Aufmerksamkeit sicherstellt, dass jeder Schritt nur von einem festen Kontext abhängt.
In diesem Beitrag werden Modifikationen an der Standard Transformer Architektur vorgestellt, mit dem Ziel, die Interpretierbarkeit zu verbessern und gleichzeitig die Leistung bei NLP-Aufgaben zu erhalten.
In diesem Artikel werden drei diskrete Transformer vorgeschlagen: ein diskretes und stochastisches Gumbel-Softmax-basiertes Aufmerksamkeitsmodul, ein syntaktischer und semantischer Two-Stream Transformer und eine Sparsity-Regularisierung.
Wir zeigen empirische Belege dafür, dass Vorhersehende Coding Modelle Repräsentationen liefern, die stärker mit Gehirndaten korrelieren als überwachte Bilderkennungsmodelle.
Ein allgemeines Framework für die Handhabung von Transfer- und Multi-Task Lernen unter Verwendung von Paaren von Autoencodern mit aufgabenspezifischen und gemeinsamen Gewichten.
Vorschlagen eines generischen Frameworks für Ende-zu-Ende Transfer Lernen / Domänenanpassung mit tiefen neuronalen Netzen. 
Diese Arbeit schlägt ein Modell vor, das es Architekturen von tiefen neuronalen Netzen ermöglicht, Parameter über verschiedene Datensätze hinweg gemeinsam zu nutzen, und wendet es auf das Transferlernen an.
Der Artikel konzentriert sich auf das Erlernen gemeinsamer Merkmale aus Daten aus verschiedenen Bereichen und endet mit einer allgemeinen Architektur für Multi-Task-, Semi-Supervised- und Transfer-Lernen.
Wir schlagen ein Framework vor, um Entscheidungsbäume und neuronale Netze zu kombinieren, und zeigen anhand von Bildklassifizierungsaufgaben, dass er die komplementären Vorteile der beiden Ansätze nutzt und gleichzeitig die Grenzen früherer Arbeiten überwindet.
Die Autoren schlugen ein neues Modell, Adaptive Neural Trees, vor, indem sie das Repräsentationslernen und die Gradientenoptimierung von neuronalen Netzen mit dem Architekturlernen von Entscheidungsbäumen kombinierten.
In diesem Beitrag wird der Ansatz der adaptiven neuronalen Bäume vorgeschlagen, um die beiden Lernparadigmen der tiefen neuronalen Netze und der Entscheidungsbäume zu kombinieren.
Die Übersetzung von Teilen der Eingabe während des Trainings kann die sprachübergreifende Leistung verbessern.
In diesem Beitrag wird eine Methode zur sprachübergreifenden Datenerweiterung vorgeschlagen, um die Sprachinferenz und die Beantwortung von Fragen zu verbessern.
In diesem Beitrag wird vorgeschlagen, sprachübergreifende Daten durch heuristische Swaps mit alignierten Übersetzungen zu ergänzen, wie es zweisprachige Menschen beim Code-Switching tun.
Wir schlagen einen bedingten variationalen Autoencoder vor, der den nachgelagerten Kollaps in Szenarien abmildert, in denen das Konditionierungssignal stark genug ist, damit ein ausdrucksstarker Decoder eine plausible Ausgabe daraus erzeugen kann.
In diesem Beitrag werden stark konditionierte generative Modelle betrachtet und eine Zielfunktion sowie eine Parametrisierung der Variationsverteilung vorgeschlagen, so dass die latenten Variablen explizit von den Eingabebedingungen abhängen.
In diesem Artikel wird argumentiert, dass ein nachgelagerter Kollaps wahrscheinlicher ist als bei einer einfachen VAE, wenn der Decoder auf der Verkettung von latenten Variablen und Hilfsinformationen beruht.
Wir schlagen eine Studie über die Stabilität verschiedener Algorithmen zum Few-Shot Learning vor, die Variationen in den Hyperparametern und Optimierungsschemata unterworfen sind, während wir die zufällige Seed kontrollieren.
In dieser Arbeit wird die Reproduzierbarkeit beim Few-Shot Learning untersucht.
Wir übersetzen eine Beschränkung der Suboptimalität von Repräsentationen in ein praktisches Trainingsziel im Kontext des hierarchischen Reinforcement Learning.
Die Autoren schlagen einen neuartigen Ansatz für das Lernen einer Repräsentation für HRL vor und stellen eine interessante Verbindung zwischen dem Lernen der Repräsentation und der Begrenzung der Suboptimalität her, die zu einem gradientenbasierten Algorithmus führt.
In dieser Arbeit wird ein Weg vorgeschlagen, mit Suboptimalität im Zusammenhang mit Lernrepräsentationen umzugehen, die sich auf die Suboptimalität der hierarchischen Regeln in Bezug auf die Aufgabenbelohnung beziehen.
Metareasoning in einem situierten temporalen Planer.
Dieser Beitrag befasst sich mit dem Problem der situierten zeitlichen Planung und schlägt eine weitere Vereinfachung der zuvor von Shperberg vorgeschlagenen gierigen Strategien vor.
Die Robustheit trainierter PGD-Modelle reagiert empfindlich auf semantikerhaltende Transformationen von Bilddatensätzen, was bedeutet, dass die Bewertung robuster Lernalgorithmen in der Praxis heikel ist.
Die Arbeit verdeutlicht den Unterschied zwischen sauberer und robuster Genauigkeit und zeigt, dass eine Änderung der Randverteilung der Eingabedaten P(x) unter Beibehaltung ihrer Semantik P(y|x) die Robustheit des Modells beeinflusst.
In diesem Beitrag wird die Ursache für die mangelnde Robustheit von Klassifikatoren gegenüber Störungen der aadversarial Eingaben bei l-inf begrenzten Störungen untersucht.
Übereinstimmung von Sätzen durch Lernen der latenten Konstituentenbaumstrukturen mit einer Variante des Inside-Outside Algorithmus, eingebettet in eine neuronale Netzwerkschicht.
In diesem Beitrag wird ein strukturierter Aufmerksamkeitsmechanismus zur Berechnung von Alignment-Scores unter allen möglichen Abständen in zwei gegebenen Sätzen vorgestellt.
In diesem Artikel wird ein Modell für strukturierte Übereinstimmungen zwischen Sätzen vorgeschlagen, um Sätze durch den Abgleich ihrer latenten Strukturen zu vergleichen.
Unüberwachtes Lernen der Entflechtungsdarstellung.
Die Autoren stellen einFramework vor, in dem ein Auto Encoder (E, D) so regularisiert wird, dass seine latente Repräsentation gegenseitige Informationen mit generierten latenten Raumrepräsentation teilt.
Bedingte GANs, die so trainiert werden, dass sie datenerweiterte Beispiele ihrer bedingten Eingaben erzeugen, die zur Verbesserung von Standard Klassifizierungs- und One-Shot-Lernsystemen wie Matching-Netzwerken und Pixel-Distanz verwendet werden.
Die Autoren schlagen eine Methode zur Datenerweiterung vor, bei der die klassenübergreifenden Transformationen mit Hilfe von bedingten GAN auf einen niedrigdimensionalen latenten Raum abgebildet werden.
Wir entwickeln eine einfache, auf Regression basierende, modellagnostische Methode zur Auswahl von Merkmalen, um datengenerierende Prozesse mit FDR-Kontrolle zu interpretieren, und übertreffen mehrere populäre Grundlinien auf mehreren simulierten, medizinischen und Bilddatensätzen.
Diese Arbeit schlägt eine praktische Verbesserung des bedingten Randomisierungstests und eine neue Teststatistik vor, beweist, dass f-Divergenz eine mögliche Wahl ist, und zeigt, dass KL-Divergenz einige bedingte Verteilungen aufhebt.
Diese Arbeit befasst sich mit dem Problem, nützliche Merkmale in einer Eingabe zu finden, die von einer variablen Antwort abhängig sind, selbst wenn alle anderen Eingabevariablen konditioniert sind.
Eine modellunabhängige Methode zur Interpretation des Einflusses von Eingabemerkmalen auf die Reaktion eines Modells auf Maschinenebene bis hin zur Instanzebene sowie geeignete Teststatistiken für die modellunabhängige Merkmalsauswahl.
Ein neuer Ansatz zum Lernen eines Modells aus verrauschten Crowdsourced Annotations.
In diesem Artikel wird eine Methode zum Lernen aus störhaften Labels vorgeschlagen, die sich auf den Fall konzentriert, dass die Daten nicht redundant beschriftet sind, mit theoretischer und experimenteller Validierung
Diese Arbeit konzentriert sich auf das Problem des Lernens aus der Crowd, bei dem die gemeinsame Aktualisierung der Klassifikatorgewichte und der Konfusionsmatrizen der Arbeiter bei dem Schätzproblem mit seltenen Crowdsourced Labels helfen kann.
Es wird ein überwachter Lernalgorithmus für die Modellierung der Qualität von Labels und Mitarbeitern vorgeschlagen und der Algorithmus wird verwendet, um zu untersuchen, wie viel Redundanz beim Crowdsourcing erforderlich ist und ob eine geringe Redundanz mit reichlich Störbeispielen zu besseren Labels führt.
Neue Methode zur Erklärung, warum ein neuronales Netz ein Bild falsch klassifiziert hat.
In diesem Artikel wird eine Methode zur Erklärung der Klassifizierungsfehler von neuronalen Netzen vorgeschlagen. 
Ziel ist es, die Klassifizierung neuronaler Netze besser zu verstehen und den latenten Raum eines variationalen Autoencoders zu erforschen und die Störungen des latenten Raums zu berücksichtigen, um eine korrekte Klassifizierung zu erhalten.
Eine Methode zur automatischen Konstruktion von verzweigten Multitasking-Netzwerken mit starker experimenteller Bewertung auf verschiedenen Multitasking-Datensätzen.
In diesem Beitrag wird ein neuartiges Multi-Task Learning Framework mit sanften Parametern vorgeschlagen, das auf einer baumartigen Struktur basiert.
In diesem Artikel wird eine Methode zur Ableitung der Architektur von Multitasking Netzwerken vorgestellt, um zu bestimmen, welcher Teil des Netzwerks von den verschiedenen Aufgaben gemeinsam genutzt werden sollte.
Es ist möglich, die Gewichtsmatrix in einem Convolutional Layer zu ersetzen, um sie als strukturierte, effiziente Schicht zu trainieren, die genauso gut funktioniert wie die Low-Rank Decomposition.
Diese Arbeit wendet frühere Structured Efficient Linear Layers auf Convolutional Layers an und schlägt Structured Efficient Convolutional Layers als Ersatz für die ursprünglichen Convolutional Layers vor.
Wir stellen SVDocNet vor, ein durchgängig trainierbares U-Netz auf der Basis eines räumlich rekurrenten neuronalen Netzes (RNN) für die blinde Entschlüsselung von Dokumenten.
Wir erweitern die bilineare Sparse Codierung und nutzen Videosequenzen, um dynamische Filter zu lernen.
Wir schlagen einen neuartigen OOD-Detektor vor, der unscharfe Bilder als Negativbeispiele verwendet. Unser Modell erreicht eine signifikante OOD-Erkennungsleistung in verschiedenen Bereichen.
In diesem Beitrag wird die Idee vorgestellt, unscharfe Bilder als Regularisierungsbeispiele zu verwenden, um die Leistung bei der Erkennung von Verteilungsfehlern auf der Grundlage von Random Network Distillation zu verbessern.
In diesem Artikel wird die Out-of-Data Verteilung angegangen, indem RND auf Datenerweiterungen angewandt wird, indem ein Modell trainiert wird, um die Ausgaben eines Zufallsnetzwerks mit einer Erweiterung als Eingabe abzugleichen.
Ein schneller Optimierer für allgemeine Anwendungen und Training in großen Batches.
In dieser Arbeit haben die Autoren eine Studie zum Large-Batch-Training für BERT durchgeführt und erfolgreich ein BERT-Modell in 76 Minuten trainiert.
In diesem Beitrag wird eine schichtweise Anpassungsstrategie entwickelt, die es ermöglicht, BERT-Modelle mit großen 32k-Minibatches im Vergleich zur Basislinie von 512 Batches zu trainieren.
Wir haben die Rolle von zwei Lernraten beim modellagnostischen Meta-Lernen bei der Konvergenz analysiert.
Die Autoren haben das Problem der Instabilität der Optimierung in MAML durch die Untersuchung der beiden Lernraten angegangen.
In dieser Arbeit wird eine Methode untersucht, mit der die beiden im MAML-Trainingsalgorithmus verwendeten Lernraten eingestellt werden können.
Aufgabenunabhängiges neuronales Modell für das Lernen von Assoziationen zwischen zusammenhängenden Wortgruppen.
In dem Papier wird eine Methode zum Training funktionsspezifischer Wortvektoren vorgeschlagen, bei der jedes Wort mit drei Vektoren in jeweils einer anderen Kategorie (Subjekt-Verb-Objekt) dargestellt wird.
In diesem Beitrag wird ein neuronales Netz zum Erlernen funktionsspezifischer Arbeitsrepräsentationen vorgeschlagen und der Vorteil gegenüber Alternativen aufgezeigt.
Einsatz einer Deep-Learning-Methode zur automatischen Vermessung von SEM-Bildern in der Halbleiterindustrie.
Dieses Papier beschreibt und analysiert drei Methoden zur Terminierung von Aktivitäten mit nicht fester Dauer bei Vorhandensein von verbrauchenden Ressourcen.
In diesem Beitrag werden drei Ansätze für die Planung von Aktivitäten an Bord eines planetarischen Rovers unter Berücksichtigung von Ressourcenbeschränkungen vorgestellt.
Beschreibung der Einreichung zur NeurIPS2019 Disentanglement Challenge basierend auf hypersphärischen variationalen Autoencodern.
Eine Anomalie-Erkennung, die Zufallstransformationen zur Klassifizierung für die Verallgemeinerung auf Nicht-Bilddaten verwendet.
In diesem Beitrag wird ein tiefes Verfahren zur Erkennung von Anomalien vorgeschlagen, das die jüngsten tiefen One-class Klassifizierungs- und transformationsbasierten Klassifizierungsansätze vereint.
In dieser Arbeit wird ein Ansatz zur klassifikationsbasierten Erkennung von Anomalien für allgemeine Daten unter Verwendung der affinen Transformation y = Wx+b vorgeschlagen.
Wir verringern die Verzerrung der Stimmung auf der Grundlage einer kontrafaktischen Bewertung der Texterstellung mithilfe von Sprachmodellen.
In diesem Artikel wird die semantische Verzerrung in Sprachmodellen gemessen, wie sie sich in dem von den Modellen erzeugten Text widerspiegelt, und es werden andere objektive Begriffe zu den üblichen Sprachmodellierungszielen hinzugefügt, um die Verzerrung zu verringern.
In diesem Papier wird vorgeschlagen, die Verzerrung in vortrainierten Sprachmodellen zu bewerten, indem ein festes Sentiment-System verwendet und verschiedene Präfix-Vorlagen getestet werden.
Eine Methode, die auf semantischer Ähnlichkeit basiert, und eine Methode, die auf sentimentaler Ähnlichkeit basiert, um die neuronalen Sprachmodelle, die aus großen Datensätzen trainiert wurden, zu entzerren.
Ein nichtparametrisches Bayes'sches Themenmodell mit variationalen Autoencodern, das bei öffentlichen Benchmarks den Stand der Technik in Bezug auf Komplexität, Themenkohärenz und Abfrageaufgaben erreicht.
In diesem Beitrag wird ein unendliches Themenmodell mit variationalen Autoencodern konstruiert, indem der Stick-breaking variationale Autoencoder von Nalisnick & Smith mit latenter Dirichlet-Zuweisung und mehreren in Miao verwendeten Inferenztechniken kombiniert wird.
Wir stellen einen neuen Rahmen für die Wissensdestillation vor, der Peer-Beispiele als Lehrer verwendet.
Vorschlagen einer Methode zur Verbesserung der Effektivität der Wissensdestillation, indem die verwendeten Bezeichnungen abgeschwächt werden und ein Datensatz anstelle eines einzelnen Beispiels verwendet wird.
In dieser Arbeit wird vorgeschlagen, den zusätzlichen Rechenaufwand für das Training durch Wissensdestillation zu bewältigen, indem auf der kürzlich vorgeschlagenen Snapshot-Destillationstechnik aufgebaut wird.
Erlernen hierarchischer Teilstrategien durch Ende-zu-Ende Training über eine Verteilung von Aufgaben
Die Autoren betrachten das Problem des Lernens einer sinnvollen Menge von Subregeln, die zwischen Aufgaben geteilt werden können, um das Lernen auf neue Aufgaben aus der Aufgabenverteilung zu starten. 
In dieser Arbeit wird eine neuartige Methode zur Induzierung einer zeitlichen hierarchischen Struktur in einer spezialisierten Multi-Task-Umgebung vorgeschlagen.
Convolutional Neural Network Modell für die unbeaufsichtigte Einbettung von Dokumenten.
Es wird ein neues Modell für die allgemeine Aufgabe der Induktion von Dokumentrepräsentationen (Einbettungen) vorgestellt, das eine CNN-Architektur zur Verbesserung der Recheneffizienz verwendet.
In diesem Artikel wird vorgeschlagen, CNNs mit einem Skip-Gram-ähnlichen Ziel als schnellen Weg zur Ausgabe von Dokumenteneinbettungen zu verwenden.
Wir beweisen Verallgemeinerungsgrenzen für Convolutional neuronale Netze, die die Gewichtskopplung berücksichtigen.
Untersucht die Verallgemeinerungsfähigkeit von CNNs und verbessert die oberen Schranken des Verallgemeinerungsfehlers, wobei eine Korrelation zwischen dem Verallgemeinerungsfehler von gelernten CNNs und dem dominanten Term der oberen Schranke gezeigt wird.
In diesem Papier wird eine Verallgemeinerungsschranke für neuronale Convolutional Neural Networks vorgestellt, die auf der Anzahl der Parameter, der Lipschitz-Konstante und dem Abstand der endgültigen Gewichte von der Initialisierung basiert.
Zweifache Einsparungen bei der Modellgröße, 28 % Energieeinsparung für MobileNets auf ImageNet ohne Genauigkeitsverlust durch hybride Schichten, die aus konventionellen Filtern mit voller Genauigkeit und ternären Filtern bestehen.
Der Schwerpunkt liegt auf der Quantisierung der MobileNets-Architektur auf ternäre Werte, wodurch der Platz- und Rechenbedarf gesenkt wird, um neuronale Netze energieeffizienter zu machen.
Das Papier schlägt eine schichtweise hybride Filter Bank vor, die nur einen Teil der Convolutional Filter auf ternäre Werte für die MobileNets Architektur quantisiert.
Wir haben einen Benchmark für kontrolliertes echtes Stören erstellt und einige interessante Erkenntnisse über gestörte Daten in der realen Welt gewonnen.
In diesem Beitrag werden 6 bestehende Methoden zum Erlernen von gestörten Labeln in zwei Trainingseinstellungen verglichen: von Grund auf und mit Fine-Tuning.
Die Autoren erstellen einen großen Datensatz und einen Benchmark für kontrolliertes echte Störungen, um kontrollierte Experimente mit gestörten Daten beim Deep Learning durchzuführen.
Wir lernen, das RNA-Design Problem mit Reinforcement Learning unter Verwendung von Meta-Learning und AutoML Ansätzen zu lösen.
Anwendung der Policy-Gradienten-Optimierung zur Generierung von RNA-Sequenzen, die sich in eine Ziel-Sekundärstruktur falten, was zu einer deutlichen Verbesserung der Genauigkeit und der Laufzeit führte. 
Das Trainieren kleiner Netze schlägt das Pruning, aber das Pruning findet gute kleine Netze zum Trainieren, die leicht zu kopieren sind.
Wir untersuchen das Problem des Lernens, die zugrundeliegende Vielfalt von Überzeugungen in überwachten Lernbereichen vorherzusagen.
Wir haben eine Strategie eingeführt, die das Inpainting von Modellen auf Datensätzen unterschiedlicher Größe ermöglicht.
Hilfe beim Bild Inpainting mit GANs durch Verwendung eines vergleichenden Augmentierungsfilters und Hinzufügen von Zufallsstörungen zu jedem Pixel.
Wir finden Belege dafür, dass die Minimierung der Divergenz möglicherweise keine genaue Charakterisierung des GAN-Trainings ist.
Der Beitrag zielt darauf ab, empirische Beweise dafür zu liefern, dass die Theorie der Divergenzminimierung eher ein Werkzeug ist, um das Ergebnis des Trainings von GANs zu verstehen, als eine notwendige Bedingung, die während des Trainings selbst durchgesetzt werden muss.
Diese Arbeit untersucht nicht-sättigende GANs und die Auswirkungen von zwei bestraften Gradienten-Ansätzen, unter Berücksichtigung mehrerer Gedankenexperimente, um Beobachtungen zu demonstrieren und sie an realen Datenexperimenten zu validieren.
Ein neuer und praktischer statistischer Test der Abhängigkeit unter Verwendung neuronaler Netze, der an synthetischen und realen fMRI-Datensätzen getestet wurde.
Es wird eine auf neuronalen Netzen basierende Schätzung von gegenseitigen Informationen vorgeschlagen, die zuverlässig mit kleinen Datensätzen arbeiten kann, wobei die Komplexität der Stichprobe durch Entkopplung des Netzwerk-Lernproblems und des Schätzproblems reduziert wird.
Bildbeschriftung durch zweidimensionale Worteinbettung.
LEAP kombiniert die Stärken des adaptiven Sampling mit denen des Mini-Batch-Online-Lernens und des adaptiven Repräsentationslernens, um eine repräsentative, selbstgesteuerte Strategie in einem Ende-zu-Ende DNN Trainingsprotokoll zu formulieren. 
Es wird eine Methode zur Erstellung von Minibatches für ein Schülernetz vorgestellt, bei der ein zweiter erlernter Repräsentationsraum zur dynamischen Auswahl von Beispielen nach ihrer "Einfachheit und wahren Vielfalt" verwendet wird.
Experimente die Klassifizierungsgenauigkeit auf MNIST, FashionMNIST und CIFAR-10 Datensätze zu lernen, eine Darstellung mit Lehrplan Lern-Stil Minibatch-Auswahl in einem Ende-zu-Ende Rahmen.
Wir schlagen vor, Makro-Aktionen mit Hilfe eines genetischen Algorithmus zu konstruieren, der die Abhängigkeit der Makro-Aktionsableitung von den vergangenen Strategien des Agenten eliminiert.
Dieses Papier schlägt einen generischen Algorithmus für die Konstruktion von Makro-Aktionen für Deep Reinforcement Learning vor, indem eine Makro-Aktion an den primitiven Aktionsraum angehängt wird.
Wir schlagen eine Erweiterung von LFADS vor, die in der Lage ist, Spike Trains abzuleiten, um Kalzium-Fluoreszenzspuren mit hierarchischen VAEs zu rekonstruieren.
Wir stellen die erste erfolgreiche Methode vor, um neuronale maschinelle Übersetzung auf unüberwachte Weise zu trainieren, indem wir nichts anderes als einsprachige Korpora verwenden
Die Autoren stellen ein Modell für unüberwachte NMT vor, das keine parallelen Korpora zwischen den beiden interessierenden Sprachen erfordert. 
Dies ist eine Arbeit über unüberwachte MT, die eine Standardarchitektur mit Worteinbettungen in einem gemeinsamen Einbettungsraum nur mit zweisprachigen Wortpapieren und einem Encoder-Decoder trainiert, der mit einsprachigen Daten trainiert wird.
Wir trainieren generative adversarial Netze auf progressive Weise und können so hochauflösende Bilder mit hoher Qualität erzeugen.
Einführung von progressivem Wachstum und einer einfachen parameterfreien statistischen Minibatch-Zusammenfassung zur Verwendung beim GAN-Training, um die Synthese von hochauflösenden Bildern zu ermöglichen.
Ein graphbasiertes sphärisches CNN, das ein interessantes Gleichgewicht von Kompromissen für eine Vielzahl von Anwendungen bietet.
Kombiniert bestehende CNN-Frameworks, die auf der Diskretisierung einer Kugel als Graph basieren, um ein Konvergenzergebnis zu zeigen, das mit der Rotationsäquivalenz auf einer Kugel zusammenhängt.
Die Autoren verwenden die bestehende Graph-CNN-Formulierung und eine Pooling-Strategie, die hierarchische Pixelierungen der Kugel ausnutzt, um aus der diskretisierten Kugel zu lernen.
Wir beweisen Fluktuations-Dissipations-Beziehungen für SGD, die verwendet werden können, um (i) Lernraten adaptiv festzulegen und (ii) Verlustflächen zu untersuchen.
Die Konzepte des Artikels arbeiten im zeitdiskreten Formalismus, verwenden die Master-Gleichung und sind nicht auf eine lokal quadratische Annäherung der Verlustfunktion oder auf Gaußsche Annahmen für SGD-Störungen angewiesen. 
Die Autoren leiten die stationären Fluktuations-Dissipations-Relationen ab, die messbare Größen und Hyperparameter in SGD miteinander verknüpfen, und verwenden die Relationen, um den Trainingsplan adaptiv festzulegen und die Verlustfunktionslandschaft zu analysieren.
Wir schlagen einen Mechanismus zum Entfernen von Störfaktoren des internen Zustands eines RNN vor, um die Generalisierungsleistung zu verbessern.
Für Umgebungen, die teilweise durch externe Eingabeprozesse diktiert werden, leiten wir eine eingabeabhängige Basislinie ab, die nachweislich die Varianz für Policy-Gradienten Methoden reduziert und die Strategie-Leistung in einem breiten Spektrum von RL Aufgaben verbessert.
Die Autoren betrachten das Problem des Lernens in eingabegesteuerten Umgebungen, zeigen, wie das PG-Theorem immer noch für einen eingabebewussten Kritiker gilt, und zeigen, dass eingabeabhängige Basislinien am besten für Vermutungen mit diesem Kritiker zu verwenden sind.
In diesem Beitrag wird der Begriff der eingabeabhängigen Basislinien in Policy-Gradienten Methoden in RL eingeführt und es werden verschiedene Methoden zum Trainieren der eingabeabhängigen Basislinienfunktion vorgeschlagen, um die Varianz von Störungen durch externe Faktoren zu verringern.
Durch die Erweiterung der obersten Schicht eines Klassifizierungsnetzes mit einem Stylespeicher kann es generativ arbeiten.
In dieser Arbeit wird vorgeschlagen, ein neuronales Klassifizierungsnetz nicht nur für die Klassifizierung zu trainieren, sondern auch für die Rekonstruktion einer Repräsentation seiner Eingabe, um die Klasseninformation aus dem Erscheinungsbild zu faktorisieren.
Die Arbeit schlägt vor, einen Autoencoder so zu trainieren, dass die Repräsentation der mittleren Schicht aus dem Klassenlabel des Inputs und einer versteckten Vektordarstellung besteht.
Pro-Beispiel Routing Modelle profitieren von der architektonischen Vielfalt, haben aber immer noch Probleme bei der Skalierung auf eine große Anzahl von Routing Entscheidungen.
Erweitert die Art der architektonischen Einheit, die dem Router bei jeder Entscheidung zur Verfügung steht, und skaliert auf tiefere Netze, um die modernste Leistung auf Omniglot zu erreichen. 
Diese Arbeit erweitert Routing Netzwerke, um verschiedene Architekturen über geroutete Module hinweg zu nutzen.
Wir stellen RNNs für das Training von Ersatzmodellen von PDEs vor, bei denen Konsistenzbeschränkungen sicherstellen, dass die Lösungen physikalisch sinnvoll sind, selbst wenn beim Training viel kleinere Bereiche verwendet werden als das trainierte Modell angewendet wird.
Wir schlagen die Verwendung von optimistischen Spiegeln vor, um Zyklusprobleme beim Training von GANs anzugehen. Außerdem stellen wir den Optimistischen Adam-Algorithmus vor
In dieser Arbeit wird die Verwendung von optimistischem Mirror Descent zum Training von WGANs vorgeschlagen.
Der Artikel schlägt vor, optimistischen Gradientenabstieg für das GAN-Training zu verwenden, der das bei SGD und seinen Varianten beobachtete Zyklusverhalten vermeidet und vielversprechende Ergebnisse beim GAN-Training liefert.
In diesem Beitrag wird eine einfache Modifikation des Standard-Gradientenabstiegs vorgeschlagen, die die Konvergenz von GANs und anderen Minimax-Optimierungsproblemen verbessern soll.
Eine einfache Erweiterung der verallgemeinerten Matrixfaktorisierung kann den Stand der Technik bei Empfehlungen übertreffen.
Die Arbeit stellt ein Matrixfaktorisierungs Framework vor, um den Effekt historischer Daten beim Lernen von Benutzerpräferenzen in kollaborativen Filtereinstellungen zu verstärken.
Eine Methode, die Darstellungen von sequentiellen Daten und ihrer Dynamik durch generative Modelle mit einem aktiven Prozess erstellt.
Kombiniert neuronale Netze und Gaußsche Verteilungen, um eine Architektur und ein generatives Modell für Bilder und Videos zu schaffen, das den Fehler zwischen erzeugten und gelieferten Bildern minimiert.
Die Arbeit schlägt ein Bayes'sches Netzmodell vor, das als neuronales Netz realisiert ist und verschiedene Daten in Form eines linearen dynamischen Systems erlernt.
Wir schlagen Polynome als Aktivierungsfunktionen vor.
Die Autoren führen lernfähige Aktivierungsfunktionen ein, die durch Polynomfunktionen parametrisiert sind, und zeigen Ergebnisse, die etwas besser sind als ReLU.
Eine einfache intrinsische Motivationsmethode, die die Forward Dynamik nutzt, modelliert Fehler im Merkmalsraum des Regelwerks.
Wir zeigen, dass entwirrte VAEs robuster als einfache VAEs gegenüber Angriffen sind, die darauf abzielen, sie zur Dekodierung der adversarial Eingabe für ein ausgewähltes Ziel zu verleiten. Anschließend entwickeln wir ein noch robusteres hierarchisches entschlüsseltes VAE, Seatbelt-VAE.
Die Autoren schlagen ein neues VAE-Modell namens seatbelt-VAE vor, das sich gegenüber latenten Angriffen als robuster erweist als Benchmarks.
Wir zeigen, dass Funktionsänderungen in der Backpropagation gleichbedeutend mit einer impliziten Lernrate sind.
Ein Ansatz des Reinforcement Learrning zur Übertragung von Textstilen.
Stellt eine RL-basierte Methode vor, die ein vortrainiertes Sprachmodell nutzt, um den Textstil zu übertragen, ohne das Ziel der Entflechtung zu verfolgen, und dabei generierte Stilübertragungen eines anderen Modells verwendet.
Die Autoren schlagen eine Kombinationsbelohnung vor, die sich aus Sprachgewandtheit, Inhalt und Stil für die Übertragung von Textstil zusammensetzt.
Wir zeigen, dass in den tiefen generativen Repräsentationen eine hochstrukturierte semantische Hierarchie als Ergebnis für die Synthese von Szenen entsteht.
Die Arbeit untersucht die Aspekte, die durch die latenten Variablen kodiert werden, die in die verschiedenen Schichten von StyleGAN eingegeben werden.
Der Artikel präsentiert eine visuell geführte Interpretation der Aktivierungen der Convolution Layers im Generator von StyleGAN auf Layout, Szenenkategorie, Szeneneigenschaften und Farbe.
Wir fassen Nachrichten zwischen mehreren SMILES-Zeichenfolgen desselben Moleküls zusammen, um Informationen entlang aller Pfade durch den molekularen Graphen zu übermitteln und so latente Darstellungen zu erzeugen, die den Stand der Technik bei einer Vielzahl von Aufgaben deutlich übertreffen.
Die Methode verwendet mehrere Eingaben von SMILES-Zeichenfolgen, zeichenweise Merkmalsfusion über diese Zeichenfolgen und Netzwerktraining durch mehrere Ausgabeziele von SMILES-Zeichenfolgen, wodurch eine robuste latente Repräsentation mit fester Länge unabhängig von SMILES-Variationen geschaffen wird.  
Die Autoren beschreiben ein neuartiges Variations Autoencoder ähnliches Verfahren für Moleküle, das Moleküle als Zeichenketten kodiert, um die für den Informationsaustausch zwischen den Atomen im Molekül erforderlichen Operationen zu reduzieren.
Wir schlagen einen einfachen und allgemeinen Ansatz vor, der das Problem des Moduszusammenbruchs in verschiedenen bedingten GANs vermeidet.
Die Arbeit schlägt einen Regularisierungsterm für das bedingte GAN-Ziel vor, um eine vielfältige multimodale Generierung zu fördern und einen Moduskollaps zu verhindern.
Die Arbeit schlägt eine Methode zur Erzeugung verschiedener Ausgaben für verschiedene bedingte GAN-Frameworks vor, einschließlich Bild-zu-Bild-Übersetzung, Bild-Inpainting und Video-Vorhersage, die auf verschiedene bedingte Synthese-Frameworks für verschiedene Aufgaben angewendet werden können. 
Durch die Ausstattung des Transformermodells mit Abkürzungen zur Einbettungsschicht wird die Modellkapazität für das Lernen neuer Informationen frei.
Wir untersuchen die Beziehung zwischen Wahrscheinlichkeitsdichtewerten und Bildinhalten in nicht-invertierbaren GANs.
Die Autoren versuchen, die Wahrscheinlichkeitsverteilung des Bildes mit Hilfe von GAN zu schätzen und entwickeln eine geeignete Approximation der PDFs im latenten Raum.
Wir schlagen eine räumlich gemischte Convolution vor, bei der die reguläre Convolution die Informationen von außerhalb ihres rezeptiven Feldes einbezieht.
Er schlägt eine SS-Konvulation vor, die Informationen außerhalb ihres RF verwendet und bei Tests mit mehreren CNN-Modellen bessere Ergebnisse erzielt.
Die Autoren schlugen eine Shuffle-Strategie für Convolution Layers in Convolution Layers in Convolutional Neural Networks vor.
Eine Methode zur Modellierung der generativen Verteilung von Sequenzen, die aus graphisch verbundenen Einheiten stammen.
Die Autoren schlagen eine Methode zur Modellierung sequentieller Daten aus mehreren miteinander verbundenen Quellen unter Verwendung einer Mischung aus einem gemeinsamen Pool von HMMs vor.
Unsere Arbeit wendet Meta-Lernen auf Multi-Agenten Reinforcement Learning an, um unserem Agenten zu helfen, sich effizient an neue Gegner anzupassen.
Dieser Beitrag konzentriert sich auf die schnelle Anpassung an neue Verhaltensweisen der anderen Agenten in der Umgebung mit Hilfe einer auf MAML basierenden Methode.
Die Arbeit stellt einen Ansatz für Multi-Agenten-Lernen vor, der auf dem Rahmen des modell-agnostischen Meta-Lernens für die Aufgabe der Gegner-Modellierung für Multi-Agenten RL basiert.
Wir charakterisieren die Singulärwerte der linearen Transformation, die mit einer standardmäßigen 2D Mehrkanal Convolutional Layer verbunden sind, und ermöglichen so deren effiziente Berechnung. 
Der Beitrag widmet sich der Berechnung der Singulärwerte von Convolutional Layers.
Leitet exakte Formeln für die Berechnung der Singulärwerte von Convolutional Layers tiefer neuronaler Netze ab und zeigt, dass die Berechnung der Singulärwerte viel schneller erfolgen kann als die Berechnung der vollständigen SVD der Convolution Matrix, indem man auf schnelle FFT-Transformationen zurückgreift.
VariBAD eröffnet einen Weg zu überschaubarer approximativer Bayes-optimaler Exploration für Deep RL unter Verwendung von Ideen aus Meta-Learning, Bayesian RL und approximativer Variationsinferenz.
In diesem Beitrag wird eine neue Methode des tiefen Reinforcement Learnings vorgestellt, die einen effizienten Kompromiss zwischen Erkundung und Ausbeutung ermöglicht und Meta-Lernen, variationale Inferenz und bayesianisches RL kombiniert.
Wir zeigen, dass metrisches Lernen dazu beitragen kann, katastrophales Vergessen zu reduzieren.
In dieser Arbeit wird metrisches Lernen eingesetzt, um das katastrophale Vergessen in neuronalen Netzen zu reduzieren, indem die Ausdruckskraft der letzten Schicht verbessert wird, was zu besseren Ergebnissen beim kontinuierlichen Lernen führt.
Wir stellen NormCo vor, ein tiefes Kohärenzmodell, das sowohl die Semantik einer Entitätserwähnung als auch die thematische Kohärenz der Erwähnungen innerhalb eines einzelnen Dokuments berücksichtigt, um eine Krankheitsentitätsnormalisierung durchzuführen.
Verwendet einen GRU Autoencoder zur Darstellung des "Kontexts" (verwandte Eigenschaften einer bestimmten Krankheit innerhalb eines Satzes) und löst die BioNLP-Aufgabe mit erheblichen Verbesserungen gegenüber den bekanntesten Methoden.
Wir untersuchen die Rolle der multiplikativen Interaktion als vereinheitlichendes Framework, um eine Reihe klassischer und moderner neuronaler Netzwerkarchitekturen zu beschreiben, wie z.B. Gating, Aufmerksamkeitsschichten, Hypernetze und dynamische Convolutions.
Stellt die multiplikative Interaktion als einheitliche Charakterisierung für die Darstellung häufig verwendeter Komponenten der Modellarchitektur vor und zeigt empirische Beweise für die überlegene Leistung bei Aufgaben wie RL und Sequenzmodellierung.
Die Arbeit untersucht verschiedene Arten von multiplikativen Interaktionen und stellt fest, dass MI-Modelle in der Lage sind, bei Sprachmodellierungs- und Reinforcement Learning Problemen eine Spitzenleistung zu erzielen.
Ein effektives textkonditionierendes GAN-Framework zur Erzeugung von Videos aus Text.
In dieser Arbeit wird eine GAN-basierte Methode zur Videogenerierung auf der Grundlage von Textbeschreibungen vorgestellt, mit einer neuen Konditionierungsmethode, die Convolution Filter aus dem kodierten Text erzeugt und diese für eine Convolution im Diskriminator verwendet.
Diese Arbeit schlägt bedingte GAN-Modelle für die Text-Video-Synthese vor: die Entwicklung von CNN-Filtern mit Textmerkmalen und die Erstellung eines Datensatzes für bewegte Formen mit verbesserter Leistung bei der Video-/Bilderzeugung.
SplitLBI wird auf Deep Learning angewandt, um die strukturelle Spärlichkeit von Modellen zu erforschen. Dabei wird eine Spitzenleistung in ImageNet-2012 erzielt und eine effektive Subnetzarchitektur enthüllt.
Es wird ein optimierungsbasierter Algorithmus vorgeschlagen, um wichtige dünn besetzte Strukturen großer neuronaler Netze zu finden, indem das Lernen der Gewichtsmatrix und die Einschränkung der dünn besetzten Strukturen gekoppelt werden, was garantierte Konvergenz bei nicht-konvexen Optimierungsproblemen bietet.
Wir schlagen Gated-Mechanismen zur Verbesserung der erlernten ISTA für Sparse Coding vor, mit theoretischen Garantien für die Überlegenheit der Methode. 
Es werden Erweiterungen von LISTA vorgeschlagen, die die Unterschätzung durch die Einführung von "Gain Gates" und die Einbeziehung von Impulsen mit "Overshoot Gates" angehen und verbesserte Konvergenzraten zeigen.
Diese Arbeit konzentriert sich auf die Lösung von Sparse-Coding Problemen unter Verwendung von Netzwerken des LISTA-Typs, indem es eine "Gain Gating Funktion" vorschlägt, um die Schwäche der "no false positive" Annahme zu mildern.
Wir stellen einen effizienten und adaptiven Rahmen für den Vergleich von Bildklassifizierern vor, um die Diskrepanzen zwischen den Klassifizierern zu maximieren, anstelle eines Vergleichs auf festen Testsätzen.
Mechanismus zur Fehlersuche, der Bildklassifikatoren vergleicht, indem er ihre "am meisten nicht übereinstimmende" Testmenge stichprobenartig prüft und die Unstimmigkeit durch einen von der WordNet-Ontologie abgeleiteten semantischen Abstand misst.
Wir schlagen eine Technik vor, die CNN-Strukturen modifiziert, um die Robustheit zu verbessern und gleichzeitig eine hohe Testgenauigkeit zu erhalten. Wir stellen in Frage, ob die derzeitige Definition von adversarial Beispielen angemessen ist, indem wir Gegenbeispiele erzeugen, die Menschen täuschen können.
In dieser Arbeit wird eine einfache Technik zur Verbesserung der Robustheit neuronaler Netze gegen Blackbox-Angriffe vorgeschlagen.
Die Autoren schlagen eine einfache Methode vor, um die Robustheit von Convolutional Neural Networks gegenüber adversarial Beispielen zu erhöhen, mit überraschend guten Ergebnissen.
Wir schlagen vor, halb-überwachtes und robustes Lernen auf störhaften Etiketten in einer gemeinsamen Umgebung zu vergleichen.
Die Autoren schlagen eine Strategie für das Training eines Modells in einer formalen Umgebung vor, die auf einer Verwechslung basiert und die semi-supervised und robuste Lernaufgaben als Spezialfälle einschließt.
In dieser Arbeit wird der positive Effekt von Top-Down-Verbindungen im Hierarchical Sparse Coding Algorithmus experimentell nachgewiesen.
In diesem Beitrag wird eine Studie vorgestellt, in der Techniken für die hierarchische spärliche Kodierung verglichen werden. Es wird gezeigt, dass der Top-Down-Term bei der Reduzierung von Vorhersagefehlern von Vorteil ist und schneller lernen kann.
Ein Black-Box-Ansatz zur Erklärung der Vorhersagen eines Bildähnlichkeitsmodells.
Stellt eine Methode zur Erklärung von Bildähnlichkeitsmodellen vor, die Attribute identifiziert, die positiv zur Ähnlichkeitsbewertung beitragen, und sie mit einer generierten Auffälligkeitskarte verbindet.
Die Arbeit schlägt einen Erklärungsmechanismus vor, der die typischen Regionen von Auffälligkeitskarten mit Attributen für die Ähnlichkeitsanpassung tiefer neuronaler Netze verbindet.
Wie Sie gegnerische Angriffe auf seq2seq bewerten sollten.
Die Autoren untersuchen Möglichkeiten zur Generierung von Gegenbeispielen und zeigen, dass das adversarial Training mit dem Angriff, der am besten mit den eingeführten Kriterien für den Bedeutungserhalt übereinstimmt, zu einer verbesserten Robustheit gegenüber dieser Art von Angriff führt, ohne dass es zu einer Verschlechterung in der nicht-adversarischen Umgebung kommt.
Das Papier handelt von bedeutungserhaltenden adversen Störungen im Kontext von Seq2Seq-Modellen
Eine alternative Normalisierungstechnik zur Batch-Normalisierung
Führt eine Normalisierungstechnik ein, die die Gewichte von Convolutional Layers normalisiert. 
In diesem Manuskript wird eine neue schichtweise Transformation, EquiNorm, zur Verbesserung der Batch Normalisierung eingeführt, die nicht die Eingaben in die Schichten, sondern die Schichtgewichte verändert.
Stellen Sie jede Entität als eine Wahrscheinlichkeitsverteilung über Kontexte dar, die in einen Grundraum eingebettet sind.
Es wird vorgeschlagen, Worteinbettungen aus einem Histogramm über Kontextwörter zu konstruieren, anstatt als Punktvektoren, was die Messung von Entfernungen zwischen zwei Wörtern im Sinne eines optimalen Transports zwischen den Histogrammen durch eine Methode ermöglicht, die die Darstellung einer Entität vom Standard "Punkt in einem Vektorraum" zu einem Histogramm mit Bins an einigen Punkten in diesem Vektorraum erweitert. 
In Anbetracht der beobachteten Fehlerquoten von Modellen außerhalb der natürlichen Datenverteilung sollte man mit kleinen negativen Störungen rechnen.
In diesem Beitrag wird eine alternative Betrachtungsweise für adversarial Beispiele in hochdimensionalen Räumen vorgeschlagen, indem die "Fehlerrate" in einer Gauß-Verteilung betrachtet wird, die an jedem Testpunkt zentriert ist.
Untersucht, wie selbstüberwachtes Lernen und Wissensdestillation im Zusammenhang mit der Erstellung kompakter Modelle zusammenwirken.
Untersucht das Training von kompakten, vortrainierten Sprachmodellen durch Destillation und zeigt, dass die Verwendung eines Lehrers zur Destillation eines kompakten Schülermodells besser funktioniert als das direkte Vortraining des Modells.
Dieser Beitrag zeigt, dass das Vortraining eines Schülers direkt auf die maskierte Sprachmodellierung besser ist als die Destillation, und dass es am besten ist, beides zu kombinieren und von diesem vortrainierten Schülermodell zu destillieren.
Wir stellen das universelle Komprimierungsschema für tiefe neuronale Netze vor, das universell für die Komprimierung beliebiger Modelle anwendbar ist und unabhängig von deren Gewichtsverteilung nahezu optimal funktioniert.
Es wird eine Pipeline für die Netzwerkkomprimierung eingeführt, die der tiefen Komprimierung ähnelt und randomisierte Gitterquantisierung anstelle der klassischen Vektorquantisierung sowie universelle Quellcodierung (bzip2) anstelle der Huffman-Codierung verwendet.
In diesem Beitrag wird versucht, die Entflechtung theoretisch in einer idealistischen Situation und praktisch durch die Modellierung von Noise in einem realistischen Fall zu untersuchen.
Untersucht die Bedeutung der Modellierung mit Störungen in der Gaußschen VAE und schlägt vor, die Störungen mit Hilfe der Empirical-Bayes-Methode zu trainieren.
Änderung der Behandlung von Störfaktoren bei der Entwicklung von VAE-Modellen
Wir untersuchen die Gewichtsabnahme-Regularisierung für verschiedene Optimierer und identifizieren drei verschiedene Mechanismen, durch die die Gewichtsabnahme die Generalisierung verbessert.
Diskutiert die Auswirkung der Gewichtsabnahme auf das Training von Deep-Network-Modellen mit und ohne Batch Normalisierung und bei Verwendung von Optimierungsmethoden erster/zweiter Ordnung und stellt die Hypothese auf, dass eine größere Lernrate einen Regularisierungseffekt hat.
Der erste frei verfügbare Domänenanpassungs Datensatz für die Erkennung von Schallereignissen.
Schätzer der gegenseitigen Information auf der Grundlage der nicht-extensiven statistischen Mechanik.
In diesem Beitrag wird versucht, neuartige Variationsschranken für die gegenseitige Information aufzustellen, indem der Parameter q eingeführt und die q-Algebra definiert wird. Es wird gezeigt, dass die Schranken eine geringere Varianz haben und hohe Werte erreichen.
Wir zeigen, dass der stochastische Gradientenabstieg zu einem globalen Optimum für WGAN mit einem einschichtigen Generatoren-Netzwerk konvergiert.
Es wird versucht zu beweisen, dass der Stochastic Gradient Descent-Ascent zu einer globalen Lösung für das Min-Max Problem des WGAN konvergieren kann.
Wir zeigen empirisch, dass das adversarische Training universelle Störungen wirksam beseitigt, die adversarischen Beispiele weniger robust gegenüber Bildtransformationen macht und sie für einen Erkennungsansatz erkennbar bleiben.
Analysiert adversariales Training und seine Auswirkung auf universelle adversarische Beispiele sowie standardmäßige (Basisiteration) adversarische Beispiele und wie adversariales Training die Erkennung beeinflusst. 
Die Autoren zeigen, dass adversarial Training wirksam gegen "gemeinsame" adversarial Störungen schützt, insbesondere gegen universelle Störungen, aber weniger wirksam gegen singuläre Störungen.
Wir stellen Techniken vor, um ein einziges Netzwerk zu trainieren, das für viele Hardware-Plattformen geeignet ist.
Die Methode führt zu einem Netz, aus dem man Teilnetze für verschiedene Ressourcenbeschränkungen (Latenz, Speicher) extrahieren kann, die gute Leistungen erbringen, ohne dass ein erneutes Training erforderlich ist.
In diesem Beitrag wird versucht, das Problem der Suche nach den besten Architekturen für spezielle ressourcenbeschränkte Einsatzszenarien mit einer auf Vorhersagen basierenden NAS-Methode anzugehen.
Vorschlag für einen Ansatz zur Verstärkung generativer Modelle durch Kaskadierung von Modellen mit verborgenen Variablen
In diesem Papier wird ein neuartiger Ansatz des kaskadierten Boostings für generative Modelle vorgeschlagen, bei dem jedes Meta-Modell separat und gierig trainiert werden kann.
Wir untersuchen die Satzstruktur in ELMo und verwandten kontextuellen Einbettungsmodellen. Wir stellen fest, dass bestehende Modelle die Syntax effizient kodieren und Hinweise auf weitreichende Abhängigkeiten zeigen, aber nur geringe Verbesserungen bei semantischen Aufgaben bieten.
Die von den Autoren vorgeschlagene "Edge Probing"-Methode konzentriert sich auf die Beziehungen zwischen den Bereichen und nicht auf die einzelnen Wörter, was es den Autoren ermöglicht, syntaktische Konstituenten, Abhängigkeiten, Entitätskennzeichnungen und semantische Rollenbezeichnungen zu untersuchen.
Bietet neue Einblicke in die Erfassung kontextualisierter Worteinbettungen durch die Zusammenstellung einer Reihe von Edge Probing Aufgaben. 
Wir stellen DPFRL vor, ein Rahmenwerk für Reinforcement Learning unter partiellen und komplexen Beobachtungen mit einem voll differenzierbaren diskriminativen Partikelfilter.
Es werden Ideen für das Training von DLR-Agenten mit latenten Zustandsvariablen vorgestellt, die als Glaubensverteilung modelliert sind, so dass sie mit teilweise beobachteten Umgebungen umgehen können.
Diese Arbeit stellt eine prinzipielle Methode für POMDP RL vor: Diskriminatives Partikelfilter Reinforcement Learning, das es erlaubt, mit partiellen Beobachtungen über mehrere Zeitschritte zu argumentieren und dabei den Stand der Technik in Benchmarks zu erreichen.
Monte Carlo Ziele werden mit Hilfe der Variationsinferenz für Hilfsvariablen analysiert, was zu einer neuen Analyse von CPC und NCE sowie zu einem neuen generativen Modell führt.
Vorschlage einer anderen Sichtweise zur Verbesserung der Variationsschranken mit latenten Hilfsvariablenmodellen und untersuchen der Verwendung dieser Modelle im generativen Modell.
Wir verbessern den Ablauf aller bestehenden Gradientenabstiegsalgorithmen.
Die Autoren schlagen vor, stochastische Gradienten aus einer monotonen Funktion, die proportional zu den Gradientenstärken ist, mit Hilfe von LSH abzutasten. 
Betrachtet SGD über ein Ziel in der Form einer Summe über Beispiele eines quadratischen Verlustes.
Die Grenzen der gegenwärtigen KI sind allgemein anerkannt, aber weniger Menschen sind sich bewusst, dass wir genug über das Gehirn wissen, um sofort neue KI-Formulierungen anzubieten.
Wir verwenden die Beantwortung von Fragen, um zu bewerten, wie viel Wissen über die Umwelt Agenten durch selbstüberwachte Vorhersage lernen können.
Er schlägt QA als ein Werkzeug vor, um zu untersuchen, was Agenten in der Welt lernen, und argumentiert, dass dies eine intuitive Methode für Menschen ist, die beliebige Komplexität zulässt.
Die Autoren schlagen ein Framework vor, um die von prädiktiven Modellen erstellten Repräsentationen zu bewerten, die genügend Informationen enthalten, um Fragen über die Umgebung zu beantworten, auf die sie trainiert wurden. Sie zeigen, dass die von SimCore erstellten Repräsentationen genügend Informationen enthielten, damit das LSTM die Fragen genau beantworten konnte.
Wir entwickeln eine neue Methode zur unausgewogenen Klassifizierung unter Verwendung von adversarial Beispielen.
Schlägt ein neues Optimierungsziel vor, das synthetische Stichproben erzeugt, indem es die Mehrheitsklassen anstelle der Minderheitsklassen überabtasten lässt und so das Problem der Überanpassung der Minderheitsklassen löst.
Die Autoren schlagen vor, die Ungleichgewichtsklassifizierung mit Hilfe von Re-Sampling Methoden anzugehen. Sie zeigen, dass Gegenbeispiele in der Minderheitenklasse helfen würden, ein neues Modell zu trainieren, das besser verallgemeinert.
Eine interessante Anwendung von CNN in Experimenten zur Physik der weichen kondensierten Materie.
Die Autoren zeigen, dass ein Deep-Learning Ansatz sowohl die Erkennungsgenauigkeit als auch die Erkennungsrate von Defekten in nematischen Flüssigkristallen verbessern kann.
Anwendung eines bekannten neuronalen Modells (YOLO) zur Erkennung von Bounding Boxes von Objekten in Bildern.
Eine Analyse der Auswirkungen von Kompositionalität und Lokalität auf das Repräsentationslernen beim Zero-Shot Lernen.
Schlägt einen Evaluierungsrahmen für ZSL vor, bei dem das Modell nicht vortrainiert werden darf und stattdessen die Modellparameter zufällig initialisiert werden, um besser zu verstehen, was in ZSL passiert.
Bei allen untersuchten Datensätzen und Modellen weisen die Fehler des Gegners eine ähnliche Potenzgesetzform auf, und die Architektur spielt eine Rolle.
Wir stellen eine Formulierung von Neugier als ein visuelles Repräsentationslernproblem vor und zeigen, dass sie gute visuelle Repräsentationen in Agenten ermöglicht.
In diesem Beitrag wird neugierbasiertes RL-Training als Lernen eines visuellen Repräsentationsmodells formuliert, wobei argumentiert wird, dass die Konzentration auf bessere LR und die Maximierung des Modellverlusts für neuartige Szenen eine bessere Gesamtleistung ergibt.
Aus einem unvollständigen RGB-D-Scan einer Szene wollen wir die einzelnen Objektinstanzen, aus denen die Szene besteht, erkennen und ihre vollständige Objektgeometrie ableiten.
Vorschlagen einer durchgängigen 3D CNN-Struktur, die Farbmerkmale und 3D-Merkmale kombiniert, um die fehlende 3D-Struktur einer Szene aus RGB-D-Scans vorherzusagen.
Die Autoren schlagen ein neuartiges durchgängiges 3D Convolutional Network vor, das die semantische 3D Instanzvervollständigung in Form von Objektbegrenzungsrahmen, Klassenbezeichnungen und vollständiger Objektgeometrie vorhersagt.
XGAN ist ein unüberwachtes Modell für die Bild-zu-Bild-Übersetzung auf Merkmalsebene, das auf semantische Stilübertragungsprobleme wie die Gesicht-zu-Karikatur-Aufgabe angewendet wird, für die wir einen neuen Datensatz vorstellen.
In diesem Beitrag wird ein neues GAN-basiertes Modell für die ungepaarte Bild-zu-Bild-Übersetzung vorgeschlagen, das dem DTN ähnelt.
Die Arbeiter senden Gradientenzeichen an den Server, und die Aktualisierung wird durch Mehrheitsabstimmung entschieden. Wir zeigen, dass dieser Algorithmus konvergent, kommunikationseffizient und fehlertolerant ist, sowohl in der Theorie als auch in der Praxis.
Stellt eine verteilte Implementierung von signSGD mit Mehrheitsentscheidung als Aggregation vor.
Wir korrigieren unerwünschte Abweichungen bei der Einbettung von Bildern in verschiedenen Bereichen, wobei nur relevante Informationen erhalten bleiben.
Erörtert eine Methode zur Anpassung von Bildeinbettungen, um technische Variationen von biologischen Signalen zu trennen.
Die Autoren stellen eine Methode vor, um domänenspezifische Informationen zu entfernen und gleichzeitig die relevanten biologischen Informationen zu erhalten, indem sie ein Netzwerk trainieren, das die Wasserstein-Distanz zwischen den Distrbutionen minimiert.
Ein in Stichprobengröße und Dimensionen skalierbarer Schätzer der gegenseitigen Information.
Die neue Kombination aus verstärktem und überwachtem Lernen, die die Anzahl der erforderlichen Proben für das Training auf Videos drastisch reduziert.
In dieser Arbeit wird vorgeschlagen, gelabelte kontrollierte Daten zu nutzen, um das verstärkungsbasierte Lernen einer Kontrollpolitik zu beschleunigen.
Schnelles Lernen über das episodische Gedächtnis, verifiziert durch einen biologisch plausiblen Rahmen für den präfrontalen Kortex-Basalganglien-Hippocampus-Schaltkreis (PFC-BG).
In dieser Arbeit zeigen wir eine neue Verbindung zwischen der Ausdruckskraft von DNNs und Sharkovskys Theorem aus dynamischen Systemen auf, die es uns ermöglicht, den Tiefen-Breiten Kompromiss von ReLU-Netzen zu charakterisieren .
Zeigt, wie die Ausdruckskraft von NN von ihrer Tiefe und Breite abhängt, und fördert das Verständnis für den Nutzen tiefer Netze zur Darstellung bestimmter Funktionsklassen.
Die Autoren leiten mit Hilfe der Analyse dynamischer Systeme Bedingungen für einen Tiefen-Breiten Kompromiss ab, wenn ReLu-Netze in der Lage sind, periodische Funktionen darzustellen.
Wir untersuchen quantisierungsbewusstes Training in sehr niedrig quantisierten Keyword-Spottern, um die Kosten für das On-Device Keyword-Spotting zu reduzieren.
In diesem Beitrag wird eine Kombination aus Low-Rank-Decomposition und Quanitization-Ansatz zur Komprimierung von DNN-Modellen für das Keyword-Spotting vorgeschlagen.
Ein neuartiger Rahmen für die Graphsignalverarbeitung zur Quantifizierung der Auswirkungen von experimentellen Störungen in biomedizinischen Einzelzelldaten.
In diesem Beitrag werden mehrere Methoden zur Verarbeitung von Versuchsergebnissen zu biologischen Zellen vorgestellt und ein MELD-Algorithmus vorgeschlagen, der harte Gruppenzuordnungen auf weiche Zuordnungen abbildet, so dass relevante Gruppen von Zellen geclustert werden können.
Wir schlagen eine Klasse von Benutzermodellen vor, die auf der Anwendung von Gaußschen Prozessen auf einen durch Entscheidungsregeln definierten transformierten Raum basieren.
Wir schlagen einen Bayes-optimalen Bayes'schen Optimierungsalgorithmus für das Tuning von Hyperparametern vor, der billige Approximationen ausnutzt.
Untersucht die Optimierung von Hyperparametern durch Bayes'sche Optimierung unter Verwendung des Knowledge Gradient Frameworks und ermöglicht dem Bayes'schen Optimierer die Abstimmung von Treue und Kosten.
Wir verifizieren die Robustheit von tiefen neuronalen Modellen mit über 100.000 ReLUs, wobei wir mehr Beispiele als der Stand der Technik zertifizieren und mehr negative Beispiele finden als ein starker Angriff erster Ordnung.
Führt eine sorgfältige Studie über gemischt-ganzzahlige lineare Programmieransätze zur Überprüfung der Robustheit neuronaler Netze gegenüber Störungen durch Gegner durch und schlägt drei Verbesserungen der MILP-Formulierungen zur Überprüfung neuronaler Netze vor.
Eine Reihe von Methoden, um eine Unsicherheitsabschätzung eines beliebigen Modells zu erhalten, ohne es neu zu entwerfen, neu zu trainieren oder zu fine-tunen.
Beschreibt mehrere Ansätze zur Messung der Unsicherheit in beliebigen neuronalen Netzen, wenn während des Trainings keine Verzerrung auftritt.
Vorgeschlagene Operation höherer Ordnung für kontextbezogenes Lernen.
Es wird ein neuer 3D Convolutional Block vorgeschlagen, der den Video-Input mit seinem Kontext verarbeitet, basierend auf der Annahme, dass relevanter Kontext um das Objekt des Bildes herum vorhanden ist.
Konsistenzbasierte Modelle für halbüberwachtes Lernen konvergieren nicht zu einem einzigen Punkt, sondern erkunden weiterhin eine Reihe plausibler Lösungen am Rande eines flachen Bereichs. Die Mittelwertbildung trägt zur Verbesserung der Generalisierungsleistung bei.
Der Artikel schlägt vor, Stochastic Weight Averaging auf den Kontext des halbüberwachten Lernens anzuwenden. Es wird argumentiert, dass die halbüberwachten MT/Pi-Modelle besonders gut für SWA geeignet sind und schlägt schnelles SWA vor, um das Training zu beschleunigen.
Wir konvertieren erfolgreich einen populären Detektor RPN in einen gut funktionierenden Tracker aus der Sicht der Verlustfunktion.
Eine neuronale Architektur zur Bewertung und Einstufung von Programmreparaturkandidaten, um semantische Programmreparaturen statisch ohne Zugriff auf Unit-Tests durchzuführen.
Stellt eine neuronale Netzarchitektur vor, die aus den Teilen share, specialize und compete besteht, um Code in vier Fällen zu reparieren.
Ist es möglich, die Genauigkeit, Robustheit und Effizienz von Modellen mitzugestalten, um ihre dreifachen Ziele zu erreichen? Ja!
Nutzt input-adaptive Mehrfach-Frühausgänge für den Bereich des adversarial Angriffs und der Verteidigung, indem es die durchschnittliche Inferenzkomplexität reduziert, ohne der Annahme einer größeren Kapazität zu widersprechen.
Wir zeigen, dass einzelne Einheiten in CNN Repräsentationen, die in NLP Aufgaben erlernt werden, selektiv auf bestimmte natürliche Sprachkonzepte reagieren.
Verwendet grammatikalische Einheiten natürlicher Sprache, die Bedeutungen bewahren, um zu zeigen, dass die Einheiten von tiefen CNNs, die in NLP Aufgaben gelernt wurden, als Konzeptdetektor für natürliche Sprache fungieren können.
Es handelt sich dabei um eine überwiegend theoretische Arbeit, die die Herausforderungen bei der Entflechtung von Variationsfaktoren unter Verwendung von Autoencodern und GAN beschreibt.
Dieser Beitrag befasst sich mit der Entflechtung von Variationsfaktoren in Bildern, zeigt, dass man im Allgemeinen ohne weitere Annahmen nicht zwischen zwei verschiedenen Variationsfaktoren unterscheiden kann, und schlägt eine neuartige AE+GAN-Architektur vor, um zu versuchen, die Variationsfaktoren zu entflechten.
Diese Arbeit untersucht die Herausforderungen bei der Entflechtung unabhängiger Variationsfaktoren bei schwach markierten Daten und führt den Begriff der Referenzmehrdeutigkeit für die Zuordnung von Datenpunkten ein.
Lernen, mit mehreren Einbettungen und Aufmerksamkeiten zu rangieren.
Es wird vorgeschlagen, die Aufmerksamkeit zu nutzen, um mehrere Eingabedarstellungen sowohl für die Suchanfrage als auch für die Suchergebnisse in der Aufgabe "Lernen zu ranken" zu kombinieren.
Wir haben einen Algorithmus entwickelt, der als Eingabe Aufzeichnungen neuronaler Aktivität nimmt und Cluster von Neuronen nach Zelltyp und Modelle neuronaler Aktivität liefert, die durch diese Cluster eingeschränkt werden.
Wir überwachen neuronale Graphen-Netzwerke, um Zwischenergebnisse und schrittweise Ausgaben klassischer Graphen-Algorithmen zu imitieren, und gewinnen dabei sehr günstige Erkenntnisse.
Schlägt vor, neuronale Netze so zu trainieren, dass sie Graphenalgorithmen imitieren, indem sie Primitive und Unterroutinen lernen und nicht die endgültige Ausgabe.
Wir beschreiben eine Architektur zur Generierung verschiedener Hypothesen für Zwischenziele bei Robotermanipulationsaufgaben.
Bewertet die Qualität eines vorgeschlagenen generativen Vorhersagemodells zur Erstellung von Plänen für die Roboterausführung.
In dieser Arbeit wird eine Methode zum Erlernen einer hochrangigen Übergangsfunktion vorgeschlagen, die für die Aufgabenplanung nützlich ist.
Dieser Artikel bietet eine neuartige Analyse von adaptiven Gradientenalgorithmen zur Lösung von nicht-konvexen, nicht-konkaven Min-Max-Problemen als GANs und erklärt den Grund, warum adaptive Gradientenmethoden ihre nicht-adaptiven Gegenstücke durch empirische Studien übertreffen.
Entwickelt Algorithmen für die Lösung von Variationsungleichungen im stochastischen Umfeld und schlägt eine Variante der Extragradientenmethode vor.
Wir lernen sohpisticated Trajektorien eines Objekts rein aus Pixeln mit einem Spielzeug Videodatensatz durch die Verwendung einer VAE-Struktur mit einem Gauß Prozess Prior.
Wir untersuchen die neuronalen Grundlagen der Traumerinnerung mit Hilfe von Convolutional Neural Networks und Techniken zur Visualisierung von Merkmalen, wie tSNE und Guided Backpropagation.
In diesem Papier werden eine neue Formulierung und ein neues Kommunikationsprotokoll für vernetzte Multi-Agenten Kontrollprobleme vorgeschlagen
Befasst sich mit N-MARLs, bei denen die Agenten ihre Politik nur auf der Grundlage von Nachrichten von benachbarten Knoten aktualisieren, und zeigt, dass die Einführung eines räumlichen Diskontierungsfaktors das Lernen stabilisiert.
Die VB mit mittlerem Feld verwendet doppelt so viele Parameter; wir binden die Varianzparameter in der VB mit mittlerem Feld ohne Verlust an ELBO, wodurch wir an Geschwindigkeit und geringeren Varianzgradienten gewinnen.
Wir nutzen einige wenige Schlüsselwörter als schwache Überwachung für das Training neuronaler Netze zur Extraktion von Aspekten.
Erörtert eine Variante der Wissensdestillation, bei der ein "Lehrer" auf der Grundlage eines Bag-of-Words-Klassifikators mit Startwörtern und ein "Schüler", der ein auf Einbettung basierendes neuronales Netz ist, verwendet werden.
Horizontale und von oben nach unten gerichtete Rückkopplungsverbindungen sind für komplementäre Wahrnehmungsgruppierungsstrategien in biologischen und rekurrenten Sehsystemen verantwortlich.
Unter Verwendung neuronaler Netze als Computermodell des Gehirns wird die Effizienz verschiedener Strategien zur Lösung von zwei visuellen Aufgaben untersucht.
Wir stellen GAN-TTS vor, ein Generatives Adversariales Netzwerk für Text-to-Speech, das einen Mean Opinion Score (MOS) von 4,2 erreicht.
Löst die GAN-Herausforderung bei der Synthese von Rohwellenformen und beginnt, die bestehende Leistungslücke zwischen autoregressiven Modellen und GANs für Rohaudios zu schließen.
Wir schlagen einen Lernalgorithmus für Netzwerk Pruning vor, indem wir Strukturspärlichkeitsstrafen durchsetzen.
In diesem Beitrag wird ein Ansatz für das Pruning beim Training eines Netzes unter Verwendung von Lasso- und Split-LBI-Penalties vorgestellt.
Wir stellen unüberwachtes kontinuierliches Lernen (UCL) und eine neuroinspirierte Architektur vor, die das UCL-Problem löst.
Vorschlag Hierarchien von STAM-Modulen zu verwenden, um das UCL-Problem zu lösen, und erbringen des Nachweis, dass die von den Modulen erlernten Repräsentationen für die Few-Shot Klassifizierung gut geeignet sind.
Neue Methode zur Signalextraktion im Fourier-Bereich.
Trägt eine komplexwertige Convolutional Version der merkmalsweisen linearen Modulation bei, die eine Parameteroptimierung ermöglicht und einen Verlust entwirft, bei dem Betrag und Phase berücksichtigt werden.
Wir stellen ein neuartiges Framework vor, mit dem sich die getrennte Darstellung von Inhalt und Stil auf völlig unüberwachte Weise erlernen lässt. 
Vorschlag eines Modells auf der Grundlage eines Autoencoders zur Entflechtung der Darstellung eines Objekts; die Ergebnisse zeigen, dass das Modell Darstellungen erzeugen kann, die Inhalt und Stil erfassen.
Wir entwickeln eine CATE-Schätzungsstrategie, die sich einige der faszinierenden Eigenschaften neuronaler Netze zunutze macht. 
Zeigt Verbesserungen von X-learner durch die Modellierung der Behandlungsreaktionsfunktion, der Kontrollreaktionsfunktion und der Abbildung des unterstellten Behandlungseffekts auf den bedingten durchschnittlichen Behandlungseffekt als neuronale Netze.
Die Autoren schlagen den Y-Learner zur Schätzung des bedingten durchschnittlichen Behandlungseffekts (CATE) vor, der gleichzeitig die Parameter der Ergebnisfunktionen und den CATE-Schätzer aktualisiert.
Geräteunabhängige Firmware-Ausführung.
Eine Architektur für tabellarische Daten, die Verzweigungen von Entscheidungsbäumen nachbildet und eine dichte Restkonnektivität verwendet. 
In dieser Arbeit wird ein tiefer neuronaler Wald vorgeschlagen, ein Algorithmus, der auf tabellarische Daten abzielt und die Stärken des Gradient Boosting von Entscheidungsbäumen integriert.
Eine neuartige neuronale Netzwerkarchitektur, die die Funktionsweise von Entscheidungswäldern nachahmt, um das allgemeine Problem des Trainings von tiefen Modellen für tabellarische Daten zu lösen, und die eine mit GBDT vergleichbare Effektivität aufweist.
YellowFin ist ein SGD-basierter Optimierer, der sowohl die Dynamik als auch die Lernrate anpassen kann.
Schlägt eine Methode zur automatischen Abstimmung des Momentum-Parameters in Momentum-SGD-Methoden vor, die bessere Ergebnisse und eine schnellere Konvergenzgeschwindigkeit als der moderne Adam-Algorithmus erzielt.
Angriffe auf den latenten Raum von variationalen Autoencodern zur Veränderung der semantischen Bedeutung von Eingaben.
Dieses Papier befasst sich mit Sicherheit und maschinellem Lernen und schlägt einen Man-in-Middle-Angriff vor, der die VAE-Kodierung der Eingabedaten so verändert, dass die dekodierte Ausgabe falsch klassifiziert wird.
Eine empirische Studie, die die Wirksamkeit verschiedener Encoder-Decoder-Kombinationen für die Aufgabe des Dependency Parsing untersucht.
Empirische Analyse verschiedener Kodierer, Dekodierer und deren Abhängigkeiten für graphbasiertes Dependency Parsing.
Lehrer, der Meta-Lernende wie Menschen ausbildet.
Wir führen einen Ansatz zur Einbettung des Raums ein, um die Wahrscheinlichkeitsverteilung der Ausgabe eines neuronalen Netzes einzuschränken.
In diesem Beitrag wird eine Methode zum halbüberwachten Lernen mit tiefen neuronalen Netzen vorgestellt, und das Modell erreicht bei einem geringen Trainingsumfang eine relativ hohe Genauigkeit.
In dieser Arbeit wird die Label-Verteilung in das Modelllernen integriert, wenn nur eine begrenzte Anzahl von Trainingsinstanzen zur Verfügung steht, und es werden zwei Techniken vorgeschlagen, um das Problem der falsch verzerrten Output-Label Verteilung zu lösen.
Wir stellen eine neue Art von tiefer kontextualisierter Wortrepräsentation vor, die den Stand der Technik für eine Reihe von anspruchsvollen NLP-Aufgaben deutlich verbessert.
In dieser Arbeit wird eine neuartige Verlustfunktion für das robuste Training von DNN zur zeitlichen Lokalisierung in Gegenwart von falsch ausgerichteten Etiketten eingeführt.
Ein neuer Verlust für Trainingsmodelle, die vorhersagen, wo Ereignisse in einer Trainingssequenz mit störhaften Beschriftungen auftreten, indem sie geglättete Beschriftung und Vorhersagesequenz vergleichen.
Wir führen den Begriff der gemischten Tensorzerlegungen ein und beweisen damit, dass die Verbindung von erweiterten Convolutional Networks deren Ausdruckskraft erhöht.
In diesem Beitrag wird theoretisch bestätigt, dass die Verbindung von Netzen mit unterschiedlichen Dilatationen zu einer ausdrucksstarken Effizienz bei der gemischten Tensorzerlegung führen kann.
Die Autoren untersuchen erweiterte Convolutional Networks und zeigen, dass die Verflechtung von zwei erweiterten Convolutional Networks A und B in verschiedenen Stadien ausdrucksstärker ist als eine Nichtverflechtung.
Es zeigt sich, dass die strukturelle Annahme eines einzigen perfekten Binärbaums im WaveNet seine Leistung beeinträchtigt und dass WaveNet-ähnliche Architekturen mit komplexeren gemischten Baumstrukturen besser abschneiden.
Multi-Task-Lernen funktioniert.
In diesem Beitrag wird ein neuronales Multitasking-Netzwerk zur Klassifizierung von MNIST-ähnlichen Datensätzen vorgestellt.
Wir bieten eine prinzipielle, optimierungsbasierte Neubetrachtung des Begriffs der adversarial Beispiele und entwickeln Methoden, die Modelle hervorbringen, die gegen eine Vielzahl von Gegnern robust sind.
Untersucht eine Minimax Formulierung für das Lernen von tiefen Netzwerken, um deren Robustheit zu erhöhen, wobei der projizierte Gradientenabstieg als Hauptgegner verwendet wird. 
In dieser Arbeit wird vorgeschlagen, neuronale Netze durch den Rahmen von Sattelpunktproblemen widerstandsfähig gegen gegnerische Verluste zu machen. 
Viele Graphklassifizierungsdatensätze weisen Duplikate auf, was Fragen hinsichtlich der Generalisierungsfähigkeiten und des fairen Vergleichs der Modelle aufwirft. 
Die Autoren diskutieren den Isomorphismus-Bias in Graphen-Datensätzen, den Overfitting-Effekt beim Lernen von Netzwerken, wenn Graphen-Isomorphismus-Merkmale in das Modell aufgenommen werden, theoretisch analog zu Datenleckeffekten.
Wir führen einen Begriff von konservativ-extrapolierten Wertfunktionen ein, die nachweislich zu Strategien führen, die sich selbst korrigieren können, um nahe an den Demonstrationszuständen zu bleiben, und lernen sie mit einer neuartigen negativen Sampling-Technik.
Ein Algorithmus namens Wertiteration mit negativem Sampling, um das Problem der Kovariatenverschiebung beim Imitationslernen zu lösen.
Kontrastiv trainierte strukturierte Weltmodelle (C-SWMs) lernen objektorientierte Zustandsdarstellungen und ein relationales Modell einer Umgebung aus rohen Pixeleingaben.
Die Autoren überwinden das Problem der Verwendung von pixelbasierten Verlusten bei der Konstruktion und dem Lernen von strukturierten Weltmodellen, indem sie einen kontrastiven latenten Raum verwenden.
Unüberwachte Methoden zum Auffinden, Analysieren und Kontrollieren wichtiger Neuronen in der NMT
In dieser Arbeit wird vorgeschlagen, "sinnvolle" Neuronen in neuronalen maschinellen Übersetzungsmodellen zu finden, indem ein Ranking auf der Grundlage der Korrelation zwischen Modellpaaren, verschiedenen Epochen oder verschiedenen Datensätzen erstellt wird, und es wird ein Kontrollmechanismus für die Modelle vorgeschlagen.
Wir präsentieren doppelt spärlichen Softmax, die spärliche Mischung aus spärlichen Experten, um die Effizienz der Softmax-Inferenz durch Ausnutzung der zweistufigen überlappenden Hierarchie zu verbessern. 
Die Arbeit schlägt die neue Softmax-Algorithmus-Implementierung mit zwei hierarchischen Ebenen der Spärlichkeit, die den Betrieb in der Sprach Modellierung beschleunigt.
In diesem Beitrag werden empirische Belege für die Entdeckung eines Generalisierungsindikators vorgestellt: die Entwicklung des Kosinusabstands zwischen dem Gewichtsvektor jeder Schicht und ihrer Initialisierung während des Trainings.
Modelle von Quellcode, die globale und strukturelle Merkmale kombinieren, lernen leistungsfähigere Darstellungen von Programmen.
Eine neue Methode zur Modellierung des Quellcodes für die Fehlerbehebung unter Verwendung eines Sandwich-Modells wie [RNN GNN RNN], das die Lokalisierungs- und Reparaturgenauigkeit erheblich verbessert.
Inkrementelle RNNs lösen das Problem der Explosion/des verschwindenden Gradienten, indem sie die Zustandsvektoren auf der Grundlage der Differenz zwischen dem vorherigen Zustand und dem durch eine ODE vorhergesagten Zustand aktualisieren.
Die Autoren befassen sich mit dem Problem der Signalausbreitung in rekurrenten neuronalen Netzen, indem sie ein Attraktorsystem für den Signalübergang aufbauen und prüfen, ob es zu einem Gleichgewicht konvergiert. 
Wir liefern Beweise gegen klassische Behauptungen über den Kompromiss zwischen Verzerrung und Varianz und schlagen eine neuartige Zerlegung der Varianz vor.
Wir haben ein neuartiges Deep Learning System zur Bildklassifizierung vorgeschlagen, das sowohl Bilder genau klassifizieren als auch die Privatsphäre der Nutzer schützen kann.
In diesem Beitrag wird ein Verfahren vorgeschlagen, das die privaten Informationen im Bild bewahrt und die Nutzbarkeit des Bildes nicht beeinträchtigt.
In dieser Arbeit wird vorgeschlagen, adversarische Netzwerke zu verwenden, um Bilder zu verschleiern und sie so ohne Bedenken hinsichtlich des Datenschutzes zu sammeln, um sie für das Training von maschinellen Lernmodellen zu verwenden.
Ein 2vec-Modell für Transaktionsgraphen von Kryptowährungen.
Die Arbeit schlägt vor, einen Autoencoder, networkX, und node2Vec zu verwenden, um vorherzusagen, ob eine Bitcoin-Adresse nach einem Jahr leer sein wird, aber die Ergebnisse sind schlechter als eine bestehende Basislinie.
Konvergenzbeweis der stochastischen Subgradientenmethode und Variationen bei konvex-konkaven Minimax-Problemen
Eine Analyse des simultanen stochastischen Subgradienten, des simultanen Gradienten mit Optimismus und des simultanen Gradienten mit Verankerung im Kontext von konvex-konkaven Minmax Spielen.
Dieses Papier analysiert die Dynamik des stochastischen Gradientenabstiegs, wenn er auf konvex-konkave Spiele angewendet wird, sowie GD mit Optimismus und einen neuen verankerten GD-Algorithmus, der unter schwächeren Annahmen konvergiert als SGD oder SGD mit Optimismus.
Wir schlagen einen algorithmischen Rahmen vor, um Konstellationen von kleinen Raumfahrzeugen mit 3-DOF-Neuorientierungsfähigkeiten zu planen, die mit Inter-Sat-Verbindungen vernetzt sind.
In diesem Beitrag wird ein Kommunikationsmodul zur Optimierung des Kommunikationsplans für das Problem von Raumfahrzeugkonstellationen vorgeschlagen und der Algorithmus in verteilten und zentralisierten Einstellungen verglichen.
Wir haben einen neuartigen komprimierten Kernelized Importance Sampling Algorithmus vorgeschlagen.
Wir untersuchen die Struktur der Ridge-Regression in einem hochdimensionalen asymptotischen Framework und gewinnen Erkenntnisse über Cross-Validation und Sketching.
Eine theoretische Untersuchung der Ridge-Regression unter Ausnutzung einer neuen asymptotischen Charakterisierung des Ridge-Regressionsschätzers.
Wir analysieren die Verlustlandschaft von neuronalen Netzen mit Aufmerksamkeit und erklären, warum Aufmerksamkeit beim Training neuronaler Netze hilfreich ist, um eine gute Leistung zu erzielen.
Diese Arbeit beweist aus theoretischer Sicht, dass Aufmerksamkeitsnetzwerke besser generalisieren können als Nicht-Aufmerksamkeits-Baselines für feste Aufmerksamkeit (ein- und mehrschichtig) und Selbstaufmerksamkeit in der einschichtigen Umgebung.
Mischungsmodell für neuronale Entflechtung.
Wir haben robuste Schätzungen der gegenseitigen Information für DNNs entwickelt und sie verwendet, um die Kompression in Netzwerken mit nicht sättigenden Aktivierungsfunktionen zu beobachten.
In dieser Arbeit wurde die weit verbreitete Annahme untersucht, dass tiefe neuronale Netze bei überwachten Aufgaben eine Informationskompression durchführen.
In diesem Papier wird eine Methode zur Schätzung der gegenseitigen Information für Netze mit unbeschränkten Aktivierungsfunktionen und die Verwendung der L2-Regularisierung vorgeschlagen, um eine stärkere Kompression zu erreichen.
Wir stellen das TimbreTron vor, eine Pipeline zur Durchführung von qualitativ hochwertigem Timbre-Transfer auf musikalische Wellenformen unter Verwendung von CQT-Domain-Transfer.
Eine Methode zur Konvertierung von Aufnahmen eines bestimmten Musikinstruments in ein anderes durch Anwendung von CycleGAN, das für die Übertragung von Bildern entwickelt wurde, um Spektrogramme zu übertragen.
Die Autoren verwenden mehrere Techniken/Werkzeuge, um eine neuronale Klangfarbenübertragung (Umwandlung von Musik von einem Instrument in ein anderes) ohne gepaarte Trainingsbeispiele zu ermöglichen. 
Beschreibt ein Modell für die Übertragung von musikalischen Klangfarben. Die Ergebnisse zeigen, dass das vorgeschlagene System sowohl für die Übertragung von Tonhöhe und Tempo als auch für die Anpassung der Klangfarbe effektiv ist.
In diesem Beitrag wird Deep Rewiring vorgestellt, ein Algorithmus, mit dem tiefe neuronale Netze trainiert werden können, wenn die Konnektivität des Netzes während des Trainings stark eingeschränkt ist.
Ein Ansatz zur Implementierung von Deep Learning direkt auf spärlich verbundenen Graphen, der ein effizientes Online-Training von Netzwerken und schnelles und flexibles Lernen ermöglicht.
Die Autoren stellen einen einfachen Algorithmus vor, der mit begrenztem Speicherplatz trainieren kann
Bestehende Pruning-Methoden versagen, wenn sie auf GANs angewendet werden, die komplexe Aufgaben bewältigen. Daher stellen wir eine einfache und robuste Methode zum Pruning von Generatoren vor, die für eine Vielzahl von Netzwerken und Aufgaben gut funktioniert.
Die Autoren schlagen eine Modifikation der klassischen Destillationsmethode für die Aufgabe der Komprimierung eines Netzwerks vor, um das Versagen früherer Lösungen bei der Anwendung auf generative adversarische Netzwerke zu beheben.
Wir stellen fest, dass 99,9 % des Gradientenaustauschs bei verteiltem SGD überflüssig sind; wir reduzieren die Kommunikationsbandbreite um zwei Größenordnungen, ohne an Genauigkeit zu verlieren. 
In dieser Arbeit wird eine zusätzliche Verbesserung gegenüber dem Gradient Dropping vorgeschlagen, um die Kommunikationseffizienz zu steigern.
Wir schlagen das Netzwerk Exemplar Guided & Semantically Consistent Image-to-Image Translation (EGSC-IT) vor, das den Übersetzungsprozess an ein Beispielbild in der Zieldomäne koppelt.
Erörtert ein zentrales Versagen und die Notwendigkeit von I2I-Übersetzungsmodellen.
Die Arbeit untersucht die Idee, dass ein Bild zwei Komponenten hat, und wendet ein Aufmerksamkeitsmodell an, bei dem die Merkmalsmasken, die den Übersetzungsprozess steuern, keine semantischen Bezeichnungen benötigen.
Einfügen von Graphenstrukturen in neuronale Netzschichten zur Verbesserung der visuellen Interpretierbarkeit.
Ein neuartiger Regularisierer, der den verborgenen Schichten eines Neuronalen Netzes eine Graphenstruktur auferlegt, um die Interpretierbarkeit der verborgenen Repräsentationen zu verbessern.
Hervorheben des Beitrags des graphischen spektralen Regularisierers zur Interpretierbarkeit neuronaler Netze.
Wir zeigen, dass energiebasierte Modelle, die auf den Residuen eines autoregressiven Sprachmodells trainiert werden, effektiv und effizient zur Texterzeugung eingesetzt werden können. 
Ein vorgeschlagenes, auf Restenergie basierendes Modell (EBM) für die Texterstellung, das auf Satzebene arbeitet und daher BERT nutzen kann, erreicht eine geringere Komplexität und wird bei der menschlichen Bewertung bevorzugt.
Systematische Untersuchung großer cachebasierter Bilderkennungsmodelle mit besonderem Augenmerk auf deren Robustheitseigenschaften.
In dieser Arbeit wurde vorgeschlagen, den Cache-Speicher zu nutzen, um die Robustheit gegenüber ungünstigen Bildbeispielen zu verbessern, und man kam zu dem Schluss, dass die Verwendung eines großen kontinuierlichen Cachespeichers der harten Aufmerksamkeit nicht überlegen ist.
Das Papier beschreibt einen flexiblen Rahmen für den Aufbau von CNNs, die äquivariant zu einer großen Klasse von Transformationsgruppen sind.
Ein Rahmenwerk für den Aufbau von Gruppen-CNN mit einer beliebigen Lie Gruppe G, das bei der Klassifizierung von Tumoren und der Lokalisierung von Landmarken eine Überlegenheit gegenüber einem CNN zeigt. 
Ein Benchmarking von neun repräsentativen globalen Pooling-Systemen zeigt einige interessante Ergebnisse.
Für feinkörnige Klassifizierungsaufgaben wurde in dieser Arbeit bestätigt, dass maxpooling spärlichere Merkmalskarten als avgpooling fördert und diese übertrifft. 
Die Selbstüberwachung verbessert die Few-Shot Erkennung in kleinen und schwierigen Datensätzen, ohne auf zusätzliche Daten angewiesen zu sein; zusätzliche Daten helfen nur, wenn sie aus demselben oder einem ähnlichen Bereich stammen.
Eine empirische Studie verschiedener Methoden des selbstüberwachten Lernens (SSL), die zeigt, dass SSL mehr hilft, wenn der Datensatz schwieriger ist, dass der Bereich für das Training wichtig ist und eine Methode zur Auswahl von Proben aus einem unbeschrifteten Datensatz. 
Wir erstellen abstrakte Modelle von Umgebungen aus Erfahrung und nutzen sie, um neue Aufgaben schneller zu lernen.
Eine Methode, die die Idee der MDP-Homomorphismen nutzt, um ein komplexes MDP mit einem kontinuierlichen Zustandsraum in ein einfacheres zu transformieren.
Wir erweitern die Netzwerk-Dissektion um die Interpretation von Handlungen und untersuchen interpretierbare Merkmalspfade, um die konzeptuelle Hierarchie zu verstehen, die zur Klassifizierung einer Handlung verwendet wird.
Wir schlagen ein neues Modell zur Darstellung von Noten und ihren Eigenschaften vor, das die automatische Melodieerzeugung verbessern kann.
In diesem Beitrag wird ein generatives Modell der symbolischen (MIDI-)Melodie in der westlichen Populärmusik vorgeschlagen, das Notensymbole zusammen mit Zeit- und Dauerinformationen kodiert, um musikalische "Worte" zu bilden.
In der Arbeit wird vorgeschlagen, die Erzeugung von Melodien zu erleichtern, indem Noten als "Wörter" dargestellt werden, die alle Eigenschaften der Note repräsentieren und so die Erzeugung von musikalischen "Sätzen" ermöglichen.
Eine Methode, die automatisch Schichten in neuronalen Netzen wachsen lässt, um die optimale Tiefe zu ermitteln.
Ein Rahmen für das Training eines flachen Netzes und das Hinzufügen neuer Schichten, der Einblicke in das Paradigma der "wachsenden Netze" bietet.
Erforschung des bereichsinternen Repräsentationslernens für Fernerkundung von Datensätze.
In dieser Arbeit wurden mehrere standardisierte Fernerkundungsdatensätze bereitgestellt und es wurde gezeigt, dass die bereichsinterne Repräsentation bessere Basisergebnisse für die Fernerkundung liefern kann als das Fine-Tuning mit ImageNet oder das Lernen von Grund auf.
Vermeiden Sie es, Antworten Wort für Wort zu generieren, indem Sie eine schwache Überwachung verwenden, um einen Klassifikator zu trainieren, der eine vollständige Antwort auswählt.
Eine Möglichkeit zur Generierung von Antworten für medizinische Dialoge unter Verwendung eines Klassifikators zur Auswahl aus von Experten kuratierten Antworten auf der Grundlage des Gesprächskontextes.
SGD-trainierte CNNs mit endlicher Breite vs. unendlich breite, vollständig Bayes'sche CNNs. Wer gewinnt?
Die Arbeit stellt eine Verbindung zwischen einem Bayes'schen Convolutional Neural Network mit unendlich vielen Kanälen und Gauß'schen Prozessen her.
Wir skalieren die Bayes'sche Inferenz auf die ImageNet-Klassifikation und erzielen wettbewerbsfähige Ergebnisse hinsichtlich Genauigkeit und Unsicherheitskalibrierung.
Ein adaptiver Noise-MCMC Algorithmus für die Bildklassifikation, der den Impuls und die Störungen, die auf jede Parameteraktualisierung angewendet werden, dynamisch anpasst, robust gegenüber Overfitting ist und ein Unsicherheitsmaß mit Vorhersagen liefert. 
Eine empirische Studie über gefälschte Bilder zeigt, dass die Textur ein wichtiger Hinweis darauf ist, dass sich gefälschte Bilder von echten Bildern unterscheiden. Unser verbessertes Modell, das globale Texturstatistiken erfasst, zeigt eine bessere GAN-übergreifende Erkennungsleistung für gefälschte Bilder.
Die Arbeit schlägt einen Weg vor, um die Leistung des Modells für die Erkennung gefälschter Gesichter in Bildern, die von einem GAN generiert wurden, zu verbessern, damit es auf der Grundlage von Texturinformationen verallgemeinerbar ist.
Die Wasserstein-Distanz ist mit stochastischem Gradientenabstieg schwer zu minimieren, während die Cramer-Distanz leicht optimiert werden kann und genauso gut funktioniert.
In dem Manuskript wird vorgeschlagen, die Cramer-Distanz als Verlust bei der Optimierung einer Zielfunktion mit stochastischem Gradientenabstieg zu verwenden, da sie unverzerrte Stichprobengradienten aufweist.
Der Beitrag des Artikels bezieht sich auf Leistungskriterien, insbesondere auf die Wasserstein/Mallows-Metrik.
Wir lernen den Pfeil der Zeit für MDPs und nutzen ihn, um die Erreichbarkeit zu messen, Nebenwirkungen zu erkennen und ein Neugier-Belohnungssignal zu erhalten. 
In dieser Arbeit wird das h-Potenzial als Lösung für ein Ziel vorgeschlagen, das die Asymmetrie der Zustandsübergänge in einem MDP misst.
Wir haben SGD als ein Bayes'sches Filterproblem formuliert und zeigen, dass dies zu RMSprop, Adam, AdamW, NAG und anderen Merkmalen moderner adaptiver Methoden führt.
Das Papier analysiert den stochastischen Gradientenabstieg durch Bayes'sche Filterung als Rahmen für die Analyse adaptiver Methoden.
Die Autoren versuchen, bestehende adaptive Gradientenmethoden im Rahmen der Bayes'schen Filterung mit dem dynamischen Prior zu vereinen.
Wir führen die Idee des kontradiktorischen Lernens in die automatische Datenerweiterung ein, um die Generalisierung eines Zielnetzes zu verbessern.
Eine Technik namens Adversarial AutoAugment, die während des Trainings mit Hilfe eines adversariellen Ansatzes dynamisch gute Datenerweiterungsstrategien erlernt.
Die Studie stellt zwei Ansätze zur Verbesserung der Generalisierung von Meta-Lernen erster Ordnung vor und präsentiert eine empirische Evaluierung zur Few-Shot Klassifizierung von Bildern.
Die Arbeit präsentiert eine empirische Studie des Meta-Lernalgorithmus erster Ordnung Reptile Algorithmus, der eine vorgeschlagene Regularisierungstechnik und tiefere Netzwerke untersucht.
In dieser Arbeit wird vorgeschlagen, die Matrixfaktorisierung zur Trainingszeit für die neuronale maschinelle Übersetzung zu verwenden, wodurch die Modellgröße und die Trainingszeit ohne Leistungseinbußen verringert werden können.
In dieser Arbeit wird vorgeschlagen, Modelle mit Hilfe der Matrixfaktorisierung während des Trainings für tiefe neuronale Netze der maschinellen Übersetzung zu komprimieren.
Verschiedene Methoden zur Analyse des BERT führen zu unterschiedlichen (aber kompatiblen) Schlussfolgerungen in einer Fallstudie über NPIs.
In dieser Arbeit stellen wir V1Net vor - ein neuartiges rekurrentes neuronales Netzwerk, das kortikale horizontale Verbindungen modelliert, die zu robusten visuellen Repräsentationen durch Wahrnehmungsgruppierung führen.
Die Autoren schlagen vor, eine Convolutional Variante des LSTM zu modifizieren, um horizontale Verbindungen einzubeziehen, die von bekannten Interaktionen im visuellen Kortex inspiriert sind.
Wir schlagen eine Verbindung zwischen Permutationsäquivarianz und kompositioneller Verallgemeinerung vor und bieten äquivariante Sprachmodelle
Diese Arbeit konzentriert sich auf das Lernen von lokal äquivarianten Repräsentationen und Funktionen über Eingabe-/Ausgabewörter für die Zwecke der SCAN-Aufgabe.
Das Papier schlägt einen Algorithmus vor, um die Flexibilität des Variationsposteriores in Bayesschen Neuronalen Netzen durch iterative Optimierung zu erhöhen.
Eine Methode zum Trainieren flexibler Variationsposteriori-Verteilungen, angewandt auf Bayes'sche neuronale Netze, um Variationsinferenz (VI) über die Gewichte durchzuführen.
Neuer hochmoderner Rahmen für die Bildwiederherstellung.
In dem Artikel wird eine Architektur für ein Convolutional Neural Network vorgeschlagen, das Blöcke für lokale und nicht-lokale Aufmerksamkeitsmechanismen enthält, die angeblich für die Erzielung hervorragender Ergebnisse in vier Bildwiederherstellungsanwendungen verantwortlich sind.
In diesem Beitrag wird ein nicht-lokales Aufmerksamkeitsnetz für die Bildwiederherstellung vorgeschlagen.
Hybrider Ansatz zur Modellerstellung, der einen Mangel an verfügbaren Daten durch bereichsspezifisches Wissen von Experten ausgleicht.
Ein Domänenerfassungsansatz, der eine andere Darstellung des partiellen Domänenmodells in Betracht zieht, indem er schematische Mutex-Relationen anstelle von Vor- und Nachbedingungen verwendet.
Wir veröffentlichen einen Datensatz, der aus Einleitungs-EKG-Daten von 11.000 Patienten besteht, denen das {DEVICENAME}(TM)-Gerät verschrieben wurde.
Diese Arbeit beschreibt einen großen EKG-Datensatz, den die Autoren zu veröffentlichen beabsichtigen, und bietet eine unüberwachte Analyse und Visualisierung des Datensatzes.
Eine neuartige Context-Gated Convolution, die durch explizite Modulation von Convolutional Kernels globale Kontextinformationen in CNNs einbezieht und so repräsentativere lokale Muster erfasst und diskriminierende Merkmale extrahiert.
In dieser Arbeit wird globaler Kontext verwendet, um die Gewichte von Convolutional Layers zu modulieren und CNNs dabei zu helfen, diskriminantere Merkmale mit hoher Leistung und weniger Parametern zu erfassen als bei der Modulation von Merkmalskarten.
Wir analysieren den Kompromiss zwischen Quantisierungsstörungen und Clipping-Verzerrung in Netzwerken mit geringer Präzision und zeigen deutliche Verbesserungen gegenüber Standard-Quantisierungsschemata, die normalerweise Clipping vermeiden.
Leitet eine Formel zur Ermittlung der minimalen und maximalen Clipping-Werte für eine gleichmäßige Quantisierung ab, die den quadratischen Fehler, der sich aus der Quantisierung ergibt, entweder für eine Laplace- oder Gauß-Verteilung über den vorquantisierten Wert minimiert.
Wir schlagen eine neuartige Normalisierungsmethode vor, um Fälle mit kleinen Batch Größen zu behandeln.
Eine Methode zur Bewältigung des Problems der kleinen Batch Größe von BN, die den gleitenden Mittelwert ohne allzu großen Aufwand anwendet und die Anzahl der Statistiken von BN für eine bessere Stabilität reduziert.
ReLU MLP Tiefenseparationsbeweis mit gemoterischen Argumenten.
Ein Beweis dafür, dass tiefere Netze weniger Einheiten benötigen als flachere für eine Familie von Problemen. 
Ein neuer GAN-basierter Few-Shot Lernalgorithmus durch Synthese verschiedener und diskriminierender Merkmale.
Eine Meta-Lernmethode, die ein generatives Modell erlernt, das die Unterstützungsmenge eines Few-Shot Lerners, der eine Kombination von Verlusten optimiert, erweitern kann.
Wir zeigen, wie sich die Struktur von Datensätzen auf neuronale Netze auswirkt, und stellen ein generatives Modell für synthetische Datensätze vor, das diese Auswirkungen reproduziert.
In dem Beitrag wird untersucht, wie sich verschiedene Einstellungen der Datenstruktur auf das Lernen neuronaler Netze auswirken und wie das Verhalten auf realen Datensätzen beim Lernen auf einem synthetischen Datensatz nachgeahmt werden kann.
Wir trainieren tiefe neuronale Netze, die auf diagonalen und zirkulanten Matrizen basieren, und zeigen, dass diese Art von Netzen sowohl kompakt als auch genau in realen Anwendungen sind.
Die Autoren liefern eine theoretische Analyse der Ausdruckskraft von diagonalen zirkulanten neuronalen Netzen (DCNN) und schlagen ein Initialisierungsschema für tiefe DCNNs vor.
Wir schlagen vor, die Modelldestillation zu nutzen, um globale additive Erklärungen in Form von Merkmalsformen (die aussagekräftiger sind als Merkmalszuweisungen) für Modelle wie neuronale Netze zu lernen, die auf tabellarischen Daten trainiert werden.
In diesem Beitrag werden verallgemeinerte additive Modelle (GAMs) mit Modelldestillation eingesetzt, um globale Erklärungen für neuronale Netze zu liefern.
Ein groß angelegter Multi-Task Lernrahmen mit verschiedenen Trainingszielen zum Erlernen von Satzrepräsentationen fester Länge.
In dieser Arbeit geht es um das Erlernen von Satzeinbettungen durch die Kombination verschiedener Trainingssignale: Überspringen von Gedanken, Vorhersage von Übersetzungen, Klassifizierung von Entailment-Beziehungen und Vorhersage der Konstituentenparse.
Wir schlagen einen neuronalen Bias Annotator vor, um Modelle auf ihre Robustheit gegenüber verzerrten Textdatensätzen zu testen.
Eine Methode zur Erzeugung voreingenommener Datensätze für NLP, die auf einem bedingten, adversarial regularisierten Autoencoder (CARA) beruht.
Wir schlagen vor, Themenmodelle im VAE-Stil zu überwachen, indem wir den Prior auf intelligenter Basis pro Dokument anpassen. Wir finden, dass ein Logit-Normal-Posterior die beste Leistung liefert.
Eine flexible Methode zur schwachen Überwachung eines Themenmodells, um eine bessere Anpassung an die Intuition der Benutzer zu erreichen.
Erste umfassende Analyse der Informationsebene von großen tiefen neuronalen Netzen unter Verwendung von matrixbasierter Entropie und Tensorkernels.
Die Autoren schlagen einen Tensor-Kernel-basierten Schätzer für die Schätzung der gegenseitigen Information zwischen hochdimensionalen Schichten in einem neuronalen Netz vor.
Wir schlagen einen modularen Rahmen vor, der durch Programme spezifizierte Aufgaben bewältigen kann und eine Zero-Shot Verallgemeinerung auf komplexere Aufgaben ermöglicht.
Diese Arbeit untersucht das Training von RL-Agenten mit Anweisungen und Aufgabendekompositionen, die als Programme formalisiert sind, und schlägt ein Modell für einen programmgesteuerten Agenten vor, der ein Programm interpretiert und Teilziele für ein Aktionsmodul vorschlägt.
Wir beweisen, dass ein zufällig initialisierter (stochastischer) Gradientenabstieg einen Convolutional Filter in polynomieller Zeit lernt.
Untersucht das Problem des Lernens eines einzelnen Convolutional Filter mit SGD und zeigt, dass SGD unter bestimmten Bedingungen einen einzelnen Convolutional Filter lernt.
In dieser Arbeit wird die Annahme der Gaußschen Verteilung auf eine allgemeinere Annahme der Winkelglätte erweitert, die eine größere Familie von Eingangsverteilungen abdeckt.
Die erste Datenerweiterungsmethode, die speziell zur Verbesserung der allgemeinen Robustheit von DNN entwickelt wurde, ohne Hypothesen über die angreifenden Algorithmen aufzustellen.
Es wird eine Trainingsmethode zur Datenerweiterung vorgeschlagen, um die Robustheit des Modells gegenüber Störungen durch den Gegner zu erhöhen, indem gleichmäßige Zufallsstichproben aus einer Kugel mit festem Radius, die auf die Trainingsdaten zentriert ist, erweitert werden. 
Verwendung von Wasserstein-GANs zur Erzeugung realistischer neuronaler Aktivität und zur Erkennung der wichtigsten Merkmale in neuronalen Populationsmustern.
Eine Methode zur Simulation von Spike Trains von Neuronenpopulationen, die mit empirischen Daten übereinstimmen, unter Verwendung eines semi-convolutional GAN.
In der Arbeit wird vorgeschlagen, GANs für die Synthese realistischer neuronaler Aktivitätsmuster zu verwenden.
Doppelt reparametrisierte Gradientenschätzer bieten eine unvoreingenommene Varianzreduktion, die zu einer verbesserten Leistung führt.
Der Autor fand experimentell heraus, dass der Schätzer der bestehenden Arbeit (STL) voreingenommen ist und schlägt vor, die Voreingenommenheit zu reduzieren, um den Gradientenschätzer des ELBO zu verbessern.
Gradientless Descent ist ein nachweislich effizienter gradientenfreier Algorithmus, der monoton-invariant und schnell für hochdimensionale Optimierung nullter Ordnung ist.
Diese Arbeit schlägt stabile GradientLess Descent (GLD) Algorithmen vor, die sich nicht auf eine Gradientenschätzung verlassen.
Wir schlagen eine neue Klasse von visuellen generativen Modellen vor: zielkonditionierte Prädiktoren. Wir zeigen experimentell, dass die Konditionierung auf das Ziel es ermöglicht, die Unsicherheit zu reduzieren und Vorhersagen über viel längere Zeiträume zu erstellen.
In dieser Arbeit wird das Problem der Videovorhersage als Interpolation statt als Extrapolation neu formuliert, indem die Vorhersage auf das Start- und Endbild (Zielbild) konditioniert wird, was zu einer höheren Qualität der Vorhersagen führt.
Wir schlagen ein tiefes Multiinstanz-Lernsystem vor, das auf rekurrenten neuronalen Netzen basiert und Pooling-Funktionen und Aufmerksamkeitsmechanismen für die Annotation von Konzepten verwendet.
Das Papier befasst sich mit der Klassifizierung medizinischer Zeitreihendaten und schlägt vor, die zeitliche Beziehung zwischen den Instanzen in jeder Reihe mithilfe einer rekurrenten neuronalen Netzwerkarchitektur zu modellieren. 
Vorschlagen einer neuartigen Multiple Instance Learning (MIL) Formulierung, die Relation MIL (RMIL) genannt wird, und erörtern eine Reihe ihrer Varianten mit LSTM, Bi-LSTM, S2S usw. und untersuchen der Integration von RMIL mit verschiedenen Aufmerksamkeitsmechanismen und demonstrieren ihrer Verwendung bei der Vorhersage medizinischer Konzepte aus Zeitreihendaten. 
Die Einbettungsschichten werden mit der Tensor-Train-Zerlegung faktorisiert, um ihren Speicherbedarf zu reduzieren.
Diese Arbeit schlägt ein Low-Rank-Tensor-Zerlegungsmodell zur Parametrisierung der Einbettungsmatrix in der natürlichen Sprachverarbeitung (NLP) vor, das das Netzwerk komprimiert und manchmal die Testgenauigkeit erhöht.
Regulierung des Gewichtsabfalls in adaptiven Gradientenmethoden wie Adam.
Schlägt vor, den Gewichtsabfall von der Anzahl der Optimierungsschritte zu entkoppeln.
In der Arbeit wird eine alternative Methode zur Umsetzung des Gewichtsverfalls in Adam vorgestellt, für die empirische Ergebnisse vorliegen.
Untersucht die Probleme des Gewichtsverfalls in den SGD-Varianten und schlägt eine Entkopplungsmethode zwischen dem Gewichtsverfall und der gradientenbasierten Aktualisierung vor.
Lebenslanges verteilungsbasiertes Lernen durch eine Schüler-Lehrer Architektur in Verbindung mit einem modellübergreifenden Posterior-Regulierer.
Deep Autoencoders zum Erlernen einer guten Darstellung für geometrische 3D-Punktwolkendaten; Generative Modelle für Punktwolken.
Ansätze zum Erlernen generativer Modelle vom Typ GAN unter Verwendung der PointNet-Architektur und des Latent-Space-GAN.
Wir schlagen eine neue Methode zur Unterdrückung der Anfälligkeit des latenten Merkmalsraums vor, um robuste und kompakte Netzwerke zu erhalten.
In dieser Arbeit wird die Methode des "adversen neuronalen Pruning" vorgeschlagen, bei der eine Pruning-Maske und ein neuer Verlust zur Unterdrückung von Schwachstellen trainiert wird, um die Genauigkeit und die Robustheit gegenüber Angriffen zu verbessern.
Wir haben zwei VAE-Modifikationen vorgeschlagen, die negative Datenbeispiele berücksichtigen, und sie für die halbüberwachte Erkennung von Anomalien verwendet.
In den Beiträgen werden zwei VAE-ähnliche Methoden für die halbüberwachte Neuheitserkennung vorgeschlagen, MML-VAE und DP-VAE.
Ein neues Verständnis der Trainingsdynamik und Metriken der Auswendiglernhärte führen zu effizientem und nachweisbarem Lernen von Curriculums.
Diese Arbeit formuliert DIH als ein Curriclum Learning Problem, das die Daten zum Trainieren von DNNs effektiver nutzen kann, und leitet die Theorie der Approximationsgrenze ab.
Geschichte der parallelen Entwicklungen von Aktualisierungsgesetzen und Konzepten zwischen adaptiver Steuerung und Optimierung beim maschinellen Lernen.
Rekurrente Convoltion für die Modellkompression und ein Trick für das Training, d.h. das Lernen unabhängiger BN-Schichten über Schritte.
Der Autor modifiziert das Recurrent Convoltional Neural Network (RCNN) mit unabhängiger Batch Normalisierung, wobei die experimentellen Ergebnisse des RCNN mit der Architektur des neuronalen Netzwerks ResNet kompatibel sind, wenn es die gleiche Anzahl von Schichten enthält.
Dynamische rezeptive Felder mit räumlicher Gaußstruktur sind genau und effizient.
In diesem Beitrag wird ein strukturierter Faltungsoperator zur Modellierung von Deformationen lokaler Bildregionen vorgeschlagen, der die Anzahl der Parameter erheblich reduziert.
Ein Trick für adversarial Beispiele, damit die falsch klassifizierten Labels im Bezeichnungsraum für menschliche Beobachter nicht wahrnehmbar sind.
Eine Methode zur Konstruktion von Angriffen, die von Menschen weniger leicht erkannt werden können, indem die Zielklasse so verändert wird, dass sie der Originalklasse des Bildes ähnlich ist.
In diesem Beitrag wird eine Klassifizierung der Lärmarten und -positionen verschiedener Trittschallgeräusche vorgestellt, die in einem Gebäude erzeugt werden und ein ernsthaftes Konfliktproblem in Wohnkomplexen darstellen.
Diese Arbeit beschreibt den Einsatz von Convolutional Neural Networks in einem neuartigen Anwendungsbereich der Klassifizierung von Gebäudelärmarten und Lärmpositionen. 
Rekurrente neuronale Netze lernen, um die Dimensionalität ihrer internen Repräsentation in Abhängigkeit von der Dynamik des Ausgangsnetzes aufgabengerecht zu erhöhen und zu verringern.
Anstelle von strikten Verteilungsanpassungen in traditionellen tiefen Domänenanpassungszielen, die versagen, wenn sich die Verteilung der Zielbeschriftungen ändert, schlagen wir vor, ein entspanntes Ziel mit neuen Analysen, neuen Algorithmen und experimenteller Validierung zu optimieren.
In dieser Arbeit werden entspannte Metriken für die Bereichsanpassung vorgeschlagen, die neue theoretische Grenzen für den Zielfehler setzen.
Wir untersuchen die Aufgabe der Generierung von Zusammenfassungen zu Artikeln und schlagen ein hierarchisches Generierungsschema zusammen mit einem gemeinsamen Ende-zu-Ende Reinforcement Learning Framework zum Trainieren des hierarchischen Modells vor.
Um das Problem der Degeneration bei der Generierung von Zusammenfassungen zu Artikeln zu lösen, wird in diesem Artikel ein hierarchischer Generierungsansatz vorgeschlagen, der zunächst eine Zwischenversion des Artikels und dann den vollständigen Artikel generiert.
Wir schlagen eine kontrafaktische Regularisierung vor, um uns vor nachteiligen Domänenverschiebungen zu schützen, die durch Verschiebungen in der Verteilung der latenten "Stilmerkmale" von Bildern entstehen.
Die Arbeit erörtert Möglichkeiten zum Schutz vor nachteiligen Domänenverschiebungen mit kontrafaktischer Regularisierung durch das Erlernen eines Klassifikators, der gegenüber oberflächlichen Veränderungen (oder "Stil"-Merkmalen) in der Vorstellungswelt invariant ist.
Diese Arbeit zielt auf eine robuste Bildklassifizierung gegen nachteilige Veränderungen im Bereich ab, und das Ziel wird erreicht, indem die Verwendung von sich ändernden Stilmerkmalen vermieden wird.
Wir schlagen einen Meta-Lerner vor, der sich schnell an mehrere Aufgaben anpassen kann, sogar in einem Schritt in einer Few-Shot Einstellung.
In diesem Beitrag wird eine Methode zum Meta-Lernen eines Gradientenkorrekturmoduls vorgeschlagen, bei der die Vorkonditionierung durch ein neuronales Netz parametrisiert wird und ein zweistufiger Gradientenaktualisierungsprozess während der Anpassung eingebaut wird. 
Modelle zur Beantwortung von Fragen, die die gemeinsame Verteilung von Fragen und Antworten modellieren, können mehr lernen als unterschiedliche Modelle.
In diesem Beitrag wird ein generativer Ansatz für die textuelle und visuelle Qualitätssicherung vorgeschlagen, bei dem eine gemeinsame Verteilung über den Frage- und Antwortraum unter Berücksichtigung des Kontexts erlernt wird, wodurch komplexere Beziehungen erfasst werden.
In diesem Beitrag wird ein generatives Modell für die Beantwortung von Fragen vorgestellt und vorgeschlagen, p(q,a|c) zu modellieren, faktorisiert als p(a|c) * p(q|a,c). 
Die Autoren schlagen ein generatives QA-Modell vor, das die Verteilung von Fragen und Antworten in einem Dokument/Kontext gemeinsam optimiert. 
Es wird eine neue Aktivierungsfunktion namens Displaced Rectifier Linear Unit vorgeschlagen. Es wurde gezeigt, dass sie die Trainings- und Inferenzleistung von Batch normalisierten neuronalen Netzen verbessern kann.
Die Arbeit vergleicht und rät ab von der Verwendung von Batch-Normalisierung nach der Verwendung von Rectifier Linear Units.
In dieser Arbeit wird eine Aktivierungsfunktion, genannt "displaced ReLU", vorgeschlagen, um die Leistung von CNNs zu verbessern, die eine Batch-Normalisierung verwenden.
Wir konstruieren skalenäquivariante Convolutional Neural Networks in der allgemeinsten Form, die sowohl rechnerisch effizient als auch nachweislich deformationsresistent sind.
Die Autoren schlagen eine CNN-Architektur vor, die theoretisch äquivariant zu isotropen Skalierungen und Translationen ist, indem eine zusätzliche Skalendimension zu den Aktivierungstensoren hinzugefügt wird.
Wir diagnostizieren tiefe neuronale Netze für die 3D-Punktwolkenverarbeitung, um den Nutzen verschiedener Netzarchitekturen zu untersuchen. 
Die Arbeit untersucht verschiedene neuronale Netzwerkarchitekturen für die Verarbeitung von 3D-Punktwolken und schlägt Metriken für die Robustheit gegenüber nachteiligen Einflüssen, die Rotationsrobustheit und die Nachbarschaftskonsistenz vor.
Die Nutzung der Struktur von Verteilungen verbessert die semi-implizite variationale Inferenz.
Self-Imitation Learning von verschiedenen Trajektorien mit trajektorienbedingten Regeln.
Diese Arbeit befasst sich mit schwierigen Explorationsaufgaben, indem die Selbstimitation auf eine vielfältige Auswahl von Trajektorien aus der Vergangenheit angewendet wird, um eine effizientere Exploration in spärlichen Belohnungsproblemen zu ermöglichen und SOTA-Ergebnisse zu erzielen.
Eine Methode, die neuronale Netze mit großer Kapazität mit deutlich verbesserter Genauigkeit und geringeren dynamischen Rechenkosten trainiert.
Ein Verfahren zum Trainieren eines Netzes mit großer Kapazität, von dem nur Teile zum Zeitpunkt der Inferenz abhängig von der Eingabe verwendet werden, unter Verwendung einer feinkörnigen bedingten Auswahl und einer neuen Regularisierungsmethode, dem "Batch Shaping".
Wir stellen eine differenzierbare Ende-zu-Ende Architektur vor, die lernt, Pixel auf Prädikate abzubilden, und evaluieren sie anhand einer Reihe einfacher relationaler Argumentationsaufgaben.
Eine Netzwerkarchitektur, die auf dem Multi-Head Self-Attention Modul basiert, um eine neue Form von relationalen Darstellungen zu erlernen, die die Dateneffizienz und die Generalisierungsfähigkeit beim Lernen von Curriculums verbessert.
Wir verwenden neuronale Netze, um oberflächliche Informationen für die Inferenz natürlicher Sprache zu projizieren, indem wir die oberflächlichen Informationen aus der Perspektive der Logik erster Ordnung definieren und identifizieren.
In diesem Beitrag wird versucht, oberflächliche Informationen in der natürlichsprachlichen Inferenz zu reduzieren, um eine Überanpassung zu verhindern, und es wird ein neuronales Graphennetz zur Modellierung der Beziehung zwischen Prämisse und Hypothese eingeführt. 
Ein Ansatz zur Behandlung der Inferenz natürlicher Sprache unter Verwendung der Logik erster Ordnung und zur Ergänzung von NLI-Modellen mit logischen Informationen, um die Inferenz zu verbessern.
Algorithmus für das Training eines individuell fairen Klassifizierers unter Verwendung von adversarialer Robustheit.
In diesem Beitrag wird eine neue Definition von algorithmischer Fairness und ein Algorithmus vorgeschlagen, mit dem ein ML-Modell gefunden werden kann, das die Fairness-Bedingungen erfüllt.
Sind Seeding und Augmentation alles, was Sie für die Klassifizierung von Ziffern in jeder Sprache brauchen?
In diesem Beitrag werden neue Datensätze für fünf Sprachen vorgestellt und ein neuer Rahmen (SAT) für die Erstellung von Schriftbilddatensätzen für die universelle Ziffernklassifikation vorgeschlagen.
Der Erfolg von MAML beruht auf der Wiederverwendung von Merkmalen aus der Meta-Initialisierung, die auch zu einer natürlichen Vereinfachung des Algorithmus führt, indem die innere Schleife für den Netzkörper sowie andere Erkenntnisse über den Kopf und den Körper entfernt werden.
In der Arbeit wird festgestellt, dass die Wiederverwendung von Merkmalen der wichtigste Faktor für den Erfolg von MAML ist, und es werden neue Algorithmen vorgeschlagen, die wesentlich weniger Rechenzeit benötigen als MAML.
Wir stellen einen methodenunabhängigen Algorithmus zur Verfügung, um zu entscheiden, wann ein inkrementelles Training und wann ein vollständiges Training durchgeführt werden soll. Dieser Algorithmus bietet eine signifikante Beschleunigung gegenüber dem vollständigen Training und verhindert katastrophales Vergessen.
Diese Arbeit schlägt einen Ansatz vor, um zu entscheiden, wann ein Modell im Rahmen einer iterativen Modellentwicklung bei Slot-Filling-Aufgaben inkrementell oder vollständig neu trainiert werden soll.
Wir entwickeln einen theoretischen Rahmen, um zu charakterisieren, welche Denkaufgaben ein neuronales Netz gut lernen kann.
In dem Beitrag wird ein Maß für die Klassen der algorithmischen Ausrichtung vorgeschlagen, mit dem gemessen wird, wie "nah" neuronale Netze an bekannten Algorithmen sind, und das die Verbindung zwischen mehreren Klassen bekannter Algorithmen und Architekturen neuronaler Netze nachweist.
Wir erforschen Zell-Zell Interaktionen in verschiedenen Kontexten der Tumorumgebung, die in hochgradig multiplexen Bildern beobachtet werden, durch Bildsynthese unter Verwendung einer neuartigen Aufmerksamkeits GAN Architektur.
Eine neue Methode zur Modellierung von Daten, die durch Multiplex-Ionenstrahl Imaging per Time of Flight (MIBI-TOF) generiert werden, durch Lernen der Many-to-Many Zuordnung zwischen Zelltypen und Expressionsniveaus von Proteinmarkern.
Ein zweistufiger Ansatz, der aus einer Satzauswahl und einer anschließenden Abstandauswahl besteht, kann im Vergleich zu einem einstufigen Modell, das auf der Grundlage des gesamten Kontexts trainiert wird, robuster gegenüber adversarischen Angriffen gemacht werden.
In diesem Beitrag wird ein bestehendes Modell untersucht und es wird festgestellt, dass eine zweistufige trainierte QS-Methode im Vergleich zu anderen Methoden nicht widerstandsfähiger gegen Angriffe von außen ist.
Verifizierung eines menschlichen Fahrermodells auf der Grundlage einer kognitiven Architektur und Synthese eines korrekt konstruierten ADAS auf dieser Grundlage.
Ein neuartiger, hybrider Deep-Learning-Ansatz bietet die beste Lösung für ein Problem mit begrenzten Daten (das für den Erhalt der hawaiischen Sprache wichtig ist).
Wir untersuchen quantitativ die Out-of-Distribution Erkennung in einer Situation mit wenigen Aufnahmen, ermitteln die grundlegenden Ergebnisse mit ProtoNet, MAML und ABML und verbessern sie.
In dem Artikel werden zwei neue Konfidenzwerte vorgeschlagen, die für die Erkennung von Out-of-Distribution bei der Few-Shot Klassifizierung besser geeignet sind, und es wird gezeigt, dass ein auf Distanzmetriken basierender Ansatz die Leistung verbessert.
In diesem Beitrag wird eine progressive Wissensdestillation für das Lernen generativer Modelle vorgestellt, die auf Erkennungsaufgaben ausgerichtet sind.
In diesem Beitrag wird gezeigt, wie man durch einfaches bis schweres Curriculum-Lernen ein generatives Modell trainieren kann, um die Few-Shot Klassifizierung zu verbessern.
Wir schlagen eine neue Methode zur Verbesserung der Übertragbarkeit von Negativbeispielen vor, indem wir den Störungsreduzierten Gradienten verwenden.
In dieser Arbeit wird postuliert, dass eine Störung aus einer modellspezifischen und einer datenspezifischen Komponente besteht und dass die Verstärkung der letzteren am besten für adversarial Angriffe geeignet ist.
In dieser Arbeit geht es darum, die Übertragbarkeit von Beispielen aus der Praxis von einem Modell auf ein anderes Modell zu verbessern.
Wir stellen den iterativen Zwei-Pass-CP Zerlegungsfluss vor, um bestehende Convolutional Neural Networks (CNNs) effektiv zu beschleunigen.
In der Arbeit wird ein neuartiger Arbeitsablauf für die Beschleunigung und Komprimierung von CNNs vorgeschlagen, und es wird eine Methode zur Bestimmung des Zielrangs der einzelnen Schichten angesichts der angestrebten Gesamtbeschleunigung vorgeschlagen. 
Diese Arbeit befasst sich mit dem Problem des Lernens einer Tensor-Filter-Operation niedrigen Ranges für Filterschichten in tiefen neuronalen Netzen (DNNs). 
LP-basierte obere Schranken für die Lipschitz-Konstante von neuronalen Netzen.
Die Autoren untersuchen das Problem der Schätzung der Lipschitz-Konstante eines tiefen neuronalen Netzes mit ELO Aktivierungsfunktion und formulieren es als polynomielles Optimierungsproblem.
Wir befassen uns mit der Few-Shot Klassifizierung in mehreren Bereichen, indem wir mehrere Modelle erstellen, um diese komplexe Aufgabenverteilung auf kollektive Weise darzustellen, und die aufgabenspezifische Anpassung als Auswahlproblem aus diesen vortrainierten Modellen vereinfachen.
Diese Arbeit befasst sich mit der Few-shot Klassifikation mit vielen verschiedenen Domänen, indem ein Pool von Einbettungsmodellen aufgebaut wird, um domäneninvariante und domänenspezifische Merkmale zu erfassen, ohne die Anzahl der Parameter signifikant zu erhöhen.
Neural-basierte Entfernung von Tintenartefakten in Dokumenten (Unterstreichungen, Flecken usw.) ohne manuell beschriftete Trainingsdaten.
Wir schlagen einen abfrageeffizienten Black-Box-Angriff vor, der Bayes'sche Optimierung in Kombination mit Bayes'scher Modellauswahl verwendet, um die Störung des Gegners und den optimalen Grad der Dimensionsreduktion des Suchraums zu optimieren. 
Die Autoren schlagen vor, die Bayes'sche Optimierung mit einem GP Surrogate für die Erzeugung von adversarial Bildern zu verwenden, indem sie die additive Struktur ausnutzen und die Bayes'sche Modellauswahl verwenden, um eine optimale Dimensionalitätsreduktion zu bestimmen.
Wir schlagen ein Modell zum Erlernen faktorisierter multimodaler Repräsentationen vor, die diskriminativ, generativ und interpretierbar sind.
Diese Arbeit stellt ein "Multimodales Faktorisierungsmodell" vor, das Repräsentationen in gemeinsame multimodale diskriminierende Faktoren und modalitätsspezifische generative Faktoren aufteilt. 
Wir entwickeln einen hierarchischen, akteurskritischen Algorithmus für den kompositorischen Transfer durch die gemeinsame Nutzung von Richtlinienkomponenten und demonstrieren die Komponentenspezialisierung und die damit verbundenen direkten Vorteile in Multitasking-Domänen sowie seine Anpassung für Einzelaufgaben.
Eine Kombination verschiedener Lerntechniken für den Erwerb von Strukturen und das Lernen mit asymmetrischen Daten, die zum Trainieren einer HRL-Politik verwendet werden.
Die Autoren stellen eine hierarchische Policy-Struktur vor, die sowohl für Single-Task- als auch für Multitask Reinforcement Learning verwendet werden kann, und bewerten die Nützlichkeit der Struktur bei komplexen Roboteraufgaben.
Wir zählen empirisch die Anzahl der linearen Bereiche von Gleichrichternetzen und verfeinern die oberen und unteren Grenzen.
In diesem Beitrag werden verbesserte Grenzwerte für die Zählung der Anzahl linearer Regionen in ReLU-Netzen vorgestellt.
Wir analysieren die Erinnerungseigenschaften durch ein Convnet der Trainingsmenge und schlagen verschiedene Anwendungsfälle vor, in denen wir einige Informationen über die Trainingsmenge extrahieren können. 
Beleuchtet die Verallgemeinerungs-/Erinnerungseigenschaften von großen und tiefen ConvNets und versucht, Verfahren zu entwickeln, mit denen festgestellt werden kann, ob eine Eingabe für ein trainiertes ConvNet tatsächlich zum Trainieren des Netzes verwendet wurde.
GANs können Verteilungen im Prinzip stichprobeneffizient lernen, wenn die Diskriminatorklasse kompakt ist und eine starke Unterscheidungskraft gegenüber der jeweiligen Generatorklasse hat.
Vorschlagen des Begriffs der eingeschränkten Approximierbarkeit und liefern einer Komplexitätsgrenze für Stichproben, die polynomiell in der Dimension ist und bei der Untersuchung mangelnder Vielfalt in GANs nützlich ist.
Analysiert, dass die Integrale Wahrscheinlichkeitsmetrik unter einigen milden Annahmen eine gute Annäherung an den Wassersteinabstand sein kann.
In der frühen Phase des Trainings von tiefen neuronalen Netzen gibt es einen "Break-even-Point", der die Eigenschaften der gesamten Optimierungskurve bestimmt.
In dieser Arbeit wird die Optimierung von tiefen neuronalen Netzen analysiert, indem untersucht wird, wie die Hyperparameter Batch-Größe und Schrittgröße die Lerntrajektorien verändern.
Wir schlagen HWGCN vor, um die relevanten Nachbarschaftsinformationen auf verschiedenen Ebenen zu mischen, um die Knotenrepräsentationen besser zu lernen.
Die Autoren schlagen eine Variante von GCN, HWGCN, vor, die eine Convolution jenseits von 1-Schritt-Nachbarn berücksichtigt und mit modernen Methoden vergleichbar ist.
Wir führen ein neuartiges Maß für die Flachheit bei lokalen Minima der Verlustfläche tiefer neuronaler Netze ein, das gegenüber schichtweisen Neuparametrisierungen invariant ist, und stellen eine Verbindung zwischen Flachheit und Merkmalsrobustheit und Generalisierung her.
Die Autoren schlagen einen Begriff der Merkmalsrobustheit vor, der gegenüber einer Neuskalierung der Gewichtung invariant ist, und erörtern die Beziehung des Begriffs zur Generalisierung.
In diesem Beitrag wird ein Begriff der Merkmalsrobustheit definiert und mit der Epsilon-Repräsentativität einer Funktion kombiniert, um einen Zusammenhang zwischen der Flachheit von Minima und der Generalisierung in tiefen neuronalen Netzen zu beschreiben.
Wir schlagen vor, die Voraktivierungen von Gattern und den Informationsfluss im LSTM zu sparsam zu gestalten, um sie konstant zu machen und das Spärlichkeitsniveau der Neuronen zu erhöhen.
In diesem Papier wird eine Sparsification Methode für rekurrente neuronale Netze vorgeschlagen, bei der Neuronen mit Null-Präaktivierungen eliminiert werden, um kompakte Netze zu erhalten.
Wir stellen einen Ansatz für neuronale Netze zur Unterstützung von Lösern partieller Differentialgleichungen vor.
Die Autoren zielen darauf ab, die Genauigkeit numerischer Solver zu verbessern, indem sie ein neuronales Netz auf simulierten Referenzdaten trainieren, das den numerischen Solver korrigiert.
Eine konföderierte Lernmethode, die Modelle aus horizontal und vertikal getrennten medizinischen Daten trainiert.
Eine "verbündete" maschinelle Lernmethode, die über die Grenzen medizinischer Daten hinweg lernt, die sowohl horizontal als auch vertikal getrennt sind.
In dieser Arbeit wird eine stochastische quantisierte Aktivierung vorgeschlagen, die das Problem der Überanpassung beim FGSM-Training löst und schnell eine Robustheit erreicht, die mit dem mehrstufigen Training vergleichbar ist.
In dem Papier wird ein Modell zur Verbesserung des gegnerischen Trainings vorgeschlagen, indem zufällige Störungen in die Aktivierungen einer der verborgenen Schichten eingeführt werden
Wir untersuchen die Struktur von Störfaktoren im Gehirn und stellen fest, dass es zur Generalisierung beitragen kann, indem es die Repräsentationen entlang der Reizvariationen innerhalb einer Klasse verschiebt.
Wir stellen ein neues Design vor, d.h. Self-Ensembling mit kategorie-agnostischen Clustern, sowohl für Closed-Set- als auch für Open-Set-Domain-Adaption.
Ein neuer Ansatz zur Anpassung von offenen Domänen, bei dem die Kategorien der Quelldomäne in den Kategorien der Zieldomäne enthalten sind, um Ausreißerkategorien herauszufiltern und eine Anpassung innerhalb der gemeinsamen Klassen zu ermöglichen.
Wir zeigen, wie man spektrale Zerlegungen von linearen Operatoren mit Deep Learning erlernen kann, und nutzen dies für unüberwachtes Lernen ohne generatives Modell.
Die Autoren schlagen vor, ein Deep Learning Framework zu verwenden, um die Berechnung der größten Eigenvektoren zu lösen.
In dieser Arbeit wird ein Rahmen zum Erlernen von Eigenfunktionen über einen stochastischen Prozess vorgestellt und vorgeschlagen, die Herausforderung der Berechnung von Eigenfunktionen in einem groß angelegten Kontext durch Annäherung mit Hilfe eines zweiphasigen stochastischen Optimierungsprozesses zu bewältigen.
Anwendung des Riemannschen SGD (RSGD) Algorithmus für das Training von Tensor-Train RNNs zur weiteren Reduzierung der Modellparameter.
Die Arbeit schlägt vor, den Riemannschen stochastischen Gradientenalgorithmus für das Lernen von Tensoren mit niedrigem Rang in tiefen Netzwerken zu verwenden.
Vorschlagen eines Algorithmus zur Optimierung von neuronalen Netzen, der durch Tensor-Train-Zerlegung parametrisiert ist, basierend auf der Riemannschen Optimierung und Ranganpassung, und entwerfen einer bidirektionale TT-LSTM-Architektur.
Wir schlagen einen neuen Algorithmus vor, der einschränkungsbefriedigende Strategien lernt, und bieten eine theoretische Analyse und empirische Demonstration im Kontext des Reinforcement Learning mit Einschränkungen.
In diesem Beitrag wird ein Algorithmus zur Optimierung von Richtlinien mit Beschränkungen vorgestellt, der einen zweistufigen Optimierungsprozess verwendet, bei dem Richtlinien, die die Beschränkung nicht erfüllen, in die Beschränkungsmenge zurückprojiziert werden können.
Wir schlagen eine gradientenbasierte Darstellung zur Charakterisierung von Informationen vor, die tiefe Netzwerke nicht gelernt haben.
Die Autoren stellen Darstellungen vor, die auf Gradienten in Bezug auf die Gewichte basieren, um Informationen zu ergänzen, die im Trainingsdatensatz für tiefe Netzwerke fehlen.
Wir führen ein Zero-Shot Framework zur Reduzierung von Artefakten in medizinischen Bildern ein, das die Leistungsfähigkeit von Deep Learning nutzt, ohne jedoch allgemeine vortrainierte Netzwerke oder eine saubere Bildreferenz zu verwenden. 
Wir wenden das Konzept des Informationsengpasses auf die Zuordnung an.
In diesem Beitrag wird eine neuartige, auf Störungen basierende Methode zur Berechnung von Attributions- bzw. Ähnlichkeitskarten für Bildklassifizierer auf der Grundlage tiefer neuronaler Netze vorgeschlagen, bei der künstliches Störungen in eine frühe Schicht des Netzes injiziert wird.
Wir zeigen, dass die RNNs gepruned werden können, um eine Blocksparsität zu erzeugen, die eine Beschleunigung für spärliche Operationen auf vorhandener Hardware ermöglicht.
Die Autoren schlagen einen Block Sparsity Pruning-Ansatz zur Komprimierung von RNNs vor, bei dem Gruppen-LASSO zur Förderung der Sparsity und zum Pruning verwendet wird, allerdings mit einem sehr speziellen Zeitplan für das Pruning und das Pruning-Gewicht.
Wir schlagen eine Verbesserung der Wert-Iterationsnetzwerke vor, mit Anwendungen für die Pfadplanung von Planeten-Rovern.
In dieser Arbeit wird eine Belohnungsfunktion auf der Grundlage von Expertentrajektorien mit Hilfe eines Value Iteration Module erlernt, um den Planungsschritt differenzierbar zu machen.
Ein neuartiger Attention Layer, der die Selbstaufmerksamkeit und Feed-Forward-Teilschichten von Transformer-Netzen kombiniert.
In diesem Beitrag wird eine Modifikation des Transformer-Modells vorgeschlagen, bei der die Aufmerksamkeit über "persistente" Speichervektoren in die Selbstaufmerksamkeitsschicht integriert wird, was zu einer Leistung führt, die mit bestehenden Modellen vergleichbar ist, während weniger Parameter benötigt werden.
Wir finden effizient eine Untergruppe von Bildern, die höhere Aktivierungen als erwartet für eine Untergruppe von Knoten aufweisen.  Diese Bilder erscheinen anomaler und sind leichter zu erkennen, wenn sie als Gruppe betrachtet werden. 
In dem Beitrag wird ein Verfahren zur Erkennung anomaler Eingaben vorgeschlagen, das auf einem "Subset-Scanning"-Ansatz zur Erkennung anomaler Aktivierungen im Deep Learning Netzwerk basiert.
Stabile rekurrente Modelle können durch Feed Forward Netzwerke approximiert werden und schneiden empirisch genauso gut ab wie instabile Modelle bei Benchmark-Aufgaben.
Studien zur Stabilität von RNNs und Untersuchung der spektralen Normalisierung für sequenzielle Vorhersagen.
Untersuchung der Rolle der Gewichtsteilung in neuronalen Netzen unter Verwendung von Hash-Funktionen und feststellen, dass eine ausgewogene und deterministische Hash-Funktion die Netzleistung verbessert.
Vorschlag für ArbNets zur systematischeren Untersuchung der Gewichtsteilung durch Definition der Gewichtsteilungsfunktion als Hash-Funktion.
Wir stellen ein statistisches relationales Lernsystem vor, das Ideen aus der Markov-Logik entlehnt, aber eine implizite Repräsentation von Regeln als neuronales Netz erlernt.
Das Papier bietet eine Erweiterung der Markov-Logik-Netzwerke, indem es ihre Abhängigkeit von vordefinierten Logikregeln erster Ordnung aufhebt, um mehr Domänen in Wissensdatenbank-Vervollständigungsaufgaben zu modellieren.
Eine skalierbare Methode zum Erlernen eines aussagekräftigen Priors für neuronale Netze über mehrere Aufgaben hinweg.
Die Arbeit stellt eine Methode für die Ausbildung eines probabilistischen Modells für Multitasks Transfer Learning durch die Einführung einer latenten Variable pro Aufgabe, um die Gemeinsamkeit in den Aufgaben Instanzen zu erfassen.
In der Arbeit wird ein Variationsansatz für das Meta-Lernen vorgeschlagen, der latente Variablen verwendet, die aufgabenspezifischen Datensätzen entsprechen.
Ziel ist es, einen Prior über neuronale Netze für mehrere Aufgaben zu lernen. 
Entkoppelte Zustandsraummodelle.
Die Arbeit stellt ein generatives Zustandsraummodell vor, das eine globale latente Variable E verwendet, um umweltspezifische Informationen zu erfassen.
Bregman Divergenz Lernen für Few-Shot Learning. 
Wir stellen einen Netzwerkrahmen vor, der seine Struktur während des Trainings verändern kann, und zeigen, dass er zu verschiedenen ML-Netzwerk-Archetypen wie MLPs und LCNs konvergieren kann. 
Die bereichsbezogene Datenerweiterung bietet eine robuste und stabile Methode der Bereichsgeneralisierung
In diesem Artikel wird ein Ansatz zur Generalisierung von Domänen durch domänenabhängige Datenerweiterung vorgeschlagen
Die Autoren stellen die CrossGrad-Methode vor, die sowohl eine Label-Klassifizierungsaufgabe als auch eine Domänen-Klassifizierungsaufgabe trainiert.
Wir untersuchen Alternativen zu traditionellen Pixelbildmodellierungsansätzen und schlagen ein generatives Modell für Vektorbilder vor.
In diesem Beitrag wird eine neuronale Netzwerkarchitektur zur Erzeugung von Skizzenzeichnungen vorgestellt, die sich an den variationalen Autoencoder anlehnt.
Wir stellen eine Studie vor, die versucht zu sehen, wie die jüngste Online-Lernratenanpassung die Schlussfolgerung von Wilson et al. 2018 über adaptive Gradientenmethoden erweitert, zusammen mit einem Vergleich und einer Sensitivitätsanalyse.
Berichtet über die Ergebnisse von Tests verschiedener Methoden zur Anpassung der Schrittweite, einschließlich einfachem SGD, SGD mit Neserov-Momentum und ADAM, und vergleicht diese Methoden mit und ohne Hypergradient. 
Wir untersuchen das Verhalten der Q-Wert-Schätzungen bei großen Stichproben und schlagen eine effiziente Erkundungsstrategie vor, die sich auf die Schätzung der relativen Diskrepanzen zwischen den Q-Schätzungen stützt. 
In diesem Beitrag wird ein reiner Explorationsalgorithmus für das Reinforcement Learning vorgestellt, der auf einer asymptotischen Analyse der Q-Werte und ihrer Konvergenz zur zentralen Grenzverteilung basiert und damit bessere Ergebnisse als die Benchmark-Explorationsalgorithmen erzielt.
Wir trainieren ein Bild-zu-Bild-Übersetzungsnetzwerk, das als Eingabe das Quellbild und eine Probe aus einer vorherigen Verteilung nimmt, um eine Probe aus der Zielverteilung zu erzeugen
Diese Arbeit formalisiert das Problem der unbeaufsichtigten Übersetzung und schlägt ein erweitertes GAN Framework vor, der die gegenseitige Information nutzt, um den degenerierten Fall zu vermeiden.
In dieser Arbeit wird das Problem der unbeaufsichtigten Eins-zu-Viel-Bild-Übersetzung formuliert und das Problem durch Minimierung der gegenseitigen Information gelöst. 
Lernen, um unterscheidbare Schlüsselpunkte aus einer Proxy-Aufgabe zu extrahieren, Zurückweisung von Ausreißern.
Dieser Beitrag widmet sich dem selbstüberwachten Lernen von lokalen Merkmalen unter Verwendung von Neural Guided RANSAC als zusätzlicher Hilfsverlustlieferant zur Verbesserung der Deskriptorinterpolation.
Wir schlagen eine Formulierung der intrinsischen Motivation vor, die sich als Explorationsverzerrung in synergetischen Multi-Agenten-Aufgaben mit geringer Belohnung eignet, indem sie die Agenten dazu ermutigt, die Welt auf eine Weise zu beeinflussen, die nicht möglich wäre, wenn sie einzeln handeln würden.
Der Beitrag konzentriert sich auf die Nutzung intrinsischer Motivation zur Verbesserung des Explorationsprozesses von Agenten mit Reinforcement Learning bei Aufgaben, die von mehreren Agenten zu bewältigen sind.
Ein probabilistischer Inferenzalgorithmus auf der Grundlage eines neuronalen Netzes für graphisch-strukturierte Modelle.
Diese Arbeit stellt Policy Message Passing vor, ein neuronales Graphen-Netzwerk mit einem Inferenz-Mechanismus, der den Kanten auf rekurrente Weise Nachrichten zuweist, was auf eine konkurrenzfähige Leistung bei visuellen Argumentationsaufgaben hindeutet.
Wir zeigen, wie man die Leistung eines Multitasking-Netzes steigern kann, indem man eine adaptive Multitasking Verlustfunktion einstellt, die durch direkten Ausgleich der Netzgradienten gelernt wird.
In dieser Arbeit wird ein dynamisches Aktualisierungsschema für die Gewichte vorgeschlagen, das die Gewichte für verschiedene Aufgabenverluste während der Trainingszeit aktualisiert, indem es die Verlustquoten der verschiedenen Aufgaben nutzt.
DNNs für die Bildsegmentierung können Lösungen für das Insideness-Problem implementieren, aber nur einige rekurrente Netze können sie mit einer bestimmten Art von Überwachung lernen.
In diesem Beitrag wird die Innensicht zur Untersuchung der semantischen Segmentierung in der Ära des Deep Learning eingeführt, und die Ergebnisse können dazu beitragen, dass Modelle besser verallgemeinert werden.
Ausgehend von einem trainierten Modell untersuchten wir die Gradienten der Modellparameter pro Stichprobe im Verhältnis zu einem aufgabenspezifischen Verlust und konstruierten ein lineares Modell, das die Gradienten der Modellparameter und die Aktivierung des Modells kombiniert.
In diesem Beitrag wird vorgeschlagen, die Gradienten bestimmter Schichten von Convolutional Networks als Merkmale in einem linearisierten Modell für Transferlernen und schnelle Anpassung zu verwenden.
Wir trainieren unser Gesichtsrekonstruktionsmodell mit adversarialem Verlust in halbüberwachter Weise auf hybriden Batches von unbeschrifteten und beschrifteten Gesichtsbildern, um den Wert großer Mengen unbeschrifteter Gesichtsbilder aus unbeschränkten Fotosammlungen zu nutzen.
In dieser Arbeit wird ein halbüberwachtes und kontradiktorisches Trainingsverfahren vorgeschlagen, um nichtlineare entwirrte Repräsentationen von Gesichtsbildern mit Verlustfunktionen genau zu bestimmen und so die beste Leistung bei der Gesichtsrekonstruktion zu erzielen.
In diesem Beitrag wird ConceptFlow vorgestellt, das den Gesprächsfluss explizit in einem Commonsense-Wissensgraphen modelliert, um eine bessere Gesprächsgenerierung zu ermöglichen.
Die Arbeit schlägt ein System für die Generierung einer Single-Turn-Antwort auf eine gepostete Äußerung in einer offenen Domäne Dialog Einstellung mit der Diffiusion in die Nachbarn der geerdeten Konzepte.
Wir untersuchen die Hypothese, dass die Entropie von Lösungsräumen für Beschränkungen der synaptischen Gewichte (die "Flexibilität" der Beschränkung) als Kostenfunktion für die Entwicklung neuronaler Schaltkreise dienen könnte.
Unendliche Ensembles von unendlich großen neuronalen Netzen sind aus informationstheoretischer Sicht eine interessante Modellfamilie.
Wir führen eine vergleichende Studie über sprachübergreifendes Alignment und gemeinsame Trainingsmethoden durch und vereinen diese beiden bisher exklusiven Paradigmen in einem neuen Rahmen. 
Dieser Beitrag vergleicht Ansätze zur zweisprachigen Lexikoninduktion und zeigt, welche Methode bei Lexikon, Induktion, NER- und MT-Aufgaben besser abschneidet.
Kombination orthogonaler Modellkomprimierungstechniken, um die Modellgröße und die Anzahl der bei der Inferenzierung erforderlichen Flops erheblich zu verringern.
In dieser Arbeit wird eine Kombination aus Tucker Zerlegung und Filter Pruning vorgeschlagen.
Einführen von JAUNE, einer Methode die BLEU- und ROUGE-Punkte durch multidimensionale, modellbasierte Evaluatoren zur Bewertung von Zusammenfassungen ersetzt.
In diesem Beitrag wird eine neue JAUNE-Metrik für die Bewertung von maschinellen Übersetzungs- und Textzusammenfassungssystemen vorgeschlagen, die zeigt, dass ihr Modell besser mit den tatsächlichen Ähnlichkeitsbezeichnungen übereinstimmt als BLEU.
Neuer GNN-Formalismus mit umfangreichen Experimenten; die Unterschiede zwischen GGNN/GCN/GAT sind geringer als angenommen.
Die Arbeit schlägt eine neue Graph Neural Network Architektur vor, die eine merkmalsweise lineare Modulation verwendet, um die Weiterleitung von Nachrichten von der Quelle zum Zielknoten auf der Grundlage der Darstellung des Zielknotens zu konditionieren.
In diesem Beitrag wird ein neuartiger Matrixzerlegungsrahmen für die gleichzeitige Einbettung und Clustering von Netzwerkdaten vorgeschlagen.
In dieser Arbeit wird ein Algorithmus vorgeschlagen, der die Einbettung von Attributen in ein Netzwerk und das Clustering gemeinsam durchführt.
Wir schlagen eine erlernte bildgesteuerte Rendering-Technik vor, die die Vorteile von bildbasiertem Rendering und GAN-basierter Bildsynthese kombiniert und gleichzeitig sichtabhängige Effekte berücksichtigt.
In diesem Beitrag wird eine Methode zur Behandlung von sichtabhängigen Effekten beim neuronalen Rendering vorgeschlagen, die die Robustheit bestehender neuronaler Renderingmethoden verbessert.
GANs werden anhand synthetischer Datensätze bewertet.
Wir schlagen ein effizientes und effektives Verfahren zur Anpassung der Schrittweite für die Gradientenmethoden vor.
Eine neue Schrittgrößenanpassung in Gradientenmethoden erster Ordnung, die ein neues Optimierungsproblem mit der Erweiterung der Verlustfunktion erster Ordnung und Regularisierung aufstellt, wobei die Schrittgröße als Variable behandelt wird.
Wir haben festgestellt, dass eine große Klasse von Vielfältigkeiten durch ReLU- und Sigmoid-Netzwerke mit beliebiger Genauigkeit erzeugt werden kann.
Diese Arbeit bietet bestimmte grundlegende Garantien dafür, wann Vielfältigkeiten als das Bild einer Karte geschrieben werden können, die durch ein neuronales Netz approximiert wird, und verbindet Theoreme aus der Mannigfaltigkeitsgeometrie und universelle Standard-Approximationsergebnisse.
In diesem Beitrag wird theoretisch gezeigt, dass generative Modelle, die auf neuronalen Netzen basieren, Datenmannigfaltigkeiten annähern können, und es wird nachgewiesen, dass neuronale Netze unter milden Annahmen einen latenten Raum auf eine Menge abbilden können, die der gegebenen Datenmannigfaltigkeit innerhalb einer kleinen Hausdorff-Distanz nahe kommt.
Wir entwerfen modellbasierte Reinforcement Learning Algorithmen mit theoretischen Garantien und erzielen die besten Ergebnisse bei Mujuco-Benchmark-Aufgaben, wenn eine Million oder weniger Beispiele zulässig sind.
In der Arbeit wird ein Rahmen für die Entwicklung modellbasierter RL-Algorithmen auf der Grundlage von OFU vorgeschlagen, die eine SOTA-Leistung bei MuJoCo-Aufgaben erreichen.
Wir stellen zusätzliche Techniken zur Wissensdestillation vor, um das U-Netz um mehr als das 1000-fache zu komprimieren.
Die Autoren führten eine modifizierte Destillationsstrategie ein, um eine U-Netz-Architektur um mehr als das 1000-fache zu komprimieren und dabei eine Genauigkeit beizubehalten, die der des ursprünglichen U-Netzes nahe kommt.
Diese Arbeit bietet einen Ansatz zur Bekämpfung des katastrophalen Vergessens durch hessianfreie Krümmungsschätzungen.
Die Arbeit schlägt eine ungefähre Laplace-Methode für das Training neuronaler Netze im Rahmen des kontinuierlichen Lernens mit einer geringen Raumkomplexität vor.
Methode zur Verbesserung der Leistung einfacher Modelle anhand eines (genauen) komplexen Modells.
In der Arbeit wird eine Methode zur Verbesserung der Vorhersagen eines Modells mit geringer Kapazität vorgeschlagen, die Vorteile gegenüber bestehenden Ansätzen aufweist.
Ein einfacher und praktischer Algorithmus zum Erlernen eines margenmaximierenden translationsinvarianten oder sphärisch symmetrischen Kernels aus Trainingsdaten, unter Verwendung von Werkzeugen der Fourier-Analyse und der Regret-Minimierung.
Die Arbeit schlägt vor, einen benutzerdefinierten übersetzungs- oder drehungsinvarianten Kern in der Fourier-Darstellung zu lernen, um die Marge der SVM zu maximieren.
Die Autoren schlagen einen interessanten Algorithmus für das gemeinsame Lernen des l1-SVM und des Fourier-Kernels vor.
Die Autoren betrachten das Erlernen direkter Fourier-Darstellungen von verschiebungs- und translationsinvarianten Kerneln für Anwendungen des maschinellen Lernens, wobei die Ausrichtung des Kernels auf die Daten die zu optimierende Zielfunktion darstellt.
Probabilistische Programmierung, die kausale, kontrafaktische Inferenz unterstützt.
Inferenz eines Mean Field Game (MFG)-Modells für das Verhalten großer Populationen durch eine Synthese von MFG und Markov-Entscheidungsprozessen.
Die Autoren befassen sich mit der Inferenz in Modellen für kollektives Verhalten, indem sie inverses Reinforcement Learning verwenden, um die Belohnungsfunktionen der Agenten im Modell zu lernen.
Wir kombinieren tiefe generative Modelle mit programmgesteuerter schwacher Überwachung, um koordinierte Multi-Agenten-Trajektorien von deutlich höherer Qualität zu erzeugen als frühere Basislösungen.
Schlägt sequentielle generative Multi-Agenten-Modelle vor.
Die Arbeit schlägt vor, generative Modelle zu trainieren, die Multi-Agenten-Trajektorien unter Verwendung heuristischer Funktionen erzeugen, die Variablen kennzeichnen, die ansonsten in den Trainingsdaten verborgen wären.
Lernen, Lernkurven zu ordnen, um vielversprechende Trainingsaufgaben frühzeitig zu beenden. Neuheit: Verwendung von paarweisen Rangfolgeverlusten zur direkten Modellierung der Wahrscheinlichkeit von Verbesserungs- und Transferlernen über Datensätze hinweg, um die erforderlichen Trainingsdaten zu reduzieren.
In der Arbeit wird eine Methode zur Einstufung von Lernkurven neuronaler Netze vorgeschlagen, die Lernkurven über verschiedene Datensätze hinweg modellieren kann, um höhere Geschwindigkeitssteigerungen bei Bildklassifizierungsaufgaben zu erzielen.
Wir zeigen, dass in kontinuierlichen Lernumgebungen katastrophales Vergessen vermieden werden kann, indem man RL mit einer Mischung aus neuer und wiederholter Erfahrung anwendet, mit einem Verhaltensklonverlust.
Er schlägt eine besondere Variante der Erfahrungswiederholung mit Verhaltensklonen als Methode für kontinuierliches Lernen vor.
Wir stellen eine Methode vor, die lernt, zeitliche Informationen und mehrdeutige visuelle Informationen im Kontext von interagierenden Agenten zu integrieren.
Die Autoren schlagen Graph VRNN vor, das die Interaktion mehrerer Agenten modelliert, indem für jeden Agenten ein VRNN eingesetzt wird.
In diesem Beitrag wird eine auf einem neuronalen Graphennetz basierende Architektur vorgestellt, die trainiert wird, um die Interaktionen von Agenten in einer Umgebung direkt aus Pixeln zu lokalisieren und zu modellieren, und die Vorteile des Modells für die Verfolgung von Aufgaben und die Vorhersage von Agentenstandorten aufzeigt.
Wir betrachten ein vereinfachtes Modell eines tiefen Convolutional Neural Networks. Wir zeigen, dass alle Schichten dieses Netzes mit einer geeigneten Anwendung der Tensorzerlegung annähernd gelernt werden können.
Bietet theoretische Garantien für das Lernen von tiefen faConvolutional Neural Networks unter Verwendung der Rang-Eins-Tensorzerlegung.
In dieser Arbeit wird eine Lernmethode für einen eingeschränkten Fall von tiefen Convolutional Networks vorgeschlagen, bei dem die Schichten auf den nicht überlappenden Fall beschränkt sind und nur einen Ausgangskanal pro Schicht haben.
Analysiert das Problem des Lernens einer sehr speziellen Klasse von CNNs: Jede Schicht besteht aus einem einzigen Filter, der auf nicht überlappende Bereiche der Eingabe angewendet wird.
Neuronale Netze mit Vorwärtskopplung, bei denen die Gewichte nach dem Training pruned werden können, könnten die gleichen Gewichte vor dem Training gepruned worden sein.
Zeigt, dass es spärliche Subnetze gibt, die von Grund auf mit guter Generalisierungsleistung trainiert werden können, und schlägt unpruned, zufällig initialisierte NNs enthaltende Subnetze vor, die von Grund auf mit ähnlicher Generalisierungsgenauigkeit trainiert werden können.
In der Arbeit wird die Hypothese untersucht, dass zufällig initialisierte neuronale Netze Teilnetze enthalten, die gleich schnell oder schneller konvergieren und die gleiche oder eine bessere Klassifizierungsgenauigkeit erreichen können.
In diesem Beitrag wird die Schwierigkeit des Trainings spärlicher neuronaler Netze durch Interpolationsexperimente in der Energielandschaft aufgezeigt 
Die Gewichtsraum-Symmetrie in den Landschaften neuronaler Netze führt zu einer Vielzahl von Sätteln und flachen hochdimensionalen Unterräumen.
In der Arbeit wird eine verlustarme Methode zur Untersuchung der Verlustfunktion in Bezug auf die Parameter eines neuronalen Netzes unter dem Gesichtspunkt der Gewichtsraumsymmetrie vorgestellt.
Signalausbreitungstheorie angewandt auf kontinuierliche Surrogates binärer Netze; kontraintuitive Initialisierung; Reparameterisierungstrick nicht hilfreich.
Die Autoren untersuchen die Trainingsdynamik binärer neuronaler Netze bei Verwendung kontinuierlicher Surrogates, untersuchen, welche Eigenschaften Netze bei der Initialisierung haben sollten, um optimal zu trainieren, und geben konkrete Ratschläge zu stochastischen Gewichten bei der Initialisierung.
Eine eingehende Untersuchung von stochastischen binären Netzen, kontinuierlichen Surrogaten und ihrer Trainingsdynamik, mit Einblicken in die Initialisierung von Gewichten für beste Leistung.
Wir schlagen einen Ansatz zum halbüberwachten Lernen von semantischen Dependenzparsern vor, der auf dem CRF-Autoencoder-Rahmen basiert.
Diese Arbeit konzentriert sich auf semi-supervised semantische Abhängigkeit Parsing mit dem CRF-Auto-Kodierer, um das Modell in einem semi-supervised Stil zu trainieren, zeigt die Wirksamkeit auf niedrigen Ressource gelabelten Daten Aufgaben.
DeFINE verwendet ein tiefes, hierarchisches, spärliches Netzwerk mit neuen Sprungverbindungen, um effizient bessere Worteinbettungen zu lernen. 
Dieses Papier beschreibt eine neue Methode zum effizienten Erlernen tiefer Wortrepräsentationen durch die Verwendung einer hierarchischen Struktur mit Skip-Verbindungen für die Verwendung von niedrigdimensionalen Eingabe- und Ausgabeschichten.
Wir reproduzieren erfolgreich einen Meta-Learning-Ansatz für die Klassifizierung von wenigen Aufnahmen, der durch Backpropagieren der Lösung einer geschlossenen Form funktioniert, und geben Anmerkungen zum Vergleich mit Baselines.
Die dynamische Neuzuweisung von Parametern ermöglicht ein erfolgreiches direktes Training kompakter spärlicher Netze und spielt selbst dann eine unverzichtbare Rolle, wenn wir das optimale spärliche Netz von vornherein kennen.
3 Schwerpunkte, die als Sprungbretter für das Roboter-Erfahrungslernen des Bildverarbeitungsmoduls dienen.
Untersucht die Leistung bestehender Bildklassifizierer und Objektdetektoren. 
Mit Ausnahme der ersten beiden Schichten unserer CNN-basierten akustischen Modelle wiesen alle ein gewisses Maß an Sprachspezifität auf, aber das Einfriertraining ermöglichte einen erfolgreichen Transfer zwischen den Sprachen.
Die Arbeit misst die Übertragbarkeit von Merkmalen für jede Schicht in CNN-basierten akustischen Modellen über verschiedene Sprachen hinweg und kommt zu dem Schluss, dass AMs, die mit der Technik des "Freeze-Trainings" trainiert wurden, andere übertragene Modelle übertreffen.
Verknüpfung von Wasserstein-Vertrauensgebieten, entropischen politischen Gradienten und der Wärmegleichung.
Die Arbeit untersucht die Verbindungen zwischen Reinforcement Learning und der Theorie des quadratischen optimalen Transports.
Die Autoren untersuchten den Policy-Gradienten mit einem Wechsel der Policies, der durch eine Vertrauensregion mit Wasserstein-Distanz im Multi-Armed-Bandit-Setting begrenzt ist, und zeigten, dass die Policy-Dynamik im Grenzbereich kleiner Schritte durch die Wärmegleichung (Fokker-Planck-Gleichung) bestimmt wird.
Die Unterscheidungsfähigkeit von Softmax beim Lernen von Merkmalsvektoren von Objekten wird durch die isotrope Normalisierung der globalen Verteilung der Datenpunkte effektiv verbessert.
Wir passen das Q-Lernen mit UCB-Explorationsbonus an ein MDP mit unendlichem Zeithorizont und diskontierten Belohnungen an, ohne auf ein generatives Modell zuzugreifen, und verbessern das bisher beste bekannte Ergebnis.
In dieser Arbeit wurde ein Q-Learning-Algorithmus mit UCB-Explorationspolitik für MDP mit unendlichem Horizont betrachtet.
Störungen können verwendet werden, um Rückkopplungsgewichte zu trainieren, die in Fully Connected und Convolutional Neural Networks lernen.
In dieser Arbeit wird eine Methode vorgeschlagen, die das Problem des "Gewichtstransports" löst, indem die Gewichte für den Rückwärtsdurchlauf mit Hilfe eines rauschbasierten Schätzers geschätzt werden.
Wir identifizieren einige universelle Muster (d.h. über alle Architekturen hinweg) im Verhalten verschiedener Ersatzverluste (CE, MSE, 0-1 Verlust) beim Training neuronaler Netze und präsentieren unterstützende empirische Beweise.
