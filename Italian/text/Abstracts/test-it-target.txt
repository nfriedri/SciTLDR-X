FearNet è una rete neurale efficiente in termini di memoria, ispirata alla formazione della memoria nel cervello dei mammiferi, che è in grado di apprendere classi incrementali senza dimenticanze catastrofiche.
Questo articolo presenta una nuova soluzione a un problema di classificazione incrementale basato su un sistema a doppia memoria. 
L'apprendimento multi vista migliora l'apprendimento non supervisionato della rappresentazione della frase
L'approccio utilizza diversi codificatori complementari della frase in ingresso e la massimizzazione del consenso.
L'articolo presenta un quadro multi-vista per migliorare la rappresentazione delle frasi nei compiti NLP usando architetture generative e discriminative.
Questo articolo mostra che le strutture multi-vista sono più efficaci dell'uso di codificatori individuali per l'apprendimento di rappresentazioni di frasi.
Mostriamo come gli oggetti discreti possono essere appresi in modo non supervisionato dai pixel, e come eseguire l'apprendimento per rinforzo usando questa rappresentazione degli oggetti.
Un metodo per imparare rappresentazioni di oggetti dai pixel per fare l'apprendimento di rinforzo. 
L'articolo propone un'architettura neurale per mappare i flussi video su una collezione discreta di oggetti, senza annotazioni umane, utilizzando una perdita di ricostruzione dei pixel non supervisionata. 
Un set di dati su larga scala per l'addestramento di modelli di attenzione per il riconoscimento degli oggetti porta a un riconoscimento degli oggetti più accurato, interpretabile e simile a quello umano.
Sostiene che i recenti guadagni nel riconoscimento visivo derivano dall'utilizzo di meccanismi di attenzione visiva nelle reti convoluzionali profonde, che imparano dove concentrarsi attraverso una forma debole di supervisione basata sulle etichette di classe delle immagini.
Presenta un nuovo approccio all'attenzione in cui un grande set di dati sull'attenzione viene raccolto e utilizzato per addestrare una NN in modo supervisionato per sfruttare l'attenzione umana auto-riferita.
Questo articolo propone un nuovo approccio per utilizzare segnali più informativi, in particolare, le regioni che gli esseri umani ritengono importanti nelle immagini, per migliorare le reti neurali convoluzionali profonde.
Abbiamo proposto un metodo di difesa efficiente in termini di tempo contro gli attacchi avversari a un passo e iterativi.
Proporre un nuovo metodo efficiente dal punto di vista computazionale chiamato e2SAD che genera insiemi di due campioni di formazione avversaria per ogni campione di formazione pulito.
L'articolo introduce un metodo di difesa avversaria in due fasi, per generare due esempi avversari per ogni campione pulito e includerli nel ciclo di formazione attuale per raggiungere la robustezza e affermare che può superare i metodi iterativi più costosi.
L'articolo presenta un approccio in 2 fasi per generare forti esempi avversari ad un costo molto inferiore rispetto ai recenti attacchi avversari iterativi in più fasi.
Un confronto tra cinque architetture di reti neurali profonde per il rilevamento di nomi di dominio malevoli mostra sorprendentemente poche differenze.
Gli autori propongono di utilizzare cinque architetture profonde per il compito di cybersicurezza del rilevamento degli algoritmi di generazione del dominio.
Applica diverse architetture NN per classificare gli URL tra quelli benigni e quelli relativi al malware.
Questo articolo propone di riconoscere automaticamente i nomi di dominio come malevoli o benigni tramite reti profonde addestrate per classificare direttamente la sequenza di caratteri come tale.
I metodi di apprendimento avversariale incoraggiano i modelli NLI a ignorare le distorsioni specifiche del dataset e aiutano i modelli a trasferirsi attraverso i dataset.
L'articolo propone una configurazione avversaria per mitigare gli artefatti di annotazione nei dati di inferenza del linguaggio naturale
Questo articolo presenta un metodo per rimuovere i bias di un modello di entailment testuale attraverso un obiettivo di formazione avversaria. 
Un nuovo approccio allo stato dell'arte per l'incorporazione dei grafi di conoscenza.
Presenta una funzione di punteggio neurale di predizione dei collegamenti che può dedurre modelli di simmetria, antisimmetria, inversione e composizione delle relazioni in una base di conoscenza.
Questo articolo propone un approccio all'incorporazione dei grafi di conoscenza modellando le relazioni come rotazioni nello spazio vettoriale complesso.
Propone un metodo per l'incorporazione di grafici da utilizzare per la predizione dei link
Abbiamo modificato la CNN usando HyperNetworks e abbiamo osservato una migliore robustezza contro gli esempi avversari.
Migliorare la robustezza e l'affidabilità delle reti neurali di convoluzione profonda usando kernel di convoluzione dipendenti dai dati
Proponiamo un approccio di meta-apprendimento per la classificazione a pochi scatti che raggiunge forti prestazioni ad alta velocità tramite il back-propagating attraverso la soluzione di solutori veloci, come la regressione ridge o la regressione logistica.
L'articolo propone un algoritmo per il meta-apprendimento che equivale a fissare le caratteristiche (cioè tutti gli strati nascosti di una NN profonda), e trattare ogni compito come se avesse il proprio strato finale che potrebbe essere una regressione ridge o una regressione logistica.
Questo articolo propone un approccio di meta-apprendimento per il problema della classificazione di pochi scatti, usano un metodo basato sulla parametrizzazione del discente per ogni compito da un risolutore in forma chiusa.
Forniamo una nuova prospettiva sull'addestramento di un modello di apprendimento automatico da zero nell'impostazione delle etichette gerarchiche, cioè pensandolo come una comunicazione a due vie tra l'uomo e gli algoritmi, e studiamo come possiamo sia misurare che migliorare l'efficienza. 
Introduce una nuova impostazione di Active Learning in cui l'oracolo offre un'etichetta parziale o debole invece di interrogare l'etichetta di un particolare esempio, portando a un recupero più semplice delle informazioni.
Questo articolo propone un metodo di apprendimento attivo con feedback parziale che supera le linee di base esistenti sotto un budget limitato.
L'articolo considera un problema di classificazione multiclasse in cui le etichette sono raggruppate in un dato numero M di sottoinsiemi, che contengono tutte le etichette individuali come singleton.
Mostriamo che gli spazi di Wasserstein sono buoni obiettivi per incorporare dati con una struttura semantica complessa.
Impara le incorporazioni in uno spazio discreto di distribuzioni di probabilità, utilizzando una versione minimizzata e regolarizzata delle distanze di Wasserstein.
L'articolo descrive un nuovo metodo di incorporazione che incorpora i dati nello spazio delle misure di probabilità dotato della distanza di Wasserstein. 
L'articolo propone di incorporare i dati in spazi Wasserstein a bassa dimensione, che possono catturare la struttura sottostante dei dati in modo più accurato.
Un algoritmo di clustering che esegue la riduzione non lineare della dimensionalità e il clustering congiunto ottimizzando un obiettivo globale continuo.
Presenta un algoritmo di clustering risolvendo congiuntamente l'autoencoder profondo e il clustering come obiettivo continuo globale, mostrando risultati migliori rispetto agli schemi di clustering all'avanguardia.
Deep Continuous Clustering è un metodo di clustering che integra l'obiettivo dell'autocodificatore con l'obiettivo del clustering e poi si allena usando SGD.
Per accelerare il calcolo delle reti neurali convoluzionali, proponiamo una nuova tecnica di pruning in due fasi che raggiunge una maggiore sparsità dei pesi nel dominio di Winograd senza cambiare la struttura della rete.
Propone un quadro di pruning spaziale-Winograd che permette di mantenere il peso potato dal dominio spaziale nel dominio Winograd e migliora la sparsità del dominio Winograd.
Propone due tecniche per la potatura degli strati convoluzionali che utilizzano l'algoritmo di Winograd
Proponiamo un modello bayesiano non parametrico per l'apprendimento federato con reti neurali.
Utilizza il processo beta per fare la corrispondenza neurale federata.
L'articolo considera l'apprendimento federato delle reti neurali, dove i dati sono distribuiti su più macchine e l'assegnazione dei punti dati è potenzialmente disomogenea e sbilanciata.
Metodo generale per addestrare kernel MCMC espressivi parametrizzati con reti neurali profonde. Data una distribuzione di destinazione p, il nostro metodo fornisce un campionatore veloce, in grado di esplorare in modo efficiente lo spazio di stato.
Propone un HMC generalizzato modificando l'integratore leapfrog utilizzando reti neurali per far convergere e miscelare rapidamente il campionatore. 
Mostriamo che i fallimenti rari ma catastrofici possono essere mancati interamente dai test casuali, il che pone problemi per una distribuzione sicura. Il nostro approccio proposto per i test avversari risolve questo problema.
Propone un metodo che impara un predittore di probabilità di fallimento per un agente appreso, portando a predire quali stati iniziali causano il fallimento di un sistema.
Questo articolo propone un approccio di campionamento di importanza per campionare i casi di fallimento per gli algoritmi RL basati su una funzione appresa tramite una rete neurale sui fallimenti che si verificano durante l'addestramento dell'agente
Questo articolo ha proposto un approccio adversariale per identificare i casi di fallimento catastrofico nel reinforcement learning.
Per affrontare il collasso posteriore nelle VAE, proponiamo una procedura di formazione nuova ma semplice che ottimizza in modo aggressivo la rete di inferenza con più aggiornamenti. Questa nuova procedura di formazione attenua il collasso posteriore e porta a un modello VAE migliore. 
Esamina il fenomeno del collasso posteriore, mostrando che un maggiore addestramento della rete di inferenza può ridurre il problema e portare a migliori ottimali.
Gli autori propongono di cambiare la procedura di formazione delle VAE solo come soluzione al collasso posteriore, lasciando il modello e l'obiettivo intatti.
Scoprire generativamente nuove coppie di entità significative con una certa relazione medica imparando puramente dalle coppie di entità significative esistenti, senza il requisito di un corpus di testo aggiuntivo per l'estrazione discriminante.
Presenta un autocodificatore variazionale per generare coppie di entità data una relazione in un ambiente medico.
Nel contesto medico, questo articolo descrive il classico problema del "completamento della base di conoscenza" dai soli dati strutturati.
Analizziamo da vicino la funzione obiettivo VAE e traiamo nuove conclusioni che portano a semplici miglioramenti.
Propone un metodo VAE a due stadi per generare campioni di alta qualità ed evitare la sfocatura.
Questo articolo analizza le VAE gaussiane.
L'articolo fornisce una serie di risultati teorici sugli autocodificatori variazionali gaussiani "vanilla", che sono poi utilizzati per costruire un nuovo algoritmo chiamato "2 stage VAEs".
Un algoritmo di regolazione dell'iperparametro utilizzando l'analisi di Fourier discreta e il rilevamento compresso
Indaga il problema dell'ottimizzazione degli iperparametri sotto l'ipotesi che la funzione sconosciuta possa essere approssimata, mostrando che la minimizzazione approssimata può essere eseguita sull'ipercubo booleano.
L'articolo esplora l'ottimizzazione degli iperparametri assumendo una struttura nella funzione sconosciuta che mappa gli iperparametri all'accuratezza della classificazione
Un nuovo metodo per l'inferenza a gradiente di permutazioni, con applicazioni all'inferenza di corrispondenza latente e all'apprendimento supervisionato di permutazioni con reti neurali
L'articolo utilizza un'approssimazione finita dell'operatore Sinkhorn per descrivere come si può costruire una rete neurale per l'apprendimento da dati di formazione valutati in permutazione. 
L'articolo propone un nuovo metodo che approssima il peso massimo discreto per l'apprendimento delle permutazioni latenti
Un nuovo quadro di ricerca dell'architettura neurale differenziabile per la quantizzazione mista di ConvNets.
Gli autori introducono un nuovo metodo per la ricerca dell'architettura neurale che seleziona la quantizzazione di precisione dei pesi in ogni strato della rete neurale, e lo usano nel contesto della compressione della rete.
L'articolo presenta un nuovo approccio nella quantizzazione della rete quantizzando diversi strati con diverse larghezze di bit e introduce un nuovo quadro di ricerca dell'architettura neurale differenziabile.
Funzione di perdita liscia per la minimizzazione dell'errore Top-k
Propone di utilizzare la perdita top-k con modelli profondi per affrontare il problema della confusione di classi con classi simili presenti o assenti nel set di dati di formazione.
Attenua le perdite top-k.
Questo articolo introduce una funzione di perdita surrogata liscia per la SVM top-k, allo scopo di collegare la SVM alle reti neurali profonde.
Introduciamo Mol-CycleGAN - un nuovo modello generativo per l'ottimizzazione delle molecole per aumentare la progettazione dei farmaci.
L'articolo presenta un approccio per l'ottimizzazione delle proprietà molecolari basato sull'applicazione di CycleGANs agli autocodificatori variazionali per le molecole e impiega un VAE specifico del dominio chiamato Junction Tree VAE (JT-VAE).
Questo articolo usa un autoencoder variazionale per imparare una funzione di traduzione, dall'insieme delle molecole senza la proprietà interessata all'insieme delle molecole con la proprietà. 
Prendiamo il riconoscimento dei volti come punto di rottura e proponiamo la distillazione dei modelli con il trasferimento di conoscenze dalla classificazione dei volti all'allineamento e alla verifica
Questo articolo propone di trasferire il classificatore dal modello per la classificazione dei volti al compito di allineamento e verifica.
Il manoscritto presenta esperimenti sulla distillazione della conoscenza da un modello di classificazione dei volti a modelli di studenti per l'allineamento e la verifica dei volti.
Miglioramento delle raccomandazioni basate sulla sessione con RNNs (GRU4Rec) del 35% utilizzando funzioni di perdita e campionamento di nuova concezione.
Questo articolo analizza le funzioni di perdita esistenti per le raccomandazioni basate sulla sessione e propone due nuove funzioni di perdita che aggiungono una ponderazione alle funzioni di perdita esistenti basate sul ranking
Presenta modifiche in cima al lavoro precedente per la raccomandazione basata sulla sessione usando RNN pesando gli esempi negativi per la loro "rilevanza".
Questo articolo discute i problemi di ottimizzazione delle funzioni di perdita in GRU4Rec, propone dei trucchi per l'ottimizzazione e suggerisce una versione migliorata.
Sviluppiamo un approccio di apprendimento permanente per l'apprendimento di trasferimento basato sulla teoria PAC-Bayes, in cui i priori vengono aggiustati quando si incontrano nuovi compiti, facilitando così l'apprendimento di nuovi compiti.
Un nuovo limite di rischio PAC-Bayesiano che serve come funzione obiettivo per l'apprendimento automatico multi-task, e un algoritmo per minimizzare una versione semplificata di quella funzione obiettivo.
Estende i limiti PAC-Bayes esistenti all'apprendimento multitasking, per consentire l'adattamento del priore a diversi compiti.
Una versione su misura di Adam per l'addestramento delle DNN, che colma il divario di generalizzazione tra Adam e SGD.
Propone una variante dell'algoritmo di ottimizzazione ADAM che normalizza i pesi di ogni unità nascosta usando la normalizzazione batch
Estensione dell'algoritmo di ottimizzazione di Adam per preservare la direzione di aggiornamento adattando il tasso di apprendimento per i pesi in entrata a un'unità nascosta utilizzando congiuntamente la norma L2 del vettore del gradiente
Mostriamo come possiamo usare la rappresentazione del successore per scoprire auto-opzioni in domini stocastici, a partire da pixel grezzi. Le auto-opzioni sono opzioni apprese per navigare nelle dimensioni latenti di una rappresentazione appresa.
Estende l'idea di eigenopzioni a domini con transizioni stocastiche e dove le caratteristiche dello stato vengono apprese.
Mostra l'equivalenza tra le funzioni di valore proto e le rappresentazioni dei successori e deriva l'idea delle opzioni eigen come meccanismo nella scoperta delle opzioni
L'articolo è un seguito del precedente lavoro di Machado et al. (2017) che mostra come le funzioni di proto-valore possono essere usate per definire opzioni chiamate "eigenopzioni".
Forniamo limiti superiori migliorati per il numero di regioni lineari usate nell'espressività della rete, e un algoritmo altamente efficiente (rispetto al conteggio esatto) per ottenere limiti inferiori probabilistici sul numero effettivo di regioni lineari.
Contribuisce allo studio del numero di regioni lineari nelle reti neurali RELU utilizzando un algoritmo di conteggio probabilistico approssimato e l'analisi
Costruisce il lavoro precedente che studia il conteggio delle regioni lineari nelle reti neurali profonde, e migliora il limite superiore precedentemente proposto cambiando il vincolo di dimensionalità
L'articolo si occupa dell'espressività di una rete neurale piecewise linear, caratterizzata dal numero di regioni lineari della funzione modellata, e sfrutta algoritmi probabilistici per calcolare i limiti più velocemente, e dimostra limiti più stretti.
Una memoria di lavoro biologicamente ispirata che può essere integrata in modelli di attenzione visiva ricorrente per prestazioni allo stato dell'arte
Introduce una nuova architettura di rete ispirata alla memoria di lavoro visiva attenta e la applica a compiti di classificazione e la usa come modello generativo
L'articolo aumenta il modello di attenzione ricorrente con un nuovo modello di memoria di lavoro di Hebb-Rosenblatt e raggiunge risultati competitivi su MNIST
L'articolo utilizza l'autocodifica variazionale e il condizionamento di rete per il trasferimento del timbro musicale, sviluppiamo e generalizziamo la nostra architettura per trasferimenti da molti a molti strumenti insieme a visualizzazioni e valutazioni.
Propone un auto-Encoder Variazionale Modulato per eseguire il trasferimento del timbro musicale sostituendo il solito criterio di traduzione avversaria con un Maxiimum Mean Discrepancy
Descrive un modello molti-a-molti per il trasferimento del timbro musicale che si basa sui recenti sviluppi nel trasferimento del dominio e dello stile
Propone un modello ibrido basato su VAE per eseguire il trasferimento del timbro su registrazioni di strumenti musicali.
Studiamo il comportamento degli autocodificatori di vaniglia multistrato legati al peso sotto l'ipotesi di pesi casuali. Attraverso una caratterizzazione esatta nel limite di grandi dimensioni, la nostra analisi rivela interessanti fenomeni di transizione di fase.
Un'analisi teorica degli autoencoder con pesi legati tra codificatore e decodificatore (weight-tied) tramite l'analisi del campo medio
Analizza le prestazioni degli autocodificatori legati ponderati basandosi sui recenti progressi nell'analisi dei problemi di statistica ad alta dimensione e in particolare, l'algoritmo di passaggio dei messaggi
Questo articolo studia gli autocodificatori sotto diverse ipotesi, e sottolinea che questo modello di autocodificatore casuale può essere analizzato in modo elegante e rigoroso con equazioni monodimensionali.
Ispirati da lavori precedenti su Sliced-Wasserstein Autoencoders (SWAE) e kernel smoothing costruiamo un nuovo modello generativo - Cramer-Wold AutoEncoder (CWAE).
Questo articolo propone una variante WAE basata su una nuova distanza statistica tra la distribuzione dei dati codificati e la distribuzione a priori latente
Introduce una variazione dell'AudoEncoders di Wasserstein che è una nuova architettura di auto-encoder regolarizzato che propone una scelta specifica della penalità di divergenza
Questo articolo propone l'autocodificatore Cramer-Wold, che utilizza la distanza Cramer-Wold tra due distribuzioni basata sul teorema Cramer-Wold.
Usiamo un discriminatore GAN per eseguire uno schema di campionamento approssimativo di rifiuto sull'uscita del generatore GAN.
 Propone un algoritmo di campionamento di rifiuto per il campionamento dal generatore GAN.
Questo articolo ha proposto uno schema di campionamento di rifiuto post-elaborazione per le GAN, chiamato Discriminator Rejection Sampling, per aiutare a filtrare i campioni "buoni" dal generatore delle GAN.
Un metodo semplice e veloce per estrarre le caratteristiche visive dalle reti neurali convoluzionali
Propone un modo veloce per imparare caratteristiche convoluzionali che in seguito possono essere utilizzate con qualsiasi classificatore utilizzando un numero ridotto di epoc di allenamento e ritardi specifici del tasso di apprendimento
Utilizzare uno schema di decadimento del tasso di apprendimento che è fisso rispetto al numero di epoche utilizzate nell'addestramento ed estrarre l'uscita del penultimo strato come caratteristiche per addestrare un classificatore convenzionale.
Forniamo nuove intuizioni e interpretazioni delle RNN dal punto di vista degli operatori spline max-affine.
Riscrive le equazioni di Elman RNN in termini dei cosiddetti operatori spline max-affine
Fornire un nuovo approccio verso la comprensione delle RNN utilizzando operatori spline max-affline (MASO) riscrivendoli con attivazioni affini e convesse piecewise MASO
Gli autori si basano sull'interpetazione dell'operatore spline max-affine di una classe sostanziale di reti profonde, concentrandosi sulle reti neurali ricorrenti che utilizzano il rumore nello stato nascosto iniziale come regolarizzazione
Scaliamo i Neural Theorem Prover a grandi insiemi di dati, miglioriamo il processo di apprendimento delle regole e lo estendiamo per ragionare insieme su testo e basi di conoscenza.
Propone un'estensione del sistema Neural Theorem Provers che affronta i problemi principali di questo modello riducendo la complessità temporale e spaziale del modello
Scala gli NTP usando la ricerca approssimativa dei vicini più vicini su fatti e regole durante l'unificazione e suggerisce di parametrizzare i predicati usando l'attenzione su predicati conosciuti
migliora l'approccio Neural Theorem Prover proposto in precedenza utilizzando la ricerca del vicino più vicino.
Generalizzazione delle relazioni apprese tra coppie di immagini utilizzando una piccola quantità di dati di addestramento a tipi di immagini precedentemente non visti utilizzando un modello di sistemi dinamici spiegabili, Reservoir Computing, e una tecnica di apprendimento biologicamente plausibile basata sulle analogie.
I risultati di "combinare le trasformazioni" nel contesto di RC utilizzando una rete di eco-stato con accattivazioni tanh standard con la differenza che i pesi ricorrenti non sono addestrati
Nuovo metodo per classificare diverse distorsioni dei dati MNIST
L'articolo utilizza una rete a stati d'eco per imparare a classificare le trasformazioni di immagini tra coppie di immagini in una delle cinque classi.
Presentiamo Generative Adversarial Privacy and Fairness (GAPF), una struttura data-driven per l'apprendimento di rappresentazioni private ed eque con garanzie certificate di privacy/equità
Questo articolo usa un modello GAN per fornire una panoramica del lavoro correlato all'apprendimento di rappresentazioni private/equo (PRL).
Questo articolo presenta un approccio basato sull'avversario per rappresentazioni private ed eque attraverso la distorsione appresa dei dati che minimizza la dipendenza dalle variabili sensibili mentre il grado di distorsione è vincolato.
Gli autori descrivono un quadro di come imparare una rappresentazione di parità demografica che può essere utilizzata per addestrare alcuni classificatori.
Presentiamo una metrica e un attacco ottimale per valutare i modelli che si difendono dagli esempi avversari usando la soglia di fiducia
Questo articolo introduce una famiglia di attacchi agli algoritmi di soglia di fiducia, concentrandosi principalmente sulle metodologie di valutazione.
Propone un metodo di valutazione per i modelli di difesa a soglia di fiducia e un approccio per generare esempi avversari scegliendo la classe sbagliata con più fiducia quando si usano attacchi mirati
L'articolo presenta una metodologia di valutazione degli attacchi ai metodi di soglia di fiducia e propone un nuovo tipo di attacco.
un nuovo metodo per imparare con ricompensa sparsa usando la rietichettatura adversariale della ricompensa
Propone di utilizzare un ambiente competitivo multi-agente per incoraggiare l'esplorazione e mostra che CER + HER > HER ~ CER
Proporre un nuovo metodo per imparare da ricompense sparse in impostazioni di apprendimento di rinforzo senza modello e densificare la ricompensa
Per affrontare i problemi di ricompensa sparsa e incoraggiare l'esplorazione negli algoritmi RL, gli autori propongono una strategia di rietichettatura chiamata Competitive Experience Reply (CER).
Costruire un modello TTS con Gaussian Mixture VAEs permette un controllo a grana fine dello stile di conversazione, delle condizioni di rumore e altro.
Descrive il modello GAN condizionato per generare gli spettri di Mel condizionati dal parlante aumentando lo spazio z corrispondente all'identificazione
Questo articolo propone un modello di variabile latente a due strati per ottenere una rappresentazione latente dissociata, facilitando così un controllo a grana fine su vari attributi
Questo articolo propone un modello che può controllare gli attributi non annotati, come lo stile di conversazione, l'accento, il rumore di fondo, ecc.
Abilitare i modelli di Visual Question Answering a contare gestendo le proposte di oggetti sovrapposti.
Questo articolo propone un'architettura di rete progettata a mano su un grafico di proposte di oggetti per eseguire una soppressione morbida non massima per ottenere il conteggio degli oggetti.
Si concentra su un problema di conteggio nella risposta a domande visive utilizzando il meccanismo dell'attenzione e propone un componente di conteggio differenziabile che conta esplicitamente il numero di oggetti
Questo articolo affronta il problema del conteggio degli oggetti nella risposta alle domande visive, propone molte euristiche per trovare il conteggio corretto.
Un approccio semplice e senza addestramento per l'embedding delle frasi con prestazioni competitive rispetto ai modelli sofisticati che richiedono una grande quantità di dati di addestramento o un tempo di addestramento prolungato.
Presentato un nuovo modo senza addestramento di generare l'incorporamento delle frasi con un'analisi sistematica
Propone un nuovo metodo basato sulla geometria per l'incorporazione delle frasi dai vettori di incorporazione delle parole, quantificando la novità, il significato e l'unicità del corpus di ogni parola
Questo articolo esplora l'incorporamento delle frasi basato sulla decomposizione ortogonale dello spazio attraversato da embeddings di parole
Proponiamo nuove estensioni delle reti prototipiche che sono aumentate con la capacità di utilizzare esempi non etichettati quando si producono prototipi.
Questo documento è un'estensione di una rete prototipica che considera l'impiego degli esempi non etichettati disponibili per aiutare ad addestrare ogni episodio
Studia il problema della classificazione semi-supervisionata di pochi colpi estendendo le reti prototipiche nell'impostazione dell'apprendimento semi-supervisionato con esempi di classi distratte
Estende la rete prototipica all'impostazione semi-supervisionata aggiornando i prototipi usando pseudo-etichette assegnate, trattando i distrattori e pesando i campioni usando la distanza dai prototipi originali.
Proviamo teoricamente che le interpolazioni lineari sono inadatte all'analisi dei modelli generativi impliciti addestrati. 
Studia il problema di quando l'interpolante lineare tra due variabili casuali segue la stessa distribuzione, in relazione alla distribuzione a priori di un modello generativo implicito
Questo lavoro chiede come interpolare nello spazio latente dato un modello di variabile latente.
Rilevamento del nodulo polmonare a partire da dati di proiezione piuttosto che da immagini.
Le DNN sono utilizzate per il rilevamento dei noduli polmonari basato su patch nei dati di proiezione CT.
Modellazione congiunta della ricostruzione della tomografia computerizzata e del rilevamento delle lesioni nel polmone addestrando la mappatura dal sinogramma grezzo agli output di rilevamento in modo end-to-end
Presenta una formazione end-to-end di un'architettura CNN che combina l'elaborazione del segnale dell'immagine CT e l'analisi dell'immagine.
Valutiamo quantitativamente e qualitativamente i metodi di navigazione basati sull'apprendimento di rinforzo profondo in una varietà di condizioni per rispondere alla domanda su quanto siano vicini a sostituire i pianificatori di percorso classici e gli algoritmi di mappatura.
Valutare un modello basato su Deep RL su labirinti di allenamento misurando la latenza ripetuta all'obiettivo e il confronto con il percorso più breve
Valutiamo l'apprendimento di modelli di rumore eteroscedastici all'interno di diversi filtri di Bayes differenziabili
Propone di apprendere modelli di rumore eteroscedastici dai dati ottimizzando la probabilità di predizione end-toend attraverso filtri bayesiani differenziabili e due diverse versioni del filtro di Kalman non brevettato
Rivede i filtri di Bayes e valuta il beneficio di addestrare i modelli di osservazione e di rumore di processo mantenendo fissi tutti gli altri modelli
Questo articolo presenta un metodo per imparare e utilizzare il rumore dipendente dallo stato e dall'osservazione nei tradizionali algoritmi di filtraggio bayesiano. L'approccio consiste nel costruire un modello di rete neurale che prende come input i dati grezzi di osservazione e produce una rappresentazione compatta e una covarianza diagonale associata.
Ripensiamo al modo in cui le informazioni possono essere sfruttate in modo più efficiente nel grafo della conoscenza per migliorare le prestazioni nel compito di apprendimento Zero-Shot e proponiamo un modulo DGP (dense graph propagation) per questo scopo.
Questi autori propongono una soluzione al problema dell'over-smoothing nelle reti Graph conv permettendo una propagazione densa tra tutti i nodi correlati, ponderata dalla distanza reciproca.
Propone una nuova rete neurale convoluzionale a grafo per affrontare il problema della classificazione a zero colpi utilizzando strutture relazionali tra le classi come input delle reti convoluzionali a grafo per imparare classificatori di classi non viste
Una segmentazione semantica basata su capsule, in cui le probabilità delle etichette di classe sono rintracciate attraverso la pipeline di capsule. 
Gli autori presentano un meccanismo di tracciamento per associare il livello più basso di Capsule con le loro rispettive classi
Propone uno strato di tracciamento per le reti di capsule per fare la segmentazione semantica e fa uso esplicito della relazione parte-intero negli strati di capsule
Propone un metodo trace-back basato sul concetto CapsNet di Sabour per eseguire una segmentazione semantica in parallelo alla classificazione.
Consideriamo SGD come una traiettoria nello spazio delle misure di probabilità, mostriamo la sua connessione con i processi di Markov, proponiamo un semplice modello di Markov per l'apprendimento di SGD e lo confrontiamo sperimentalmente con SGD utilizzando quantità teoriche dell'informazione. 
Costruisce una catena di Markov che segue un percorso corto nella metrica TV su P e mostra che le traiettorie di SGD e \alpha-SMLC hanno entropia condizionale simile
Studia la traiettoria di H(\hat{y}) contro H(\hat{y}|y) sul piano dell'informazione per i metodi di discesa del gradiente stocastico per l'addestramento delle reti neurali
Descrive SGD dal punto di vista della distribuzione p(y',y) dove y è una vera etichetta di classe (eventualmente corrotta) e y' una previsione del modello.
Questo articolo propone un metodo per automatizzare la progettazione della proposta MCMC a gradiente stocastico usando un approccio di meta apprendimento. 
Propone un approccio di meta-apprendimento per progettare automaticamente un campionatore MCMC basato sulla dinamica hamiltoniana per mescolarsi più velocemente su problemi simili a quelli di allenamento
Parametrizza le matrici di diffusione e di ricciolo tramite reti neurali e meta-apprende e ottimizza un algoritmo sg-mcmc. 
Le tecniche di valutazione della qualità dell'immagine migliorano l'addestramento e la valutazione delle reti generative adversariali basate sull'energia
Propone una formulazione basata sull'energia per il modello BEGAN e la modifica per includere un termine basato sulla valutazione della qualità dell'immagine
Propone alcune nuove funzioni energetiche nel BEGAN (boundary equilibrium GAN framework), tra cui l_1 score, Gradient magnitude similarity score, e chrominance score.
Introduciamo una semplice variante dell'ottimizzazione del momentum che è in grado di superare il momentum classico, Nesterov e Adam su compiti di apprendimento profondo con una sintonizzazione minima degli iperparametri.
Introduce una variante della quantità di moto che aggrega diverse velocità con diversi coefficienti di smorzamento che diminuisce significativamente l'oscillazione
Proposto un metodo di momento aggregato per l'ottimizzazione basata sul gradiente utilizzando più vettori di velocità con diversi fattori di smorzamento invece di un singolo vettore di velocità per migliorare la stabilità
Gli autori combinano diversi passi di aggiornamento insieme per ottenere il momentum aggregato dimostrando anche che è più stabile degli altri metodi di momentum
Introduciamo la Recurrent Discounted Unit che applica l'attenzione a qualsiasi sequenza di lunghezza in tempo lineare
Questo articolo propone la Recurrent Discounted Attention (RDA), un'estensione della Recurrent Weighted Average (RWA) aggiungendo un fattore di sconto.
Estende la media dei pesi ricorrenti per superare la limitazione del metodo originale mantenendo il suo vantaggio e propone il metodo di usare le reti di Elman come RNN di base
È possibile imparare una distribuzione gaussiana centrata sullo zero sui pesi di una rete neurale imparando solo le varianze, e funziona sorprendentemente bene.
Questo articolo studia gli effetti della media del posteriore variazionale e propone lo strato della varianza, che usa solo la varianza per memorizzare le informazioni
Studia le reti neurali a varianza che approssimano il posteriore delle reti neurali bayesiane con distribuzioni gaussiane a media zero
Facciamo una rete di Graph Convolution Networks, alimentando ciascuno una potenza diversa della matrice di adiacenza, combinando tutta la loro rappresentazione in una sottorete di classificazione, raggiungendo lo stato dell'arte sulla classificazione semi-supervisionata dei nodi.
Propone una nuova rete di GCN con due approcci: uno strato completamente connesso sopra le caratteristiche impilate e un meccanismo di attenzione che usa un peso scalare per GCN.
Presenta una rete di reti convoluzionali a grafo che usa la statistica del cammino casuale per estrarre informazioni da vicini e lontani nel grafo
Un algoritmo di pruning veloce per strati DNN completamente connessi con analisi teorica della degradazione dell'errore di generalizzazione.
Presenta un algoritmo di pruning economico per strati densi di DNN.
Propone una soluzione al problema della potatura delle DNN ponendo la funzione obiettivo Net-trim come una funzione Difference of convex(DC).
Proponiamo una nuova rete temporale ibrida che raggiunge prestazioni all'avanguardia nella segmentazione di azioni video su tre set di dati pubblici.
Discute il problema della segmentazione delle azioni in video lunghi, fino a 10 minuti, utilizzando un'architettura di codifica-decodifica convoluzionale temporale
Propone una combinazione di rete temporale convoluzionale e ricorrente per la segmentazione di azioni video
Questo articolo propone di trasferire la conoscenza dal modello profondo a quello superficiale imitando le caratteristiche fase per fase.
Spiega un metodo di trasmissione della conoscenza fase per fase utilizzando diverse strutture di reti
Questo articolo propone di dividere una rete in più parti e distillare ogni parte in modo sequenziale per migliorare le prestazioni di distillazione nelle reti di insegnanti profondi
Miglioramenti alla robustezza avversaria, così come garanzie di robustezza dimostrabili, sono ottenuti aumentando l'addestramento avversaria con una regolarizzazione Lipschitz trattabile
Esplora l'aumento della perdita di addestramento con un termine aggiuntivo di regolarizzazione del gradiente per migliorare la robustezza dei modelli contro gli esempi avversi
Utilizza un trucco per semplificare la perdita avversaria con una in cui la perturbazione avversaria appare in forma chiusa.
Un modello che combina eliminazione e selezione per rispondere a domande a scelta multipla
Dà un'elaborazione del Gated Attention Reader aggiungendo cancelli basati sull'eliminazione delle risposte nella comprensione della lettura a scelta multipla
Questo articolo propone l'uso di una porta di eliminazione in architetture di modelli per compiti di comprensione della lettura, ma non raggiunge risultati all'avanguardia
Questo articolo sostiene un nuovo modello di comprensione della lettura a scelta multipla basato sull'idea che alcune opzioni dovrebbero essere eliminate per dedurre una migliore rappresentazione del passaggio/della domanda.
Abbiamo proposto un nuovo quadro di ragionamento ricorsivo probabilistico (PR2) per compiti di apprendimento di rinforzo profondo multi-agente.
Propone un nuovo approccio per l'addestramento completamente decentralizzato nell'apprendimento di rinforzo multi-agente
Affronta il problema di dotare gli agenti RL di capacità di ragionamento ricorsivo in un ambiente multi-agente basato sull'ipotesi che il ragionamento ricorsivo sia vantaggioso per loro per convergere verso equilibri non trivalenti
L'articolo introduce un metodo di addestramento decentralizzato per l'apprendimento di rinforzo multi-agente, dove gli agenti deducono le politiche di altri agenti e usano i modelli dedotti per prendere decisioni. 
Un nuovo algoritmo per ridurre l'overhead di comunicazione dell'apprendimento profondo distribuito distinguendo i gradienti "non ambigui".
Propone un metodo di compressione del gradiente basato sulla varianza per ridurre l'overhead di comunicazione dell'apprendimento profondo distribuito
Propone un nuovo modo di comprimere gli aggiornamenti del gradiente per SGD distribuiti al fine di accelerare l'esecuzione complessiva
Introduce un metodo di compressione del gradiente basato sulla varianza per un efficiente addestramento distribuito delle reti neurali e per misurare l'ambiguità.
Una nuova tecnica non supervisionata di adattamento del dominio profondo che unifica in modo efficiente l'allineamento delle correlazioni e la minimizzazione dell'entropia
Migliora l'approccio di allineamento delle correlazioni all'adattamento del dominio sostituendo la distanza euclidea con la distanza log-euclidea geodetica tra due matrici di covarianza, e selezionando automaticamente il costo di bilanciamento per l'entropia sul dominio di destinazione.
Proposta per l'allineamento di correlazione a minima entropia, un algoritmo di adattamento di dominio non supervisionato che collega insieme la minimizzazione dell'entropia e i metodi di allineamento di correlazione.
Proponiamo una variante dell'algoritmo di backpropagation, in cui i gradienti sono schermati dai concettatori contro la degradazione dei compiti precedentemente appresi.
Questo articolo applica la nozione di concettori, una forma di regolarizzatore, per prevenire l'oblio nell'apprendimento continuo nell'addestramento di reti neurali su compiti sequenziali.
Introduce un metodo per imparare nuovi compiti, senza interferire con i compiti precedenti, usando i concettatori.
Analizzare la ragione per cui i modelli generativi a risposta neurale preferiscono le risposte universali; proporre un metodo per evitarlo.
Indaga il problema delle risposte universali che affliggono i modelli di generazione neurale Seq2Seq
L'articolo cerca di migliorare il compito di generazione di risposte neurali de-enfatizzando le risposte comuni utilizzando la modifica della funzione di perdita e presentando le risposte comuni/universali durante la fase di formazione.
Sfruttiamo la struttura sintattica del codice sorgente per generare sequenze di linguaggio naturale.
Presenta un metodo per generare sequenze dal codice analizzando e producendo un albero di sintassi
Questo articolo introduce una codifica basata su AST per il codice di programmazione e mostra la sua efficacia nei compiti di sintesi estrema del codice e di didascalia del codice.
Questo articolo presenta un nuovo modello code-to-sequence che sfrutta la struttura sintattica dei linguaggi di programmazione per codificare frammenti di codice sorgente e poi decodificarli in linguaggio naturale
Miglioriamo le CNN con un nuovo meccanismo di attenzione per il riconoscimento a grana fine. Prestazioni superiori sono ottenute su 5 set di dati.
Descrive un nuovo meccanismo attenzionale applicato al riconoscimento a grana fine che migliora costantemente l'accuratezza del riconoscimento della linea di base
Questo articolo propone un meccanismo di attenzione feed-forward per la classificazione delle immagini a grana fine
Questo articolo presenta un interessante meccanismo di attenzione per la classificazione delle immagini a grana fine.
Sostituiamo le normali convoluzioni con le convoluzioni adattive per migliorare il generatore di GANs.
Propone di sostituire le convoluzioni nel generatore con un blocco di convoluzione adattiva che impara a generare pesi di convoluzione e polarizzazioni delle operazioni di upsampling in modo adattivo per posizione del pixel
Utilizza Adaptive Convolution nel contesto delle GAN con un blocco chiamato AdaConvBlock che sostituisce la normale Convoluzione, questo dà più contesto locale per peso del kernel in modo da poter generare oggetti localmente flessibili.
Eseguiamo esperimenti su larga scala per dimostrare che una semplice variante online di distillazione può aiutarci a scalare l'addestramento di reti neurali distribuite a più macchine.
Propone un metodo per scalare l'addestramento distribuito oltre i limiti attuali della discesa del gradiente stocastico mini-batch
Proposta di un metodo di distillazione online chiamato co-distillazione, applicato su scala, in cui due diversi modelli sono addestrati per abbinare le previsioni dell'altro modello oltre a minimizzare la propria perdita.
Viene introdotta una tecnica di distillazione online per accelerare gli algoritmi tradizionali per l'addestramento di reti neurali distribuite su larga scala
Presentiamo un algoritmo per accelerare l'addestramento SVM su massicci set di dati costruendo rappresentazioni compatte che forniscono un'inferenza efficiente e provabilmente approssimativa.
Studia l'approccio del coreset per SVM e mira a campionare un piccolo insieme di punti ponderati tali che la funzione di perdita sui punti approssimi in modo dimostrabile quella sull'intero dataset
L'articolo suggerisce una costruzione di Coreset basata sul campionamento d'importanza per rappresentare grandi dati di allenamento per SVM
Dimostriamo un tasso di convergenza non convesso per il metodo del gradiente stocastico del segno. L'algoritmo ha collegamenti con algoritmi come Adam e Rprop, così come con schemi di quantizzazione del gradiente usati nell'apprendimento automatico distribuito.
Fornito un'analisi di convergenza dell'algoritmo Sign SGD per i casi non covessivi
L'articolo esplora un algoritmo che utilizza il segno dei gradienti invece dei gradienti effettivi per l'addestramento di modelli profondi
Introduciamo un middleware trasparente per l'accelerazione delle reti neurali, con un proprio motore di compilazione, raggiungendo fino a 11.8x di velocità sulle CPU e 2.3x sulle GPU.
Questo articolo propone un livello di middleware trasparente per l'accelerazione delle reti neurali e ottiene alcuni risultati di accelerazione su architetture CPU e GPU di base
Proponiamo la convoluzione conica e la 2D-DFT per codificare l'equivarianza di rotazione in una rete neurale.
Nel contesto della classificazione delle immagini, l'articolo propone un'architettura di rete neurale convoluzionale con mappe di caratteristiche univarianti alla rotazione che sono infine rese invarianti alla rotazione usando la grandezza della trasformata discreta di Fourier 2D (DFT).
Gli autori forniscono una rete neurale invariante alla rotazione attraverso la combinazione di convoluzione conica e 2D-DFT
Introduciamo una nuova struttura feed-forward per generare metameri visivi
Propone un modello NeuroFovea per la generazione di metameri di punto di fissazione utilizzando un approccio di trasferimento di stile tramite un'architettura di stile Encoder-Decoder
Un'analisi del metamerismo e un modello capace di produrre rapidamente metameri di valore per la psicofisica sperimentale e altri domini.
L'articolo propone un metodo veloce per generare metameri visivi - immagini fisicamente diverse che non possono essere distinte da un originale - attraverso un trasferimento di stile foveated, veloce e arbitrario
Mostriamo che l'addestramento delle reti relu feedforward con un regolatore debole risulta in un margine massimo e analizziamo le implicazioni di questo risultato.
Studia la teoria del margine per gli insiemi neurali e mostra che il margine massimo è monotonicamente crescente nella dimensione della rete
Questo articolo studia la polarizzazione implicita dei minimizzatori di una perdita di entropia incrociata regolarizzata di una rete a due strati con attivazioni ReLU, ottenendo un limite superiore di generalizzazione che non aumenta con la dimensione della rete.
Un'architettura distribuita per l'apprendimento di rinforzo profondo su scala, utilizzando la generazione di dati in parallelo per migliorare lo stato dell'arte sul benchmark Arcade Learning Environment in una frazione del tempo di addestramento in wall-clock degli approcci precedenti.
Esamina un sistema di RL profondo distirbuito in cui le esperienze, piuttosto che i gradienti, sono condivisi tra i lavori paralleli e il discente centralizzato
Un approccio parallelo all'addestramento DQN, basato sull'idea di avere più attori che raccolgono dati in parallelo mentre un singolo apprendista addestra il modello da esperienze campionate dalla memoria centrale di replay.
Questo articolo propone un'architettura distribuita per l'apprendimento di rinforzo profondo su scala, concentrandosi sull'aggiunta di parallelizzazione nell'algoritmo dell'attore nel framework Prioritized Experience Replay
Architetture neurali che forniscono rappresentazioni di segnali osservati in modo irregolare e che permettono la ricostruzione del segnale.
Dimostra che le reti neurali convoluzionali con funzione di attivazione Leaky ReLU sono strutture non lineari, con risultati simili per serie temporali campionate in modo non uniforme
Questo articolo considera le reti neurali su serie temporali e mostra che i primi filtri convoluzionali possono essere scelti per rappresentare una trasformata wavelet discreta.
Meccanismi di attenzione basati sulla frase per assegnare l'attenzione sulle frasi, ottenendo allineamenti di attenzione token-to-phrase, phrase-token, phrase-to-phrase, oltre alle attenzioni token-token esistenti.
L'articolo presenta un meccanismo di attenzione che calcola una somma pesata non solo su singoli token ma su ngrammi (frasi).
Le reti profonde hanno maggiori probabilità di sbagliare in modo confidente quando testano su dati inaspettati. Proponiamo una metodologia sperimentale per studiare il problema, e due metodi per ridurre gli errori fiduciari su distribuzioni di input sconosciute.
Propone due idee per ridurre le previsioni errate troppo sicure: "G-distillazione" di un insieme con dati extra non supervisionati e riduzione della fiducia nella novità utilizzando un rilevatore di novità
Gli autori propongono due metodi per stimare la fiducia di classificazione su nuove distribuzioni di dati non visti. La prima idea è quella di utilizzare metodi di insieme come approccio di base per aiutare a identificare i casi incerti e poi utilizzare metodi di distillazione per ridurre l'insieme in un singolo modello che imita il comportamento dell'insieme. La seconda idea è di usare un classificatore rilevatore di novità e pesare l'output della rete in base al punteggio di novità.
Descriviamo un pratico algoritmo di ottimizzazione per reti neurali profonde che funziona più velocemente e genera modelli migliori rispetto agli algoritmi ampiamente utilizzati.
Propone un nuovo algoritmo in cui si afferma di usare implicitamente l'Hessiano e si usa una motivazione dalle serie di potenza
Presenta un nuovo algoritmo del 2° ordine che usa implicitamente l'informazione sulla curvatura e mostra l'intuizione dietro gli schemi di approssimazione negli algoritmi e convalida l'euristica in vari esperimenti.
Introduciamo l'approssimazione della larghezza di bit frazionaria e mostriamo che ha dei vantaggi significativi.
Suggerisce un metodo per variare il grado di quantizzazione in una rete neurale durante la fase di propagazione in avanti
Mantenere la precisione di 2bits netword mentre si usano meno di 2bits di pesi
La sostituzione media è un metodo efficiente per migliorare la perdita dopo la potatura e le funzioni di punteggio basate sull'approssimazione di Taylor funzionano meglio con i valori assoluti. 
Propone un semplice miglioramento dei metodi per la potatura delle unità utilizzando la "sostituzione media".
Questo articolo presenta una strategia di pruning di sostituzione della media e utilizza l'espansione di Taylor a valore assoluto come funzione di punteggio per il pruning
 Evitare il collasso posteriore limitando il tasso.
Presenta un approccio per prevenire il collasso posteriore nelle VAE limitando la famiglia dell'approssimazione variazionale al posteriore
Questo articolo introduce un vincolo sulla famiglia dei posteriori variazionali in modo che il termine KL possa essere controllato per combattere il collasso dei posteriori nei modelli generativi profondi come i VAE
Abbiamo sviluppato un batch momentum adattivo che può raggiungere una perdita inferiore rispetto ai metodi mini-batch dopo la scansione delle stesse epoche di dati, ed è più robusto contro le grandi dimensioni del passo.
Questo articolo affronta il problema di sintonizzare automaticamente la dimensione del batch durante l'addestramento dell'apprendimento profondo, e sostiene di estendere l'SGD adattivo del batch al momentum adattivo e di adottare gli algoritmi ai problemi delle reti neurali complesse.
L'articolo propone di generalizzare un algoritmo che esegue SGD con dimensioni adattive dei lotti aggiungendo momentum alla funzione di utilità
Usiamo i meta-gradienti per attaccare la procedura di addestramento delle reti neurali profonde per i grafi.
Studia il problema dell'apprendimento di un migliore parametro del grafico avvelenato che può massimizzare la perdita di una rete neurale a grafo. 
Un algoritmo per alterare la struttura del grafico aggiungendo/eliminando bordi in modo da degradare la performance globale della classificazione dei nodi, e l'idea di usare il meta-apprendimento per risolvere il problema di ottimizzazione bilivello.
Mostriamo che i modelli strutturati modulari sono i migliori in termini di generalizzazione sistematica e che le loro versioni end-to-end non generalizzano altrettanto bene.
Questo articolo valuta la generalizzazione sistemica tra reti neurali modulari e modelli altrimenti generici attraverso l'introduzione di un nuovo dataset di ragionamento spaziale
Una valutazione empirica mirata della generalizzazione nei modelli per il ragionamento visivo, focalizzata sul problema del riconoscimento delle triple (oggetto, relazione, oggetto) in scene sintetiche con lettere e numeri.
I modelli relazionali a termine per l'apprendimento multi-agente fanno previsioni accurate del comportamento futuro degli agenti, producono rappresentazioni intepretabili e possono essere utilizzati all'interno degli agenti.
Un modo per ridurre la varianza nell'apprendimento senza modello avendo un modello esplicito, che usa un'architettura tipo grafo conv net, delle azioni che gli altri agenti prenderanno. 
Predire il comportamento di multi-agenti usando un modello relazionale in avanti con una componente ricorrente, superando due linee di base e due ablazioni
La soluzione normalizzata della discesa del gradiente sulla regressione logistica (o una perdita che decade in modo simile) converge lentamente alla soluzione L2 max margin su dati separabili.
L'articolo offre una prova formale che la discesa del gradiente sulla perdita logistica converge molto lentamente alla soluzione SVM dura nel caso in cui i dati siano linearmente separabili. 
Questo articolo si concentra sulla caratterizzazione del comportamento della minimizzazione di log-loss su dati linearmente separabili, e mostra che log-loss, minimizzato con discesa a gradiente, porta alla convergenza alla soluzione di max-margine.
Basandoci sul lavoro precedente sulla generalizzazione del dominio, speriamo di produrre un classificatore che generalizzi a domini precedentemente non visti, anche quando gli identificatori di dominio non sono disponibili durante l'addestramento.
Un approccio di generalizzazione del dominio per rivelare informazioni semantiche basate su uno schema di proiezione lineare da CNN e strati di uscita NGLCM.
L'articolo propone un approccio non supervisionato per identificare le caratteristiche delle immagini che non sono significative per i compiti di classificazione delle immagini
Proponiamo un metodo per imparare la mimetizzazione fisica del veicolo per attaccare avversariamente i rilevatori di oggetti in natura. Troviamo il nostro camuffamento efficace e trasferibile.
Gli autori studiano il problema dell'apprendimento di un modello di mimetizzazione che, quando applicato a un veicolo simulato, impedirà a un rilevatore di oggetti di rilevarlo.
Questo articolo mira all'apprendimento adversariale per il rilevamento di auto che interferiscono apprendendo modelli di mimetizzazione
Combiniamo alberi decisionali differenziabili con autocodificatori variazionali supervisionati per migliorare l'interpretabilità della classificazione. 
Questo articolo propone un modello ibrido di un autocodificatore variazionale composto con un albero decisionale differenziabile, e uno schema di formazione di accompagnamento, con esperimenti che dimostrano le prestazioni di classificazione dell'albero, le prestazioni di neg. log likelihood e l'interpretabilità dello spazio latente.
L'articolo cerca di costruire un classificatore interpretabile e accurato impilando un VAE supervisionato e un albero decisionale differenziabile
Un approccio pratico e provabilmente garantito per l'addestramento di classificatori efficienti in presenza di spostamenti di etichette tra i set di dati di origine e di destinazione
Gli autori propongono un nuovo algoritmo per migliorare la stabilità della procedura di stima dell'importanza della classe con una procedura a due fasi.
Gli autori considerano il problema dell'apprendimento sotto spostamenti di etichette, dove le proporzioni delle etichette differiscono mentre le condizioni sono uguali, e propongono uno stimatore migliorato con regolarizzazione.
Approccio per migliorare la precisione di classificazione sulle classi in coda.
L'obiettivo principale di questo articolo è quello di imparare un classificatore ConvNet che si comporta meglio per le classi nella coda della distribuzione delle occorrenze delle classi.
Proposta di un quadro bayesiano con un modello di miscela gaussiana per affrontare un problema nelle applicazioni di classificazione, che il numero di dati di formazione da classi diverse è sbilanciato.
Presentiamo un metodo per sintetizzare gli stati di interesse per gli agenti di apprendimento del rinforzo al fine di analizzare il loro comportamento. 
Questo articolo propone un modello generativo di osservazioni visive in RL che è in grado di generare osservazioni di interesse.
Un approccio per la visualizzazione degli stati di interesse che coinvolge un autoencoder variazionale che impara a ricostruire lo spazio degli stati e una fase di ottimizzazione che trova i parametri di condizionamento per generare immagini sintetiche.
Una rete neurale profonda di astensione addestrata con una nuova funzione di perdita che impara rappresentazioni per quando astenersi permettendo un apprendimento robusto in presenza di diversi tipi di rumore.
Una nuova funzione di perdita per l'addestramento di una rete neurale profonda che può astenersi, con prestazioni esaminate da angoli in esistenza di rumore strutturato, in esistenza di rumore non strutturato, e rilevamento del mondo aperto.
Questo manoscritto introduce classificatori di astensione profonda che modificano la perdita di cross-entropia multiclasse con una perdita di astensione, che viene poi applicata a compiti di classificazione di immagini perturbate
Una tecnica di regolarizzazione per l'apprendimento TD che evita la sovrageneralizzazione temporale, specialmente nelle reti profonde
Una variazione sull'apprendimento della differenza temporale per il caso dell'approssimazione della funzione che tenta di risolvere il problema della sovra-generalizzazione attraverso stati temporalmente successivi.
L'articolo introduce HR-TD, una variante dell'algoritmo TD(0), intesa a migliorare il problema della sovrageneralizzazione nel TD convenzionale
Presentiamo un nuovo kernel CNN per griglie non strutturate per segnali sferici, e mostriamo una precisione significativa e un guadagno di efficienza dei parametri su compiti come la classificazione 3D e la segmentazione omnidirezionale delle immagini.
Un metodo efficiente che permette l'apprendimento profondo su dati sferici che raggiunge numeri competitivi/stato dell'arte con molti meno parametri rispetto agli approcci popolari.
L'articolo propone un nuovo kernel di convoluzione per CNN sulle griglie non strutturate e formula la convoluzione con una combinazione lineare di operatori differenziali.
Nei compiti di predizione visiva, lasciare che il modello predittivo scelga quali tempi predire fa due cose: (i) migliora la qualità della predizione e (ii) porta a predizioni semanticamente coerenti dello "stato di collo di bottiglia", che sono utili per la pianificazione.
Un metodo sulla predizione dei fotogrammi in un video, l'approccio compreso che la predizione dell'obiettivo è galleggiante, risolto da un minimo sull'errore di predizione.
Riformula il compito di predizione/interpolazione video in modo che un predittore non sia costretto a generare fotogrammi a intervalli di tempo fissi, ma sia invece addestrato a generare fotogrammi che accadono in qualsiasi punto del futuro.
Abbiamo usato un LSTM per rilevare quando uno smartphone entra in un edificio. Poi abbiamo previsto il livello del pavimento del dispositivo utilizzando i dati dei sensori a bordo dello smartphone.
L'articolo introduce un sistema per stimare il livello di un piano attraverso i dati del sensore del loro dispositivo mobile utilizzando un LSTM e i cambiamenti nella pressione barometrica
Proposta di un metodo in due fasi per determinare su quale piano si trova un telefono cellulare all'interno di un edificio alto.
Combinare la rappresentazione degli obiettivi linguistici con i replay dell'esperienza a posteriori.
Questo articolo considera l'assunzione implicita nella riproduzione dell'esperienza a posteriori, che c'è accesso a una mappatura dagli stati agli obiettivi, e propone una rappresentazione degli obiettivi in linguaggio naturale.
Questa presentazione utilizza il framework Hindsight Experience Replay con obiettivi in linguaggio naturale per migliorare l'efficienza del campione dei modelli di istruzione-seguimento.
Proponiamo un codebook congiunto e uno schema di fattorizzazione per migliorare il pooling di secondo ordine.
Questo articolo presenta un modo per combinare le rappresentazioni di secondo ordine fattorizzate esistenti con un'assegnazione rigida in stile codebook.
Proposta di una nuova rappresentazione bilineare basata su un modello codebook, e una formulazione efficiente in cui le proiezioni basate su codebook sono fattorizzate tramite proiezione condivisa per ridurre ulteriormente la dimensione dei parametri.
Proponiamo e applichiamo una metodologia di meta-apprendimento basata sulla Supervisione Debole, per combinare l'Apprendimento Semi-Supervisionato ed Ensemble sul compito di Estrazione di Relazioni Biomediche.
Un metodo semi-supervisionato per la classificazione delle relazioni, che addestra più apprendisti di base usando un piccolo set di dati etichettati e applica alcuni di loro per annotare esempi non etichettati per l'apprendimento semi-supervisionato.
Questo articolo affronta il problema della generazione di dati di addestramento per l'estrazione di relazioni biologiche, e utilizza le previsioni dei dati etichettati da classificatori deboli come dati di addestramento aggiuntivi per un algoritmo di meta apprendimento.
Questo articolo propone una combinazione di apprendimento semi-supervisionato e apprendimento d'insieme per l'estrazione di informazioni, con esperimenti condotti su un compito di estrazione di relazioni biomediche
Una classe di reti che generano modelli semplici al volo (chiamati spiegazioni) che agiscono come un regolarizzatore e permettono una diagnostica del modello coerente e l'interpretabilità.
Gli autori sostengono che l'arte precedente integra direttamente le reti neurali nei modelli grafici come componenti, il che rende i modelli non interpretabili.
Proposta di una combinazione di reti neurali e modelli grafici utilizzando una rete neurale profonda per prevedere i parametri di un modello grafico.
Proponiamo un algoritmo di apprendimento dell'imitazione senza modello che è in grado di ridurre il numero di interazioni con l'ambiente rispetto all'algoritmo di apprendimento dell'imitazione allo stato dell'arte, cioè GAIL.
Propone di estendere l'algoritmo determinista del gradiente della politica per imparare dalle dimostrazioni, mentre è combinato con un tipo di stima della densità dell'esperto.
Questo articolo considera il problema dell'apprendimento dell'imitazione senza modello e propone un'estensione dell'algoritmo generativo di apprendimento dell'imitazione avversaria, sostituendo la politica stocastica dell'allievo con una deterministica.
L'articolo combina IRL, addestramento avversario e idee dai gradienti di politica deterministica con l'obiettivo di diminuire la complessità del campione
CNN a grafo a bassa complessità computazionale (senza approssimazione) con una migliore accuratezza di classificazione
Propone un nuovo approccio CNN alla classificazione dei grafi usando un filtro basato su passeggiate in uscita di lunghezza crescente per incorporare informazioni da vertici più distanti in un passo di propagazione.
Proposta di una nuova architettura di rete neurale per la classificazione semi-supervisionata dei grafi, basandosi sui filtri polinomiali dei grafi e utilizzandoli su strati successivi della rete neurale con funzioni di attivazione ReLU.
L'articolo introduce Topology Adaptive GCN per generalizzare le reti di convoluzione ai dati strutturati a grafo
Mostriamo che la dimenticanza catastrofica si verifica all'interno di quello che è considerato un singolo compito e troviamo che gli esempi che non sono inclini alla dimenticanza possono essere rimossi dal set di allenamento senza perdita di generalizzazione.
Studia il comportamento di dimenticanza degli esempi di formazione durante SGD, e mostra che esistono "esempi di supporto" nella formazione delle reti neurali attraverso diverse architetture di rete.
Questo articolo analizza la misura in cui le reti imparano a classificare correttamente esempi specifici e poi dimenticano questi esempi nel corso della formazione.
L'articolo studia se alcuni esempi nell'addestramento delle reti neurali sono più difficili da imparare di altri. Tali esempi sono dimenticati e ri-appresi più volte attraverso l'apprendimento.
Un approccio non supervisionato per l'apprendimento di rappresentazioni dissociate di oggetti interamente da video monoculari senza etichetta.
Progetta una rappresentazione delle caratteristiche da sequenze video catturate da una scena da diversi punti di vista.
Proposta di un metodo di apprendimento di rappresentazione non supervisionato per gli input visivi che incorpora un approccio di apprendimento metrico che tira le coppie di patch di immagini più vicine vicine nello spazio di incorporazione mentre spinge a parte altre coppie.
Questo articolo esplora l'apprendimento auto-supervisionato di rappresentazioni di oggetti, con l'idea principale di incoraggiare gli oggetti con caratteristiche simili ad essere ulteriormente "attratti" l'uno dall'altro.
Alleniamo con il vettore allineato allo stato premia un agente che predice i cambiamenti di stato dalle distribuzioni delle azioni, usando una nuova tecnica di apprendimento di rinforzo ispirata alla regressione quantile.
Presenta un algoritmo che mira ad accelerare l'apprendimento per rinforzo in situazioni in cui la ricompensa è allineata con lo spazio di stato. 
Questo articolo affronta la RL nello spazio d'azione continuo, utilizzando una politica ri-parametrata e un nuovo obiettivo di formazione basato su vettori.
Questo lavoro propone di mescolare la RL distributiva con una rete incaricata di modellare l'evoluzione del mondo in termini di quantili, sostenendo miglioramenti nell'efficienza del campione.
Proponiamo Episodic Backward Update, un nuovo algoritmo di apprendimento di rinforzo profondo che campiona le transizioni episodio per episodio e aggiorna i valori in modo ricorsivo all'indietro per ottenere un apprendimento veloce e stabile.
Propone una nuova DQN in cui gli obiettivi sono calcolati su un episodio completo con un aggiornamento all'indietro (dalla fine all'inizio) per una propagazione più veloce delle ricompense dalla fine dell'episodio.
Gli autori propongono di modificare l'algoritmo DQN applicando l'operatore max di Bellman ricorsivamente su una traiettoria con un certo decadimento per evitare di accumulare errori con il max annidato.
Nelle reti deep-Q, aggiornare i valori Q a partire dalla fine dell'episodio per facilitare la propagazione rapida delle ricompense lungo l'episodio.
In questo lavoro introduciamo una nuova architettura di rete neurale profonda siamese che è in grado di imparare efficacemente dai dati in presenza di più eventi avversi.
Questo articolo introduce le reti neurali siamesi nel quadro dei rischi concorrenti ottimizzando direttamente l'indice c
Gli autori affrontano i problemi di stima del rischio in un contesto di analisi della sopravvivenza con rischi concorrenti e propongono di ottimizzare direttamente l'indice di discriminazione dipendente dal tempo utilizzando una rete di sopravvivenza siamese
Presentiamo un modello di rete di puntatori basato sul tipo insieme a un metodo di perdita basato sul valore per addestrare efficacemente un modello neurale per tradurre il linguaggio naturale in SQL.
L'articolo sostiene di sviluppare un nuovo metodo per mappare le query del linguaggio naturale in SQL utilizzando una grammatica per guidare la decodifica e utilizzando una nuova funzione di perdita per il meccanismo di puntatore/copia
Uno stimatore di gradiente imparziale e a bassa varianza per modelli di variabili latenti discreti
Propone una nuova tecnica di riduzione della varianza da usare quando si calcola un gradiente di perdita attesa dove l'aspettativa è rispetto a variabili casuali binarie indipendenti.
Un algoritmo che combina Rao-Blackwellization e numeri casuali comuni per abbassare la varianza dello stimatore del gradiente della funzione di punteggio nel caso speciale delle reti binarie stocastiche
Uno stimatore imparziale e a bassa varianza augment-REINFORCE-merge (ARM) per il calcolo e il backpropagating dei gradienti nelle reti neurali binarie
Dimostriamo che SGD locale parallelo raggiunge una velocità lineare con una comunicazione molto minore rispetto a SGD parallelo in mini-batch.
Fornisce una prova di convergenza per SGD locale, e dimostra che SGD locale può fornire gli stessi guadagni di accelerazione del minibatch, ma può essere in grado di comunicare molto meno.
Questo articolo presenta un'analisi di SGD locale e dei limiti sulla frequenza con cui gli stimatori ottenuti dall'esecuzione di SGD devono essere mediati per ottenere un aumento lineare della velocità di parallelizzazione.
Gli autori analizzano l'algoritmo SGD locale, dove vengono eseguite $K$ catene parallele di SGD, e gli iterati sono occasionalmente sincronizzati tra le macchine facendo la media
Percezione compatta del processo dinamico
Studia il problema di rappresentare in modo compatto il modello di un sistema dinamico complesso preservando l'informazione usando un metodo di collo di bottiglia dell'informazione.
Questo articolo studia la dinamica lineare gaussiana e propone un algoritmo per calcolare l'Information Bottleneck Hierarchy (IBH).
RNN densa che ha connessioni complete da ogni stato nascosto a più stati nascosti precedenti di tutti gli strati direttamente.
Propone una nuova architettura RNN che modella meglio le dipendenze a lungo termine, può imparare la rappresentazione multiscala dei dati sequenziali e aggirare il problema dei gradienti usando unità di gating parametrizzate.
Questo articolo propone un'architettura RNN densa completamente connessa con connessioni gated per ogni strato e connessioni dello strato precedente, e i suoi risultati sul compito di modellazione a livello di caratteri PTB.
Le SD-GAN distinguono i codici latenti in base alle comunanze note in un set di dati (ad esempio, fotografie che ritraggono la stessa persona).
Questo articolo indaga il problema della generazione controllata di immagini e propone un algoritmo che produce una coppia di immagini con la stessa identità.
Questo articolo propone, SD-GAN, un metodo di addestramento delle GAN per distinguere le informazioni di identità e non identità nel vettore latente di input Z.
Proponiamo un quadro di apprendimento per le traduzioni tra domini incrociati che è esattamente ciclo-consistente e può essere appreso tramite addestramento avversario, stima di massima verosimiglianza, o un ibrido.
Propone AlignFlow, un modo efficiente di implementare il principio di coerenza del ciclo usando flussi invertibili.
Modelli di flusso per la traduzione da immagine a immagine non accoppiata
In un contesto di sintesi di programma in cui l'input è un insieme di esempi, riduciamo il costo calcolando un sottoinsieme di esempi rappresentativi
Propone un metodo per identificare esempi rappresentativi per la sintesi del programma per aumentare la scalabilità delle soluzioni di programmazione vincolata esistenti.
Un metodo per scegliere un sottoinsieme di esempi su cui eseguire un solutore di vincoli per risolvere problemi di sintesi di programmi.
Questo articolo propone un metodo per accelerare i sintetizzatori di programmi general-purpose.
Introduciamo Recurrent Relational Networks, un modulo di rete neurale potente e generale per il ragionamento relazionale, e lo usiamo per risolvere il 96,6% dei compiti Sudokus e 19/20 BaBi più difficili.
Introdotta la rete relazionale ricorrente (RRNs) che può essere aggiunta a qualsiasi rete neurale per aggiungere capacità di ragionamento relazionale.
Introduzione di una rete neurale profonda per la predizione strutturata che raggiunge prestazioni all'avanguardia sui puzzle Soduku e sul compito BaBi.
Questo articolo descrive un metodo chiamato rete relazionale per aggiungere capacità di ragionamento relazionale alle reti neurali profonde.
Tre priori di classe sono tutti necessari per addestrare modelli profondi da soli dati U, mentre due qualsiasi non dovrebbero essere sufficienti.
Propone uno stimatore imparziale che permette di addestrare modelli con supervisione debole su due set di dati non etichettati con priori di classe noti e discute le proprietà teoriche degli stimatori.
Una metodologia per l'addestramento di qualsiasi classificatore binario da soli dati non etichettati, e un metodo di minimizzazione del rischio empirico per due insiemi di dati non etichettati in cui sono dati i priori di classe.
L'aggregazione di prove di classe da molte piccole patch di immagini è sufficiente per risolvere ImageNet, produce modelli più interpretabili e può spiegare aspetti del processo decisionale delle DNN popolari.
Questo articolo suggerisce un'architettura di rete neurale nuova e compatta che utilizza le informazioni all'interno delle caratteristiche di bag-of-words. L'algoritmo proposto utilizza solo le informazioni delle patch in modo indipendente ed esegue il voto di maggioranza utilizzando patch classificate in modo indipendente.
 Gli attuali metodi di mutazione somatica non funzionano con le biopsie liquide (cioè il sequenziamento a bassa copertura), applichiamo un'architettura CNN a una rappresentazione unica di una lettura e il suo ailgnment, mostriamo un miglioramento significativo rispetto ai metodi precedenti nell'impostazione a bassa frequenza.
Propone una soluzione basata su CNN chiamata Kittyhawk per la chiamata di mutazioni somatiche a frequenze alleliche ultra basse.
Un nuovo algoritmo per rilevare le mutazioni del cancro dal sequenziamento del DNA libero delle cellule che identificherà il contesto di sequenza che caratterizza gli errori di sequenziamento dalle vere mutazioni.
Questo articolo propone un framework di deep learning per prevedere le mutazioni somatiche a frequenze estremamente basse che si verificano nel rilevamento del tumore dal DNA privo di cellule
L'articolo introduce un nuovo corpus gold-standard di letteratura scientifica biomedica annotato manualmente con le menzioni dei concetti UMLS.
Dettaglia la costruzione di un dataset annotato manualmente che copre concetti biomedici che è più grande e coperto da un'ontologia più grande dei dataset precedenti.
Questo articolo utilizza MedMentions, un modello TaggerOne semi-Markov per il riconoscimento e il collegamento dei concetti end-to-end su un insieme di abstracts di Pubmed per etichettare i documenti con concetti/entità biomediche
Proponiamo un metodo di deep clustering dove invece di un centroide ogni cluster è rappresentato da un autoencoder
Presenta un clustering profondo basato su una miscela di autoencoder, dove i punti di dati sono assegnati a un cluster basato sull'errore di rappresentazione se la rete di autoencoder fosse usata per rappresentarlo.
Un approccio di clustering profondo che utilizza un quadro autoencoder per imparare un'incorporazione bidimensionale dei dati simultaneamente mentre si raggruppano i dati utilizzando una rete neurale profonda.
Un metodo di deep clustering che rappresenta ogni cluster con diversi autocodificatori, funziona in modo end-to-end, e può anche essere usato per clusterizzare nuovi dati in arrivo senza rifare l'intera procedura di clustering.
Definiamo una nuova metrica di probabilità integrale (Sobolev IPM) e mostriamo come può essere utilizzata per l'addestramento di GAN per la generazione di testi e l'apprendimento semi-supervisionale.
Suggerisce un nuovo schema di regolarizzazione per le GAN basato su una norma Sobolev, che misura le deviazioni tra le norme L2 delle derivate.
Gli autori forniscono un altro tipo di GAN usando l'impostazione tipica di una GAN ma con una classe di funzione diversa, e producono una ricetta per addestrare le GAN con quel tipo di classe di funzione.
L'articolo propone una diversa penalità del gradiente per i critici GAN che costringe la norma quadratica attesa del gradiente ad essere uguale a 1
Proponiamo un nuovo approccio per addestrare le GAN con una miscela di generatori per superare il problema del collasso delle modalità.
Affrontare il problema del collasso della modalità nelle GAN utilizzando una distribuzione di miscela vincolata per il generatore e un classificatore ausiliario che predice la componente di miscela di origine.
L'articolo propone una miscela di generatori per addestrare le GAN senza costi computazionali aggiuntivi
Gli autori presentano che l'uso di MGAN, che mira a superare il problema del collasso dei modelli mediante generatori di miscele, raggiunge risultati all'avanguardia
Presentiamo la piattaforma BabyAI per studiare l'efficienza dei dati dell'apprendimento del linguaggio con un umano nel ciclo
Presenta una piattaforma di ricerca con un bot nel loop per imparare a eseguire istruzioni linguistiche in cui il linguaggio ha strutture compositive
Introduce una piattaforma per l'apprendimento delle lingue a terra che sostituisce qualsiasi umano nel ciclo con un insegnante euristico e utilizza una lingua sintetica mappata in un mondo a griglia 2D
Un priore k-means combinato con la regolarizzazione L1 produce risultati di compressione all'avanguardia.
Questo articolo esplora la legatura dei parametri soft e la compressione di DNNs/CNNNs
Il metodo SVRG fallisce sui moderni problemi di apprendimento profondo
Questo articolo presenta un'analisi dei metodi in stile SVRG, mostrando che dropout, batch norm, aumento dei dati (raccolto/rotazione/traslazioni casuali) tendono ad aumentare la distorsione e/o la varianza degli aggiornamenti.
Questo articolo studia l'applicabilità di SVGD alle moderne reti neurali e mostra che l'applicazione ingenua di SVGD tipicamente fallisce.
Un metodo per applicare l'apprendimento profondo alle superfici 3D usando i loro descrittori sferici e la convoluzione anisotropa alt-az su 2-sfere.
Presenta uno schema di convoluzione polare anisotropa su una sfera unitaria sostituendo la traslazione del filtro con la rotazione del filtro.
Questo articolo esplora l'apprendimento profondo delle forme 3D utilizzando la convoluzione anisotropa a 2 sfere di alt-az
Addestramento di reti binarie/ternarie usando la riparametrizzazione locale con l'approssimazione CLT
Addestra le reti di distribuzione dei pesi binarie e ternarie usando la backpropagation per campionare le preattivazioni dei neuroni con il trucco della riparametrizzazione
Questo articolo suggerisce l'uso di parametri stocastici in combinazione con il trucco della riparametrizzazione locale per addestrare reti neurali con pesi binari o ternari, che porta a risultati allo stato dell'arte.
Optimal Completion Distillation (OCD) è una procedura di addestramento per ottimizzare i modelli sequenza per sequenza basati sulla distanza di modifica che raggiunge lo stato dell'arte nei compiti di riconoscimento vocale end-to-end.
Approccio alternativo alla formazione di modelli seq2seq utilizzando un programma dinamico per calcolare le prosecuzioni ottimali dei prefissi previsti
Un algoritmo di addestramento per modelli auto-regressivi che non richiede alcun pre-addestramento MLE e può ottimizzare direttamente dal campionamento.
L'articolo considera una lacuna dei modelli sequenza per sequenza addestrati usando la stima di massima verosimiglianza e propone un approccio basato sulle distanze di modifica e l'uso implicito di sequenze di etichette date durante l'addestramento.
un modello congiunto e un metodo di sparsificazione del gradiente per l'apprendimento federato
Applica il dropout variazionale per ridurre il costo di comunicazione dell'addestramento distribuito delle reti neurali, e fa esperimenti sui dataset mnist, cifar10 e svhn. 
Gli autori propongono un algoritmo che riduce i costi di comunicazione nell'apprendimento federato inviando gradienti sparsi dal dispositivo al server e viceversa.
Combina l'algoritmo di ottimizzazione distribuita con il dropout variazionale per sparsificare i gradienti inviati al server principale dagli studenti locali.
Dimostriamo una teoria di boosting multiclasse per le architetture ResNet che crea contemporaneamente una nuova tecnica per il boosting multiclasse e fornisce un nuovo algoritmo per le architetture in stile ResNet.
Presenta un algoritmo in stile boosting per l'addestramento di reti residuali profonde, un'analisi di convergenza per l'errore di addestramento e un'analisi della capacità di generalizzazione.
Un metodo di apprendimento per ResNet utilizzando il quadro di boosting che decompone l'apprendimento di reti complesse e utilizza meno costi computazionali.
Gli autori propongono la ResNet profonda come algoritmo di boosting, e sostengono che questo è più efficiente della backpropagation standard end-to-end.
L'articolo analizza il panorama di ottimizzazione delle reti neurali a uno strato nascosto e progetta un nuovo obiettivo che non ha un minimo locale spurio. 
Questo articolo studia il problema dell'apprendimento delle reti neurali a uno strato nascosto, stabilisce una connessione tra la perdita della popolazione dei minimi quadrati e i polinomi di Hermite, e propone una nuova funzione di perdita.
Un metodo di tipo di fattorizzazione tensoriale per il leaning di una rete neurale a uno strato nascosto
Un corpus aperto di estrazione di informazioni e la sua analisi approfondita
Costruisce un nuovo corpus per l'estrazione di informazioni che è più grande dei corpora pubblici precedenti e contiene informazioni non esistenti nei corpora attuali.
Presenta un dataset di triple open-IE che sono state raccolte da Wikipedia con l'aiuto di un sistema di estrazione recente. 
L'articolo descrive la creazione di un corpus Open IE su Wikipedia inglese attraverso un modo automatico
Definiamo un DSL flessibile per la generazione di architetture RNN che permette RNN di varie dimensioni e complessità e proponiamo una funzione di classificazione che rappresenta RNN come reti neurali ricorsive, simulando le loro prestazioni per decidere le architetture più promettenti.
Introduce un nuovo metodo per generare architetture RNNs utilizzando un linguaggio specifico del dominio per due tipi di generatori (casuale e basato su RL) insieme a una funzione di classificazione e un valutatore.
Questo articolo presenta la ricerca di buone architetture RNN Cell come un problema di ottimizzazione black-box in cui gli esempi sono rappresentati come un albero di operatori e valutati sulla base di funzioni apprese o generate da un agente RL.
Questo articolo indaga la strategia di meta-apprendimento per la ricerca automatica dell'architettura nel contesto di RNN utilizzando un DSL che specifica le operazioni ricorrenti RNN.
Applichiamo l'addestramento e l'inferenza solo con numeri interi a bassa larghezza di bit nelle DNN
Un metodo chiamato WAGE che quantizza tutti gli operandi e gli operatori in una rete neurale per ridurre il numero di bit di rappresentazione in una rete.
Gli autori propongono pesi, attivazioni, gradienti ed errori discretizzati sia in fase di allenamento che di test sulle reti neurali
In questo articolo, sviluppiamo metodi di sparsificazione veloci e privi di retraining che possono essere impiegati per la sparsificazione on-the-fly delle CNN in molti contesti industriali.
Questo articolo propone approcci per il pruning delle CNN senza riqualificazione, introducendo tre schemi per determinare le soglie dei pesi di pruning.
Questo articolo descrive un metodo per la sparsificazione delle CNN senza riqualificazione.
Proponiamo che l'addestramento con set crescenti fase per fase fornisce un'ottimizzazione per le reti neurali.
Gli autori paragonano l'apprendimento del curriculum all'apprendimento in un ordine casuale con fasi che aggiungono un nuovo campione di esempi all'insieme precedentemente costruito in modo casuale
Questo articolo studia l'influenza dell'ordinamento nel Curriculum e nell'apprendimento autodidattico, e mostra che in una certa misura l'ordinamento delle istanze di formazione non è importante.
Un metodo di traduzione da immagine a immagine che aggiunge a un'immagine il contenuto di un'altra creando così una nuova immagine.
Questo articolo affronta il compito del trasferimento di contenuti, con la novaltà di essere in perdita.
Un set di dati per testare il ragionamento matematico (e la generalizzazione algebrica), e i risultati sugli attuali modelli sequenza-sequenza.
Presenta un nuovo set di dati sintetico per valutare la capacità di ragionamento matematico dei modelli sequenza-sequenza, e lo usa per valutare diversi modelli.
Modello per risolvere problemi matematici di base.
Questo articolo introduce parametrizzazioni efficienti ed economiche di reti neurali convoluzionali motivate da equazioni differenziali parziali 
Introduce quattro alternative "a basso costo" all'operazione di convoluzione standard che possono essere usate al posto dell'operazione di convoluzione standard per ridurre la loro complessità computazionale.
Questo articolo introduce metodi per ridurre il costo computazionale delle implementazioni CNN, e introduce nuove parametrizzazioni di CNN come architetture che limitano l'accoppiamento dei parametri.
L'articolo propone una prospettiva basata su PDE per comprendere e parametrizzare le CNN
Utilizzare la teoria della distorsione del tasso per limitare quanto un modello di variabile latente può essere migliorato
Affronta i problemi di ottimizzazione del priore nel modello di variabile latente e la selezione della funzione di verosimiglianza proponendo criteri basati su un limite inferiore della log-likelihood negativa.
Presenta un teorema che dà un limite inferiore alla likelihood negativa del tasso di distorsione per la modellazione delle variabili latenti
Gli autori sostengono che la teoria della distorsione del tasso per la compressione lossy fornisce un toolkit naturale per studiare i modelli di variabili latenti propone un limite inferiore.
Ignoriamo le non linearità e non calcoliamo i gradienti nel passaggio a ritroso per risparmiare calcoli e per assicurare che i gradienti scorrano sempre. 
L'autore ha proposto algoritmi di backprop lineare per garantire il flusso dei gradienti per tutte le parti durante la backpropagation.
Capire l'autocodificatore discreto VQ-VAE sistematicamente usando EM e usarlo per progettare un modello di traduzione non autogressivo che corrisponde a una linea di base autoregressiva forte.
Questo articolo introduce un nuovo modo di interpretare il VQ-VAE e propone un nuovo algoritmo di formazione basato sul soft EM clustering.
L'articolo presenta una visione alternativa sulla procedura di addestramento per il VQ-VAE utilizzando l'algoritmo soft EM
Questo articolo presenta una rete neurale profonda che incorpora una funzione di perdita per quanto riguarda la distribuzione ottimale del margine, che allevia il problema dell'overfitting teoricamente ed empiricamente.
Presenta un limite PAC-bayesiano per una perdita di margine
Cerchiamo di capire le rappresentazioni apprese nelle reti compresse attraverso un regime sperimentale che chiamiamo deep net triage
Confronta vari metodi di inizializzazione e di addestramento per trasferire la conoscenza dalla rete VGG a una rete di studenti più piccola, sostituendo blocchi di strati con strati singoli.
Questo articolo presenta cinque metodi per fare il triage o la compressione dei livelli di blocco per le reti profonde.
L'articolo propone un metodo per comprimere un blocco di strati in una NN che valuta diversi sotto-approcci
La prova empirica di un nuovo fenomeno richiede nuove intuizioni teoriche ed è rilevante per le discussioni attive nella letteratura su SGD e la comprensione della generalizzazione.
L'articolo discute un fenomeno per cui l'addestramento delle reti neurali in impostazioni molto specifiche può trarre molto profitto da un programma che include grandi tassi di apprendimento
Gli autori analizzano l'addestramento delle reti residue usando grandi tassi di apprendimento ciclici, e dimostrano una convergenza veloce con i tassi di apprendimento ciclici e la prova dei grandi tassi di apprendimento che agiscono come regolarizzazione.
Proponiamo un metodo per la costruzione di reti di larghezza infinita arbitrariamente profonde, sulla base del quale deriviamo un nuovo schema di inizializzazione dei pesi per le reti di larghezza finita e dimostriamo le sue prestazioni competitive.
Propone un approccio di inizializzazione dei pesi per consentire reti infinitamente profonde e di larghezza infinita con risultati sperimentali su piccoli set di dati.
Propone reti neurali profonde di larghezza infinita.
Abbiamo derivato regole di apprendimento della plasticità sinaptica biologicamente plausibili per una rete neurale ricorrente per memorizzare le rappresentazioni degli stimoli. 
Un modello di rete neurale composto da neuroni connessi in modo ricorrente e uno o più redout che mira a mantenere un certo output nel tempo.
Questo articolo presenta un meccanismo di memoria auto-organizzante in un modello neurale, e introduce una funzione obiettivo che minimizza i cambiamenti nel segnale da memorizzare.
Per comprendere la formazione GAN, definiamo una semplice dinamica GAN e mostriamo le differenze quantitative tra gli aggiornamenti ottimali e quelli del primo ordine in questo modello.
Gli autori studiano l'impatto delle GAN in impostazioni in cui ad ogni iterazione, il discriminatore si allena fino alla convergenza e il generatore si aggiorna con passi di gradiente, o dove pochi passi di gradiente sono fatti per il disciminatore e il generatore.
Questo articolo studia la dinamica dell'addestramento avversario delle GAN su un modello a miscela gaussiana
Proponiamo un metodo basato sul gradiente per trasferire la conoscenza da più fonti attraverso diversi domini e compiti.
Questo articolo propone di combinare i gradienti dei domini di origine per aiutare l'apprendimento nel dominio di destinazione. 
La prima formulazione Bayes variazionale dell'inferenza filogenetica, un impegnativo problema di inferenza su strutture con componenti discrete e continue intrecciate
Esplora una soluzione di inferenza approssimativa al problema dell'inferenza bayesiana degli alberi filogenetici sfruttando le reti bayesiane subsplit recentemente proposte e i moderni stimatori di gradiente per VI.
Propone un approccio variazionale all'inferenza posteriore bayesiana negli alberi filogenetici.
È un'architettura neurale ibrida per accelerare il modello autoregressivo. 
Conclude che per scalare la dimensione del modello senza aumentare il tempo di inferenza per la predizione sequenziale, usa un modello che predice più tempi contemporaneamente.
Questo articolo presenta HybridNet, un sistema di sintesi neurale del parlato e di altri audio che combina il modello WaveNet con un LSTM con l'obiettivo di offrire un modello con una generazione audio a tempo di inferenza più veloce.
Interpretazione identificando le caratteristiche apprese dal modello che servono come indicatori per il compito di interesse. Spiegare le decisioni del modello evidenziando la risposta di queste caratteristiche nei dati di test. Valutare obiettivamente le spiegazioni con un set di dati controllato.
Questo articolo propone un metodo per produrre spiegazioni visive per gli output delle reti neurali profonde e rilascia un nuovo set di dati sintetico.
Un metodo per Reti Neurali Profonde che identifica automaticamente le caratteristiche rilevanti dell'insieme delle classi, supportando l'interpretazione e la spiegazione senza fare affidamento su annotazioni aggiuntive.
Una struttura per l'apprendimento efficiente di rappresentazioni di frasi di alta qualità.
Propone un algoritmo più veloce per l'apprendimento di rappresentazioni di frasi in stile SkipThought da corpora di frasi ordinate che sostituisce il decodificatore a livello di parola con una perdita di classificazione contrastiva.
Questo articolo propone una struttura per l'apprendimento non supervisionato di rappresentazioni di frasi massimizzando un modello della probabilità di frasi di contesto vere rispetto a frasi candidate casuali
Ricaviamo una penalità di norma sull'uscita della rete neurale dal punto di vista del collo di bottiglia dell'informazione
Propone Activation Norm Penalty, una regolarizzazione di tipo L_2 sulle attivazioni, derivandola dal principio Information Bottleneck
Questo articolo crea una mappatura tra le penalità della norma di attivazione e il quadro dei colli di bottiglia dell'informazione usando il quadro di dropout variazionale.
Un metodo completamente non supervisionato, per integrare naturalmente la riduzione della dimensionalità e il clustering temporale in un unico quadro di apprendimento end-to-end.
Propone un algoritmo che integra l'autoencoder con il clustering di dati di serie temporali usando una struttura di rete che si adatta ai dati di serie temporali.
Un algoritmo per eseguire congiuntamente la riduzione della dimensionalità e il clustering temporale in un contesto di apprendimento profondo, utilizzando un autoencoder e un obiettivo di clustering.
Gli autori hanno proposto un metodo di clustering non supervisionato di serie temporali costruito con reti neurali profonde e dotato di un codificatore-decodificatore e una modalità di clustering per accorciare le serie temporali, estrarre le caratteristiche temporali locali e ottenere le rappresentazioni codificate.
Una rete neurale con memoria che affronta il problema dei pochi colpi di molte classi sfruttando la gerarchia delle classi sia nell'apprendimento supervisionato che nel meta-apprendimento.
Questo articolo presenta metodi per aggiungere un bias induttivo a un classificatore attraverso la predizione grossolana a fine lungo una gerarchia di classi e l'apprendimento di un classificatore KNN basato sulla memoria che tiene traccia delle istanze etichettate male durante l'apprendimento.
Questo articolo formula il problema di classificazione di molte classi e pochi colpi da una prospettiva di apprendimento supervisionato e una prospettiva di meta-apprendimento.
Un nuovo componente di perdita che costringe la rete ad apprendere una rappresentazione adatta al clustering durante l'allenamento per un compito di classificazione.
Questo articolo propone due termini di regolarizzazione basati su una perdita composta a cerniera sulla divergenza KL tra due argomenti di input normalizzati con softmax per incoraggiare l'apprendimento di rappresentazioni disgiunte
Proposta di due regolatori destinati a rendere le rappresentazioni apprese nel penultimo strato di un classificatore più conformi alla struttura intrinseca dei dati.
Mostriamo come ottenere buone rappresentazioni dal punto di vista della Ricerca di Simiarità.
Studia l'impatto del cambiamento della parte di classificazione delle immagini sopra la DNN sulla capacità di indicizzare i descrittori con un LSH o un algoritmo kd-tree.
Propone di utilizzare la perdita softmax cross-entropia per imparare una rete che cerca di ridurre gli angoli tra gli ingressi e i vettori di classe corrispondenti in un quadro supervisionato utilizzando.
Introduciamo una tecnica che permette l'addestramento basato sul gradiente delle reti neurali quantizzate.
Propone un modo unificato e generale di addestrare reti neurali con pesi e attivazioni sinaptiche quantizzate a precisione ridotta.
Un nuovo approccio alla quantizzazione delle attivazioni che è allo stato dell'arte o competitivo su diversi problemi di immagini reali.
Un metodo per l'apprendimento di reti neurali con pesi e attivazioni quantizzate quantizzando stocasticamente i valori e sostituendo la distribuzione categoriale risultante con un rilassamento continuo
Mostriamo che il problema del collasso della modalità nelle GAN può essere spiegato da una mancanza di condivisione delle informazioni tra le osservazioni in un batch di formazione, e proponiamo un quadro basato sulla distribuzione per la condivisione globale delle informazioni tra i gradienti che porta a una formazione avversaria più stabile ed efficace.
Propone di sostituire i discriminatori a campione singolo nell'addestramento avversario con discriminatori che operano esplicitamente su distribuzioni di esempi.
Teoria sui test a due campioni e MMD e come possono essere beneficamente incorporati nel quadro GAN.
Abbiamo progettato un quadro end-to-end utilizzando il modello sequence to sequence per fare la standardizzazione dei nomi chimici.
Standardizza i nomi non sistematici nell'estrazione di informazioni chimiche creando un corpus parallelo di nomi non sistematici e sistematici e costruendo un modello seq2seq.
Questo lavoro presenta un metodo per tradurre nomi non sistematici di composti chimici nei loro equivalenti sistematici utilizzando una combinazione di meccanismi
SGD viene indirizzato all'inizio dell'addestramento verso una regione in cui il suo passo è troppo grande rispetto alla curvatura, il che ha un impatto sul resto dell'addestramento. 
Analizza la relazione tra la convergenza/generalizzazione e l'aggiornamento sui più grandi autovettori di Hessiano delle perdite empiriche delle DNN.
Questo lavoro studia la relazione tra la dimensione del passo SGD e la curvatura della superficie di perdita
Introduciamo un nuovo algoritmo di apprendimento per rinforzo, che predice azioni multiple e campioni da esse.
Questo lavoro introduce una miscela uniforme di politiche deterministiche, e trova che questa parametrizzazione delle politiche stocastiche supera DDPG su diversi benchmark della palestra OpenAI.
Gli autori studiano un metodo per migliorare le prestazioni delle reti addestrate con DDPG, e mostrano prestazioni migliorate su un gran numero di ambienti di controllo continuo standard.
Rendendosi conto degli inconvenienti nell'applicazione del dropout originale su DenseNet, abbiamo realizzato il design del metodo di dropout da tre aspetti, la cui idea potrebbe essere applicata anche ad altri modelli CNN.
Applicazione di diverse strutture e programmi di dropout binario con l'obiettivo specifico di regolarizzare l'architettura DenseNet.
Propone una tecnica di pre-dropout per densenet che implementa il dropout prima della funzione di attivazione non lineare.
Guidare i modelli profondi consapevoli delle relazioni verso un migliore apprendimento con la conoscenza umana.
Questo lavoro propone una variante della rete a colonne basata sull'iniezione della guida umana modificando i calcoli nella rete.
Un metodo per incorporare consigli umani all'apprendimento profondo estendendo Column Network, una rete neurale a grafo per la classificazione collettiva.
I recenti successi delle Reti Neurali Binarie possono essere compresi sulla base della geometria dei vettori binari ad alta densità
Indaga numericamente e teoricamente le ragioni dietro il successo empirico delle reti neurali binarizzate.
Questo articolo analizza l'efficacia delle reti neurali binarie e perché la binarizzazione è in grado di preservare le prestazioni del modello.
Dopo aver dimostrato che un neurone agisce come un risolutore di problemi inversi per la super-risoluzione e una rete di neuroni è garantita per fornire una soluzione, abbiamo proposto un'architettura di rete doppia che esegue più velocemente dello stato dell'arte.
Discute l'uso delle reti neurali per la super-risoluzione
Una nuova architettura per risolvere compiti di super-risoluzione delle immagini, e un'analisi che mira a stabilire una connessione tra CNN per risolvere la super-risoluzione e risolvere problemi inversi regolarizzati sparsi.
Modello dinamico che impara strategie di divide et impera tramite una supervisione debole.
Propone di aggiungere un nuovo bias induttivo all'architettura delle reti neurali utilizzando una strategia divide et impera.
Questo articolo studia i problemi che possono essere risolti usando un approccio di programmazione dinamica, e propone un'architettura di rete neurale per risolvere tali problemi che batte le linee di base della sequenza.
L'articolo propone un'architettura di rete unica che può imparare strategie divide et impera per risolvere compiti algoritmici.
un gradiente Rep-like per distribuzioni continue/discrete non riparametrizzabili; ulteriormente generalizzato a modelli probabilistici profondi, ottenendo una back-propagation statistica
Presenta uno stimatore del gradiente per obiettivi basati sulle aspettative che è imparziale, ha una bassa varianza e si applica a variabili casuali continue e discrete.
Un metodo migliorato per calcolare le derivate dell'aspettativa e un nuovo stimatore di gradiente a bassa varianza che permette l'addestramento di modelli generativi in cui le osservazioni o le variabili latenti sono discrete.
Disegna un gradiente di bassa varianza per distribuzioni associate a variabili casuali continue o discrete.
Mostriamo che i parametri della NN e i paesaggi dei costi degli iperparametri possono essere generati come stati quantici usando un singolo circuito quantico e che questi possono essere usati per l'addestramento e il meta-addestramento.
Descrive un metodo in cui un quadro di apprendimento profondo può essere quantizzato considerando la forma a due stati di una sfera di Bloch/qubit e creando una rete neurale binaria quantistica.
Questo articolo propone l'amplificazione di ampiezza quantistica, un nuovo algoritmo per la formazione e la selezione del modello nelle reti neurali binarie.
Propone un'idea nuova di produrre uno stato quantico che rappresenta un panorama completo dei costi di tutti i parametri per una data rete neurale binaria, costruendo una rete neurale binaria quantica (QBNN).
Un metodo generale per l'addestramento di classificatori robusti certificati sensibili ai costi contro le perturbazioni avversarie
Calcola e inserisce i costi degli attacchi avversari nell'obiettivo dell'ottimizzazione per ottenere un modello che sia costo-sensibilmente robusto contro gli attacchi avversari. 
Costruisce sul lavoro seminale di Dalvi et al. ed estende l'approccio alla robustezza certificabile con una matrice di costo che specifica per ogni coppia di classi sorgente-target se il modello deve essere robusto agli esempi avversari.
Usare le triplette per imparare una metrica per confrontare le risposte neurali e migliorare le prestazioni di una protesi.
Gli autori sviluppano nuove metriche di distanza dei treni di spike, comprese le reti neurali e le metriche quadratiche. Queste metriche hanno dimostrato di superare la metrica ingenua della distanza di Hamming, e catturano implicitamente alcune strutture nel codice neurale.
Con l'applicazione di migliorare le protesi neurali in mente, gli autori propongono di imparare una metrica tra le risposte neurali ottimizzando una forma quadratica o una rete neurale profonda.
Questo articolo introduce un metodo per generare domande (spunti) e domande (suggerimenti) per aiutare gli utenti a eseguire il mind-mapping.
Presenta uno strumento per assistere la mappatura mentale attraverso il contesto suggerito relativo ai nodi esistenti e attraverso domande che espandono i rami meno sviluppati.
Questo articolo presenta un approccio per assistere le persone nei compiti di mindmapping, progettando un'interfaccia e caratteristiche algoritmiche per supportare il mindmapping, e contribuisce a uno studio valutativo.
Rilevamento di campioni fuori distribuzione utilizzando statistiche di caratteristiche di basso ordine senza richiedere alcun cambiamento nella DNN sottostante.
Presenta un algoritmo per rilevare i campioni fuori distribuzione utilizzando la stima in corso della media e della varianza all'interno dei livelli BatchNorm per costruire rappresentazioni di caratteristiche successivamente alimentate in un classificatore lineare.
Un approccio per il rilevamento di campioni fuori distribuzione in cui gli autori propongono di utilizzare la regressione logistica sulle statistiche semplici di ogni strato di normalizzazione batch della CNN.
L'articolo suggerisce di usare i punteggi Z per confrontare i campioni ID e OOD per valutare ciò che le reti profonde stanno cercando di fare.
Proponiamo un nuovo metodo chiamato Maximal Divergence Sequential Auto-Encoder che sfrutta la rappresentazione Variational AutoEncoder per il rilevamento delle vulnerabilità del codice binario.
Questo articolo propone un'architettura basata sull'autoencoder variazionale per l'embedding del codice per il rilevamento delle vulnerabilità del software binario, con embedding appresi più efficaci nel distinguere tra codice binario vulnerabile e non vulnerabile rispetto alle linee di base.
Questo articolo propone un modello per estrarre automaticamente le caratteristiche per il rilevamento della vulnerabilità utilizzando la tecnica del deep learning. 
Calcolare l'attenzione in base alla distribuzione posteriore porta a un'attenzione più significativa e a prestazioni migliori
Questo articolo propone un modello sequenza per sequenza in cui l'attenzione è trattata come una variabile latente, e deriva nuove procedure di inferenza per questo modello, ottenendo miglioramenti nella traduzione automatica e nei compiti di generazione di inflessioni morfologiche.
Questo articolo presenta un nuovo modello di attenzione posteriore per problemi seq2seq
Comprimere i modelli DNN addestrati minimizzando la loro complessità mentre si limita la loro perdita.
Questo articolo propone un metodo per la compressione delle reti neurali profonde sotto vincoli di precisione.
Questo articolo presenta un metodo di codifica k-means vincolato al valore di perdita per la compressione della rete e sviluppa un algoritmo iterativo per l'ottimizzazione del modello.
Sviluppiamo una tecnica per visualizzare i meccanismi di attenzione in reti neurali arbitrarie. 
Propone di imparare una rete di attenzione latente che può aiutare a visualizzare la struttura interna di una rete neurale profonda.
Gli autori di questo articolo propongono uno schema di visualizzazione black-box guidato dai dati. 
Indaghiamo una varietà di algoritmi RL per la generazione molecolare e definiamo nuovi benchmark (da rilasciare come OpenAI Gym), trovando che PPO e un algoritmo hill-climbing MLE funzionano meglio.
Considera la valutazione del modello per la generazione di molecole proponendo 19 benchmark, espandendo piccoli set di dati a un grande set di dati standardizzato, ed esplorando come applicare le tecniche RL per la progettazione molecolare.
Questo articolo mostra che i metodi RL più sofisticati sono meno efficaci della semplice tecnica hill-climbing, con PPO come eccezione, quando si modellano e sintetizzano le molecole.
La capacità più robusta per il ragionamento analogico è indotta quando le reti imparano le analogie contrastando le strutture relazionali astratte nei loro domini di input.
L'articolo studia la capacità di una rete neurale di imparare l'analogia, mostrando che una semplice rete neurale è in grado di risolvere alcuni problemi di analogia
Questo articolo descrive un approccio per addestrare reti neurali per compiti di ragionamento analogico, considerando in particolare l'analogia visiva e le analogie simboliche.
Un modello di conversazione neurale orientato all'obiettivo attraverso l'auto-gioco
Un modello di auto-gioco per la generazione di dialoghi orientati agli obiettivi, che mira a rafforzare l'accoppiamento tra la ricompensa del compito e il modello del linguaggio.
Questo articolo descrive un metodo per migliorare un sistema di dialogo orientato agli obiettivi usando il selfplay. 
completamento delle query di ricerca in tempo reale utilizzando modelli linguistici LSTM a livello di carattere
Questo articolo presenta metodi per il completamento delle query che includono la correzione dei prefissi, e alcuni dettagli tecnici per soddisfare particolari requisiti di latenza su una CPU.
Gli autori propongono un algoritmo per risolvere il problema del completamento della query con la correzione degli errori, e adottano una modellazione basata su RNN a livello di carattere e ottimizzano la parte di inferenza per raggiungere gli obiettivi in tempo reale.
In questo articolo dimostriamo la convergenza alla criticità di RMSProp (stocastico e deterministico) e ADAM deterministico per obiettivi lisci non convessi e dimostriamo un'interessante sensibilità beta_1 per ADAM su autoencoder. 
Questo articolo presenta un'analisi di convergenza di RMSProp e ADAM nel caso di funzioni lisce non convesse
L'elaborazione di meccanismi di difesa non supervisionati contro gli attacchi avversari è fondamentale per garantire la generalizzabilità della difesa. 
Questo articolo presenta un metodo per rilevare esempi avversi in un ambiente di classificazione di apprendimento profondo
Questo articolo presenta un metodo non supervisionato per rilevare esempi di reti neurali avverse.
Ricerca di architetture neurali senza proxy per l'apprendimento diretto di architetture su attività su larga scala (ImageNet) riducendo il costo allo stesso livello del normale addestramento.
Questo articolo affronta il problema della ricerca dell'architettura, e in particolare cerca di farlo senza doversi allenare su compiti "proxy" dove il problema è semplificato attraverso un'ottimizzazione più limitata, la complessità dell'architettura o la dimensione del dataset.
Un nuovo quadro basato sull'inferenza variazionale per il rilevamento di fuori distribuzione
Descrive un approccio probabilistico per quantificare l'incertezza nei compiti di classificazione DNN che supera altri metodi SOTA nel compito di rilevamento di fuori distribuzione.
Un nuovo quadro per il rilevamento di out-of-distribution, basato sull'inferenza variazionale e una distribuzione di Dirichlet a priori, che riporta risultati allo stato dell'arte su diversi set di dati.
Un rilevamento di una distribuzione fuori dal normale tramite un nuovo metodo per approssimare la distribuzione di fiducia della probabilità di classificazione usando l'inferenza variazionale della distribuzione di Dirichlet.
Impariamo una rappresentazione dello spazio d'azione di un agente dalle pure osservazioni visive. Usiamo un approccio di variabile latente ricorrente con una nuova perdita di componibilità.
Propone un modello compositivo a variabili latenti per imparare modelli che prevedano cosa accadrà dopo in scenari in cui le etichette-azione non sono disponibili in abbondanza.
Un approccio basato su IB variazionale per imparare le rappresentazioni delle azioni direttamente dai video delle azioni compiute, ottenendo una migliore efficienza dei metodi di apprendimento successivi e richiedendo una minore quantità di video di etichette delle azioni.
Questo articolo propone un approccio alla predizione video che trova autonomamente uno spazio d'azione che codifica le differenze tra i fotogrammi successivi
L'apprendimento per rinforzo può essere usato per addestrare gli agenti a negoziare la formazione della squadra attraverso molti protocolli di negoziazione
Questo articolo studia la RL profonda multi-agente in ambienti in cui tutti gli agenti devono cooperare per portare a termine un compito (ad esempio, ricerca e salvataggio, videogiochi multi-player), e utilizza semplici giochi di voto ponderato cooperativo per studiare l'efficacia della RL profonda e per confrontare le soluzioni trovate dalla RL profonda con una soluzione equa.
Un approccio di apprendimento per rinforzo per la negoziazione di coalizioni in impostazioni di teoria dei giochi cooperativi che può essere utilizzato nei casi in cui sono disponibili simulazioni di allenamento illimitate.
Metodi non supervisionati per trovare, analizzare e controllare i neuroni importanti nella NMT
Questo articolo presenta approcci non supervisionati per scoprire i neuroni importanti nei sistemi di traduzione automatica neurale e analizza le proprietà linguistiche controllate da questi neuroni.
Metodi non supervisionati per classificare i neuroni nella traduzione automatica, dove i neuroni importanti sono così identificati e utilizzati per controllare l'output della traduzione automatica.
Proponiamo un quadro DRL che distingue la conoscenza specifica del compito e dell'ambiente.
Gli autori propongono di decomporre l'apprendimento di rinforzo in una funzione PATH e una funzione GOAL
Un'architettura modulare con l'obiettivo di separare la conoscenza specifica dell'ambiente e la conoscenza specifica del compito in diversi moduli, alla pari con l'A3C standard in una vasta gamma di compiti.
pix2scene: un approccio generativo profondo per modellare implicitamente le proprietà geometriche di una scena 3D dalle immagini
Esplora la spiegazione di scene con surfelli in un modello di riconoscimento neurale, e dimostra i risultati sulla ricostruzione dell'immagine, la sintesi e la rotazione mentale delle forme.
Gli autori introducono un metodo per creare un modello di scena 3D dato un'immagine 2D e una posa della telecamera utilizzando un modello auto-superficiale
Identificare le relazioni che collegano le parole è importante per vari compiti NLP. Modelliamo la rappresentazione delle relazioni come un problema di apprendimento supervisionato e impariamo operatori parametrici che mappano le incorporazioni di parole pre-addestrate alle rappresentazioni delle relazioni.
Questo articolo presenta un nuovo metodo per rappresentare le relazioni lessicali come vettori usando solo le embeddings di parole pre-addestrate e una nuova funzione di perdita che opera su coppie di coppie di parole.
Una nuova soluzione al problema della composizione delle relazioni quando si hanno già embeddings di parole/entità preaddestrati e si è interessati solo ad imparare a comporre rappresentazioni di relazioni.
Proponiamo di addestrare due copie identiche di una rete neurale ricorrente (che condividono i parametri) con diverse maschere di dropout, minimizzando la differenza tra le loro previsioni (pre-softmax).
Presenta Fraternal dropout come un miglioramento rispetto a Expectation-linear dropout in termini di convergenza, e dimostra l'utilità di Fraternal dropout su una serie di compiti e set di dati.
Apprendimento di ponderazioni e deformazioni di set di dati spazio-temporali per approssimazioni altamente efficienti del comportamento dei liquidi.
Un modello basato su reti neurali è usato per interpolare simulazioni per nuove condizioni di scena da superfici implicite 4D densamente registrate per una scena strutturata.
Questo articolo presenta un approccio di deep learning accoppiato per generare dati realistici di simulazione di liquidi che possono essere utili per applicazioni di supporto decisionale in tempo reale.
Questo articolo introduce un approccio di deep learning per la simulazione fisica che combina due reti per sintetizzare dati 4D che rappresentano simulazioni fisiche 3D
Costruiamo e valutiamo reti neurali invarianti al colore su un nuovo set di dati realistici
Propone un metodo per rendere invarianti al colore le reti neurali per il riconoscimento delle immagini e lo valuta sul dataset cifar 10.
Gli autori studiano uno strato di input modificato che risulta in reti invarianti al colore, e mostrano che certi strati di input invarianti al colore possono migliorare l'accuratezza per le immagini di prova da una distribuzione di colore diversa dalle immagini di allenamento.
Gli autori testano una CNN su immagini con canali di colore modificati per essere invarianti alle permutazioni, con prestazioni non troppo degradate. 
Analizziamo come il grado di sovrapposizioni tra i campi recettivi di una rete convoluzionale influenza la sua potenza espressiva.
L'articolo studia la potenza espressiva fornita dalla "sovrapposizione" negli strati di convoluzione delle DNN considerando attivazioni lineari con il pooling del prodotto.
Questo articolo analizza l'espressività dei circuiti aritmetici convoluzionali e mostra che un numero esponenzialmente grande di ConvACs non sovrapposti è richiesto per approssimare il tensore di griglia di un ConvACs sovrapposto.
Un algoritmo teorico per testare l'ottimalità locale ed estrarre le direzioni di discesa in punti non differenziabili di rischi empirici di reti ReLU a uno strato nascosto.
Propone un algoritmo per verificare se un dato punto è un punto stazionario generalizzato del secondo ordine.
Un algoritmo teorico, che coinvolge la risoluzione di programmi quadratici convessi e non convessi, per verificare l'ottimalità locale e sfuggire alle selle quando si addestrano reti ReLU a due strati.
L'autore propone un metodo per controllare se un punto è un punto stazionario o no e poi classificare i punti stazionari come min locali o stazionari di secondo ordine
Una nuova perdita basata su negativi relativamente duri che raggiunge prestazioni allo stato dell'arte nel recupero delle didascalie delle immagini.
Imparare l'embedding congiunto di frasi e immagini usando la perdita di terzine che viene applicata ai negativi più difficili invece di fare la media su tutte le terzine
Utilizziamo il principio di minimizzazione alternata per fornire una nuova tecnica efficace per addestrare gli autocodificatori profondi.
Struttura di minimizzazione alternata per l'addestramento di reti autoencoder e encoder-decoder
Gli autori esplorano un approccio di ottimizzazione alternata per l'addestramento degli Auto Encoder, trattando ogni strato come un modello lineare generalizzato, e suggeriscono di usare la GD stocastica normalizzata come algoritmo di minimizzazione in ogni fase.
Apprendimento di trasferimento per stimare gli effetti causali usando le reti neurali.
Sviluppa algoritmi per stimare l'effetto di trattamento medio condizionato per set di dati ausiliari in diversi ambienti, sia con che senza learner di base.
Gli autori propongono metodi per affrontare un nuovo compito di apprendimento di trasferimento per stimare la funzione CATE, e li valutano utilizzando un'impostazione sintetica e un set di dati sperimentali del mondo reale.
Utilizzando la regressione di rete neurale e confrontando i quadri di apprendimento di trasferimento per stimare un effetto di trattamento medio condizionato sotto ipotesi di ignorabilità delle stringhe
Presentiamo LeMoNADe, un metodo di rilevamento di motivi appresi end-to-end che opera direttamente sui video di imaging del calcio.
Questo articolo propone un modello in stile VAE per identificare i motivi dai video di imaging del calcio, basandosi sulle variabili di Bernouli e richiede il trucco Gumbel-softmax per l'inferenza.
Proponiamo una struttura per imparare una buona politica attraverso l'apprendimento per imitazione da un set di dimostrazioni rumorose attraverso il meta-training di un valutatore di idoneità della dimostrazione.
Contribuisce un algoritmo basato su MAML all'apprendimento per imitazione che determina automaticamente se le dimostrazioni fornite sono "adatte".
Un metodo per fare l'apprendimento per imitazione da un insieme di dimostrazioni che include comportamenti inutili, che seleziona le dimostrazioni utili in base ai loro guadagni di performance forniti al momento del meta-training.
Introduciamo modelli generativi causali impliciti, che possono campionare da distribuzioni condizionali e interventistiche e proponiamo anche due nuove GAN condizionali che usiamo per addestrarle.
Un metodo per combinare un grafico casuale, che descrive la struttura di dipendenza delle etichette con due architetture GAN condizionali che generano immagini condizionando l'etichetta binaria
Gli autori affrontano la questione dell'apprendimento di un modello causale tra le variabili dell'immagine e l'immagine stessa dai dati osservazionali, quando è data una struttura causale tra le etichette dell'immagine.
Dimostriamo che NCE è auto-normalizzato e lo dimostriamo su set di dati
Presenta una prova dell'auto normalizzazione di NCE come risultato dell'essere un'approssimazione della matrice di basso rango della matrice delle probabilità condizionali normalizzate.
Questo articolo considera il problema dei modelli auto-normalizzanti e spiega il meccanismo di auto-normalizzazione interpretando NCE in termini di fattorizzazione della matrice.
Arricchire le incorporazioni di parole con informazioni sull'affetto migliora le loro prestazioni nei compiti di predizione del sentimento.
Propone di utilizzare la lessica degli affetti per migliorare le embeddings di parole per superare lo standard Word2vec e Glove.
Questo articolo propone di integrare le informazioni di una risorsa semantica che quantifica l'influenza delle parole in un algoritmo di incorporazione di parole basato sul testo per rendere i modelli linguistici più riflessivi dei fenomeni semantici e pragmatici.
Questo articolo introduce delle modifiche alle funzioni di perdita word2vec e GloVe per incorporare il lessico degli affetti e facilitare l'apprendimento di embeddings di parole sensibili agli affetti.
Per l'incorporazione non supervisionata e induttiva della rete, proponiamo un nuovo approccio per esplorare i vicini più rilevanti e preservare la conoscenza precedentemente appresa dei nodi utilizzando l'architettura di bi-attenzione e introducendo un bias globale, rispettivamente
Questo propone un'estensione di GraphSAGE utilizzando una matrice di bias di incorporazione globale nelle funzioni di aggregazione locale e un metodo per campionare i nodi interessanti.
Noi sosteniamo che la generalizzazione dell'incorporazione dei grafi lineari non è dovuta al vincolo di dimensionalità ma piuttosto alla piccola norma dei vettori di incorporazione.
Gli autori mostrano che l'errore di generalizzazione dei metodi di embedding dei grafi lineari è limitato dalla norma dei vettori di embedding piuttosto che dai vincoli di dimensionalità
Gli autori propongono un limite teorico sulle prestazioni di generalizzazione dell'apprendimento di embeddings di grafi e sostengono che la norma delle coordinate determina il successo della rappresentazione appresa.
Mescolate SGD e momentum (o fate qualcosa di simile con Adam) per un grande profitto.
L'articolo propone semplici modifiche a SGD e Adam, chiamate varianti QH, che possono recuperare il metodo padre e una serie di altri trucchi di ottimizzazione.
Una variante del momentum classico che prende una media ponderata del momentum e dell'aggiornamento del gradiente, e una valutazione delle sue relazioni con altri schemi di ottimizzazione basati sul momentum.
Un nuovo modo di generalizzare i lambda-returns permettendo all'agente RL di decidere quanto vuole pesare ciascuno degli n-step returns.
Estende l'algoritmo A3C con ritorni lambda, e propone un approccio per imparare i pesi dei ritorni.
Gli autori presentano i ritorni autodidattici basati sulla fiducia, un metodo RL di apprendimento profondo per regolare i pesi di un vettore di ammissibilità nella stima del valore TD(lambda)-like per favorire stime più stabili dello stato.
abbiamo proposto un nuovo modello di guida autonoma che è composto da modulo di percezione per vedere e pensare e modulo di guida per comportarsi per acquisire una migliore generalizzazione e capacità di spiegazione degli incidenti.
Presenta un'architettura di apprendimento multitask per la stima della mappa di profondità e segmentazione e la previsione di guida utilizzando un modulo di percezione e un modulo di decisione di guida.
Un metodo per un'architettura end-to-end modificata che ha una migliore generalizzazione e capacità di spiegazione, è più robusto per una diversa impostazione di test, e ha un output di decodifica che può aiutare nel debugging del modello.
Gli autori presentano una rete neurale convoluzionale multi-task per la guida end-to-end e forniscono valutazioni con il simulatore open source CARLA mostrando migliori prestazioni di generalizzazione in nuove condizioni di guida rispetto alle linee di base
Un quadro generico per scalare le tecniche esistenti di embedding di grafi a grandi grafi.
Questo articolo propone una struttura di embedding multilivello da applicare sopra i metodi di embedding di rete esistenti al fine di scalare a reti su larga scala con maggiore velocità.
Gli autori propongono una struttura a tre stadi per l'incorporazione di grafici su larga scala con una migliore qualità di incorporazione.
Un nuovo metodo per aumentare la resistenza degli OCSVM contro attacchi mirati e di integrità mediante trasformazioni selettive non lineari dei dati a dimensioni inferiori.
Gli autori propongono una difesa contro gli attacchi alla sicurezza dei rilevatori di anomalie basati su una classe SVM
Questo articolo esplora come le proiezioni casuali possono essere utilizzate per rendere OCSVM robusta ai dati di allenamento perturbati in modo avverso.
Apprendimento di una migliore rappresentazione delle reti neurali con il principio dell'Information Bottleneck
Propone un metodo di apprendimento basato sul quadro del collo di bottiglia dell'informazione, dove gli strati nascosti delle reti profonde comprimono l'input X mantenendo informazioni sufficienti per prevedere l'output Y.
Questo articolo presenta un nuovo modo di addestrare una rete neurale stocastica seguendo un quadro di rilevanza/compressione delle informazioni simile all'Information Bottleneck.
Proponiamo uno stimatore per la massima discrepanza media, appropriato quando una distribuzione di destinazione è accessibile solo attraverso una procedura di selezione del campione distorta, e mostriamo che può essere utilizzato in una rete generativa per correggere questa distorsione.
Propone uno stimatore ponderato per l'importanza della MMD per stimare la MMD tra distribuzioni basate su campioni distorti secondo uno schema noto o stimato sconosciuto.
Gli autori affrontano il problema della distorsione della selezione del campione nelle MMD-GAN e propongono una stima della MMD tra due distribuzioni utilizzando la discrepanza media massima ponderata.
Questo articolo presenta una modifica dell'obiettivo utilizzato per addestrare le reti generative con un avversario MMD 
Utilizzando la regressione bayesiana per stimare il posteriore sulle funzioni Q e distribuire Thompson Sampling come una strategia di esplorazione mirata con un efficiente trade-off tra esplorazione e sfruttamento
Gli autori propongono un nuovo algoritmo per l'esplorazione in Deep RL dove applicano la regressione lineare bayesiana con le caratteristiche dell'ultimo strato di una rete DQN per stimare la funzione Q per ogni azione.
Gli autori descrivono come usare le reti neurali bayesiane con il campionamento di Thompson per un'esplorazione efficiente in q-learning e propongono un approccio che supera gli approcci di esplorazione epsilon-greedy.
PolyCNN ha solo bisogno di imparare un filtro convoluzionario di seme ad ogni strato. Questa è una variante efficiente della CNN tradizionale, con prestazioni alla pari.
I tentativi di ridurre il numero di parametri del modello CNN utilizzando la trasformazione polinomiale dei filtri per creare blow-up delle risposte dei filtri.
Gli autori propongono un'architettura di condivisione dei pesi per ridurre il numero di parametri delle reti neurali convoluzionali con filtri di semi
In questo articolo, proponiamo KL-CPD, un nuovo quadro di apprendimento kernel per la CPD di serie temporali che ottimizza un limite inferiore della potenza di prova attraverso un modello generativo ausiliario come surrogato della distribuzione anomala. 
Descrive un nuovo approccio per ottimizzare la scelta del kernel verso una maggiore potenza di test e dimostra di offrire miglioramenti rispetto alle alternative.
Cluster prima di classificare; usare etichette deboli per migliorare la classificazione 
Propone l'uso di una funzione di perdita basata sul clustering a più livelli di una deepnet e l'uso della struttura gerarchica dello spazio delle etichette per allenare rappresentazioni migliori.
Questo articolo usa le informazioni gerarchiche sulle etichette per imporre ulteriori perdite sulle rappresentazioni intermedie nell'addestramento delle reti neurali.
Advantage-based regret minimization è un nuovo algoritmo di deep reinforcement learning che è particolarmente efficace su compiti parzialmente osservabili, come la navigazione in prima persona in Doom e Minecraft.
Questo articolo introduce i concetti di minimizzazione del rimpianto controfattuale nel campo della RL profonda e un algoritmo chiamato ARM che può trattare meglio l'osservabilità parziale.
L'articolo fornisce una variante ispirata alla teoria dei giochi dell'algoritmo policy-gradient basata sull'idea di minimizzazione del rimpianto controfattuale e sostiene che l'approccio può trattare il dominio parziale osservabile meglio dei metodi standard.
un modello di apprendimento profondo multi-task che adatta la rappresentazione ad anello dei tensori
Una variante della formulazione dell'anello tensore per l'apprendimento multi-task condividendo alcuni dei nuclei TT per l'apprendimento del "compito comune" mentre si imparano singoli nuclei TT per ogni compito separato
Un modello di regressione che impara le distribuzioni condizionali di un processo stocastico, incorporando l'attenzione nei processi neurali.
Propone di risolvere il problema dell'underfitting nel metodo del processo neurale aggiungendo un meccanismo di attenzione al percorso deterministico.
Un'estensione della struttura dei Processi Neurali che aggiunge un meccanismo di condizionamento basato sull'attenzione, permettendo al modello di catturare meglio le dipendenze nel set di condizionamento.
Gli autori estendono i processi neurali incorporando l'autoattenzione per arricchire le caratteristiche dei punti di contesto e l'attenzione incrociata per produrre una rappresentazione specifica della domanda. Risolvono il problema dell'underfitting delle NP e mostrano che le ANP convergono meglio e più velocemente delle NP.
Risolvere il problema della scacchiera nel livello deconvolutivo costruendo dipendenze tra i pixel
Questo lavoro propone strati di deconvoluzione dei pixel per le reti neurali convoluzionali come un modo per alleviare l'effetto scacchiera.
Una nuova tecnica per generalizzare le operazioni di deconvoluzione utilizzate nelle architetture CNN standard, che propone di fare la predizione sequenziale delle caratteristiche dei pixel adiacenti, ottenendo output più spazialmente lisci per gli strati di deconvoluzione.
Quantizziamo e potiamo i pesi delle reti neurali usando l'inferenza bayesiana variazionale con un priore multimodale che induce la sparsità.
Propone di utilizzare una miscela di spike continui propto 1/abs come priore per una rete neurale bayesiana e dimostra le buone prestazioni con convnet relativamente sparsificati per minist e cifar-10.
Questo articolo presenta un approccio variazionale bayesiano per quantificare i pesi delle reti neurali a valori ternari dopo l'addestramento in modo principesco.
Implementiamo un approccio di potatura dei pesi DNN che raggiunge i più alti tassi di potatura.
Questo articolo si concentra sulla potatura dei pesi per la compressione delle reti neurali, raggiungendo un tasso di compressione 30x per AlexNet e VGG per ImageNet.
Una tecnica di potatura progressiva che impone un vincolo di sparsità strutturale sul parametro del peso e riscrive l'ottimizzazione come un quadro ADMM, ottenendo una maggiore accuratezza rispetto alla discesa del gradiente proiettato.
Questo articolo presenta una nuova architettura di apprendimento profondo per affrontare il problema dell'apprendimento supervisionato con serie temporali multivariate sparse e campionate in modo irregolare.
Propone una struttura per fare previsioni su dati di serie temporali sparsi e campionati in modo irregolare usando un modulo di interpolazione che modella i valori mancanti usando interpolazione liscia, interpolazione non liscia e intensità. 
Risolve il problema dell'apprendimento supervisionato con serie temporali multivariate sparse e campionate in modo irregolare usando una rete di interpolazione semi-parametrica seguita da una rete di predizione.
Funzione di perdita invariante per permutazione per la predizione di set di punti.
Propone una nuova perdita per la registrazione dei punti (allineamento di due insiemi di punti) con una proprietà invariante di permutazione preferibile. 
Questo articolo introduce una nuova funzione di distanza tra insiemi di punti, applica altre due distanze di permutazione in un compito di rilevamento di oggetti end-to-end, e mostra che in due dimensioni tutti i minimi locali della perdita olografica sono minimi globali.
Propone funzioni di perdita invarianti di permutazione che dipendono dalla distanza degli insiemi.
Introduciamo un modello gerarchico per il posizionamento efficiente, end-to-end, di grafi computazionali su dispositivi hardware.
Propone di apprendere congiuntamente gruppi di operatori da colocare e di collocare i gruppi appresi sui dispositivi per distribuire le operazioni per l'apprendimento profondo tramite l'apprendimento di rinforzo.
Gli autori hanno lo scopo di una rete completamente collegata per sostituire il passo di co-locazione in un metodo di auto-placement proposto per accelerare il tempo di esecuzione di un modello TensorFlow.
Propone un algoritmo di posizionamento del dispositivo per collocare le operazioni di tensorflow sui dispositivi.
Proponiamo il metodo di usare le proprietà del gruppo per imparare una rappresentazione del movimento senza etichette e dimostriamo l'uso di questo metodo per rappresentare il movimento 2D e 3D.
Propone di imparare il gruppo di movimento rigido da una rappresentazione latente di sequenze di immagini senza la necessità di etichette esplicite e dimostra sperimentalmente il metodo su sequenze di cifre MINST e il dataset KITTI.
Questo articolo propone un approccio per l'apprendimento delle caratteristiche del movimento video in modo non supervisionato, utilizzando i vincoli per ottimizzare la rete neurale per produrre caratteristiche che possono essere utilizzate per regredire all'odometria.
Questo articolo propone un nuovo strato di convoluzione che opera in uno spazio di Hilbert continuo a kernel riproducente.
Proiettare gli esempi in uno spazio RK Hilbert ed eseguire la convoluzione e il filtraggio in quello spazio.
Questo articolo formula una variante delle reti neurali convoluzionali che modella sia le attivazioni che i filtri come funzioni continue composte da basi kernel
Le CNN addestrate in ImageNet sono orientate verso la texture dell'oggetto (invece che verso la forma come gli umani). Il superamento di questa grande differenza tra la visione umana e la visione della macchina produce prestazioni di rilevamento migliorate e una robustezza mai vista prima alle distorsioni dell'immagine.
Usare lo stylizaton delle immagini per aumentare i dati di addestramento per le CNN addestrate in ImageNet per far apparire le reti risultanti più allineate ai giudizi umani
Questo articolo studia le CNN come AlexNet, VGG, GoogleNet e ResNet50, mostra che questi modelli sono distorti verso le texture quando addestrati su ImageNet, e propone un nuovo set di dati ImageNet.
Valutiamo l'efficacia di avere compiti discriminatori ausiliari eseguiti sopra le statistiche della distribuzione posteriore apprese dagli autocodificatori variazionali per imporre la dipendenza dal parlante.
Proporre un modello autoencoder per imparare una rappresentazione per la verifica degli altoparlanti utilizzando finestre di analisi di breve durata.
Una versione modificata del modello di autocodifica variazionale che affronta il problema del riconoscimento degli altoparlanti nel contesto di segmenti di breve durata
L'inferenza variazionale è di parte, debiasiamola.
Introduce l'inferenza variazionale jackknife, un metodo per il debiasing di obiettivi Monte Carlo come l'auto-encoder ponderato per importanza.
Gli autori analizzano la distorsione e la varianza del limite IWAE e derivano un approccio jacknife per stimare i momenti come un modo per debias IWAE per campioni ponderati con importanza finita.
Un framework che fornisce una politica per il cambio di corsia autonomo imparando a prendere decisioni tattiche di alto livello con il deep reinforcement learning, e mantenendo una stretta integrazione con un controller di basso livello per prendere azioni di basso livello.
Considera il problema del cambio di corsia autonomo per le auto a guida autonoma nell'impostazione multi-lane multi-agente slot car, propone una nuova strategia di apprendimento Q-masking - accoppiando un controller definito di basso livello con una politica decisionale tattica di alto livello.
Questo articolo propone un approccio di deep Q-learning al problema del cambio di corsia usando il "Q-masking", che riduce lo spazio d'azione in base a vincoli o conoscenze precedenti.
Gli autori propongono un metodo che utilizza una politica di alto livello basata sul Q-learning che è combinata con una maschera contestuale derivata dai vincoli di sicurezza e dai controllori di basso livello, che disabilitano certe azioni dall'essere selezionabili in certi stati. 
Ricerca automatica di design robotico con reti neurali a grafo
Propone un approccio per la progettazione automatica di robot basato sull'evoluzione del grafico neurale. Gli esperimenti dimostrano che ottimizzare sia il controller che l'hardware è meglio che ottimizzare solo il controller.
Gli autori propongono uno schema basato su una rappresentazione a grafo della struttura del robot, e una rete neurale a grafo come controllori per ottimizzare le strutture del robot, combinate con i loro controllori.  
Dimostriamo un autocodificatore per i grafi.
Imparare a generare grafi usando metodi di deep learning in "one shot", producendo direttamente le probabilità di esistenza di nodi e bordi e i vettori di attributi dei nodi.
Un codificatore automatico variazionale per generare grafici
Proponiamo un nuovo algoritmo per l'addestramento LSTM imparando verso porte a valore binario che abbiamo dimostrato avere molte belle proprietà.
Proporre una nuova funzione "gate" per LSTM per abilitare i valori dei gate verso 0 o 1. 
L'articolo mira a spingere le porte LSTM ad essere binarie impiegando il recente trucco Gumbel-Softmax per ottenere una distribuzione categorica addestrabile end-to-end.
Migliorare le raccomandazioni utilizzando la modellazione sensibile al tempo con reti neurali in più categorie di prodotti su un sito web di vendita al dettaglio
L'articolo propone un nuovo metodo basato su reti neurali per la raccomandazione.
Gli autori descrivono una procedura di costruzione del loro sistema di raccomandazione di produzione da zero e integrano il decadimento temporale degli acquisti nel quadro di apprendimento.
Accoppiare la struttura di restauro dell'immagine basata su GAN con un'altra rete specifica per il compito per generare un'immagine realistica preservando le caratteristiche specifiche del compito.
Un nuovo metodo di Task-GAN di accoppiamento dell'immagine che accoppia GAN e una rete specifica del compito, che allevia per evitare l'allucinazione o il collasso della modalità.
Gli autori propongono di aumentare il restauro di immagini basato su GAN con un altro ramo specifico, come i compiti di classificazione, per un ulteriore miglioramento.
Una rete neurale profonda addestrata end-to-end che sfrutta la Gaussian Mixture Modeling per eseguire la stima della densità e il rilevamento non supervisionato delle anomalie in uno spazio bidimensionale appreso da un profondo autoencoder.
L'articolo presenta un quadro di apprendimento profondo congiunto per la riduzione delle dimensioni e il clustering, che porta a un rilevamento competitivo delle anomalie.
Una nuova tecnica per il rilevamento delle anomalie in cui la riduzione delle dimensioni e la stima della densità sono ottimizzate congiuntamente.
Abbiamo proposto le reti a sottospazio proiettivo per l'apprendimento a pochi colpi e semi-supervisionato a pochi colpi
Questo articolo propone un nuovo approccio basato sull'embedding per il problema dell'apprendimento di pochi colpi e un'estensione di questo modello all'impostazione di apprendimento semi-supervisionato di pochi colpi.
Nuovo metodo per la classificazione completamente e semi-supervisionata di pochi colpi basato sull'apprendimento di un'embedding generale e poi sull'apprendimento di un sottospazio di esso per ogni classe
Studiamo la consapevolezza della contingenza e gli aspetti controllabili nell'esplorazione e raggiungiamo prestazioni allo stato dell'arte su Montezuma's Revenge senza dimostrazioni di esperti.
Questo articolo indaga il problema dell'estrazione di una rappresentazione di stato significativa per aiutare l'esplorazione quando ci si confronta con un compito di ricompensa sparsa, identificando le caratteristiche controllabili (apprese) dello stato
Questo articolo propone l'idea nuova di usare la consapevolezza della contingenza per aiutare l'esplorazione in compiti di apprendimento di rinforzo a ricompensa sparsa, ottenendo risultati allo stato dell'arte.
Abbiamo proposto un algoritmo supervisionato, DNA-GAN, per distinguere più attributi delle immagini.
Questo articolo indaga il problema della generazione di immagini condizionata dagli attributi usando reti generative avversarie, e propone di generare immagini da attributi e codice latente come rappresentazione di alto livello.
Questo articolo ha proposto un nuovo metodo per distinguere diversi attributi delle immagini utilizzando una nuova struttura di DNA GAN
Questo articolo presenta una nuova tecnica di modellazione generativa di variabili latenti che permette la rappresentazione di informazioni globali in una variabile latente e di informazioni locali in un'altra variabile latente.
L'articolo presenta un VAE che usa le etichette per separare la rappresentazione appresa in una parte invariante e una covariante.
Ci avviciniamo al problema dell'apprendimento attivo come un problema di selezione del core-set e mostriamo che questo approccio è particolarmente utile nell'impostazione di apprendimento attivo batch che è cruciale quando si addestrano le CNN.
Gli autori forniscono un algoritmo di apprendimento attivo indipendente dall'algoritmo per la classificazione multiclasse
L'articolo propone un algoritmo di apprendimento attivo in modalità batch per CNN come un problema di core-set che supera il campionamento casuale e il campionamento di incertezza.
Studia l'apprendimento attivo per le reti neurali convoluzionali e formula il problema dell'apprendimento attivo come selezione di core-set e presenta una nuova strategia
Un semplice algoritmo per migliorare l'ottimizzazione e la gestione delle dipendenze a lungo termine in LSTM
L'articolo introduce un semplice algoritmo stocastico chiamato h-detach che è specifico per l'ottimizzazione LSTM e mirato ad affrontare questo problema.
Propone una semplice modifica al processo di addestramento del LSTM per facilitare la propagazione del gradiente lungo gli stati delle cellule, o il "percorso temporale lineare".
Addestrare correttamente le CNN con la classe dustbin aumenta la loro robustezza agli attacchi avversari e la loro capacità di gestire i campioni fuori distribuzione.
Questo articolo propone di aggiungere un'etichetta aggiuntiva per rilevare i campioni OOD e gli esempi avversari nei modelli CNN.
L'articolo propone una classe aggiuntiva che incorpora immagini naturali di distribuzione esterna e immagini interpolate per i campioni avversari e di distribuzione esterna nelle CNN
In una rete neurale convoluzionale profonda addestrata con un livello sufficiente di aumento dei dati, ottimizzato da SGD, i regolarizzatori espliciti (decadimento dei pesi e dropout) potrebbero non fornire alcun miglioramento aggiuntivo della generalizzazione.
Questo articolo propone l'aumento dei dati come alternativa alle tecniche di regolarizzazione comunemente usate, e mostra che per alcuni modelli/compiti di riferimento la stessa performance di generalizzazione può essere ottenuta usando solo l'aumento dei dati.
Questo articolo presenta uno studio sistematico dell'aumento dei dati nella classificazione delle immagini con reti neurali profonde, suggerendo che l'aumento dei dati può replicare alcuni regolarizzatori comuni come il decadimento del peso e il dropout.
In questo lavoro, presentiamo Gedit, un sistema di gesti sulla tastiera per un comodo editing di testo mobile.
Riporta la progettazione e la valutazione delle tecniche di interazione di Gedit.
Presenta una nuova serie di gesti tattili per eseguire la transizione senza soluzione di continuità tra l'inserimento del testo e la modifica del testo nei dispositivi mobili
Dimostriamo che DNN è una soluzione approssimata ricorsivamente al principio di massima entropia.
Presenta una derivazione che collega una DNN all'applicazione ricorsiva dell'adattamento del modello di massima entropia.
L'articolo mira a fornire una visione dell'apprendimento profondo dalla prospettiva del principio di massima entropia.
Presentiamo un nuovo approccio all'apprendimento di rinforzo che sfrutta una funzione di ricompensa intrinseca indipendente dal compito, addestrata sulle misurazioni delle pulsazioni periferiche che sono correlate alle risposte del sistema nervoso autonomo umano. 
Propone un quadro di apprendimento di rinforzo basato sulla reazione emotiva umana nel contesto della guida autonoma.
Gli autori propongono di utilizzare i segnali, come le risposte viscerali autonome di base che influenzano il processo decisionale, all'interno della struttura RL aumentando le funzioni di ricompensa RL con un modello appreso direttamente dalle risposte del sistema nervoso umano.
Propone di utilizzare i segnali fisiologici per migliorare le prestazioni degli algoritmi di apprendimento di rinforzo e costruire una funzione di ricompensa intrinseca che è meno sparsa misurando l'ampiezza delle pulsazioni cardiache
Le CNN sono robuste o fragili al rumore delle etichette? In pratica, robuste.
Gli autori sfidano la robustezza delle CNN al rumore delle etichette usando l'albero ImageNet 1k di WordNet.
Un'analisi delle prestazioni del modello di rete neurale convoluzionale quando viene introdotto rumore dipendente dalla classe e indipendente dalla classe
Dimostra che le CNN sono più robuste al rumore delle etichette rilevante per la classe e sostiene che il rumore del mondo reale dovrebbe essere rilevante per la classe
Sintesi audio di alta qualità con i GAN
Propone un approccio che utilizza il quadro GAN per generare l'audio attraverso la modellazione di magnitudini log e frequenze istantanee con una risoluzione di frequenza sufficiente nel dominio spettrale. 
Una strategia per generare campioni audio dal rumore con le GAN, con modifiche all'architettura e alla rappresentazione necessarie per generare un audio convincente che contenga un codice latente interpretabile.
Presenta una semplice idea per rappresentare meglio i dati audio in modo da poter applicare modelli convoluzionali come le reti generative avversarie
Ottimizzazione dei grafici con filtraggio del segnale nel dominio dei vertici.
L'articolo studia l'apprendimento della matrice di adiacenza di un grafo con un grafo indiretto scarsamente connesso con pesi di bordo non negativi utilizza un algoritmo di discesa sub-gradiente proiettato.
Sviluppa un nuovo schema per il backpropogating sulla matrice di adiacenza di un grafo di rete neurale
Questo articolo descrive uno strumento di authoring 3D per fornire AR nelle linee di assemblaggio dell'industria 4.0
L'articolo affronta il modo in cui gli strumenti di authoring AR supportano la formazione dei sistemi della catena di montaggio e propone un approccio
Un sistema di guida AR per linee di assemblaggio industriali che permette l'authoring in loco di contenuti AR.
Presenta un sistema che permette agli operai di una fabbrica di essere addestrati in modo più efficiente usando un sistema di realtà aumentata. 
Mostriamo che, con una scelta appropriata di stepsize, l'algoritmo iterativo del primo ordine ampiamente utilizzato nell'addestramento delle GAN convergerebbe infatti a una soluzione stazionaria con un tasso sublineare.
Questo articolo usa le GAN e l'apprendimento multi-task per fornire una garanzia di convergenza per algoritmi primal-dual su certi problemi min-max.
Analizza la dinamica di apprendimento delle GAN formulando il problema come un problema di ottimizzazione primaria-duale assumendo una classe limitata di modelli
Mostriamo come usare la RL profonda per costruire agenti che possono risolvere dilemmi sociali al di là dei giochi di matrice.
Imparare a giocare giochi a somma generale a due giocatori con stato con informazioni imperfette
Specifica una strategia di innesco (CCC) e l'algoritmo corrispondente, dimostrando la convergenza verso risultati efficienti nei dilemmi sociali senza bisogno che gli agenti osservino le azioni degli altri.
L'articolo propone e analizza due schemi di quantizzazione per comunicare i gradienti stocastici nell'apprendimento distribuito che ridurrebbero i costi di comunicazione rispetto allo stato dell'arte, mantenendo la stessa precisione.  
Gli autori propongono di applicare la quantizzazione dithered ai gradienti stocastici calcolati attraverso il processo di formazione, che migliora l'errore di quantizzazione e raggiunge risultati superiori rispetto alle linee di base, e propongono uno schema annidato per ridurre i costi di comunicazione.
Gli autori stabiliscono una connessione tra la riduzione della comunicazione nell'ottimizzazione distribuita e la quantizzazione dithered e sviluppano due nuovi algoritmi di formazione distribuita in cui l'overhead della comunicazione è significativamente ridotto.
Addestrare congiuntamente una rete generatrice di rumore avversario con una rete di classificazione per fornire una migliore robustezza agli attacchi avversari.
Una soluzione GAN per modelli profondi di classificazione, di fronte agli attacchi white e black box, che produce modelli robusti. 
L'articolo propone un meccanismo difensivo contro gli attacchi avversari utilizzando GANs con perturbazioni generate usate come esempi avversari e un discriminatore usato per distinguerli
Utilizzo di metodi d'insieme come difesa dalle perturbazioni avversarie contro le reti neurali profonde.
Questo articolo propone di utilizzare l'ensembling come un meccanismo di difesa avversaria.
Ha studiato empiricamente la robustezza di diversi insiemi di opere neurali profonde ai due tipi di attacchi, FGSM e BIM, su due dataset popolari, MNIST e CIFAR10
Proposta del metodo di generazione di frasi basato sulla fusione tra informazioni testuali e informazioni visive associate alle informazioni testuali
Questo lavoro descrive un modello di deep learning per sistemi di dialogo che sfrutta le informazioni visive.
Questo articolo propone un nuovo set di dati per il grounded dialog e fa un'osservazione computazionale che potrebbe aiutare a ragionare sulla visione anche quando si esegue un dialogo basato sul testo.
Propone di aumentare i tradizionali approcci di generazione di frasi/dialogo basati sul testo incorporando informazioni visive, raccogliendo un insieme di dati che consistono sia di testo che di immagini o video associati
abbiamo proposto una nuova rete convoluzionale ricorrente contestuale con proprietà robuste di apprendimento visivo 
Questo articolo introduce la connessione di feedback per migliorare l'apprendimento delle caratteristiche attraverso l'incorporazione di informazioni di contesto.
L'articolo propone di aggiungere connessioni "ricorrenti" all'interno di una rete di convoluzione con meccanismo di gating.
Mostriamo che l'addestramento di una rete profonda usando la normalizzazione batch è equivalente all'inferenza approssimativa nei modelli bayesiani, e dimostriamo come questa scoperta ci permette di fare stime utili dell'incertezza del modello nelle reti convenzionali.
Questo articolo propone di usare la normalizzazione dei lotti al momento del test per ottenere l'incertezza predittiva, e mostra che la predizione Monte Carlo al momento del test usando la norma dei lotti è migliore del dropout.
Propone che la procedura di regolarizzazione chiamata normalizzazione dei lotti può essere intesa come l'esecuzione di un'inferenza bayesiana approssimativa, che si comporta in modo simile al dropout MC in termini di stime di incertezza che produce.
Miglioriamo il gradient dropping (una tecnica per scambiare solo gradienti di grandi dimensioni nell'addestramento distribuito) incorporando i gradienti locali mentre facciamo un aggiornamento dei parametri per ridurre la perdita di qualità e migliorare ulteriormente il tempo di addestramento.
Questo articolo propone 3 modalità per combinare i gradienti locali e globali per utilizzare meglio più nodi di calcolo
Si occupa del problema della riduzione dei requisiti di comunicazione per l'implementazione delle tecniche di ottimizzazione distribuita, in particolare SGD
Esplorazione con RL distributivo e varianza troncata.
Presenta un metodo RL per gestire i trade-off di esplorazione-sfruttamento tramite tecniche UCB.
Un metodo per utilizzare la distribuzione appresa dalla Quantile Regression DQN per l'esplorazione, al posto della solita strategia epsilon-greedy.
Propone nuovi algoritmi (QUCB e QUCB+) per gestire il tradeoff di esplorazione in Multi-Armed Bandits e più in generale nel Reinforcement Learning
Motivati dalle teorie del linguaggio e della comunicazione, introduciamo autocodificatori basati sulla comunità, in cui codificatori e decodificatori multipli imparano collettivamente rappresentazioni strutturate e riutilizzabili.
Gli autori affrontano il problema dell'apprendimento della rappresentazione, mirano a costruire una rappresentazione riutilizzabile e strutturata, sostengono che il co-adattamento tra codificatore e decodificatore nell'AE tradizionale produce una scarsa rappresentazione, e introducono autocodificatori basati sulla comunità.
L'articolo presenta un quadro di autocodificatore basato sulla comunità per affrontare il co-adattamento di codificatori e decodificatori e mira a costruire rappresentazioni migliori.
Presentiamo MetaMimic, un algoritmo che prende come input un set di dati dimostrativi e produce (i) una politica di imitazione ad alta fedeltà one-shot e (ii) una politica di compito incondizionata.
L'articolo esamina il problema dell'imitazione one-shot con alta precisione di imitazione, estendendo DDPGfD per usare solo traiettorie di stato.
Questo articolo propone un approccio per l'imitazione one-shot con alta precisione, e affronta il problema comune dell'esplorazione nell'apprendimento dell'imitazione.
Presenta un metodo RL per l'apprendimento dalla dimostrazione video senza accesso alle azioni degli esperti
Presentiamo un nuovo metodo di normalizzazione per le reti neurali profonde che è robusto alle multi-modalità nelle distribuzioni di caratteristiche intermedie.
Metodo di normalizzazione che impara la distribuzione multi-modale nello spazio delle caratteristiche
Propone una generalizzazione della Batch Normalization sotto l'ipotesi che la statistica delle attivazioni unitarie sui lotti e sulle dimensioni spaziali non sia unimodale
Abbiamo proposto un metodo basato sulla distillazione della conoscenza per aumentare la precisione della traduzione automatica neurale multilingue.
Un modello di traduzione automatica multilingue che prima addestra modelli separati per ogni coppia di lingue e poi esegue la distillazione.
L'articolo mira all'addestramento di un modello di traduzione automatica aumentando la perdita standard di cross-entropia con una componente di distillazione basata su modelli di insegnanti individuali (coppia di lingue).
Indaghiamo i vari tipi di conoscenza preliminare che aiutano l'apprendimento umano e troviamo che i priori generali sugli oggetti giocano il ruolo più critico nel guidare il gioco umano.
Gli autori studiano per esperimento, quali aspetti dei priori umani sono importanti per l'apprendimento di rinforzo nei videogiochi.
Gli autori presentano uno studio sui priori impiegati dagli esseri umani nel giocare ai videogiochi e dimostra l'esistenza di una tassonomia di caratteristiche che influenzano la capacità di completare i compiti nel gioco in vari gradi.
Spinti dalla necessità di metodi di ottimizzazione iperparametrica parallelizzabili e a ciclo aperto, proponiamo l'uso di processi di punti k-determinanti nell'ottimizzazione iperparametrica tramite ricerca casuale.
Propone di usare il k-DPP per selezionare i punti candidati nelle ricerche di iperparametri.
Gli autori propongono k-DPP come un metodo ad anello aperto per l'ottimizzazione degli iperparametri e forniscono il suo studio empirico e il confronto con altri metodi.
Considera la ricerca iperparametrica non sequenziale e non informata usando processi puntuali determinanti, che sono distribuzioni di probabilità su sottoinsiemi di un insieme terreno con la proprietà che i sottoinsiemi con elementi più 'diversi' hanno una probabilità più alta
Nell'apprendimento induttivo di trasferimento, la messa a punto di reti convoluzionali pre-addestrate supera sostanzialmente la formazione da zero.
Affronta il problema dell'apprendimento di trasferimento nelle reti profonde e propone di avere un termine di regolarizzazione che penalizza la divergenza dall'inizializzazione.
Propone un'analisi su diverse tecniche di regolarizzazione adattiva per il deep transfer learning, concentrandosi in particolare sull'uso di una condizione L@-SP
Guardiamo le reti neurali con strati di prodotti interni diagonali a blocchi per l'efficienza.
Questo articolo propone che gli strati interni di una rete neurale siano diagonali a blocchi, e discute che le matrici diagonali a blocchi sono più efficienti del pruning e che gli strati diagonali a blocchi portano a reti più efficienti.
Sostituzione di strati completamente connessi con strati completamente connessi block-diagonal
Proponiamo una nuova tecnica di normalizzazione dei pesi chiamata normalizzazione spettrale per stabilizzare l'addestramento del discriminatore delle GAN.
Questo articolo usa la regolarizzazione spettrale per normalizzare gli obiettivi GAN, e il GAN che ne risulta, chiamato SN-GAN, assicura essenzialmente la proprietà Lipschitz del discriminatore.
Questo articolo propone la "normalizzazione spettrale", facendo un bel passo avanti nel miglioramento dell'addestramento delle GAN.
Le politiche di transizione permettono agli agenti di comporre competenze complesse collegando in modo fluido le competenze primitive precedentemente acquisite.
Propone uno schema per la transizione a stati di strating favorevoli per l'esecuzione di opzioni date in domini continui. Utilizza due processi di apprendimento eseguiti simultaneamente.
Presenta un metodo per l'apprendimento delle politiche di transizione da un compito all'altro con l'obiettivo di completare compiti complessi utilizzando lo stimatore di prossimità di stato per premiare la politica di transizione.
Propone un nuovo schema di addestramento con una funzione di ricompensa ausiliaria appresa per ottimizzare le politiche di transizione che collegano lo stato finale di una precedente macro azione/opzione con buoni stati di inizio della seguente macro azione/opzione
Classifichiamo le caratteristiche dinamiche che una e due celle GRU possono e non possono catturare in tempo continuo, e verifichiamo sperimentalmente i nostri risultati con la previsione di serie temporali in k-step. 
Gli autori analizzano le GRU con dimensioni nascoste di uno e due come sistemi dinamici a tempo continuo, sostenendo che la potenza espressiva della rappresentazione degli stati nascosti può fornire una conoscenza preliminare su quanto bene una GRU si comporterà su un dato set di dati
Questo articolo analizza le GRU da una prospettiva di sistemi dinamici, e mostra che le GRU 2d possono essere addestrate ad adottare una varietà di punti fissi e possono approssimare gli attrattori di linea, ma non possono imitare un attrattore ad anello.
Converte le equazioni di GRU in tempo continuo e usa la teoria e gli esperimenti per studiare reti GRU a 1 e 2 dimensioni e mostrare ogni varietà di topologia dinamica disponibile in questi sistemi
Gli input differenziati causano una differenziazione funzionale della rete, e l'interazione delle funzioni di perdita tra le reti può influenzare il processo di ottimizzazione.
Una modifica alla rete originale a clessidra per la stima della posa singola che produce miglioramenti rispetto alla linea di base originale.
Gli autori estendono una rete a clessidra impilata con moduli inception-resnet-A e propongono un approccio multiscala per la stima della posa umana in immagini RGB fisse.
Per addestrare un sentence embedding utilizzando documenti tecnici, il nostro approccio considera la struttura del documento per trovare un contesto più ampio e gestire le parole fuori dal vocabolario.
Presenta idee per migliorare l'incorporazione delle frasi attingendo a più contesti.
Apprendimento di rappresentazioni di frasi con informazioni sulle dipendenze delle frasi
Estende l'idea di formare una rappresentazione non supervisionata delle frasi usata nell'approccio SkipThough utilizzando un insieme più ampio di prove per formare la rappresentazione di una frase
Esploriamo la struttura delle funzioni di perdita neurali e l'effetto dei paesaggi di perdita sulla generalizzazione, usando una serie di metodi di visualizzazione.
Questo articolo propone un metodo per visualizzare la funzione di perdita di una NN e fornisce approfondimenti sull'addestrabilità e la generalizzazione delle NN.
Studia la non convessità della superficie di perdita e dei percorsi di ottimizzazione.
Dimostriamo che sfruttando una codifica dell'uscita a più vie, piuttosto che la codifica one-hot ampiamente utilizzata, possiamo rendere i modelli profondi più robusti agli attacchi avversari.
Questo articolo propone di sostituire lo strato finale di cross-entropia addestrato sulle etichette one-hot nei classificatori codificando ogni etichetta come un vettore ad alta densità e addestrando il classificatore per minimizzare la distanza L2 dalla codifica della classe corretta.
Gli autori propongono un nuovo metodo contro gli attacchi avversari che mostra una quantità significativa di guadagni rispetto alle linee di base
Introduciamo il primo modello NMT con decodifica completamente parallela, riducendo la latenza di inferenza di 10 volte.
Questo lavoro propone un decoder non autoregressivo per il quadro codificatore-decodificatore in cui la decisione di generare una parola non dipende dalla decisione precedente delle parole generate
Questo articolo descrive un approccio alla decodifica non autoregressiva per la traduzione automatica neurale con la possibilità di una decodifica più parallela che può risultare in un significativo aumento della velocità.
Propone l'introduzione di un insieme di variabili latenti per rappresentare la fertilità di ogni parola sorgente per rendere la generazione della frase target non autoregressiva
Dimostriamo un metodo certificabile, addestrabile e scalabile per difendersi dagli esempi avversari.
Propone una nuova difesa contro gli attacchi di sicurezza alle reti neurali con il modello di attacco che produce un certificato di sicurezza sull'algoritmo.
Deriva un limite superiore sulla perturbazione avversaria per le reti neurali con uno strato nascosto
proponiamo un regolatore che migliora le prestazioni di classificazione delle reti neurali
gli autori propongono di addestrare un modello da un punto di massimizzazione dell'informazione reciproca tra le previsioni e le uscite vere, con un termine di regolarizzazione che minimizza l'informazione irrilevante durante l'apprendimento.
Propone di decomporre i parametri in una mappa di caratteristiche invertibile F e una trasformazione lineare w nell'ultimo strato per massimizzare l'informazione reciproca I(Y, \hat{T}) mentre vincola l'informazione irrilevante
Questo articolo introduce un nuovo quadro di modellazione generativa che evita il collasso delle variabili latenti e chiarisce l'uso di alcuni fattori ad-hoc nell'addestramento degli autocodificatori variazionali.
L'articolo propone di risolvere il problema di un auto-encoder variazionale che ignora le variabili latenti.
Questo articolo propone di aggiungere un autoencoder stocastico al modello VAE originale per affrontare il problema che il decodificatore LSTM di un modello linguistico potrebbe essere troppo forte per ignorare le informazioni della variabile latente.
Questo articolo presenta AutoGen, che combina un autoencoder variazionale generativo con un modello di ricostruzione ad alta fedeltà basato su autoencoder per utilizzare meglio la rappresentazione latente
 Questo articolo studia il problema della divisione del dominio segmentando le istanze tratte da diverse distribuzioni probabilistiche.  
Questo articolo si occupa del problema del riconoscimento della novità nell'apprendimento di set aperti e nell'apprendimento generalizzato a zero colpi e propone una possibile soluzione
Un approccio alla separazione dei domini basato sul bootstrapping per identificare le soglie di cutoff di somiglianza per le classi note, seguito da un test di Kolmogorov-Smirnoff per raffinare le zone di in-distribuzione del bootstrapping.
Propone di introdurre un nuovo dominio, il dominio incerto, per gestire meglio la divisione tra domini visti/non visti nell'apprendimento a set aperto e a zero colpi generalizzato
SGD esegue implicitamente l'inferenza variazionale; il rumore del gradiente è altamente non isotropo, quindi SGD non converge nemmeno ai punti critici della perdita originale
Questo articolo fornisce un'analisi variazionale di SGD come processo di non-equilibrio.
Questo articolo discute la funzione obiettivo regolarizzata minimizzata da SGD standard nel contesto delle reti neurali, e fornisce una prospettiva di inferenza variazionale utilizzando l'equazione di Fokker-Planck.
Sviluppa una teoria per studiare l'impatto del rumore del gradiente stocastico per SGD, specialmente per i modelli di rete neurale profonda
Gli agenti possono imparare a imitare solo dimostrazioni visive (senza azioni) al momento del test dopo aver imparato dalla loro esperienza senza alcuna forma di supervisione al momento dell'addestramento.
Questo articolo propone un approccio per l'apprendimento visivo a zero colpi tramite l'apprendimento di funzioni di abilità parametriche.
Un articolo sull'imitazione di un compito presentato proprio durante l'inferenza, dove l'apprendimento viene eseguito in modo auto-supervisionato e durante l'addestramento l'agente esplora compiti correlati ma diversi.
Propone un metodo per aggirare il problema della costosa dimostrazione da parte di esperti utilizzando l'esplorazione casuale di un agente per imparare abilità generalizzabili che possono essere applicate senza uno specifico addestramento preliminare
L'articolo fornisce una descrizione di una procedura per migliorare il modello di spazio vettoriale delle parole con una valutazione dei modelli Paragram e GloVe per i benchmark di similarità.
Questo articolo suggerisce un nuovo algoritmo che regola i vettori di parole di GloVe e poi usa una funzione di somiglianza non euclidea tra loro.
Gli autori presentano osservazioni sulle debolezze dei modelli di spazio vettoriale esistenti ed elencano un approccio in 6 fasi per raffinare i vettori di parole esistenti
Proponiamo un nuovo metodo di quantizzazione e lo applichiamo per quantizzare le RNN sia per la compressione che per l'accelerazione
Questo articolo propone un metodo di quantizzazione multi-bit per reti neurali ricorrenti.
Una tecnica per quantizzare le matrici di peso delle reti neurali e una procedura di ottimizzazione alternata per stimare l'insieme di k vettori binari e coefficienti che rappresentano meglio il vettore originale.
Sostituiamo gli strati completamente connessi di una rete neurale con il multi-scala entanglement renormalization ansatz, un tipo di operazione quantistica che descrive le correlazioni a lungo raggio. 
Nell'articolo gli autori suggeriscono di usare la tecnica di tensorizzazione MERA per comprimere le reti neurali.
Una nuova parametrizzazione delle mappe lineari per l'uso delle reti neurali, utilizzando una fattorizzazione gerarchica della mappa lineare che riduce il numero di parametri pur permettendo di modellare interazioni relativamente complesse.
Studi che comprimono gli strati feed forward usando decomposizioni tensoriali di basso rango ed esplorano una decomposizione ad albero
Presentiamo un'analisi a colpo singolo di una rete neurale addestrata per rimuovere la ridondanza e identificare la struttura ottimale della rete
Questo articolo propone una serie di euristiche per identificare una buona architettura di rete neurale, basata sulla PCA delle attivazioni delle unità sul dataset
Questo articolo presenta un quadro per ottimizzare le architetture delle reti neurali attraverso l'identificazione dei filtri ridondanti tra gli strati
Conduciamo la prima analisi di sicurezza approfondita degli attacchi di fingerprinting DNN che sfruttano i canali laterali della cache, il che rappresenta un passo verso la comprensione della vulnerabilità della DNN agli attacchi side-channel.
Questo articolo considera il problema del fingerprinting delle architetture di reti neurali utilizzando i canali laterali della cache, e discute le difese di sicurezza attraverso l'oscurità.
Questo articolo esegue attacchi cache side-channel per estrarre gli attributi di un modello vittima e dedurre la sua architettura, oltre a mostrare che possono raggiungere una precisione di classificazione quasi perfetta.
Proponiamo il Complement Objective Training (COT), un nuovo paradigma di formazione che ottimizza sia gli obiettivi primari che quelli complementari per apprendere efficacemente i parametri delle reti neurali.
Considera l'aumento dell'obiettivo di cross-entropia con la massimizzazione dell'obiettivo "complemento", che mira a neutralizzare le probabilità previste di classi diverse dalle etichette di verità.
Gli autori propongono un obiettivo secondario per la minimizzazione di softmax basato sulla valutazione delle informazioni raccolte dalle classi errate, portando a un nuovo approccio di formazione.
Si occupa dell'addestramento di reti neurali per compiti di classificazione o generazione di sequenze utilizzando la perdita di entropia trasversale
Stima dell'incertezza in un singolo passaggio in avanti senza parametri aggiuntivi apprendibili.
Un nuovo metodo per calcolare le stime di incertezza dell'uscita nelle DNN per problemi di classificazione che corrisponde ai metodi all'avanguardia per la stima dell'incertezza e li supera nei compiti di rilevamento fuori distribuzione.
Gli autori presentano la softmax inibita, una modifica della softmax attraverso l'aggiunta di un'attivazione costante che fornisce una misura per l'incertezza. 
Abbiamo realizzato un sistema ricco di funzionalità per l'apprendimento profondo con input criptati, producendo output criptati, preservando la privacy.
Un quadro per l'inferenza privata del modello di apprendimento profondo utilizzando schemi FHE che supportano il bootstrapping veloce e quindi possono ridurre il tempo di calcolo.
L'articolo presenta un mezzo per valutare una rete neurale in modo sicuro usando la crittografia omomorfa.
Introduciamo un sistema chiamato GamePad per esplorare l'applicazione di metodi di apprendimento automatico alla dimostrazione di teoremi nell'assistente di prova Coq.
Questo articolo descrive un sistema per applicare l'apprendimento automatico alla dimostrazione interattiva dei teoremi, si concentra sui compiti di previsione della tattica e di valutazione della posizione, e mostra che un modello neurale supera un SVM su entrambi i compiti.
Propone che le tecniche di apprendimento automatico siano utilizzate per aiutare a costruire prove nel prover di teoremi Coq.
In questo articolo, abbiamo studiato l'addestramento efficiente delle reti a peso quantizzato con gradiente quantizzato in un ambiente distribuito, sia teoricamente che empiricamente.
Questo articolo studia le proprietà di convergenza della quantizzazione del peso consapevole delle perdite con diverse precisioni del gradiente nell'ambiente distribuito, e fornisce un'analisi di convergenza per la quantizzazione del peso con gradienti completi, quantizzati e quantizzati ritagliati.
Gli autori propongono un'analisi dell'effetto della quantizzazione simultanea dei pesi e dei gradienti nella formazione di un modello parametrizzato in un ambiente distribuito completamente sincronizzato.
Una strategia di regolarizzazione per migliorare le prestazioni dell'apprendimento sequenziale
Un nuovo approccio basato sulla regolarizzazione al problema dell'apprendimento sequenziale utilizzando un modello a dimensione fissa che aggiunge termini extra alla perdita, incoraggiando la sparsità della rappresentazione e combattendo la dimenticanza catastrofica.
Questo articolo affronta il problema della dimenticanza catastrofica nell'apprendimento permanente proponendo strategie di apprendimento regolarizzate
Una rete neurale sinaptica con grafico di sinapsi e apprendimento che ha la caratteristica della coniugazione topologica e della distribuzione di Bose-Einstein nello spazio di sorpresa.  
Gli autori propongono un lavoro neurale ibrido composto da un grafico di sinapsi che può essere incorporato in una rete neurale standard
Presenta un modello di rete neurale biologicamente ispirato basato sui canali ionici eccitatori e inibitori nelle membrane delle cellule reali
Modelli generalizzati di inclusione dei grafici
Un approccio generalizzato di embedding dei grafi di conoscenza che impara le embeddings basate su tre diversi obiettivi simultanei, ed esegue alla pari o addirittura supera gli approcci esistenti allo stato dell'arte.
Affronta il compito di apprendere embeddings di grafi multi-relazionali usando una rete neurale
Propone un nuovo metodo, GEN, per calcolare embeddings di grafi multirelazionali, in particolare che le cosiddette E-Cells e R-Cells possono rispondere a query della forma (h,r,?),(?r,t), e (h,?,t)
Il Minimax Curriculum Learning è un metodo di insegnamento automatico che comporta l'aumento della durezza desiderabile e la riduzione programmata della diversità.
 Un approccio di apprendimento curricolare che utilizza una funzione set submodulare che cattura la diversità degli esempi scelti durante la formazione. 
L'articolo introduce il MiniMax Curriculum learning come un approccio per l'addestramento adattivo dei modelli fornendo diversi sottoinsiemi di dati. 
Modelli impliciti applicati alla causalità e alla genetica
Gli autori propongono di utilizzare il modello implicito per affrontare il problema dell'associazione Genome-Wide.
Questo articolo propone soluzioni per i problemi negli studi di associazione genome-wide di confondimento dovuti alla struttura della popolazione e la potenziale presenza di interazioni non lineari tra diverse parti del genoma, e collega la genetica statistica e la ML.
Presenta un modello generativo non lineare per GWAS che modella la struttura della popolazione in cui le non linearità sono modellate utilizzando reti neurali come approssimatori di funzioni non lineari e l'inferenza viene eseguita utilizzando l'inferenza variazionale senza likelihood
Apprendimento a pochi scatti sfruttando la relazione a livello di oggetto per imparare la relazione a livello di immagine (somiglianza)
Questo articolo affronta il problema dell'apprendimento di pochi colpi proponendo un approccio basato sull'embedding che impara a confrontare le caratteristiche a livello di oggetto tra il supporto e gli esempi del set di query
Propone un metodo di apprendimento a pochi colpi che sfrutta la relazione a livello di oggetto tra diverse immagini basate sulla ricerca dei vicini vicini e concatena le mappe di caratteristiche di due immagini di input in una mappa di caratteristiche
I ricercatori che esplorano le tecniche di elaborazione del linguaggio naturale applicate al codice sorgente non stanno usando alcuna forma di embeddings pre-addestrati, noi dimostriamo che dovrebbero esserlo.
Questo articolo si prefigge di capire se la preformazione di embeddings di parole per il codice del linguaggio di programmazione utilizzando modelli linguistici simili a quelli della PNL ha un impatto sul compito di sintesi del codice estremo.
Questo lavoro mostra come il pre-addestramento dei vettori di parole usando corpus di codice porta a rappresentazioni che sono più adatte delle rappresentazioni inizializzate e addestrate a caso per la predizione dei nomi di funzioni/metodi
Risolviamo il cubo di Rubik con il puro apprendimento per rinforzo
Soluzione per risolvere il cubo di Rubik usando l'apprendimento di rinforzo (RL) con la ricerca ad albero di Monte-Carlo (MCTS) attraverso l'iterazione autodidattica. 
Questo lavoro risolve il cubo di Rubik usando un metodo di iterazione approssimativa delle politiche chiamato iterazione autodidattica, superando il problema delle ricompense sparse creando il proprio sistema di ricompense.
Introduce un algoritmo di RL profondo per risolvere il cubo di Rubik che gestisce l'enorme spazio di stato e la ricompensa molto sparsa del cubo di Rubik
Descriviamo un modello differenziabile end-to-end per l'AQ che impara a rappresentare spanne di testo nella domanda come denotazioni nel grafo della conoscenza, imparando sia moduli neurali per la composizione che la struttura sintattica della frase.
Questo articolo presenta un modello per la risposta a domande visive che può imparare sia i parametri che i predittori di struttura per una rete neurale modulare, senza strutture supervisionate o assistenza da un parser sintattico.
Propone di addestrare un modello di risposta alle domande a partire dalle sole risposte e da una KB imparando alberi latenti che catturano la sintassi e imparano la semantica delle parole
Introduciamo una nuova infrastruttura di compilatore che affronta le carenze dei framework esistenti per l'apprendimento profondo.
Proposta di passare dalla generazione di codice ad-hoc nei motori di apprendimento profondo alle migliori pratiche del compilatore e delle lingue.
Questo articolo presenta un framework per compilatori che permette la definizione di linguaggi specifici per i sistemi di apprendimento profondo e definisce fasi di compilazione che possono trarre vantaggio dalle ottimizzazioni standard e dalle ottimizzazioni specializzate per le reti neurali.
Questo articolo introduce una DLVM per sfruttare gli aspetti di compilazione di un compilatore tensoriale
Architettura basata sull'attenzione per la messa a terra della lingua attraverso l'apprendimento di rinforzo in un nuovo ambiente di griglia 2D personalizzabile  
L'articolo affronta il problema della navigazione data da un'istruzione e propone un approccio per combinare informazioni testuali e visive attraverso un meccanismo di attenzione
Questo articolo considera il problema di seguire istruzioni in linguaggio naturale data una visione in prima persona di un ambiente sconosciuto a priori, e propone un metodo di architettura neurale.
Studia il problema della navigazione verso un oggetto target in un ambiente a griglia 2D seguendo una descrizione in linguaggio naturale e ricevendo informazioni visive come pixel grezzi.
Una semplice architettura composta da convoluzioni e attenzione raggiunge risultati alla pari con i migliori modelli ricorrenti documentati.
Un metodo di aumento dei dati basato sulla parafrasi ad alte prestazioni e un modello di comprensione della lettura non ricorrente che usa solo convoluzioni e attenzione.
Questo articolo propone di applicare i moduli CNN+autoattenzione al posto degli LSTM e di migliorare l'addestramento del modello RC con le parafrasi dei passaggi generate da un modello neurale di parafrasi al fine di migliorare le prestazioni RC.
Questo articolo presenta un modello di comprensione della lettura utilizzando le convoluzioni e l'attenzione e propone di aumentare i dati di formazione aggiuntivi attraverso la parafrasi basata sulla traduzione automatica neurale off-the-shelf
Introduciamo Spherical CNNs, una rete convoluzionale per segnali sferici, e la applichiamo al riconoscimento di modelli 3D e alla regressione dell'energia molecolare.
L'articolo propone un quadro per la costruzione di reti convoluzionali sferiche basato su una nuova sintesi di diversi concetti esistenti
Questo articolo si concentra su come estendere le reti neurali convoluzionali per avere l'invarianza sferica incorporata, e adatta gli strumenti dell'analisi armonica non abeliana per raggiungere questo obiettivo.
Gli autori sviluppano un nuovo schema per rappresentare i dati sferici da zero
Un metodo per eseguire la progettazione automatizzata su oggetti del mondo reale come dissipatori di calore e profili alari che fa uso di reti neurali e discesa del gradiente.
Rete neurale (parametrizzazione e previsione) e gradient descent (back propogation) per progettare automaticamente i compiti di ingegneria. 
Questo articolo introduce l'uso di una rete profonda per approssimare il comportamento di un sistema fisico complesso, e poi progettare dispositivi ottimali ottimizzando questa rete rispetto ai suoi input.
 Proponiamo una versione duale della distanza logistico-avversaria per l'allineamento delle caratteristiche e mostriamo che produce iterazioni a gradiente più stabili dell'obiettivo min-max.
L'articolo si occupa di fissare le GAN a livello computazionale
Questo articolo studia una formulazione duale di una perdita avversaria basata su un upper-bound della perdita logistica, e trasforma il problema min max standard della formazione avversaria in un singolo problema di minimizzazione.
Propone di riformulare l'obiettivo del punto di sella del GAN (per un discriminatore di regressione logistica) come un problema di minimizzazione dualizzando l'obiettivo di massima verosimiglianza per la regressione logistica regolarizzata
implementazione delle prestazioni computazionali allo stato dell'arte delle reti neurali binarie
L'articolo presenta una libreria scritta in C/CUDA che presenta tutte le funzionalità necessarie per la propagazione in avanti delle BCNN
Questo documento si basa su Binary-NET e lo espande alle architetture CNN, fornisce ottimizzazioni che migliorano la velocità del passaggio in avanti, e fornisce codice ottimizzato per Binary CNN.
Proponiamo un algoritmo di selezione dei sottoinsiemi che è addestrabile con metodi basati sul gradiente ma che raggiunge prestazioni quasi ottimali tramite l'ottimizzazione submodulare.
Propone un modello basato sulla rete neurale che integra la funzione submodulare combinando la tecnica di ottimizzazione basata sul gradiente con un quadro submodulare chiamato 'Differentiable Greedy Network' (DGN).
Propone una rete neurale che mira a selezionare un sottoinsieme di elementi (ad esempio, selezionando k frasi che sono per lo più legate a un reclamo da un insieme di documenti recuperati)
Introduciamo l'apprendimento della rappresentazione gerarchicamente raggruppata (HCRL), che ottimizza simultaneamente l'apprendimento della rappresentazione e il raggruppamento gerarchico nello spazio di incorporazione.
L'articolo propone di usare il CRP annidato come un modello di clustering piuttosto che un modello di argomento
Presenta un nuovo metodo di clustering gerarchico su uno spazio di incorporazione in cui sia lo spazio di incorporazione che il clustering gerarchico sono appresi simultaneamente
Sviluppiamo un quadro di aumento dell'apprendimento non supervisionato statistico-geometrico per reti neurali profonde per renderle robuste agli attacchi avversari.
Trasforma le reti neurali profonde tradizionali in calssificatori robusti avversari utilizzando le GRN
Propone una difesa basata su distribuzioni di caratteristiche condizionali di classe per trasformare le reti neurali profonde in classificatori robusti
Rendere più efficiente l'apprendimento di rinforzo profondo in grandi spazi di stato-azione usando l'esplorazione strutturata con politiche gerarchiche profonde.
Un metodo per coordinare il comportamento degli agenti usando politiche che hanno una struttura latente condivisa, un metodo di ottimizzazione variazionale delle politiche per ottimizzare le politiche coordinate, e una derivazione dell'aggiornamento variazionale e gerarchico degli autori.
Questo articolo suggerisce un'innovazione algoritmica costituita da variabili latenti gerarchiche per l'esplorazione coordinata in ambienti multi-agente
Forniamo molte intuizioni sulla generalizzazione delle reti neurali dal caso lineare teoricamente trattabile.
Gli autori studiano un semplice modello di reti lineari per comprendere la generalizzazione e l'apprendimento di trasferimento
La normalizzazione dei lotti mantiene la varianza del gradiente durante l'allenamento, stabilizzando così l'ottimizzazione.
Questo articolo ha analizzato l'effetto della normalizzazione dei lotti sulla backpropagation a gradiente nelle reti residuali
I giudizi comportamentali umani sono utilizzati per ottenere rappresentazioni sparse e interpretabili degli oggetti che si generalizzano ad altri compiti
Questo articolo descrive un esperimento su larga scala sulle rappresentazioni umane di oggetti/semi e un modello di tali rappresentazioni.
Questo articolo sviluppa un nuovo sistema di rappresentazione per le rappresentazioni di oggetti dall'addestramento sui dati raccolti da giudizi umani dispari di immagini.
Un nuovo approccio per apprendere uno spazio semantico rado, positivo e interpretabile che massimizza i giudizi di somiglianza umana attraverso l'addestramento per massimizzare specificamente la previsione dei giudizi di somiglianza umana.
Proponiamo un agente che si pone tra l'utente e un sistema di risposta alle domande a scatola nera e che impara a riformulare le domande per ottenere le migliori risposte possibili
Questo articolo propone una risposta attiva alle domande attraverso un approccio di apprendimento di rinforzo che impara a riformulare le domande in modo da fornire le migliori risposte possibili.
Descrive chiaramente come i ricercatori hanno progettato e addestrato attivamente due modelli per la riformulazione delle domande e la selezione delle risposte durante gli episodi di risposta alle domande
Apprendimento di priori per autocodificatori adversariali
Propone una semplice estensione degli autocodificatori adversariali per la generazione di immagini condizionali.
Si concentra sugli autocodificatori avversari e introduce una rete di generatori di codice per trasformare un semplice priore in uno che insieme al generatore può adattarsi meglio alla distribuzione dei dati
Generazione di testo usando le embeddings di frasi dai vettori di pensiero saltati con l'aiuto delle reti generative adversariali.
Descrive l'applicazione delle reti generative avversarie per la modellazione di dati testuali con l'aiuto di vettori di pensiero e gli esperimenti con diversi tipi di GAN per due diversi set di dati.
Introduce una stima del gradiente online, imparziale e facilmente implementabile per modelli ricorrenti.
Gli autori introducono un nuovo approccio all'apprendimento online dei parametri delle reti neurali ricorrenti da lunghe sequenze che supera l'imitazione della backpropagation troncata nel tempo
Questo articolo affronta l'addestramento online di RNNs in un modo principesco, e propone una modifica a RTRL e di usare l'approccio forward per il calcolo del gradiente.
Super-risoluzione di etichette grossolane in etichette a livello di pixel, applicate a immagini aeree e scansioni mediche.
Un metodo per risolvere le etichette di segmentazione grossolane a bassa risoluzione se la distribuzione congiunta delle etichette a bassa e alta risoluzione è nota.
Proponiamo un metodo per allineare le caratteristiche latenti apprese da diversi set di dati utilizzando le correlazioni armoniche.
Propone l'uso di corrispondenze di caratteristiche per preformare l'allineamento manifold tra lotti di dati dagli stessi campioni per evitare la raccolta di misure rumorose.
L'evoluzione della forma del corpo negli agenti controllati da RL migliora le loro prestazioni (e aiuta l'apprendimento)
Algoritmo PEOM che incorpora il valore Shapley per accelerare l'evoluzione identificando il contributo di ogni parte del corpo
Modellare la ricompensa con la motivazione intrinseca per evitare stati catastrofici e mitigare l'oblio catastrofico.
Un algoritmo RL che combina l'algoritmo DQN con un modello di paura addestrato in parallelo per prevedere gli stati catastrofici.
L'articolo studia la dimenticanza catastrofica nella RL, enfatizzando i compiti in cui una DQN è in grado di imparare ad evitare eventi catastrofici finché evita la dimenticanza.
Un nuovo operatore di convoluzione per l'apprendimento automatico della rappresentazione all'interno della sfera di unità
Questo lavoro è legato ai recenti articoli sulle reti sferiche CNN e SE(n) equivarianti ed estende le idee precedenti ai dati volumetrici nella sfera unitaria.
Propone l'uso di convoluzioni volumetriche su reti di convoluzioni per imparare la palla di unità e discute la metodologia e i risultati del processo.
Addestriamo politiche di apprendimento di rinforzo utilizzando l'aumento della ricompensa, l'apprendimento del curriculum e il meta-apprendimento per navigare con successo le pagine web.
Sviluppa un metodo di apprendimento del curriculum per addestrare un agente RL a navigare in un web, basato sull'idea di decomporre un'istruzione in più sotto-istruzioni.
Classificazione del testo in più lingue mediante codifica universale
Questo articolo propone un approccio alla classificazione multilingue dei testi attraverso l'uso di corpora comparabili.
Apprendimento di embeddings multilingue e addestramento di un classificatore utilizzando dati etichettati nella lingua di origine per affrontare l'apprendimento di un categorizzatore di testo multilingue senza informazioni etichettate nella lingua di destinazione
In questo lavoro proponiamo deep inside-outside recursive auto-encoders (DIORA) un metodo completamente non supervisionato per scoprire la sintassi e contemporaneamente imparare rappresentazioni per i costituenti scoperti. 
Un modello neurale ad albero latente addestrato con un obiettivo di autocodifica che raggiunge lo stato dell'arte nel parsing non supervisionato dei costituenti e cattura la struttura sintattica meglio di altri modelli ad albero latente.
L'articolo propone un modello per il parsing non supervisionato delle dipendenze (induzione dell'albero latente) che è basato su una combinazione dell'algoritmo inside-outside con la modellazione neurale (autocodificatori ricorsivi). 
Indaghiamo la distorsione nell'obiettivo di meta-ottimizzazione a breve termine.
Questo articolo propone un modello e un problema semplificato per dimostrare la distorsione a breve termine della meta-ottimizzazione del tasso di apprendimento.
Questo articolo studia la questione della backpropagation troncata per la meta-ottimizzazione attraverso una serie di esperimenti su un problema giocattolo
un modo gerarchico e compositivo di generare didascalie
Questo articolo presenta un metodo più interpretabile per la didascalia delle immagini.
Sviluppiamo una nuova misura di complessità topologica per le reti neurali profonde e dimostriamo che cattura le loro proprietà salienti.
Questo articolo propone la nozione di persistenza neurale, una misura topologica per assegnare punteggi agli strati completamente connessi in una rete neurale.
L'articolo propone di analizzare la complessità di una rete neurale usando la sua omologia zero-esimo persistente.
Guardare i confini decisionali intorno a un input ti dà più informazioni di un piccolo quartiere fisso
Gli autori presentano un nuovo attacco per la generazione di esempi avversari in cui attaccano i classificatori creati classificando casualmente L2 piccole perturbazioni
Un nuovo approccio per generare attacchi avversari a una rete neurale e un metodo per difendere una rete neurale da questi attacchi.
Addestriamo una rete neurale per produrre pesi approssimativamente ottimali in funzione degli iperparametri.
Iper-reti per l'ottimizzazione degli iper-parametri nelle reti neurali.
Stima della matrice di covarianza delle attività finanziarie con modelli di variabili latenti a processo gaussiano
Illustra come il Gaussian Process Latent Variable Model (GP-LVM) può sostituire i classici modelli a fattori lineari per la stima delle matrici di covarianza nei problemi di ottimizzazione del portafoglio.
Questo articolo utilizza GPLVM standard per modellare la struttura di covarianza e una rappresentazione dello spazio latente delle serie temporali finanziarie S&P500, per ottimizzare i portafogli e prevedere i valori mancanti.
Questo articolo propone di utilizzare una GPLVM per modellare i rendimenti finanziari
Introduciamo l'apprendimento meta-adversariale, una nuova tecnica per regolarizzare le GAN, e proponiamo un metodo di formazione controllando esplicitamente la distribuzione dell'uscita del discriminatore.
L'articolo propone l'apprendimento adversariale regolarizzante della varianza per l'addestramento delle GAN per assicurare che il gradiente del generatore non svanisca
Un agente di apprendimento di rinforzo profondo con rumore parametrico aggiunto ai suoi pesi può essere utilizzato per aiutare l'esplorazione efficiente.
Questo articolo introduce le NoisyNets, reti neurali i cui parametri sono perturbati da una funzione di rumore parametrico, che ottengono un sostanziale miglioramento delle prestazioni rispetto agli algoritmi di apprendimento di rinforzo profondo di base.
Nuovo metodo di esplorazione per la RL profonda iniettando rumore nei pesi delle reti profonde, con il rumore che assume varie forme
"Active Neural Localizer", una rete neurale completamente differenziabile che impara a localizzare in modo efficiente utilizzando l'apprendimento di rinforzo profondo.
Questo articolo formula il problema della localizzazione su una mappa nota usando una rete di credenze come un problema RL in cui l'obiettivo dell'agente è di minimizzare il numero di passi per localizzarsi.
Questo è un documento chiaro e interessante che costruisce una rete parametrizzata per selezionare azioni per un robot in un ambiente simulato
Sviluppiamo un algoritmo di addestramento per modelli di traduzione automatica non autoregressivi, ottenendo un'accuratezza paragonabile alle basi autoregressive forti, ma un ordine di grandezza più veloce nell'inferenza.  
Distilla la conoscenza dagli stati nascosti intermedi e dai pesi di attenzione per migliorare la traduzione automatica neurale non autoregressiva.
Propone di sfruttare il modello autoregressivo ben addestrato per informare gli stati nascosti e l'allineamento delle parole dei modelli di traduzione automatica non autoregressivi.
Usando le operazioni mofologiche (dilatazione ed erosione) abbiamo definito una classe di rete che può approssimare qualsiasi funzione continua. 
Questo articolo propone di sostituire le unità RELU/tanh standard con una combinazione di operazioni di dilatazione ed erosione, osservando che il nuovo operatore crea più iperpiani e ha più potenza espressiva.
Gli autori introducono Morph-Net, una rete neurale a singolo strato in cui la mappatura viene eseguita utilizzando la dilatazione e l'erosione morfologica.
Questo lavoro fa avanzare la compressione DNN oltre i pesi alle attivazioni integrando il pruning di attivazione con il pruning dei pesi. 
Un metodo di compressione integrale del modello che gestisce sia il pruning dei pesi che l'attivazione, portando a un calcolo di rete più efficiente e a una riduzione efficace del numero di moltiplicazioni e accumulazioni.
Questo articolo presenta un nuovo approccio per ridurre il costo di calcolo delle reti neurali profonde integrando la potatura dell'attivazione insieme alla potatura dei pesi e mostra che le tecniche comuni di potatura esclusiva dei pesi aumenta il numero di attivazioni non nulle dopo ReLU.
Proponiamo un metodo semplice per addestrare Variational Auto Encoders (VAE) con rappresentazioni latenti discrete, utilizzando il campionamento di importanza
Introdurre una distribuzione di campionamento d'importanza e usare campioni dalla distribuzione per calcolare la stima ponderata d'importanza del gradiente
Questo articolo propone di usare il campionamento importante per ottimizzare VAE con variabili latenti discrete.
Un nuovo algoritmo distribuito asincrono SGD che raggiunge un'accuratezza allo stato dell'arte sulle architetture esistenti senza alcuna sintonizzazione aggiuntiva o overhead.
Propone un miglioramento degli approcci ASGD esistenti a medie dimensioni utilizzando il momentum con SGD per la formazione asincrona attraverso un pool di lavoratori distribuiti.
Questo articolo affronta il problema della staltezza del gradiente contro le prestazioni parallele nell'addestramento distribuito dell'apprendimento profondo, e propone un approccio per stimare i futuri parametri del modello agli schiavi per ridurre gli effetti della latenza di comunicazione.
Proponiamo un nuovo algoritmo di apprendimento delle reti neurali profonde, che sblocca la dipendenza a livello di backpropagation.
Un paradigma di addestramento alternativo per le DNI in cui il modulo ausiliario è addestrato per approssimare direttamente l'output finale del modello originale, offrendo benefici collaterali.
Descrive un metodo di addestramento delle reti neurali senza blocco degli aggiornamenti.
Fornisce una versione imparziale della backpropagation troncata campionando le lunghezze di troncamento e ripesando di conseguenza.
Propone metodi di determinazione stocastica per i punti di troncamento nella backpropagation attraverso il tempo.
Una nuova approssimazione alla backpropagation attraverso il tempo per superare i carichi computazionali e di memoria che sorgono quando si deve imparare da lunghe sequenze.
attacco non mirato e mirato su GCN aggiungendo nodi falsi
Gli autori propongono una nuova tecnica avversaria per aggiungere nodi "falsi" per ingannare un classificatore basato su GCN
Apprendimento di trasferimento per la sequenza tramite l'apprendimento per allineare le informazioni a livello di cellula attraverso i domini.
L'articolo ha proposto di usare RNN/LSTM con allineamento di collocazione come un metodo di apprendimento della rappresentazione per l'apprendimento di trasferimento/adattamento al dominio in NLP.
Formuliamo l'incertezza del modello nel Reinforcement Learning come un processo decisionale continuo di Bayes-Adaptive Markov e presentiamo un metodo per l'ottimizzazione pratica e scalabile della politica bayesiana.
Usando un approccio bayesiano, c'è un migliore compromesso tra esplorazione e sfruttamento nella RL
Noi sosteniamo che i benchmark GAN devono richiedere un grande campione dal modello per penalizzare la memorizzazione e indagare se le divergenze delle reti neurali hanno questa proprietà.
Gli autori propongono un criterio per valutare la qualità dei campioni prodotti da una Generative Adversarial Network.
generazione di dialogo a dominio aperto con atti di dialogo
Gli autori usano una tecnica di supervisione a distanza per aggiungere tag di atti di dialogo come fattore condizionante per generare risposte nei dialoghi a dominio aperto
L'articolo descrive una tecnica per incorporare atti di dialogo in agenti di conversazione neurali
La nostra ipotesi è che, dati due domini, la mappatura di minor complessità che ha una bassa discrepanza approssima la mappatura di destinazione.
L'articolo affronta il problema dell'apprendimento di mappature tra diversi domini senza alcuna supervisione, affermando tre congetture.
Dimostra che nell'apprendimento non supervisionato su dati non allineati è possibile imparare la mappatura tra domini usando solo GAN senza una perdita di ricostruzione.
Raffiniamo i risultati di sovra-approssimazione dei verificatori incompleti usando solutori MILP per dimostrare proprietà di robustezza maggiori rispetto allo stato dell'arte. 
Introduce un verificatore che ottiene un miglioramento della precisione dei verificatori incompleti e della scalabilità dei verificatori completi usando la sovra-parametrizzazione, la programmazione lineare intera mista e il rilassamento della programmazione lineare.
Una strategia mista per ottenere una migliore precisione nelle verifiche di robustezza delle reti neurali a feed-forward con funzioni di attivazione lineari piecewise, ottenendo una migliore precisione rispetto ai verificatori incompleti e una maggiore scalabilità rispetto ai verificatori completi.
Le HMM sono un caso speciale di RNN? Indaghiamo una serie di trasformazioni architettoniche tra HMMs e RNNs, sia attraverso derivazioni teoriche che ibridazioni empiriche e forniamo nuove intuizioni.
Questo articolo esplora se le HMM sono un caso speciale di RNN utilizzando la modellazione del linguaggio e il POS tagging
Proponiamo un nuovo metodo di regolarizzazione che penalizza la covarianza tra le dimensioni degli strati nascosti in una rete.
Questo articolo presenta un meccanismo di regolarizzazione che penalizza la covarianza tra tutte le dimensioni nella rappresentazione latente di una rete neurale al fine di dissociare la rappresentazione latente
Lo schema proposto imita il processo di classificazione mediato da una serie di prelievi di un componente.
Un metodo per aumentare l'accuratezza delle reti profonde su compiti di classificazione multiclasse apparentemente tramite una riduzione della classificazione multiclasse a quella binaria.
Una nuova procedura di classificazione di discernimento, risposta massima e controllo multiplo per migliorare la precisione delle reti mediocri e migliorare le reti feedforward.
Empiricamente dimostra che i modelli più grandi si allenano in meno passi di allenamento, perché tutti i fattori di attraversamento dello spazio dei pesi migliorano.
Questo articolo mostra che RNN più larghe migliorano la velocità di convergenza quando sono applicate a problemi NLP, e per estensione l'effetto di aumentare le larghezze nelle reti neurali profonde sulla convergenza dell'ottimizzazione
Questo articolo caratterizza l'impatto della sovra-parametrizzazione nel numero di iterazioni necessarie a un algoritmo per convergere, e presenta ulteriori osservazioni empiriche sugli effetti della sovra-parametrizzazione nell'addestramento delle reti neurali.
Reti di puntatori a più teste per l'apprendimento congiunto per localizzare e riparare i bug di abuso di variabili
Propone un modello basato su LSTM con puntatori per rompere il problema del VarMisuse in più fasi.
Questo articolo presenta un modello basato su LSTM per il rilevamento e la riparazione del bug VarMisuse, e dimostra miglioramenti significativi rispetto agli approcci precedenti su diversi set di dati.
Clustering di tipo umano con CNN
L'articolo convalida l'idea che le reti neurali convoluzionali profonde potrebbero imparare a raggruppare i dati di input meglio di altri metodi di raggruppamento notando la loro capacità di interpretare il contesto di ogni punto di input grazie a un ampio campo visivo.
Questo lavoro combina l'apprendimento profondo per la rappresentazione delle caratteristiche con il compito di raggruppamento non supervisionato simile a quello umano.
Sviluppiamo due algoritmi a complessità lineare per l'interpretazione di modelli agnostici basati sul valore di Shapley, nelle impostazioni in cui il contributo delle caratteristiche all'obiettivo è ben approssimato da una fattorizzazione strutturata a grafo.
L'articolo propone due approssimazioni al valore di Shapley usato per generare i punteggi delle caratteristiche per l'interpretabilità.
Questo articolo propone due metodi per la valutazione dell'importanza delle caratteristiche in base all'istanza utilizzando i valori di Shapely, e fornisce due metodi efficienti per calcolare i valori di Shapely approssimati quando c'è una struttura nota che mette in relazione le caratteristiche.
Sono stati trovati codici locali nelle reti neurali feed-forward
Un metodo per determinare fino a che punto i singoli neuroni in uno strato nascosto di una MLP codificano un codice localista, che viene studiato per diverse rappresentazioni di input.
Studia lo sviluppo di rappresentazioni localistiche negli strati nascosti delle reti neurali feed-forward.
Estensione della modellazione relazionale per supportare i dati multimodali usando codificatori neurali.
Questo articolo propone di eseguire la predizione dei link nelle basi di conoscenza integrando le entità originali con informazioni multimodali, e presenta un modello in grado di codificare tutti i tipi di informazioni quando si segnano le triple.
L'articolo riguarda l'incorporazione di informazioni da diverse modalità negli approcci di predizione dei collegamenti
Proporre un nuovo metodo integrando il campionamento SG-MCMC, il priore sparso di gruppo e la potatura della rete per imparare Sparse Structured Ensemble (SSE) con prestazioni migliori e costi significativamente ridotti rispetto ai metodi tradizionali. 
Gli autori propongono una procedura per generare un insieme di modelli strutturati sparsi
Un nuovo quadro per l'addestramento di reti neurali ensemble che utilizza metodi SG-MCMC all'interno dell'apprendimento profondo, e poi aumenta l'efficienza computazionale tramite sparsità+pruning di gruppo.
Questo articolo esplora l'uso di FNN e LSTM per rendere la media dei modelli bayesiani più computabile e migliorare le prestazioni medie dei modelli.
Nuova struttura per il meta-apprendimento che unifica ed estende un'ampia classe di metodi di apprendimento a pochi colpi esistenti. Raggiunge una forte performance sui benchmark di apprendimento a pochi colpi senza richiedere un'inferenza iterativa a tempo di test.   
Questo lavoro affronta l'apprendimento a pochi colpi da un punto di vista di inferenza probabilistica, raggiungendo lo stato dell'arte nonostante una configurazione più semplice di molti concorrenti
Definizione di una perdita softmax parzialmente mutua esclusiva per dati positivi e implementazione di uno schema di campionamento basato sulla cooperazione
Questo articolo presenta il Cooperative Importance Sampling per risolvere il problema dell'assunzione reciprocamente esclusiva del softmax tradizionale che è distorto quando i campioni negativi non sono esplicitamente definiti
Questo articolo propone metodi PMES per rilassare l'ipotesi di risultato esclusivo nella perdita softmax, dimostrando il merito empirico nel migliorare i modelli di embedding di tipo word2vec.
Struttura insegnante-studente per una classificazione video efficiente utilizzando un minor numero di fotogrammi 
L'articolo propone un'idea per distillare da un modello di classificazione video completo un piccolo modello che riceve solo un numero minore di fotogrammi.
Gli autori presentano una rete insegnante-allievo per risolvere il problema della classificazione dei video, proponendo algoritmi di addestramento seriali e paralleli volti a ridurre i costi computazionali.
Una visione statistica unificata dell'ampia classe di modelli generativi profondi 
L'articolo sviluppa un quadro che interpreta gli algoritmi GAN come l'esecuzione di una forma di inferenza variazionale su un modello generativo che ricostruisce una variabile indicatrice del fatto che un campione appartenga alla vera delle distribuzioni di dati generativi.
un metodo che combina l'apprendimento di liste di regole e l'apprendimento di prototipi 
Presenta un nuovo quadro di predizione interpretabile, che combina l'apprendimento basato su regole, l'apprendimento di prototipi e le NN, che è particolarmente applicabile ai dati longitudinali.
Questo articolo mira ad affrontare la mancanza di interpretabilità dei modelli di deep learning, e propone Prototype lEArning via Rule Lists (PEARL), che combina l'apprendimento delle regole e l'apprendimento dei prototipi per ottenere una classificazione più accurata e rende il compito di interpretabilità più semplice.
Questo articolo propone un nuovo Generative Adversarial Network che è più stabile, più efficiente e produce immagini migliori di quelle dello status-quo 
Questo documento combina Fisher-GAN e Deli-GAN
Questo documento combina Deli-GAN, che ha una distribuzione a priori mista nello spazio latente, e Fisher GAN, che usa Fisher IPM invece di JSD come obiettivo.
Introduciamo un'architettura di rete modulare multisensore con un meccanismo attenzionale che permette la selezione dinamica del sensore su dati rumorosi del mondo reale da CHiME-3.
Un'architettura neurale generica in grado di apprendere l'attenzione che deve essere prestata ai diversi canali di ingresso a seconda della qualità relativa di ogni sensore rispetto agli altri.
 Considera l'uso dell'attenzione per la selezione del sensore o del canale con risultati su TIDIGITS e GRID che mostrano un beneficio dell'attenzione rispetto alla concatenazione delle caratteristiche.
Per consentire l'addestramento DNN basato sul cloud e proteggere contemporaneamente la privacy dei dati, proponiamo di sfruttare le rappresentazioni intermedie dei dati, il che si ottiene dividendo le DNN e distribuendole separatamente su piattaforme locali e sul cloud.
Questo articolo propone una tecnica per privatizzare i dati imparando una rappresentazione delle caratteristiche che è difficile da usare per la ricostruzione delle immagini, ma utile per la classificazione delle immagini.
GAN ricorrenti condizionali per la generazione di sequenze mediche con valore reale, mostrando nuovi approcci di valutazione e un'analisi empirica della privacy.
Propone di utilizzare i dati sintetici generati da GANs come sostituzione dei dati personali identificabili nell'addestramento di modelli ML per applicazioni sensibili alla privacy
Gli autori propongono una nuova architettura GAN ricorrente che genera sequenze di dominio continue, e la valutano su diversi compiti sintetici e su un compito di dati ICU timeseries.
Propone di utilizzare RGAN e RCGAN per generare sequenze sintetiche di dati reali.
I nostri studi e modelli empirici forniscono nuove informazioni preziose per i designer che vogliono capire e controllare come gli effetti dell'enfasi saranno percepiti dagli utenti
Questo articolo considera quale evidenziazione visiva viene percepita più velocemente nella visualizzazione dei dati e come i diversi metodi di evidenziazione si confrontano tra loro
Due studi sull'efficacia degli effetti di enfasi, uno che valuta i livelli di differenze utili, e uno più applicato utilizzando visualizzazioni effettive diverse per un'indagine più ecologicamente valida.
Una semplice architettura di ragionamento basata sulla rete di memoria (MemNN) e la rete di relazioni (RN), riducendo la complessità temporale rispetto alla RN e raggiungendo lo stato dell'arte del risultato sulla storia bAbI basata su QA e sul dialogo bAbI.
Introduce la Rete di memoria correlata (RMN), un miglioramento rispetto alle Reti di relazioni (RN).
Mostriamo che dividere una rete neurale in rami paralleli migliora le prestazioni e che un corretto accoppiamento dei rami migliora ulteriormente le prestazioni.
Il lavoro propone una riconfigurazione del modello CNN esistente allo stato dell'arte utilizzando una nuova architettura di ramificazione, con prestazioni migliori.
Questo articolo mostra i benefici di risparmio di parametri dell'ensembling accoppiato.
Presenta un'architettura di rete profonda che elabora i dati utilizzando più rami paralleli e combina il posteriore di questi rami per calcolare i punteggi finali.
Combina l'iniezione di rumore, la quantizzazione graduale e l'apprendimento del clamping di attivazione per ottenere una quantizzazione a 3, 4 e 5 bit all'avanguardia
Propone di iniettare rumore durante l'addestramento e di bloccare i valori dei parametri in uno strato così come l'output di attivazione nella quantizzazione della rete neurale.
Un metodo per la quantizzazione delle reti neurali profonde per la classificazione e la regressione, utilizzando l'iniezione di rumore, il clamping con attivazioni massime apprese e la quantizzazione graduale a blocchi per eseguire alla pari o meglio dei metodi all'avanguardia.
Proponiamo Leap, una struttura che trasferisce la conoscenza tra i processi di apprendimento minimizzando la distanza prevista che il processo di formazione percorre sulla superficie di perdita di un compito.
L'articolo propone un nuovo obiettivo di meta-apprendimento volto a superare gli approcci allo stato dell'arte quando si tratta di collezioni di compiti che mostrano una sostanziale diversità tra i compiti
Un'alternativa al transfer learning che impara più velocemente, richiede molti meno parametri (3-13%), di solito raggiunge risultati migliori e conserva precisamente le prestazioni sui vecchi compiti.
Moduli di controllo per l'apprendimento incrementale su set di dati di classificazione delle immagini
Presentiamo una tecnica generale per l'inferenza a 8 bit a bassa precisione delle reti neurali convoluzionali. 
Questo articolo progetta un sistema per quantizzare automaticamente i modelli preaddestrati CNN
Proponiamo di incorporare bias induttivi e operazioni provenienti dalla geometria iperbolica per migliorare il meccanismo di attenzione delle reti neurali.
Questo articolo sostituisce la similitudine punto-prodotto usata nei meccanismi di attenzione con la distanza iperbolica negativa, e la applica al modello Transformer esistente, alle reti di attenzione a grafo e alle reti di relazioni
Gli autori propongono un nuovo approccio per migliorare l'attenzione relazionale cambiando la corrispondenza e le funzioni di aggregazione per utilizzare la geometrica iperbolica. 
Questo articolo dimostra come la teoria del controllo H-infinity può aiutare a progettare meglio politiche profonde e robuste per i taks dei motori dei robot
Propone di incorporare elementi di controllo robusto nella ricerca sulla politica guidata per concepire un metodo che sia resiliente alle perturbazioni e al mismatch del modello.
L'articolo presenta un metodo per valutare la sensibilità e la robustezza delle politiche di RL profonde, e propone un approccio di gioco dinamico per l'apprendimento di politiche robuste.
Analisi della vulnerabilità dei classificatori alle perturbazioni universali e relazione alla curvatura del confine di decisione.
L'articolo fornisce un'interessante analisi che collega la geometria dei confini decisionali del classificatore a piccole perturbazioni universali avversarie.
Questo articolo discute le perturbazioni universali - perturbazioni che possono fuorviare un classificatore addestrato se aggiunte alla maggior parte dei punti di dati di input.
L'articolo sviluppa modelli che tentano di spiegare l'esistenza di perturbazioni universali che ingannano le reti neurali
Proponiamo un metodo di meta-apprendimento per la correzione interattiva delle politiche con il linguaggio naturale.
Questo articolo fornisce un quadro di meta apprendimento che mostra come apprendere nuovi compiti in una configurazione interattiva. Ogni compito viene appreso attraverso un setup di apprendimento di rinforzo, e poi il compito viene aggiornato osservando nuove istruzioni.
Questo documento insegna agli agenti a completare compiti tramite istruzioni in linguaggio naturale in un processo iterativo.
Studiamo la modularità dei modelli generativi profondi.
L'articolo fornisce un modo per indagare la struttura modulare del modello generativo profondo, con il concetto chiave di distribuire su canali di architetture generatrici.
Introduciamo Seq2SQL, che traduce le domande in query SQL utilizzando le ricompense dell'esecuzione delle query online, e WikiSQL, un set di tabelle/domande/domande SQL di ordini di grandezza maggiori dei set di dati esistenti.
Un nuovo set di dati di parsing semantico che si concentra sulla generazione di SQL dal linguaggio naturale utilizzando un modello basato sul reinforcement learning
La modellazione del rumore in ingresso durante l'addestramento discriminativo migliora la robustezza avversaria. Proporre una metrica di valutazione basata sulla PCA per la robustezza avversaria
Questo articolo propone, ExL, un metodo di formazione adversariale che utilizza il rumore moltiplicato e che si dimostra utile per difendere dagli attacchi blackbox su tre serie di dati.
Questo documento include il rumore moltiplicativo N nei dati di formazione per raggiungere la robustezza avversaria, quando si allena sia sui parametri del modello theta che sul rumore stesso.
Un metodo per rispondere a "perché non la classe B?" per spiegare le reti profonde
L'articolo propone un approccio per fornire spiegazioni visive contrastanti per le reti neurali profonde.
Analizziamo l'invertibilità delle reti neurali profonde studiando le preimmagini degli strati ReLU e la stabilità dell'inverso.
Questo articolo studia il volume di preimmagine dell'attivazione di una rete ReLU ad un certo strato, e si basa sulla linearità frammentaria della funzione forward di una rete ReLU. 
Questo articolo presenta un'analisi dell'invarianza inversa delle reti ReLU e fornisce limiti superiori sui valori singolari di una rete di treni.
L'addestramento avversario degli ensemble fornisce robustezza agli esempi avversari oltre a quella osservata nei modelli addestrati in modo avversario e negli ensemble addestrati in modo indipendente.
 Propone di addestrare un insieme di modelli congiuntamente, dove ad ogni passo temporale, un insieme di esempi che sono avversi all'insieme stesso è incorporato nell'apprendimento.
reti di routing: un nuovo tipo di rete neurale che impara a instradare in modo adattivo il suo input per l'apprendimento multi-task
L'articolo suggerisce di usare una rete modulare con un controllore che prende decisioni, ad ogni passo temporale, riguardo al prossimo nodulo da applicare.
L'articolo presenta una nuova formulazione per l'apprendimento dell'architettura ottimale di una rete neurale in un quadro di apprendimento multi-task utilizzando l'apprendimento di rinforzo multi-agente per trovare una politica, e mostra un miglioramento rispetto alle architetture hard-coded con strati condivisi.
Mostriamo come ottimizzare la norma L_0 attesa dei modelli parametrici con la discesa del gradiente e introduciamo una nuova distribuzione che facilita l'hard gating.
Gli autori introducono un approccio basato sul gradiente per minimizzare una funzione obiettivo con una penalità L0 sparsa per aiutare l'apprendimento di reti neurali sparse
Proponiamo una nuova architettura di Rete Neurale Grafica interpretabile basata sull'attenzione che supera le attuali Reti Neurali Grafiche allo stato dell'arte in set di dati standard di riferimento
Gli autori propongono due estensioni delle GCN, rimuovendo le non linearità intermedie dal calcolo GCN e aggiungendo un meccanismo di attenzione nel livello di aggregazione.
L'articolo propone un algoritmo di apprendimento semi-supervisionato per la classificazione dei nodi dei grafi, ispirato alle Reti Neurali Grafiche.
Un quadro per l'addestramento di modelli generativi basati su autoencoder, con perdite non-adversariali e architetture di reti neurali non limitate.
Questo articolo usa autoencoder per fare la corrispondenza della distribuzione in uno spazio ad alta dimensione.
Gli spazi di incorporamento di manifold prodotti con curvatura eterogenea producono rappresentazioni migliori rispetto agli spazi di incorporamento tradizionali per una varietà di strutture.
Propone un metodo di riduzione della dimensionalità che incorpora i dati in un collettore prodotto di collettori sferici, euclidei e iperbolici. L'algoritmo si basa sulla corrispondenza delle distanze geodetiche sul prodotto manifold alle distanze dei grafici.
Integriamo metodi simbolici (deduttivi) e statistici (basati sui neuroni) per permettere la sintesi di programmi in tempo reale con una generalizzazione quasi perfetta da 1 esempio di input-output.
L'articolo presenta un approccio branch-and-bound per imparare buoni programmi dove un LSTM è usato per predire quali rami nell'albero di ricerca dovrebbero portare a buoni programmi
Propone un sistema che sintetizza programmi da un singolo esempio che generalizzano meglio dello stato dell'arte precedente
Esploriamo l'intersezione di VAEs e la codifica sparsa.
Questo articolo propone un'estensione delle VAE con priori e posteriori sparsi per imparare rappresentazioni sparse interpretabili.
L'eliminazione graduale delle connessioni saltanti in un modo di principio evita la degradazione nelle reti feed-forward profonde.
Gli autori presentano una nuova strategia di addestramento, VAN, per l'addestramento di reti feed-forward molto profonde senza saltare le connessioni
L'articolo introduce un'architettura che interpola linearmente tra ResNets e le reti profonde vaniglia senza saltare le connessioni.
Compressione di reti neurali profonde distribuite su dispositivi embedded. 
Gli autori presentano un algoritmo di formazione basato su SVRG regolarizzato l-1 che è in grado di forzare molti pesi della rete ad essere 0.
Questo lavoro riduce i requisiti di memoria.
Un algoritmo di apprendimento basato sulla codifica predittiva per costruire modelli di reti neurali profonde del cervello
L'articolo considera l'apprendimento di una rete neurale generativa utilizzando una configurazione di codifica predittiva
Il riconoscimento di istanze di oggetti con autocodificatori avversari è stato eseguito con un nuovo obiettivo "immagine mentale" che è una rappresentazione canonica dell'immagine di input.
L'articolo propone un metodo per imparare le caratteristiche per il riconoscimento degli oggetti che è invariante a varie trasformazioni dell'oggetto, in particolare la posa dell'oggetto.
Questo articolo ha studiato il compito di riconoscimento di pochi scatti tramite un'immagine mentale generata come rappresentazione intermedia data l'immagine di input.
Combinare la logica temporale con l'apprendimento di rinforzo gerarchico per la composizione delle abilità
L'articolo offre una strategia per costruire un MDP prodotto da un MDP originale e l'automa associato a una formula LTL.
Propone di unire la logica temporale con l'apprendimento gerarchico di rinforzo per semplificare la composizione delle abilità.
Proponiamo uno schema di quantizzazione per i pesi e le attivazioni delle reti neurali profonde. Questo riduce l'impronta di memoria in modo sostanziale e accelera l'inferenza.
Compressione del modello CNN e accelerazione dell'inferenza usando la quantizzazione.
Quando un robot viene schierato in un ambiente in cui gli umani hanno agito, lo stato dell'ambiente è già ottimizzato per ciò che gli umani vogliono, e possiamo usare questo per dedurre le preferenze umane.
Gli autori propongono di aumentare la funzione di ricompensa esplicitamente dichiarata di un agente RL con ricompense/costi ausiliari dedotti dallo stato iniziale e un modello della dinamica dello stato
Questo lavoro propone un modo per dedurre le informazioni implicite nello stato iniziale usando IRL e combinare la ricompensa dedotta con una ricompensa specificata.
Categorizzazione sistematica dei metodi di regolarizzazione per l'apprendimento profondo, rivelando le loro somiglianze.
Tenta di costruire una tassonomia per le tecniche di regolarizzazione impiegate nell'apprendimento profondo.
Dimostriamo l'efficienza esponenziale delle reti neurali di tipo ricorrente rispetto alle reti poco profonde.
Gli autori confrontano la complessità delle reti a treno di tensori con reti strutturate dalla decomposizione CP
Un nuovo trattamento probabilistico per GAN con garanzia teorica.
Questo articolo propone una GAN bayesiana che ha garanzie teoriche di convergenza alla distribuzione reale e mette le verosimiglianze sul generatore e sul discriminatore con logaritmi proporzionali alle funzioni obiettivo GAN tradizionali.
Difesa contro le perturbazioni avversarie delle reti neurali dall'assunzione di manifold 
Il manoscritto propone due funzioni obiettivo basate sull'assunzione di manifold come meccanismi di difesa contro gli esempi avversari.
Difendersi dagli attacchi avversari basati sull'assunzione di manifold di dati naturali
ricerca dell'architettura neurale a colpo singolo tramite ottimizzazione diretta sparsa
Presenta un metodo di ricerca dell'architettura in cui le connessioni vengono rimosse con una regolarizzazione sparsa.
Questo articolo propone Direct Sparse Optimization, che è un metodo per ottenere architetture neurali su problemi specifici, ad un costo computazionale ragionevole.
Questo articolo propone un metodo di ricerca dell'architettura neurale basato su un'ottimizzazione sparsa diretta
Ottiene una precisione allo stato dell'arte per reti quantizzate e poco profonde sfruttando la distillazione. 
Propone modelli piccoli e a basso costo combinando distillazione e quantizzazione per esperimenti di visione e traduzione automatica neurale
Questo articolo presenta un quadro di utilizzo del modello di insegnante per aiutare la compressione del modello di apprendimento profondo nel contesto della compressione del modello.
migliorare NMT con alberi latenti
Questo articolo descrive un metodo per indurre strutture di dipendenza dal lato sorgente al servizio della traduzione automatica neurale.
Imparate lavorando a ritroso da una singola dimostrazione, anche inefficiente, e progressivamente fate fare all'agente una parte maggiore della risoluzione.
Questo articolo presenta un metodo per aumentare l'efficienza dei metodi RL a ricompensa sparsa attraverso un curriculum a ritroso sulle dimostrazioni di esperti. 
L'articolo presenta una strategia per risolvere compiti di ricompensa sparsi con RL campionando gli stati iniziali dalle dimostrazioni.
Memoria esterna per l'apprendimento di rinforzo online basato sulla stima dei gradienti su una nuova tecnica di campionamento del serbatoio.
L'articolo propone un approccio modificato alla RL, dove una "memoria episodica" aggiuntiva è tenuta dall'agente e usa una "rete di interrogazione" che si basa sullo stato attuale.
Raggiungiamo la decomposizione bias-varianza per le macchine di Boltzmann usando una formulazione geometrica dell'informazione.
L'obiettivo di questo articolo è analizzare l'efficacia e la generalizzabilità dell'apprendimento profondo presentando un'analisi teorica della decomposizione bias-varianza per i modelli gerarchici, in particolare le macchine di Boltzmann  
L'articolo arriva alla conclusione principale che è possibile ridurre sia la distorsione che la varianza in un modello gerarchico.
Combinando il pruning di rete e i kernel persistenti in un'implementazione di rete pratica, veloce e accurata.
Questo articolo introduce le RNN sparse persistenti, un meccanismo per aggiungere il pruning al lavoro esistente di memorizzazione dei pesi RNN su un chip.
Presentiamo un nuovo metodo di potatura e un formato di matrice rada per permettere un alto rapporto di compressione dell'indice e un processo di decodifica dell'indice parallelo.
Gli autori usano la codifica Viterbi per comprimere drasticamente l'indice della matrice sparsa di una rete potata, riducendo uno dei principali sovraccarichi di memoria e accelerando l'inferenza nell'impostazione parallela.
Una nuova rete di politiche gerarchiche che può riutilizzare le competenze precedentemente apprese accanto e come sottocomponenti di nuove competenze, scoprendo le relazioni sottostanti tra le competenze.
Questo articolo mira ad apprendere politiche gerarchiche utilizzando una struttura di politiche ricorsiva regolata da una grammatica temporale stocastica
Questo articolo propone un approccio all'apprendimento delle politiche gerarchiche in un contesto di apprendimento permanente, impilando le politiche e poi usando una politica esplicita di "switch".
Proponiamo di usare la proiezione esplicita delle formule algebriche vettoriali come un modo alternativo per visualizzare gli spazi di incorporamento specificamente adattati ai compiti di analisi orientati agli obiettivi e supera t-SNE nel nostro studio utente.
Analisi degli spazi di incorporazione in modo non parametrico (basato su esempi)
Proponiamo un approccio di principio che conferisce ai classificatori la capacità di resistere a grandi variazioni tra i dati di allenamento e di test in modo intelligente ed efficiente.
Usare l'apprendimento introspettivo per gestire le variazioni dei dati al momento del test
Questo articolo suggerisce l'uso di reti di trasformazione apprese, incorporate in reti introspettive per migliorare le prestazioni di classificazione con esempi sintetizzati.
La discretizzazione dell'input porta alla robustezza contro gli esempi avversi
Gli autori presentano uno studio approfondito sulla discretizzazione/quantizzazione dell'input come difesa contro gli esempi avversari
abbiamo dimostrato limiti indipendenti dalla dimensione per algoritmi di formazione a bassa precisione
Questo articolo discute le condizioni in cui la convergenza dei modelli di addestramento con pesi di bassa precisione non dipende dalla dimensione del modello.
Modifiche a MAML e RL2 che dovrebbero permettere una migliore esplorazione. 
L'articolo propone un trucco per estendere le funzioni obiettivo per guidare l'esplorazione in meta-RL sopra due recenti algoritmi meta-RL
Un modello simbolico neurale probabilistico con uno spazio di programma latente, per una risposta alle domande più interpretabile
Questo articolo propone un modello di variabile latente discreto e strutturato per la risposta a domande visive che coinvolge la generalizzazione e il ragionamento compositivo con un guadagno significativo in termini di prestazioni e capacità.
Usiamo la verifica formale per valutare l'efficacia delle tecniche per trovare esempi avversari o per difendersi dagli esempi avversari.
Questo articolo propone un metodo per calcolare esempi contraddittori con la minima distanza dagli input originali.
Gli autori propongono di impiegare esempi di distanza minima provabile come strumento per valutare la robustezza di una rete addestrata.
L'articolo descrive un metodo per generare esempi contraddittori che hanno una distanza minima dall'esempio di allenamento usato per generarli
Paragrafo retriever e lettore automatico interagisce con l'altro attraverso l'apprendimento di rinforzo per produrre grandi miglioramenti su set di dati di dominio aperto
L'articolo introduce un nuovo quadro di interazione bidirezionale tra il recuperatore di documenti e il lettore per la risposta a domande a dominio aperto con l'idea di "stato del lettore" dal lettore al recuperatore.
L'articolo propone un modello di lettura meccanica estrattiva multi-documento composto da 3 parti distinte e un algoritmo.
Presenta una nuova architettura che sfrutta il potere di globalizzazione dell'informazione delle reti a U in una rete più profonda ed esegue bene tutti i compiti senza alcun campanello d'allarme.
Un'architettura di rete per la segmentazione semantica delle immagini, basata sulla composizione di una pila di architetture U-Net di base, che riduce il numero di parametri e migliora i risultati.
Questo propone un'architettura U-Net impilata per la segmentazione delle immagini.
Proponiamo una rete neurale che è in grado di generare domande specifiche per argomento.
Presenta un approccio basato su reti neurali per generare domande specifiche per argomento con la motivazione che le domande topiche sono più significative nelle applicazioni pratiche.
Propone un metodo di generazione basato su argomenti utilizzando un LSTM per estrarre gli argomenti utilizzando una tecnica di codifica a due stadi
Implementiamo una rete di adattamento al dominio avversario per stabilizzare un'interfaccia Brain-Machine fissa contro i cambiamenti graduali nei segnali neurali registrati.
Descrive un nuovo approccio per l'interfaccia cervello-macchina impiantata per affrontare il problema della calibrazione e lo spostamento delle covariate. 
Gli autori definiscono un BMI che utilizza un autoencoder e poi affrontano il problema della deriva dei dati nel BMI.
Analizzare e capire come gli agenti delle reti neurali imparano a capire un linguaggio semplice a terra
Gli autori collegano i metodi sperimentali psicologici alla comprensione di come la scatola nera dei metodi di apprendimento profondo risolve i problemi.
Questo articolo presenta un'analisi degli agenti che imparano il linguaggio a terra attraverso l'apprendimento per rinforzo in un ambiente semplice che combina istruzioni verbali con informazioni visive
Imparare le parti degli oggetti, la struttura gerarchica e le dinamiche guardando come si muovono
Propone un modello di apprendimento non supervisionato che impara a separare gli oggetti in parti, prevedere la struttura gerarchica delle parti e, sulla base delle parti separate e della gerarchia, prevedere il movimento.
Costruiamo una comprensione delle tecniche efficienti in termini di risorse sulla Super-Risoluzione
L'articolo propone una valutazione empirica dettagliata dei compromessi raggiunti da varie reti neurali convoluzionali sul problema della super risoluzione.
Questo articolo ha proposto di migliorare l'efficienza delle risorse del sistema per le reti di super risoluzione.
Studiamo le reti ReLU nel dominio di Fourier e dimostriamo un comportamento particolare.
Analisi di Fourier della rete ReLU, trovando che sono orientati verso l'apprendimento di bassa frequenza 
Questo articolo ha contributi teorici ed empirici sul tema dei coefficienti di Fourier delle reti neurali
L'articolo propone di usare distribuzioni di probabilità invece di punti per compiti di embedding di istanze come il riconoscimento e la verifica.
La carta propone un'alternativa all'attuale incorporazione di punti e una tecnica per addestrarli.
L'articolo propone un modello che utilizza le incertezze per estendere l'apprendimento profondo alle applicazioni bayesiane
Proponiamo la contrazione dei tensori e gli strati di regressione dei tensori a basso rango per preservare e sfruttare la struttura multi-lineare in tutta la rete, ottenendo un enorme risparmio di spazio con poco o nessun impatto sulle prestazioni.
Questo articolo propone nuove architetture a strati di reti neurali utilizzando una rappresentazione a basso rango di tensori
Questo articolo incorpora la decomposizione tensoriale e la regressione tensoriale nella CNN utilizzando un nuovo strato di regressione tensoriale.
Usiamo i dizionari bilingui per aumentare i dati per la traduzione automatica neurale
Questo articolo studia l'uso di dizionari bilingui per creare fonti sintetiche per i dati monolingui lato target per migliorare i modelli NMT addestrati con piccole quantità di dati paralleli.
Proponiamo un nuovo modello di curiosità basato sulla memoria episodica e le idee di raggiungibilità che ci permette di superare i noti problemi di "couch-potato" dei lavori precedenti.
Propone di dare bonus di esplorazione negli algoritmi RL dando bonus più grandi alle osservazioni che sono più lontane nei passi dell'ambiente.
Gli autori propongono un bonus di esplorazione che ha lo scopo di aiutare nei problemi RL a ricompensa sparsa e considera molti esperimenti su ambienti 3D complessi
Introduciamo un nuovo set di dati di implicazioni logiche allo scopo di misurare la capacità dei modelli di catturare e sfruttare la struttura delle espressioni logiche in un compito di previsione di implicazione.
L'articolo propone un nuovo modello per utilizzare modelli profondi per rilevare l'implicazione logica come prodotto di funzioni continue su mondi possibili.
Propone un nuovo modello progettato per l'apprendimento automatico con la previsione dell'implicazione logica.
L'articolo riguarda una nuova metodologia ad alta efficienza energetica per l'apprendimento incrementale
Propone una procedura per l'apprendimento incrementale come apprendimento di trasferimento.
L'articolo presenta un metodo per addestrare reti neurali convoluzionali profonde in modo incrementale, in cui i dati sono disponibili in piccoli lotti per un periodo di tempo.
Presenta un approccio all'apprendimento incrementale di classe utilizzando reti profonde, proponendo tre diverse strategie di apprendimento nell'approccio finale/migliore.
utilizzare la scansione parallela per parallelizzare le reti neurali ricorrenti lineari. addestrare il modello sulla lunghezza di 1 milione di dipendenze
Propone di accelerare RNN applicando il metodo di Blelloch.
Gli autori propongono un algoritmo parallelo per Linear Surrogate RNNs, che produce accelerazioni rispetto alle implementazioni esistenti di Quasi-RNN, SRU e LSTM.
GAN in linguaggio naturale per il riempimento dello spazio vuoto
Questo articolo propone di generare testo usando le GAN.
Generazione di campioni di testo utilizzando GAN e un meccanismo per riempire le parole mancanti condizionato dal testo circostante
Confronto tra le rappresentazioni delle texture psicofisiche e quelle codificate dalla CNN in un'applicazione di rilevamento di novità con rete neurale a una classe.
Questo articolo si concentra sul rilevamento delle novità e mostra che le rappresentazioni psicofisiche possono superare le caratteristiche del VGG-encoder in una parte di questo compito
Questo articolo considera il rilevamento delle anomalie nelle texture e propone una funzione di perdita originale.
Propone l'addestramento di due rilevatori di anomalie da tre modelli diversi per rilevare anomalie percettive nelle texture visive.
Proponiamo un nuovo tipo di approccio di regolarizzazione che incoraggia la non sovrapposizione nell'apprendimento della rappresentazione, al fine di migliorare l'interpretabilità e ridurre l'overfitting.
L'articolo introduce un regolatore di matrice per indurre simultaneamente sia la sparsità che l'ortogonalità approssimativa.
L'articolo studia un metodo di regolarizzazione per promuovere la sparsità e ridurre la sovrapposizione tra i supporti dei vettori di peso nelle rappresentazioni apprese per migliorare l'interpretabilità ed evitare l'overfitting
L'articolo ha proposto un nuovo approccio di regolarizzazione che incoraggia simultaneamente i vettori di peso (W) ad essere radi e ortogonali tra loro.
Dimostra che i meccanismi di gating forniscono invarianza alle trasformazioni temporali. Introduce e testa una nuova inizializzazione per le LSTM da questa intuizione.
La carta collega il deisgn delle reti ricorrenti e il suo effetto su come la rete reagisce alle trasformazioni del tempo, e usa questo per sviluppare un semplice schema di inizializzazione dei bias.
Proponiamo e verifichiamo l'efficacia dell'apprendimento per insegnare, un nuovo quadro per guidare automaticamente il processo di apprendimento automatico.
Questo articolo si concentra sull'"insegnamento automatico" e propone di sfruttare l'apprendimento per rinforzo definendo la ricompensa come la velocità di apprendimento dell'allievo e usando il gradiente della politica per aggiornare i parametri dell'insegnante
Gli autori definiscono un modello di apprendimento profondo composto da quattro componenti: un modello di studente, un modello di insegnante, una funzione di perdita e un set di dati. 
Suggerisce un quadro di "apprendimento per insegnare", corrispondente alle scelte sui dati presentati al discente.
Una perdita differenziabile per i vincoli logici per l'addestramento e l'interrogazione delle reti neurali.
Una struttura per trasformare le interrogazioni su parametri e coppie di input e ouput alle reti neurali in funzioni di perdita differenziabili e un linguaggio dichiarativo associato per specificare queste interrogazioni
Questo articolo affronta il problema di combinare approcci logici con reti neurali traducendo una formula logica in una funzione di perdita non negativa per una rete neurale.
Approccio basato su algoritmi genetici per l'ottimizzazione delle politiche delle reti neurali profonde
Gli autori presentano un algoritmo per la formazione di insiemi di reti di politiche che mescola regolarmente diverse politiche nell'insieme.
Questo articolo propone un metodo di ottimizzazione delle politiche ispirato all'algoritmo genetico, che imita gli operatori di mutazione e crossover sulle reti di politiche.
Un quadro di principio per la quantizzazione del modello usando il metodo del gradiente prossimale, con valutazione empirica e analisi teorica della convergenza.
Propone il metodo ProxQuant per addestrare reti neurali con pesi quantizzati.
Propone di risolvere le reti binarie e le sue varianti usando la discesa del gradiente prossimale.
I minimi locali "cattivi" svaniscono in una rete neurale multistrato: una prova con ipotesi più ragionevoli di prima
Nelle reti con un solo strato nascosto, il volume dei minimi locali subottimali diminuisce esponenzialmente rispetto ai minimi globali.
Questo articolo mira a rispondere al perché gli algoritmi standard basati su SGD su rete neurale convergono a soluzioni "buone".
Proponiamo un metodo di attacco invariante all'attenzione per generare esempi avversari più trasferibili per gli attacchi black-box, che possono ingannare le difese allo stato dell'arte con un alto tasso di successo.
L'articolo propone un nuovo modo di superare lo stato dell'arte delle difese contro gli attacchi avversari alla CNN.
Questo articolo suggerisce che lo "spostamento dell'attenzione" è una proprietà chiave dietro il fallimento degli attacchi avversari da trasferire e propone un metodo di attacco invariante all'attenzione
Come allenare 100.000 classi su una singola GPU
Propone un efficiente metodo di hashing MACH per l'approssimazione di softmax nel contesto di un grande spazio di uscita, che risparmia sia la memoria che il calcolo.
Un metodo per lo schema di classificazione per problemi che coinvolgono un gran numero di classi in un ambiente multiclasse dimostrato sui dataset ODP e Imagenet-21K
L'articolo presenta uno schema basato sull'hashing per ridurre la memoria e il tempo di calcolo per la classificazione K-way quando K è grande
Presentiamo un metodo generale per la stima imparziale dei gradienti di funzioni black-box di variabili casuali. Applichiamo questo metodo all'inferenza variazionale discreta e all'apprendimento per rinforzo. 
Suggerisce un nuovo approccio per eseguire la discesa del gradiente per l'ottimizzazione di blackbox o l'addestramento di modelli di variabili latenti discreti.
Proponiamo uno stimatore della dimensione del supporto della distribuzione appresa dalle GAN per dimostrare che soffrono davvero di collasso della modalità, e dimostriamo che anche le GAN codificatrici-decodificatrici non evitano il problema.
L'articolo tenta di stimare sperimentalmente la dimensione del supporto delle soluzioni prodotte dalle tipiche GAN. 
Questo articolo propone un nuovo test intelligente basato sul paradosso del compleanno per misurare la diversità nel campione generato, con risultati di esperimenti interpretati per significare che il collasso del modo è forte in un certo numero di modelli generativi all'avanguardia.
L'articolo usa il paradosso del compleanno per mostrare che alcune architetture GAN generano distribuzioni con un supporto abbastanza basso.
Proponiamo un quadro per generare avversari naturali contro i classificatori black-box sia per domini visivi che testuali, facendo la ricerca di avversari nello spazio semantico latente.
Suggerisce un metodo per la creazione di esempi di avversari semantici.
Propone un quadro per generare esempi avversari naturali cercando avversari in uno spazio latente di rappresentazione dei dati densi e continui 
Estendiamo il metodo K-FAC alle RNN sviluppando una nuova famiglia di approssimazioni di Fisher.
Gli autori estendono il metodo K-FAC a RNNs e presentano 3 modi di approssimare F, mostrando risultati di ottimizzazione su 3 dataset, che superano ADAM sia nel numero di aggiornamenti che nel tempo di calcolo.
Propone di estendere il metodo di ottimizzazione di Kronecker-factor Appropriate Curvature all'impostazione delle reti neurali ricorrenti.
Gli autori presentano un metodo del secondo ordine che è specificamente progettato per le RNN
Il modello generativo per i kernel delle reti neurali convoluzionali, che agisce come una distribuzione a priori durante l'allenamento su nuovi set di dati.
Un metodo per modellare le reti neurali convoluzionali usando un metodo di Bayes.
Propone il "priore del peso profondo": l'idea è quella di ottenere un priore su un set di dati ausiliari e poi usare quel priore sui filtri CNN per avviare l'inferenza per un set di dati di interesse.
Questo articolo esplora l'apprendimento di priori informativi per modelli di reti neurali convoluzionali con domini di problemi simili utilizzando autoencoder per ottenere un priore espressivo sui pesi filtrati delle reti allenate.
Abbiamo applicato tecniche di deep learning alla segmentazione delle immagini iperspettrali e al campionamento iterativo delle caratteristiche.
Propone uno schema greedy per selezionare un sottoinsieme di caratteristiche spettrali altamente correlate in un compito di classificazione.
L'articolo esplora l'uso delle reti neurali per la classificazione e la segmentazione dell'imaging iperspettrale (HSI) delle cellule.
Classificazione delle cellule e implementazione della segmentazione cellulare basata su tecniche di deep learning con riduzione delle caratteristiche di input
Abbiamo creato un nuovo set di dati per l'interpretazione dei dati sulle trame e proponiamo anche una linea di base per lo stesso.
Gli autori propongono una pipeline per risolvere il problema DIP che comporta l'apprendimento da dataset contenenti terzine della forma {trama, domanda, risposta}
Propone un algoritmo che può interpretare i dati mostrati nei grafici scientifici.
Migliorare le reti neurali ricorrenti dello stato predittivo tramite caratteristiche casuali ortogonali
Propone di migliorare le prestazioni delle reti neurali ricorrenti a stati predittivi considerando le caratteristiche casuali ortogonali.
L'articolo affronta il problema dell'addestramento di reti neurali ricorrenti a stato predittivo e apporta due contributi.
Proponiamo il Fidelity-weighted Learning, un approccio semi-supervisionato insegnante-studente per l'addestramento delle reti neurali usando dati debolmente etichettati.
Questo articolo suggerisce un approccio per l'apprendimento con supervisione debole utilizzando un set di dati pulito e uno rumoroso e assumendo un insegnante e reti di studenti
L'articolo tenta di addestrare modelli di reti neurali profonde con pochi campioni di formazione etichettati.
Gli autori propongono un approccio per la formazione di modelli di deep learning per situazioni in cui non ci sono abbastanza dati annotati affidabili.
Abbiamo proposto due nuovi approcci, la regressione inversa incrementale a fette e la regressione inversa incrementale a fette sovrapposte, per implementare la riduzione delle dimensioni supervisionata in un modo di apprendimento online.
Studia il problema della riduzione delle dimensioni sufficienti e propone un algoritmo di regressione inversa a fette incrementale.
Questo articolo propone un algoritmo di apprendimento online per la riduzione delle dimensioni supervisionata, chiamato regressione inversa a fette incrementale
Proponiamo un modello di apprendimento che permette alla DNN di imparare con solo 2 bit/peso, che è particolarmente utile per l'apprendimento su dispositivo
Propone un metodo per discretizzare una NN in modo incrementale per migliorare la memoria e le prestazioni.
L'apprendimento di operatori di trasporto su collettori forma una rappresentazione preziosa per fare compiti come l'apprendimento di trasferimento.
Utilizza una struttura di apprendimento a dizionario per imparare operatori di trasporto multipli su cifre USPS aumentate.
L'articolo considera il quadro di apprendimento dell'operatore di trasporto manifold di Culpepper e Olshausen (2009), e lo interpreta come ottenere una stima MAP sotto un modello generativo probabilistico.
Presentiamo un modello neurale variazionale per l'apprendimento di concetti visivi compositivi guidati dal linguaggio.
Propone una nuova architettura di rete neurale che impara i concetti degli oggetti combinando un beta-VAE e SCAN.
Questo articolo introduce un modello basato su VAE per tradurre tra immagini e testo, con la loro rappresentazione latente ben adattata all'applicazione di operazioni simboliche, dando loro un linguaggio più espressivo per il campionamento di immagini dal testo. 
Questo articolo propone un nuovo modello chiamato SCAN (Symbol-Concept Association Network) per l'apprendimento gerarchico dei concetti e permette la generalizzazione a nuovi concetti composti da concetti esistenti usando operatori logici.
Latent Topic Conversational Model, un ibrido di seq2seq e modello neurale di argomento per generare risposte più diverse e interessanti.
Questo articolo ha proposto la combinazione del modello di argomento e del modello di conversazione seq2seq
Propone un modello di conversazione con informazioni topiche combinando il modello seq2seq con modelli neurali di argomenti e mostra che il modello proposto supera alcuni modelli di base seq2seq e altre varianti di modelli di variabili latenti di seq2seq.
L'articolo affronta il problema dell'attualità duratura nei modelli di conversazione e propone un modello che è una combinazione di un modello di argomento neurale e un sistema di dialogo basato su seq2seq. 
Il problema dell'analisi dei grafici viene trasformato in un problema di analisi delle nuvole di punti. 
Propone una rete GNN profonda per i problemi di classificazione dei grafi utilizzando il loro strato di pooling grafico adattivo.
Gli autori propongono un metodo per l'apprendimento di rappresentazioni per i grafi
Proponiamo di generare un esempio avverso basato su reti generative avverse in un'impostazione semi-whitebox e black-box.
Descrive AdvGAN, una GAN condizionale più perdita avversaria, e valuta AdvGAN su impostazione semi-white box e black box, riportando risultati allo stato dell'arte.
Questo articolo propone un modo di generare esempi avversari che ingannano i sistemi di classificazione e vince la sfida mnist di MadryLab.
Questo articolo dimostra come addestrare autocodificatori profondi end-to-end per ottenere risultati SoA sul set di dati Netflix suddivisi nel tempo.
Questo articolo presenta un modello di autocodifica profonda per la predizione del rating che supera altri approcci all'avanguardia sul set di dati del premio Netflix. 
Propone di utilizzare un'AE profonda per svolgere compiti di predizione del rating nei sistemi di raccomandazione.
Gli autori presentano un modello per raccomandazioni Netflix più accurate dimostrando che un autocodificatore profondo può superare i modelli più complessi basati su RNN che hanno informazioni temporali. 
Introduciamo il trasformatore universale, un modello di sequenza ricorrente parallelo in tempo che supera i trasformatori e gli LSTM su una vasta gamma di compiti sequenza-sequenza, compresa la traduzione automatica.
Propone un nuovo modello UT, basato sul modello Transformer, con aggiunta di ricorrenza e arresto dinamico della ricorrenza.
Questo documento estende Transformer applicando ricorsivamente un blocco di auto-attenzione a più teste, piuttosto che impilare più blocchi nel Transformer vanilla.
L'articolo sviluppa un quadro di apprendimento continuo interpretabile in cui le spiegazioni dei compiti finiti sono utilizzate per migliorare l'attenzione dell'allievo durante i compiti futuri, e in cui viene proposta anche una metrica di spiegazione. 
Gli autori propongono un quadro per l'apprendimento continuo basato su spiegazioni per classificazioni eseguite di compiti precedentemente appresi
Questo articolo propone un'estensione del quadro di apprendimento continuo usando l'apprendimento continuo variazionale esistente come metodo di base con il peso delle prove.
Pipeline di addestramento a precisione mista usando numeri interi a 16 bit su HW generico; accuratezza SOTA per CNN di classe ImageNet; migliore accuratezza riportata per il compito di classificazione ImageNet-1K con qualsiasi addestramento a precisione ridotta;
Questo articolo mostra che un'attenta implementazione del calcolo dinamico in virgola fissa a precisione mista può raggiungere una precisione allo stato dell'arte utilizzando un modello di apprendimento profondo a precisione ridotta con una rappresentazione a 16 bit interi
Propone uno schema a "punto fisso dinamico" che condivide la parte esponenziale per un tensore e sviluppa procedure per fare calcoli NN con questo formato e lo dimostra per un addestramento a precisione limitata.
Un modello acustico ConvNet basato sulle lettere porta a una pipeline di riconoscimento vocale semplice e competitiva.
Questo articolo applica le reti neurali convoluzionali gated al riconoscimento vocale, utilizzando il criterio di formazione ASG.
Un framework GAN novale che utilizza caratteristiche invarianti alla trasformazione per imparare rappresentazioni ricche e generatori forti.
Propone un obiettivo GAN modificato composto da un termine GAN classico e un termine di codifica invariante.
Questo articolo presenta l'IVE-GAN, un modello che introduce l'encoder nel quadro delle Generative Adversarial Network.
Proponiamo un metodo per l'apprendimento della struttura di dipendenza latente negli autocodificatori variazionali.
Utilizza una matrice di variabili casuali binarie per catturare le dipendenze tra le variabili latenti in un modello generativo gerarchico profondo.
Questo articolo presenta un approccio VAE in cui una struttura di dipendenza sulla variabile latente viene appresa durante la formazione.
Gli autori propongono di aumentare lo spazio latente di una VAE con una struttura auto-regressiva, per migliorare l'espressività sia della rete di inferenza che della priorità latente
Introduciamo un'architettura di rete neurale invariante in scala per il rilevamento dei punti di cambiamento nelle serie temporali multivariate.
Il documento sfrutta il concetto di trasformazione wavelet all'interno di un'architettura profonda per risolvere il rilevamento dei punti di cambiamento.
Questo articolo propone una rete neurale basata su una piramide e la applica a segnali 1D con processi sottostanti che si verificano a diverse scale temporali dove il compito è la rilevazione del punto di cambiamento
RL trova migliori euristiche per algoritmi di ragionamento automatico.
Mira ad apprendere un'euristica per un algoritmo di ricerca backtracking utilizzando il Reinforcement learning e propone un modello che fa uso di Reti Neurali Grafiche per produrre l'incorporazione di letterali e clausole, e li usa per prevedere la qualità di ogni letterale per decidere la probabilità di ogni azione.
L'articolo propone un approccio all'apprendimento automatico di euristiche di selezione delle variabili per QBF utilizzando l'apprendimento profondo
Valutate se il vostro GAN sta effettivamente facendo qualcosa di diverso dalla memorizzazione dei dati di allenamento.
Mira a fornire una misura/test di qualità per le GAN e propone di valutare l'approssimazione attuale di una distribuzione appresa da una GAN utilizzando la distanza di Wasserstein tra due distribuzioni composte da una somma di Dirac come performance di base. 
Questo articolo ha proposto una procedura per valutare le prestazioni delle GAN riconsiderando la chiave di osservazione, utilizzando la procedura per testare e migliorare le GAN attuali
Usiamo tecniche di ricerca per scoprire nuove funzioni di attivazione, e la nostra migliore funzione di attivazione scoperta, f(x) = x * sigmoide(beta * x), supera ReLU su una serie di compiti impegnativi come ImageNet.
Propone un approccio basato sull'apprendimento di rinforzo per trovare la non linearità cercando attraverso combinazioni da un insieme di operatori unari e binari.
Questo articolo utilizza l'apprendimento per rinforzo per cercare la combinazione di un insieme di funzioni unarie e binarie che risultano in una nuova funzione di attivazione
L'autore usa il reinforcement learning per trovare nuove potenziali funzioni di attivazione da un ricco insieme di possibili candidati. 
Un algoritmo bottom-up che espande le CNN a partire da una caratteristica per strato fino ad architetture con sufficiente capacità di rappresentazione.
Propone di regolare dinamicamente la profondità della mappa delle caratteristiche di una rete neurale completamente convoluzionale, formulando una misura di autosomiglianza e aumentando le prestazioni.
Introduce una semplice metrica basata sulla correlazione per misurare se i filtri nelle reti neurali sono usati efficacemente, come proxy della capacità effettiva.
Mira ad affrontare il problema della ricerca dell'architettura di apprendimento profondo attraverso l'aggiunta e la rimozione incrementale di canali negli strati intermedi della rete.
Addestriamo una rete feedforward senza backprop utilizzando un modello basato sull'energia per fornire obiettivi locali
Questo articolo mira a velocizzare la procedura di inferenza iterativa nei modelli basati sull'energia addestrati con Equilibrium Propagation (EP), proponendo di addestrare una rete feedforward per prevedere un punto fisso della "rete equilibrante". 
Addestrare una rete separata per inizializzare le reti ricorrenti addestrate utilizzando la propagazione dell'equilibrio 
Imparare le rappresentazioni per le immagini che fattorizzano un singolo attributo.
Questo documento si basa su GAN condizionali VAE per consentire la manipolazione degli attributi nel processo di sintesi.
Questo articolo propone un modello generativo per imparare la rappresentazione che può separare l'identità di un oggetto da un attributo, ed estende l'autoencoder avversario aggiungendo una rete ausiliaria.
Presentiamo un modello per la ricostruzione 3D coerente e la predizione video saltellante, ad esempio la produzione di fotogrammi di immagini a più passi temporali nel futuro senza generare fotogrammi intermedi.
Questo articolo propone un metodo generale per la modellazione dei dati indicizzati codificando le informazioni dell'indice insieme all'osservazione in una rete neurale, e poi decodificare la condizione di osservazione sull'indice di destinazione.
Propone di utilizzare un VAE che codifica il video in ingresso in modo invariante alla permutazione per prevedere i futuri fotogrammi di un video.
Analizzando il popolare ottimizzatore Adam
L'articolo cerca di migliorare Adam basato sull'adattamento della varianza con slancio proponendo due algoritmi
Questo articolo analizza l'invarianza di scala e la forma particolare del tasso di apprendimento utilizzato in Adam, sostenendo che l'aggiornamento di Adam è una combinazione di un sign-update e un tasso di apprendimento basato sulla varianza.
L'articolo divide l'algoritmo ADAM in due componenti: direzione stocastica nel segno del gradiente e stepwise adattivo con varianza relativa, e due algoritmi sono proposti per testare ciascuno di essi.
Proponiamo un nuovo quadro per regolare in modo adattivo i tassi di abbandono per la rete neurale profonda basata su un limite di complessità di Rademacher.
Gli autori collegano i parametri di dropout a un limite della complessità Rademacher della rete
Mette in relazione la complessità dell'apprendibilità delle reti con i tassi di abbandono nella backpropagation.
Vengono proposte architetture di deep learning gated ottimizzate per la fusione dei sensori.
Gli autori migliorano diverse limitazioni dell'architettura negata di base proponendo un'architettura di fusione gated a grana più grossa e un'architettura di fusione gated a due stadi
Propone due architetture di deep learning gated per la fusione dei sensori e, avendo le caratteristiche raggruppate, dimostra prestazioni migliori, soprattutto in presenza di rumore casuale dei sensori e di guasti.
La normalizzazione dei lotti causa l'esplosione dei gradienti nelle reti feedforward vaniglia.
Sviluppa una teoria del campo medio per la normalizzazione dei lotti (BN) in reti completamente connesse con pesi inizializzati in modo casuale.
Fornisce una prospettiva dinamica sulla rete neurale profonda utilizzando l'evoluzione della matrice di covarianza insieme agli strati.
Addestriamo una rete a grafi per prevedere la soddisfabilità booleana e mostriamo che impara a cercare soluzioni, e che le soluzioni che trova possono essere decodificate dalle sue attivazioni.
L'articolo descrive un'architettura generale di rete neurale per prevedere la soddisfabilità
Questo articolo presenta l'architettura NeuroSAT che utilizza una rete neurale profonda a passaggio di messaggi per prevedere la soddisfabilità delle istanze CNF
Un modello di sequenza neurale che impara a prevedere su un grafico diretto.
L'articolo propone l'architettura Diffusion Convolutional Recurrent Neural Network per il problema della previsione del traffico spazio-temporale
Propone di costruire un modello di previsione del traffico usando un processo di diffusione per reti neurali ricorrenti convoluzionali per affrontare l'autocorrelazione saptio-temporale.
Addestriamo le reti neurali ad essere incerte su input rumorosi per evitare previsioni troppo sicure al di fuori della distribuzione di addestramento.
Presenta un approccio per ottenere stime di incertezza per le previsioni delle reti neurali che ha buone prestazioni quando si quantifica l'incertezza predittiva in punti che sono al di fuori della distribuzione di formazione.
L'articolo considera il problema della stima dell'incertezza delle reti neurali e propone di utilizzare l'approccio bayesiano con priore contrastivo noice
Questo studio mette in evidenza una differenza chiave tra la visione umana e le CNN: mentre il riconoscimento degli oggetti negli esseri umani si basa sull'analisi della forma, le CNN non hanno una tale forma-bias.
Cerca di stabilire attraverso una serie di esperimenti ben progettati che le CNN addestrate per la classificazione delle immagini non codificano gli errori di forma come la visione umana.
Questo articolo mette in evidenza il fatto che le CNN non imparano necessariamente a riconoscere gli oggetti in base alla loro forma e mostra che si affezionano troppo alle caratteristiche basate sul rumore.
Descriviamo un nuovo modello generativo multi-vista che può generare più viste dello stesso oggetto, o più oggetti nella stessa vista senza bisogno di etichette sulle viste.
Questo articolo presenta un metodo basato su GAN per la generazione di immagini che cerca di separare le variabili latenti che descrivono il contenuto dell'immagine da quelle che descrivono le proprietà della vista.
Questo articolo propone un'architettura GAN che mira a decomporre la distribuzione sottostante di una particolare classe in "contenuto" e "vista".
Propone un nuovo modello generativo basato sulla Generative Adversarial Network (GAN) che separa il contenuto e la vista degli oggetti senza supervisione della vista ed estende GMV in un modello generativo condizionale che prende un'immagine di input e genera diverse viste dell'oggetto nell'immagine di input. 
Viene proposto un algoritmo di quantizzazione del peso consapevole della perdita che considera direttamente il suo effetto sulla perdita.
Propone un metodo di compressione della rete per mezzo della ternarizzazione dei pesi. 
L'articolo propone un nuovo metodo per addestrare DNN con pesi quantizzati, includendo la quantizzazione come vincolo in un algoritmo quasi-Newton prossimale, che apprende simultaneamente una scalatura per i valori quantizzati.
L'articolo estende lo schema di binarizzazione a peso consapevole delle perdite alla terarizzazione e alla quantizzazione arbitraria di m-bit e dimostra le sue promettenti prestazioni.
Sviluppiamo un nuovo metodo di gradiente della politica per l'apprendimento automatico di politiche con opzioni utilizzando una fase di inferenza differenziabile.
L'articolo presenta una nuova tecnica di gradiente della politica per l'apprendimento delle opzioni, dove un singolo campione può essere usato per aggiornare tutte le opzioni.
Propone un metodo off-policy per l'apprendimento di opzioni in problemi continui complessi.
Selezione non supervisionata delle caratteristiche attraverso la cattura della struttura lineare locale dei dati
Propone una selezione di caratteristiche non supervisionata localmente lineare.
L'articolo propone il metodo LLUFS per la selezione delle caratteristiche.
Utilizzando un semplice compito di navigazione guidato dalla lingua, studiamo le capacità compositive delle moderne reti ricorrenti seq2seq.
Questo articolo si concentra sulle capacità compositive di apprendimento a zero colpi delle moderne RNN sequenza-sequenza ed espone i difetti delle attuali architetture RNN seq2seq.
L'articolo analizza le capacità di composizione delle RNN, in particolare, la capacità di generalizzazione delle RNN su un sottoinsieme casuale di comandi SCAN, su comandi SCAN più lunghi, e di composizione su comandi primitivi. 
Gli autori introducono un nuovo set di dati che facilita l'analisi di un caso di apprendimento Seq2Seq
Affrontiamo il problema dell'apprendimento della somiglianza per oggetti strutturati con applicazioni in particolare nella sicurezza informatica, e proponiamo un nuovo modello di reti di corrispondenza del grafico che eccelle in questo compito.
Gli autori introducono un Graph Matching Network per il recupero e la corrispondenza di oggetti strutturati a grafo.
Gli autori attaccano il problema della corrispondenza dei grafi proponendo un'estensione delle reti di embedding dei grafi
Gli autori presentano due metodi per l'apprendimento di un punteggio di somiglianza tra coppie di grafi e mostrano l'utilità di introdurre idee dalla corrispondenza dei grafi alle reti neurali dei grafi.
Abbiamo proposto un modello RNN-CNN encoder-decoder per un veloce apprendimento non supervisionato della rappresentazione della frase.
Modifiche al quadro di skip-thought per l'apprendimento di embeddings di frasi.
Questo articolo presenta un nuovo codificatore RNN e un decodificatore CNN ibrido per l'uso nel pretraining, che non richiede un decodificatore autoregressivo quando si pretraina i codificatori.
Gli autori estendono Skip-thought decodificando solo una frase target utilizzando un decodificatore CNN.
Un approccio statistico per calcolare le verosimiglianze del campione nelle reti generative avversarie
Mostrare che WGAN con regolarizzazione entropica massimizza un limite inferiore sulla probabilità della distribuzione dei dati osservati.
Gli autori sostengono che è possibile sfruttare il limite superiore di un trasporto ottimale regolarizzato dall'entropia per ottenere una misura di "probabilità del campione".
Introduciamo geomstats, un efficiente pacchetto Python per la modellazione e l'ottimizzazione Riemanniana su collettori compatibile sia con numpy che con tensorflow.
L'articolo introduce il pacchetto software geomstats, che fornisce un uso semplice dei collettori e delle metriche Riemanniane all'interno dei modelli di apprendimento automatico
Propone un pacchetto Python per l'ottimizzazione e le applicazioni su collettori Riemanniani ed evidenzia le differenze tra il pacchetto Geomstats e altri pacchetti.
Introduce un toolbox geometrico, Geomstats, per l'apprendimento automatico su collettori Riemanniani.
Costruiamo il grafico dinamico sparso tramite la ricerca di riduzione delle dimensioni per ridurre i costi di calcolo e di memoria sia nell'addestramento DNN che nell'inferenza.
Gli autori propongono di utilizzare il grafico di calcolo dinamico sparso per ridurre la memoria di calcolo e il costo del tempo nella rete neurale profonda (DNN).
Questo articolo propone un metodo per accelerare l'addestramento e l'inferenza delle reti neurali profonde usando la potatura dinamica del grafico di calcolo.
Sviluppiamo un'estensione pratica dell'Information-Directed Sampling for Reinforcement Learning, che tiene conto dell'incertezza parametrica e dell'eteroscedasticità nella distribuzione di ritorno per l'esplorazione.
Gli autori propongono un modo di estendere l'Information-Directed Sampling al reinforcement learning combinando due tipi di incertezza per ottenere una semplice strategia di esplorazione basata su IDS. 
Questo articolo studia approcci sofisticati di esplorazione per l'apprendimento di rinforzo costruiti su Information Direct Sampling e su Distributional Reinforcement Learning
utilizzando la rete neurale a grafo per modellare le informazioni strutturali degli agenti per migliorare la politica e la trasferibilità 
Un metodo per la rappresentazione e l'apprendimento di una politica strutturata per compiti di controllo continuo utilizzando reti neurali grafiche
La presentazione propone l'incorporazione di una struttura aggiuntiva nei problemi di apprendimento per rinforzo, in particolare la struttura della morfologia dell'agente
Proporre un'applicazione delle Reti Neurali Grafiche all'apprendimento di politiche per il controllo di robot "millepiedi" di diverse lunghezze.
Questo articolo presenta un quadro di apprendimento di rinforzo gerarchico basato su politiche di opzioni deterministiche e massimizzazione dell'informazione reciproca. 
Propone un algoritmo HRL che cerca di imparare le opzioni che massimizzano la loro mutua informazione con la densità di stato-azione sotto la politica ottimale.
Questo articolo propone un sistema HRL in cui l'informazione mutua della variabile latente e delle coppie stato-azione è approssimativamente massimizzata.
Propone un criterio che mira a massimizzare l'informazione reciproca tra le opzioni e le coppie stato-azione e mostra empiricamente che le opzioni apprese decompongono lo spazio stato-azione ma non lo spazio stato. 
Introduciamo e convalidiamo le interpretazioni locali gerarchiche, la prima tecnica per cercare e visualizzare automaticamente le interazioni importanti per le previsioni individuali fatte da LSTMs e CNNs.
Un nuovo approccio per spiegare le previsioni delle reti neurali imparando rappresentazioni gerarchiche di gruppi di caratteristiche di input e il loro contributo alla previsione finale
Estende un metodo di interpretazione delle caratteristiche esistente per LSTMs a DNNs più generiche e introduce un clustering gerarchico delle caratteristiche di input e i contributi di ogni cluster alla predizione finale.
Questo articolo propone un'estensione gerarchica della decomposizione contestuale.
Proponiamo un metodo facile da implementare, ma efficace per la compressione delle reti neurali. PFA sfrutta la correlazione intrinseca tra le risposte dei filtri all'interno degli strati della rete per raccomandare un'impronta di rete più piccola.
Propone di sfrondare le reti di convoluzione analizzando la correlazione osservata tra i filtri di uno stesso strato come espresso dallo spettro di autovalori della loro matrice di covarianza.
Questo articolo introduce un approccio alla compressione delle reti neurali guardando la correlazione delle risposte dei filtri in ogni strato attraverso due strategie.
Questo documento propone un metodo di compressione basato sull'analisi spettrale
Agenti di riformulazione di query multiple diverse addestrati con l'apprendimento di rinforzo per migliorare i motori di ricerca.
Parellelizzazione del metodo dell'ensemble nell'apprendimento di reinforement per la riformulazione delle query, accelerando l'addestramento e migliorando la diversità delle freformulazioni apprese
Gli autori propongono di addestrare più agenti distinti, ciascuno su un diverso sottoinsieme del set di allenamento.
Gli autori propongono un approccio ensemble per la riformulazione delle query
Introduciamo un metodo di incorporazione della rete che tiene conto delle informazioni precedenti sulla rete, ottenendo prestazioni empiriche superiori.
L'articolo ha proposto di utilizzare una distribuzione a priori per vincolare l'incorporazione della rete, per la formulazione questo articolo ha utilizzato distribuzioni gaussiane molto ristrette.
Propone l'apprendimento non supervisionato delle embeddings dei nodi considerando le proprietà strutturali delle reti.
Analizziamo la convergenza degli algoritmi di tipo Adam e forniamo lievi condizioni sufficienti per garantire la loro convergenza, mostriamo anche che la violazione delle condizioni può far divergere un algoritmo.
Presenta un'analisi della convergenza in un ambiente non convesso per una famiglia di algoritmi di ottimizzazione.
Questo articolo studia la condizione di convergenza degli ottimizzatori di tipo Adam nei problemi di ottimizzazione non vincolati non convessi.
L'auto-encoder a pesi legati con la funzione abs come funzione di attivazione, impara a fare la classificazione in direzione avanti e la regressione in direzione indietro grazie alla funzione di costo appositamente definita.
L'articolo propone di utilizzare la funzione di attivazione del valore assoluto in un'architettura autoencoder con un termine aggiuntivo di apprendimento supervisionato nella funzione obiettivo
Questo documento introduce una rete reversibile con il valore assoluto usato come funzione di attivazione
Proponiamo un modello di estrazione di relazioni basato su Transformer che usa rappresentazioni linguistiche pre-addestrate invece di caratteristiche linguistiche esplicite.
Presenta un modello di estrazione di relazioni basato su trasformatori che sfrutta il pre-addestramento su testo non etichettato con un obiettivo di modellazione del linguaggio.
Questo articolo descrive una nuova applicazione di reti di trasformatori per l'estrazione di relazioni.
L'articolo presenta un'architettura basata su Transformer per l'estrazione del rilassamento, valutando su due set di dati.
Proponiamo un metodo semplice ed efficace per la ricerca di architetture per le reti neurali convoluzionali.
Propone un metodo di ricerca dell'architettura neurale che raggiunge una precisione vicina allo stato dell'arte su CIFAR10 e richiede molte meno risorse computazionali.
Presenta un metodo per cercare architetture di reti neurali allo stesso tempo dell'addestramento che risparmia drasticamente il tempo di addestramento e il tempo di ricerca dell'architettura.
Propone una variante di ricerca dell'architettura neurale usando morfismi di rete per definire uno spazio di ricerca usando architetture CNN che completano il compito di classificazione delle immagini CIFAR
Usare le GAN per generare grafici tramite passeggiate casuali.
Gli autori hanno proposto un modello generativo di passeggiate casuali su grafi che permette l'apprendimento modello-agnostico, il fitting controllabile, la generazione di grafi ensemble
Propone una formulazione WGAN per generare grafi basati su passeggiate casuali usando embeddings dei nodi e un'architettura LSTM per la modellazione.
Proponiamo di risolvere un problema di classificazione simultanea e di rilevamento di novità nel quadro GAN.
Propone una GAN per unificare la classificazione e il rilevamento delle novità.
L'articolo presenta un metodo per il rilevamento delle novità basato su una GAN multiclasse che è addestrata a produrre immagini generate da una miscela di distribuzioni nominali e nuove.
L'articolo propone una GAN per il rilevamento di novità utilizzando un generatore di miscele con perdita di corrispondenza delle caratteristiche
Le prestazioni di verifica dei parlanti possono essere migliorate significativamente adattando il modello ai dati del dominio utilizzando le reti generative avversarie. Inoltre, l'adattamento può essere eseguito in modo non supervisionato.
Proporre una serie di varianti di GAN sul compito di riconoscimento degli altoparlanti nella condizione di dominio non corrispondente.
Minimizzare l'informazione reciproca sinergica tra i latenti e i dati per il compito di disentanglement usando il quadro VAE.
Propone una nuova funzione obiettivo per l'apprendimento di rappresentazioni dientangled in un quadro variazionale minimizzando la sinergia delle informazioni fornite.
Gli autori mirano ad addestrare una VAE che ha dissociato le rappresentazioni latenti in modo "sinergicamente" massimo. 
Questo articolo propone un nuovo approccio per imporre il disentanglement nelle VAE utilizzando un termine che penalizza l'informazione reciproca sinergica tra le variabili latenti.
Accelerare SGD disponendo gli esempi in modo diverso
L'articolo presenta un metodo per migliorare il tasso di convergenza di Stochastic Gradient Descent per l'apprendimento di embeddings raggruppando campioni di formazione simili.
Propone una strategia di campionamento non uniforme per costruire minibatch in SGD per il compito di imparare embeddings per associazioni di oggetti.
Combina le informazioni tra l'incorporazione di parole pre-costruite e la rappresentazione di parole specifica per il compito per affrontare il problema dell'esaurimento del vocabolario
Questo articolo propone un approccio per migliorare la previsione di embedding fuori dal vocabolario per il compito di modellare le conversazioni di dialogo con guadagni considerevoli rispetto alle linee di base.
Propone di combinare le embeddings di parole esterne preaddestrate e le embeddings di parole preaddestrate sui dati di formazione mantenendole come due viste.
Propone un metodo per estendere la copertura di word embeddings pre-addestrati per affrontare il problema OOV che si presenta quando li si applica a dataset di conversazione e applica nuove varianti del modello basato su LSTM al compito di selezione delle risposte nella modellazione del dialogo.
Analizziamo i problemi durante l'addestramento degli ottimizzatori appresi, affrontiamo questi problemi tramite l'ottimizzazione variazionale utilizzando due stimatori di gradiente complementari, e addestriamo ottimizzatori che sono 5 volte più veloci in tempo di wall-clock rispetto agli ottimizzatori di base (per esempio Adam).
Questo articolo usa l'ottimizzazione non arrotolata per imparare le reti neurali per l'ottimizzazione.
Questo articolo affronta il problema dell'apprendimento di un ottimizzatore, in particolare gli autori si concentrano sull'ottenimento di gradienti più puliti dalla procedura di formazione srotolata.
Presenta un metodo per "imparare un ottimizzatore" usando un'ottimizzazione variazionale per la perdita "esterna" dell'ottimizzatore e propone l'idea di combinare sia il gradiente riparametrizzato che lo stimatore della funzione di punteggio per l'obiettivo variazionale e li pondera usando un prodotto di Gaussiane per la media.
Un metodo per un efficiente addestramento distribuito asincrono di modelli di apprendimento profondo insieme a limiti teorici di rammarico.
L'articolo propone un algoritmo per limitare la staleness in SGD asincrono e fornisce un'analisi teorica
Propone un algoritmo ibrido per eliminare il ritardo del gradiente dei metodi asincroni.
Abbiamo progettato una nuova metodologia di quantizzazione per ottimizzare congiuntamente l'efficienza e la robustezza dei modelli di apprendimento profondo.
Propone uno schema di regolarizzazione per proteggere le reti neurali quantizzate dagli attacchi avversari usando un filtraggio costante di Lipschitz dell'inpout-output degli strati interni.
Una modifica per le architetture RNN esistenti che permette loro di saltare gli aggiornamenti di stato mantenendo le prestazioni delle architetture originali.
Propone il modello Skip RNN che permette a una rete ricorrente di saltare selettivamente l'aggiornamento del suo stato nascosto per alcuni ingressi, portando a una riduzione dei calcoli in fase di test.
Propone un nuovo modello RNN in cui sia l'input che l'aggiornamento dello stato delle cellule ricorrenti sono saltati in modo adattivo per alcuni passi temporali.
Un veloce solutore del secondo ordine per l'apprendimento profondo che funziona su problemi su scala ImageNet senza sintonizzazione degli iperparametri
Scegliere la direzione usando un singolo passo di discesa del gradiente "verso il passo di Newton" da una stima originale, e poi prendere questa direzione invece del gradiente originale
Un nuovo metodo di ottimizzazione approssimata del secondo ordine a basso costo computazionale che sostituisce il calcolo della matrice Hessiana con un singolo passo di gradiente e una strategia di avvio a caldo.
Modello basato sull'attenzione addestrato con REINFORCE con rollout greedy baseline per imparare euristiche con risultati competitivi su TSP e altri problemi di routing
Presenta un approccio basato sull'attenzione per imparare una politica per risolvere TSP e altri problemi di ottimizzazione combinatoria di tipo routing.
Questo articolo cerca di imparare l'euristica per risolvere problemi di ottimizzazione combinatoria
Un algoritmo per ottimizzare gli iper-parametri di regolarizzazione durante l'allenamento
L'articolo propone un modo per reinizializzare y ad ogni aggiornamento di lambda e una procedura di ritaglio di y per mantenere la stabilità del sistema dinamico.
Propone un algoritmo per l'ottimizzazione degli iperparametri che può essere visto come un'estensione di Franceschi 2017 in cui alcune stime vengono riavviate a caldo per aumentare la stabilità del metodo.
Propone un'estensione di un metodo esistente per ottimizzare gli iperparametri di regolarizzazione.
Mostrare che gli LSTM sono altrettanto buoni o migliori delle recenti innovazioni per LM e che la valutazione dei modelli è spesso inaffidabile.
Questo articolo descrive una convalida completa dei modelli linguistici di parole e caratteri basati su LSTM, portando a un risultato significativo nella modellazione linguistica e a una pietra miliare nel deep learning.
Mostriamo come l'uso delle connessioni di salto può rendere i modelli di miglioramento del discorso più interpretabili, poiché li fa usare meccanismi simili che sono stati esplorati nella letteratura DSP.
Gli autori propongono di incorporare i blocchi Residual, Highway e Masking all'interno di una pipeline completamente convoluzionale per capire come viene eseguita l'inferenza iterativa dell'output e il mascheramento in un compito di miglioramento del parlato
Gli autori interpretano le connessioni autostradali, residue e di mascheramento. 
Gli autori generano il loro discorso rumoroso aggiungendo artificialmente del rumore da un set di dati sul rumore ben stabilito a un set di dati sul discorso pulito meno conosciuto.
Un metodo per eliminare la varianza del gradiente e sintonizzare automaticamente i priori per un allenamento efficace delle reti neurali bayesiane
Propone un nuovo approccio per eseguire l'inferenza variazionale deterministica per BNN feed-forward con specifiche funzioni di attivazione non lineari approssimando i momenti di livello.
L'articolo considera un approccio puramente deterministico all'apprendimento di approssimazioni posteriori variazionali per le reti neurali bayesiane.
Un approccio di metodo formale alla composizione di abilità in compiti di apprendimento con rinforzo
L'articolo combina RL e vincoli espressi da formule logiche impostando un'automazione da formule scTLTL.
Propone un metodo che aiuta a costruire la politica dai sottocompiti appresi sul tema della combinazione di compiti RL con formule logiche temporali lineari.
Derivare una formulazione generale di una VAE multimodale dalla log-likelihood marginale congiunta.
Propone un VAE multimodale con un limite variazionale derivato dalla regola della catena.
Questo articolo propone un obiettivo, M^2VAE, per le VAE multimodali, che dovrebbe imparare una rappresentazione dello spazio latente più significativa.
Ci basiamo sull'autocodifica sequenziale Monte Carlo, otteniamo nuove intuizioni teoriche e sviluppiamo una procedura di formazione migliorata basata su queste intuizioni.
L'articolo propone una versione dell'addestramento in stile IWAE che usa SMC invece del classico campionamento d'importanza.
Questo lavoro propone l'autocodifica sequenziale Monte Carlo (SMC), estendendo il quadro VAE a un nuovo obiettivo Monte Carto basato su SMC. 
Proponiamo un'architettura per l'apprendimento delle funzioni di valore che permette l'uso di qualsiasi algoritmo lineare di valutazione della politica in tandem con l'apprendimento non lineare delle caratteristiche.
L'articolo propone un quadro a due scale temporali per l'apprendimento della funzione di valore e una rappresentazione dello stato con approssimazioni non lineari.
Questo articolo propone le reti a due scale temporali (TTN) e prova la convergenza di questo metodo usando metodi di approssimazione stocastica a due scale temporali. 
Questo articolo presenta una rete a due scale temporali (TTN) che permette di utilizzare metodi lineari per imparare i valori. 
Proponiamo semplici, ma efficaci, algoritmi di fattorizzazione di matrice a basso rango (MF) per accelerare il tempo di esecuzione, risparmiare memoria e migliorare le prestazioni degli LSTM.
Propone di accelerare LSTM usando MF come strategia di compressione post-elaborazione e conduce ampi esperimenti per mostrare le prestazioni.
Una metrica forense per determinare se una data immagine è una copia (con possibile manipolazione) di un'altra immagine da un dato set di dati.
Introduce la rete siamese per identificare le immagini duplicate e copiate/modificate, che può essere usata per migliorare la sorveglianza della letteratura pubblicata e in-peer-review.
L'articolo presenta un'applicazione di reti convoluzionali profonde per il compito di rilevamento di immagini duplicate
Questo lavoro affronta il problema di trovare immagini duplicate/vicine al duplicato da pubblicazioni biomediche e propone una CNN standard e funzioni di perdita e la applica a questo campo.
Addestramento stabile di GAN in alte dimensioni utilizzando un array di discriminatori, ciascuno con una visione a bassa dimensione dei campioni generati
L'articolo propone di stabilizzare l'addestramento GAN utilizzando un insieme di discriminatori, ognuno dei quali lavora su una proiezione casuale dei dati di input, per fornire il segnale di addestramento per il modello generatore.
L'articolo propone un metodo di formazione GAN per migliorare la stabilità della formazione. 
L'articolo propone un nuovo approccio all'addestramento GAN, che fornisce gradienti stabili per addestrare il generatore.
mostriamo un metodo geometrico per codificare perfettamente le informazioni dell'albero delle categorie in word-embeddings pre-addestrati.
L'articolo propone N-ball embedding per i dati tassonomici dove una N-ball è una coppia di un vettore centroide e il raggio dal centro.
L'articolo presenta un metodo per modificare le embeddings vettoriali esistenti di oggetti categorici (come le parole), per convertirle in embeddings a sfera che seguono le gerarchie.
Si concentra sulla regolazione delle incorporazioni di parole preaddestrate in modo che rispettino la relazione ipernimia/iponimia mediante un appropriato incapsulamento n-ball.
Proponiamo Convolutional CRFs un'alternativa veloce, potente e allenabile ai Fully Connected CRFs.
Gli autori sostituiscono il grande passo di filtraggio nel reticolo permutoedrico con un kernel convoluzionario spazialmente variabile e mostrano che l'inferenza è più efficiente e la formazione è più facile. 
Propone di eseguire il message passing su una CRF a kernel gaussiano troncato usando un kernel definito e il message passing parallelizzato su GPU.
Proponiamo un approccio agnostico al modello per la convalida della robustezza del sistema Q&A e dimostriamo i risultati su modelli Q&A all'avanguardia.
Affronta il problema della robustezza alle informazioni avversarie nella risposta alle domande.
Migliorare la robustezza della comprensione automatica/risposta alle domande.
multi generatore per catturare Pdata, risolvere la concorrenza e il problema di uno-beat-all
Propone GAN parallele per evitare il collasso della modalità nelle GAN attraverso una combinazione di più generatori deboli. 
Segmentazione di immagini debolmente supervisionata usando la struttura compositiva delle immagini e modelli generativi.
Questo articolo crea una rappresentazione a strati per imparare meglio la segmentazione da immagini senza etichetta.
Questo articolo propone un modello generativo basato su GAN che decompone le immagini in strati multipli, dove l'obiettivo del GAN è quello di distinguere le immagini reali dalle immagini formate combinando gli strati.
Questo articolo propone un'architettura di rete neurale intorno all'idea di composizione stratificata della scena
Presentiamo un quadro geometrico per dimostrare le garanzie di robustezza ed evidenziare l'importanza della codimensione negli esempi avversari. 
Questo articolo fornisce un'analisi teorica degli esempi avversari, mostrando che esiste un compromesso tra la robustezza in diverse norme, l'addestramento avversario è inefficiente per il campione, e il classificatore più vicino può essere robusto in certe condizioni.
CharNMT è fragile
Questo articolo studia l'impatto del rumore a livello di carattere su 4 diversi sistemi di traduzione automatica neurale
Questo articolo studia empiricamente le prestazioni dei sistemi NMT a livello di carattere di fronte al rumore a livello di carattere, sia sintetizzato che naturale.
Questo articolo studia l'impatto dell'input rumoroso sulla traduzione automatica e prova modi per rendere i modelli NMT più robusti
Impariamo reti profonde di unità a soglia rigida impostando gli obiettivi delle unità nascoste utilizzando l'ottimizzazione combinatoria e i pesi tramite l'ottimizzazione convessa, con conseguente miglioramento delle prestazioni su ImageNet.
L'articolo spiega e generalizza gli approcci per l'apprendimento delle reti neurali con attivazione dura.
Questo articolo esamina il problema dell'ottimizzazione delle reti profonde di unità a soglia dura.
L'articolo discute il problema dell'ottimizzazione delle reti neurali con soglia rigida e propone una nuova soluzione con un insieme di euristiche/approssimazioni.
Utilizzando una nuova sfida controllata di relazioni visive, dimostriamo che i compiti same-different mettono a dura prova la capacità delle CNN; sosteniamo che le relazioni visive possono essere risolte meglio utilizzando strategie attentivo-mnemoniche.
Dimostra che le reti neurali convoluzionali e relazionali non riescono a risolvere i problemi di relazione visiva addestrando le reti su dati di relazione visiva generati artificialmente. 
Questo articolo esplora come le attuali CNN e le reti relazionali non riescono a riconoscere le relazioni visive nelle immagini.
Proponiamo AD-VAT, dove il tracker e l'oggetto target, visti come due agenti apprendibili, sono avversari e possono migliorarsi reciprocamente durante l'allenamento.
Questo lavoro mira ad affrontare il problema dell'inseguimento attivo visivo con un meccanismo di allenamento in cui l'inseguitore e il bersaglio servono come avversari reciproci
Questo articolo presenta un semplice compito di RL profondo multi-agente in cui un inseguitore in movimento cerca di seguire un bersaglio in movimento.
Propone una nuova funzione di ricompensa - "a somma parziale zero", che incoraggia solo la competizione inseguitore-obiettivo quando sono vicini e penalizza quando sono troppo lontani.
Abbiamo presentato un metodo per imparare congiuntamente un Hierarchical Word Embedding (HWE) usando un corpus e una tassonomia per identificare le relazioni di ipernimia tra le parole.
L'articolo presenta un metodo per imparare congiuntamente le incorporazioni di parole usando statistiche di co-occorrenza e incorporando informazioni gerarchiche dalle reti semantiche.
Questo articolo ha proposto un metodo di apprendimento congiunto degli ipernimi sia dal testo grezzo che dai dati della tassonomia supervisionata. 
Questo articolo propone di aggiungere una misura di differenza di "inclusione distributiva" all'obiettivo GloVE allo scopo di rappresentare le relazioni ipernemiche.
integrazione di auto-organizzazione e apprendimento supervisionato in una rete neurale gerarchica
L'articolo discute l'apprendimento in una rete neurale a tre strati, dove lo strato intermedio è organizzato topograficamente e studia l'interazione tra l'apprendimento non supervisionato e quello gerarchico supervisionato nel contesto biologico.
Una variante supervisionata della mappa auto-organizzante (SOM) di Kohonen, ma dove lo strato di output lineare è sostituito con l'errore quadratico da uno strato softmax con cross-entropia.
Propone un modello che utilizza neuroni nascosti con funzione di attivazione auto-organizzante, le cui uscite alimentano un classificatore con funzione di uscita softmax. 
autostrada di precisione; un concetto generalizzato di flusso di informazioni ad alta precisione per la quantizzazione sub 4 bit 
Indaga il problema della quantizzazione delle reti neurali impiegando un'autostrada di precisione end-to-end per ridurre l'errore di quantizzazione accumulato e consentire una precisione ultra-bassa nelle reti neurali profonde. 
Questo articolo studia i metodi per migliorare le prestazioni delle reti neurali quantizzate
Questo articolo propone di mantenere un alto flusso di attivazione/gradiente in due tipi di strutture di reti, ResNet e LSTM.
Un algoritmo per addestrare reti neurali in modo efficiente su dati temporalmente ridondanti.
L'articolo descrive uno schema di codifica neurale per l'apprendimento basato sui picchi nelle reti neurali profonde
Questo articolo presenta un metodo per l'apprendimento basato sui picchi che mira a ridurre il calcolo necessario durante l'apprendimento e il test quando si classificano dati temporalmente ridondanti.
Questo articolo applica una versione di codifica predittiva dello schema di codifica Sigma-Delta per ridurre il carico computazionale su una rete di apprendimento profondo, combinando i tre componenti in un modo non visto in precedenza.
Il collo di bottiglia dell'informazione si comporta in modi sorprendenti ogni volta che l'output è una funzione deterministica dell'input.
Sostiene che la maggior parte dei problemi reali di classificazione mostrano una tale relazione deterministica tra le etichette di classe e gli input X ed esplora diversi problemi che derivano da tali patologie.
Esplora i problemi che sorgono quando si applicano i concetti di information bottlenext ai modelli deterministici di apprendimento supervisionato
Gli autori chiariscono diversi comportamenti controintuitivi del metodo del collo di bottiglia informativo per l'apprendimento supervisionato di una regola deterministica.
Dimostriamo che le reti neurali bayesiane idealizzate non possono avere esempi avversari, e diamo prove empiriche con BNN del mondo reale.
L'articolo studia la robustezza avversaria dei classificatori bayesiani e dichiara due condizioni che dimostrano essere provabilmente sufficienti per "modelli idealizzati" su "dataset idealizzati" per non avere esempi avversi
La carta propone una classe di classificatori discriminativi bayesiani che non hanno esempi avversari.
Introduciamo la prima istanza di attacchi avversari che riprogrammano il modello bersaglio per eseguire un compito scelto dall'attaccante---senza che l'attaccante abbia bisogno di specificare o calcolare l'output desiderato per ogni input test-time.
Gli autori presentano un nuovo schema di attacco avversario in cui una rete neurale viene riproposta per svolgere un compito diverso da quello su cui è stata originariamente addestrata
Questo articolo ha proposto la "riprogrammazione avversaria" di reti neurali ben addestrate e fisse e mostra che la riprogrammazione avversaria è meno efficace sulle reti non addestrate.
L'articolo estende l'idea degli "attacchi avversari" nell'apprendimento supervisionato delle NN a una completa riproposizione della soluzione di una rete addestrata.
Sviluppiamo un nuovo schema per prevedere il gap di generalizzazione nelle reti profonde con alta precisione.
Gli autori suggeriscono di usare un margine geometrico e una distribuzione dei margini a strati per prevedere il divario di generalizzazione.
Empiricamente mostra un'interessante connessione tra le statistiche di margine proposte e il gap di generalizzazione, che può essere utilizzato per fornire alcune intuizioni prescrittive verso la comprensione della generalizzazione nelle reti neurali profonde. 
Proponiamo un algoritmo per recuperare in modo dimostrabile i parametri (pesi di convoluzione e di uscita) di una rete convoluzionale con patch sovrapposte.
Questo articolo studia l'apprendimento teorico delle reti neurali convoluzionali a uno strato nascosto, risultando in un algoritmo di apprendimento e garanzie dimostrabili utilizzando l'algoritmo.
Questo articolo fornisce un nuovo algoritmo per l'apprendimento di una rete neurale a due strati che coinvolge un singolo filtro convoluzionale e un vettore di peso per diverse posizioni.
Un nuovo termine di regolarizzazione può migliorare l'allenamento di wasserstein gans
L'articolo propone uno schema di regolarizzazione per Wasserstein GAN basato sul rilassamento dei vincoli sulla costante Lipschitz di 1.
L'articolo tratta della regolarizzazione/penalizzazione nel fitting delle GAN, quando si basa su una metrica L_1 Wasserstein.
Proponiamo il metodo prossimale di Wasserstein per l'addestramento delle GAN. 
Propone una nuova procedura GAN che tiene conto dei punti generati nell'iterazione precedente e aggiorna il generatore da eseguire l volte.
Considera l'apprendimento a gradiente naturale nell'apprendimento GAN, dove viene impiegata la struttura Riemanniana indotta dalla distanza Wasserstein-2.
L'articolo intende utilizzare il gradiente naturale indotto dalla distanza Wasserstein-2 per addestrare il generatore in GAN e gli autori propongono l'operatore prossimale Wasserstein come regolarizzazione.
Questo articolo introduce una funzione euristica basata sull'eliminazione per il processo decisionale sequenziale, adatta a guidare algoritmi di ricerca AND/OR per risolvere diagrammi di influenza.
generalizza l'euristica di inferenza dei minibucket ai diagrammi di influenza.
Approssimazione della media e della varianza dell'uscita della NN su input rumorosi / dropout / parametri incerti. Approssimazioni analitiche per gli strati argmax, softmax e max.
Gli autori si concentrano sul problema della propagazione dell'incertezza DNN
Questo articolo rivisita la propagazione feed-forward della media e della varianza nei neuroni, affrontando il problema della propagazione dell'incertezza attraverso livelli di max-pooling e softmax.
Un discriminatore che non è facilmente ingannato dall'esempio avverso rende l'addestramento GAN più robusto e porta ad un obiettivo più liscio.
Questo articolo propone un nuovo modo per stabilizzare il processo di formazione di GAN regolarizzando il discriminatore per essere robusto agli esempi avversi.
L'articolo propone un modo sistematico di addestrare le GAN con termini di regolarizzazione della robustezza, permettendo un addestramento più fluido delle GAN. 
Presenta l'idea che rendendo un discriminatore robusto alle perturbazioni avversarie l'obiettivo GAN può essere reso liscio, il che si traduce in risultati migliori sia visivamente che in termini di FID.
Modelliamo la funzione di attivazione di ogni neurone come un processo gaussiano e la impariamo insieme al peso con l'inferenza variazionale.
Proporre di mettere priori di processo gaussiano sulla forma funzionale di ogni funzione di attivazione nella rete neurale per imparare la forma delle funzioni di attivazione.
Forniamo uno studio teorico delle proprietà delle reti ReLU circolanti-diagonali profonde e dimostriamo che sono approssimatori universali a larghezza limitata.
L'articolo propone di utilizzare matrici circolanti e diagonali per accelerare il calcolo e ridurre i requisiti di memoria nelle reti neurali.
Questo articolo dimostra che le reti ReLU diagonali-circolari di larghezza limitata (DC-ReLU) sono approssimatori universali.
StarHopper è una nuova interfaccia touch screen per la navigazione efficiente e flessibile dei droni con telecamera
Gli autori delineano una nuova interfaccia di controllo del drone StarHopper che hanno sviluppato, combinando il pilotaggio automatico e manuale in una nuova interfaccia di navigazione ibrida e si sbarazza del presupposto che l'oggetto di destinazione è già nel FOV del drone utilizzando una telecamera supplementare sopra la testa.
Questo articolo presenta StarHopper, un sistema per la navigazione semiautomatica dei droni nel contesto dell'ispezione remota.
Presenta StarHopper, un'applicazione che utilizza tecniche di computer vision con input tattile per supportare il pilotaggio dei droni con un approccio centrato sull'oggetto.
Una rete di auto-attenzione per la codifica di sequenze senza RNN/CNNN con piccolo consumo di memoria, calcolo altamente parallelizzabile e prestazioni all'avanguardia su diversi compiti NLP
Propone di applicare l'auto-attenzione a due livelli per limitare la richiesta di memoria nei modelli basati sull'attenzione con un impatto trascurabile sulla velocità.
Questo articolo introduce il modello di autoattenzione bidirezionale a blocchi come un codificatore generale per vari compiti di modellazione di sequenze in NLP
Un nuovo modello all'avanguardia per la risposta alle domande multi-evidenza utilizzando l'attenzione gerarchica a grana grossa e fine.
Propone un metodo per l'AQ multi-hop basato su due moduli separati (moduli a grana grossa e a grana fine).
Questo articolo propone un'interessante architettura di rete di coattenzione a grana grossa e fine per affrontare la risposta alle domande multi-evidenza
Si concentra sull'AQ multiscelta e propone un quadro di punteggio da grossolano a fine.
Proponiamo una nuova metodologia per il trasporto ottimale sbilanciato utilizzando reti generative avversarie.
Gli autori considerano il problema del trasporto ottimale sbilanciato tra due misure con massa totale diversa usando un algoritmo stocastico min-max e una scalatura locale
Gli autori propongono un approccio per stimare il trasporto ottimale sbilanciato tra misure campionate che scala bene nella dimensione e nel numero di campioni.
L'articolo introduce una formulazione statica per il trasporto ottimale sbilanciato imparando simultaneamente una mappa di trasporto T e un fattore di scala xi.
Proponiamo un nuovo metodo di estrazione delle mappe di salienza che risulta nell'estrazione di mappe di qualità superiore.
Propone un metodo indipendente dal classificatore per l'estrazione delle mappe di salienza.
Questo articolo introduce un nuovo estrattore di mappe di salienza che sembra migliorare i risultati allo stato dell'arte.
Gli autori sostengono che quando una mappa di salienza estratta dipende direttamente da un modello, allora potrebbe non essere utile per un classificatore diverso, e suggerisce uno schema per approssimare la soluzione.
Proponiamo un nuovo metodo per incorporare l'insieme degli attributi di istanza per la traduzione da immagine a immagine.
Questo articolo propone un metodo - InstaGAN - che si basa su CycleGAN prendendo in considerazione le informazioni dell'istanza sotto forma di maschere di segmentazione per istanza, con risultati che si confrontano favorevolmente con CycleGAN e altre basi.
 Propone di aggiungere maschere di segmentazione instance-aware per il problema della traduzione immagine-immagine non abbinata.
La mappa parametro-funzione delle reti profonde è enormemente distorta; questo può spiegare perché generalizzano. Usiamo PAC-Bayes e processi gaussiani per ottenere limiti nonvacui.
L'articolo studia le capacità di generalizzazione delle reti neurali profonde, con l'aiuto della teoria dell'apprendimento PAC-Bayesiana e delle intuizioni empiricamente sostenute.
Questo articolo propone una spiegazione dei comportamenti di generalizzazione delle grandi reti neurali iper-parametrizzate sostenendo che la mappa parametro-funzione nelle reti neurali è orientata verso funzioni "semplici" e il comportamento di generalizzazione sarà buono se il concetto di destinazione è anche "semplice".
Presentiamo Evolutionary EM come un nuovo algoritmo per l'addestramento non supervisionato di modelli generativi con variabili latenti binarie che collega intimamente EM variazionale con l'ottimizzazione evolutiva
L'articolo presenta una combinazione di calcolo evolutivo ed EM variazionale per modelli con variabili latenti binarie rappresentate tramite un'approssimazione basata su particelle
L'articolo fa un tentativo di integrare strettamente gli algoritmi di formazione di massimizzazione delle aspettative con gli algoritmi evolutivi.
Proponiamo un modello efficiente di rete ricorrente per la previsione in avanti su distribuzioni variabili nel tempo.
Questo articolo propone un metodo per creare reti neurali che mappano le distribuzioni storiche sulle distribuzioni e applica il metodo a diversi compiti di previsione della distribuzione.
Propone una Reccurent Distribution Regression Network che utilizza un'architettura ricorrente su un modello precedente Distribution Regression Network.
Questo articolo riguarda la regressione su distribuzioni di probabilità studiando distribuzioni variabili nel tempo in una rete neurale ricorrente
Un nuovo approccio all'elaborazione dei dati strutturati a grafo da parte delle reti neurali, sfruttando l'attenzione sul vicinato di un nodo. Raggiunge risultati allo stato dell'arte su compiti di rete citazionale trasduttiva e un compito di interazione proteina-proteina induttiva.
Questo articolo propone un nuovo metodo per classificare i nodi di un grafo, che può essere utilizzato in scenari semi-supervisionati e su un grafo completamente nuovo. 
L'articolo introduce un'architettura di rete neurale per operare su dati strutturati a grafo chiamata Graph Attention Networks.
Fornisce una discussione corretta e quasi completa dello stato dell'arte degli approcci all'apprendimento di rappresentazioni vettoriali per i nodi di un grafico.
Un nuovo approccio basato sull'apprendimento di rinforzo per comprimere le reti neurali profonde con la distillazione della conoscenza
Questo articolo propone di usare il reinforcement learning invece di euristiche predefinite per determinare la struttura del modello compresso nel processo di distillazione della conoscenza
Introduce un metodo di principio per la compressione da rete a rete, che utilizza i gradienti delle politiche per ottimizzare due politiche che comprimono un insegnante forte in un modello di studente forte ma più piccolo.
Dimostriamo che il collasso della modalità nelle GAN condizionali è in gran parte attribuito a una mancata corrispondenza tra la perdita di ricostruzione e la perdita GAN e introduciamo una serie di nuove funzioni di perdita come alternative alla perdita di ricostruzione.
L'articolo propone una modifica al tradizionale obiettivo GAN condizionale per promuovere una generazione di immagini diverse e multimodali. 
Questo articolo propone un'alternativa agli errori L1/L2 che sono usati per aumentare le perdite avversarie quando si addestrano le GAN condizionali.
Usiamo l'inferenza causale per caratterizzare l'architettura dei modelli generativi
Questo articolo esamina la natura dei filtri convoluzionali nel codificatore e nel decodificatore di una VAE, e nel generatore e nel discriminatore di una GAN.
Questo lavoro sfrutta il principio di causalità per quantificare come i pesi degli strati successivi si adattano gli uni agli altri.
Un approccio all'apprendimento di uno spazio di incorporazione condiviso tra giochi visivamente distinti.
Un nuovo approccio per l'apprendimento della struttura di base dei giochi visivamente distinti che combina strati convoluzionali per l'elaborazione delle immagini di input, Asynchronous Advantage Actor Critic per l'apprendimento di rinforzo profondo e l'approccio avversario per forzare la rappresentazione di incorporazione per essere indipendente dalla rappresentazione visiva dei giochi
Introduce un metodo per imparare una politica su giochi visivamente distinti adattando il deep reinforcement learning.
Questo articolo discute un'architettura di agente che usa una rappresentazione condivisa per addestrare compiti multipli con diverse statistiche visive a livello di sprite
Studiamo l'equazione di stato di una rete neurale ricorrente. Mostriamo che SGD può apprendere in modo efficiente la dinamica sconosciuta da poche osservazioni di input/output sotto opportune ipotesi.
L'articolo studia sistemi dinamici a tempo discreto con un'equazione di stato non lineare, dimostrando che l'esecuzione di SGD su una traiettoria di lunghezza fissa dà una convergenza logaritmica.
Questo lavoro considera il problema dell'apprendimento di un sistema dinamico non lineare in cui l'uscita è uguale allo stato. 
Questo articolo studia la capacità di SGD di apprendere le dinamiche di un sistema lineare e di attivazione non lineare.
Un nuovo metodo di distillazione della conoscenza per l'apprendimento di trasferimento
Il lavoro introduce un metodo di distillazione della conoscenza utilizzando il concetto di manifold neuronale proposto. 
Propone un metodo di distillazione della conoscenza in cui il collettore neurale è preso come la conoscenza trasferita.
Presentiamo un metodo semplice e generale per addestrare una singola rete neurale eseguibile a diverse larghezze (numero di canali in uno strato), permettendo trade-off di precisione-efficienza istantanei e adattivi a runtime.
L'articolo propone un'idea di combinare insieme modelli di dimensioni diverse in una rete condivisa, migliorando notevolmente le prestazioni per il rilevamento
Questo documento allena una singola rete eseguibile a diverse larghezze.
Rete di somiglianza per imparare una stima di somiglianza visiva non metrica tra una coppia di immagini
Gli autori propongono l'apprendimento di misure di somiglianza per la somiglianza visiva e ottengono con questo un miglioramento in dataset molto noti di Oxford e Parigi per il recupero delle immagini.
L'articolo sostiene che è più adatto usare le distanze non metriche invece di quelle metriche.
Un nuovo apprendimento avversario ciclico aumentato con un modello di compito ausiliario che migliora le prestazioni di adattamento al dominio in situazioni supervisionate e non supervisionate con poche risorse 
Propone un'estensione dei metodi di adattamento cycle-consistent adversatial al fine di affrontare l'adattamento del dominio quando sono disponibili dati di destinazione supervisionati limitati.
Questo articolo introduce un approccio di adattamento al dominio basato sull'idea di GAN ciclica e propone due diversi algoritmi.
Sviluppiamo un metodo per l'apprendimento delle firme strutturali nelle reti basato sulla diffusione delle wavelet spettrali del grafico.
Utilizzando i modelli di diffusione wavelet del grafico spettrale del meighbothood locale di un nodo per incorporare il nodo in uno spazio bidimensionale
L'articolo ha derivato un modo per confrontare i nodi nel grafico basato sull'analisi wavelet del laplaciano del grafico. 
Un simulatore di guida in realtà mista con telecamere stereo e VR passthrough valutato in uno studio utente con 24 partecipanti.
Propone un sistema complicato per la simulazione di guida.
Questo articolo presenta una configurazione di simulatore di guida in realtà mista per migliorare la sensazione di presenza
Propone un simulatore di guida in realtà mista che incorpora la generazione di traffico e sostiene una "presenza" migliorata grazie a un sistema MR.
Regole di quadratura per l'approssimazione del kernel.
L'articolo propone di migliorare l'approssimazione del kernel delle caratteristiche casuali utilizzando regole di quadratura come le regole sferico-radiali stocastiche.
Gli autori propongono una nuova versione dell'approccio random feature map per risolvere approssimativamente problemi di kernel su larga scala.
Questo articolo mostra che le tecniche dovute a Genz & Monahan (1998) possono essere utilizzate per ottenere un basso errore di approssimazione del kernel nel quadro della caratteristica di fourier casuale, un nuovo modo di applicare le regole di quadratura per migliorare l'approssimazione del kernel.
Come costruire parlanti/ascoltatori neurali che apprendono caratteristiche a grana fine di oggetti 3D, dal linguaggio referenziale.
Gli autori forniscono uno studio sull'apprendimento dei riferimenti agli oggetti 3D, raccogliendo un dataset di espressioni referenziali e addestrando diversi modelli sperimentando una serie di scelte architettoniche
Presentiamo una struttura per l'apprendimento di rappresentazioni centrate sull'oggetto adatte alla pianificazione in compiti che richiedono una comprensione della fisica.
L'articolo presenta una piattaforma per prevedere immagini di oggetti che interagiscono tra loro sotto l'effetto di forze gravitazionali.
L'articolo presenta un metodo che impara a riprodurre "torri di blocchi" da un'immagine data.
Propone un metodo che impara a ragionare sull'interazione fisica di diversi oggetti senza supervisione delle proprietà degli oggetti.
Forniamo condizioni necessarie e sufficienti verificabili in modo efficiente per l'ottimalità globale nelle reti neurali lineari profonde, con alcune estensioni iniziali alle impostazioni non lineari.
L'articolo fornisce condizioni per l'ottimalità globale della funzione di perdita delle reti neurali lineari profonde
L'articolo fornisce risultati teorici sull'esistenza di minimi locali nella funzione obiettivo delle reti neurali profonde.
Studia alcune proprietà teoriche delle reti lineari profonde.
Utilizzando il modello auto-encoder ricorrente per estrarre le caratteristiche multidimensionali delle serie temporali
Questo articolo descrive un'applicazione di autoencoder ricorrente per analizzare serie temporali multidimensionali
L'articolo descrive un modello di auto-encoder da sequenza a sequenza che viene utilizzato per imparare rappresentazioni di sequenze, mostrando che per la loro applicazione, si ottengono prestazioni migliori quando la rete viene addestrata solo per ricostruire un sottoinsieme delle misure dei dati. 
Propone una strategia ispirata al modello auto-encoder ricorrente in modo che il clustering di dati multidimensionali di serie temporali possa essere eseguito sulla base di vettori di contesto.
Introduciamo un quadro di codifica-decodifica da grafico a grafico per imparare diverse traduzioni di grafici.
Propone un modello di traduzione da grafico a grafico per l'ottimizzazione delle molecole ispirato all'analisi delle coppie molecolari abbinate.
Estensione di JT-VAE nello scenario di traduzione da grafico a grafico aggiungendo la variabile latente per catturare la multi-modalità e una regolarizzazione adversariale nello spazio latente
Propone un sistema abbastanza complesso, che coinvolge molte scelte e componenti diverse, per ottenere composti chimici con proprietà migliorate a partire da un dato corpora.
Impariamo un solutore neurale veloce per le PDE che ha garanzie di convergenza.
Sviluppa un metodo per accelerare il metodo delle differenze finite nella risoluzione delle PDE e propone un quadro rivisto per l'iterazione del punto fisso dopo la discretizzazione.
Gli autori propongono un metodo lineare per accelerare i solutori di PDE.
Eseguiamo l'inferenza funzionale variazionale sui processi stocastici definiti dalle reti neurali bayesiane.
Adattamento di approssimazioni di reti neurali bayesiane variazionali in forma funzionale e considerando la corrispondenza a un processo stocastico a priori implicitamente tramite campioni.
Presenta un nuovo obiettivo ELBO per l'addestramento dei BNN che permette di codificare nel modello priori più significativi piuttosto che i priori di peso meno informativi presenti in letteratura.
Presenta un nuovo algoritmo di inferenza variazionale per modelli di reti neurali bayesiani in cui il priore è specificato in modo funzionale piuttosto che attraverso un priore sui pesi. 
Incorporiamo le parole nello spazio iperbolico e facciamo la connessione con le embeddings di parole gaussiane.
Questo articolo adatta la parola Glove embedding a uno spazio iperbolico dato dal modello del semipiano di Poincare
Questo articolo propone un approccio per implementare un modello di embedding di parole iperbolico basato su GLOVE, che è ottimizzato attraverso i metodi di ottimizzazione Riemanniana.
Le reti di memoria non imparano il ragionamento multi-hop a meno che non le supervisioniamo.
Le affermazioni sul ragionamento multi-hop non è facile da imparare direttamente e richiede una supervisione diretta e fare bene su WikiHop non significa necessariamente che il modello stia effettivamente imparando a saltare.
L'articolo si propone di indagare il ben noto problema dell'apprendimento delle reti di memoria e più precisamente la difficoltà della supervisione dell'apprendimento dell'attenzione con tali modelli.
Questo articolo sostiene che la rete di memoria non riesce ad apprendere un ragionevole ragionamento multi-hop.
Introduciamo reti generative che non richiedono di essere apprese con un discriminatore o un codificatore; si ottengono invertendo uno speciale operatore di embedding definito da una trasformata wavelet Scattering.
Introduce le trasformazioni di dispersione come modelli generativi di immagini nel contesto delle Reti Generative Adversariali e suggerisce perché potrebbero essere viste come trasformazioni di Gaussianizzazione con perdita di informazione controllata e invertibilità. 
L'articolo propone un modello generativo per le immagini che non richiede l'apprendimento di un discriminatore (come nei GAN) o di un embedding appreso.
Proponiamo un nuovo modello per la lettura di velocità neurale che utilizza la struttura di punteggiatura intrinseca di un testo per definire un comportamento efficace di salto e di salto.
L'articolo propone un modello Structural-Jump-LSTM per accelerare la lettura automatica con due agenti invece di uno
Propone un nuovo modello per la lettura di velocità neurale in cui il nuovo lettore ha la capacità di saltare una parola o una sequenza di parole.
L'articolo propone un metodo di lettura veloce usando azioni di salto e di salto, mostrando che il metodo proposto è accurato come LSTM ma usa molto meno calcolo.
Proponiamo l'architettura IRN per aumentare la ricompensa di acquisto sparsa e ritardata per la raccomandazione basata sulla sessione.
L'articolo propone di migliorare le prestazioni dei sistemi di raccomandazione attraverso l'apprendimento di rinforzo utilizzando una rete di ricostruzione dell'immaginazione.
L'articolo presenta un approccio di raccomandazione basato sulla sessione, concentrandosi sugli acquisti degli utenti invece che sui clic. 
Spiegare la generalizzazione degli algoritmi di apprendimento profondo stocastico, teoricamente ed empiricamente, attraverso la robustezza dell'insieme
Questo articolo presenta un adattamento della robustezza algoritmica di Xu&Mannor'12 e presenta limiti di apprendimento e un esperimento che mostra la correlazione tra la robustezza empirica dell'insieme e l'errore di generalizzazione. 
Propone uno studio della capacità di generalizzazione degli algoritmi di apprendimento profondo utilizzando un'estensione della nozione di stabilità chiamata robustezza dell'insieme e fornisce limiti all'errore di generalizzazione di un algoritmo randomizzato in termini di parametro di stabilità e fornisce uno studio empirico che cerca di collegare la teoria con la pratica.
L'articolo ha studiato la capacità di generalizzazione degli algoritmi di apprendimento dal punto di vista della robustezza in un contesto di apprendimento profondo
Apprendimento a pochi colpi PixelCNN
L'articolo propone di utilizzare la stima della densità quando la disponibilità di dati di formazione è bassa, utilizzando un modello di meta-apprendimento.
Questo articolo considera il problema della stima della densità di uno/pochi colpi, usando tecniche di metalearning che sono state applicate all'apprendimento supervisionato di uno/pochi colpi
L'articolo si concentra sull'apprendimento di pochi colpi con la stima della densità autoregressiva e migliora PixelCNN con l'attenzione neurale e le tecniche di meta apprendimento.
Mostriamo che SGD apprende reti neurali a due strati iperparametrizzate con attivazioni Leaky ReLU che generalizzano in modo dimostrabile su dati linearmente separabili.
L'articolo studia i modelli iperparametrizzati in grado di apprendere soluzioni ben generalizzate utilizzando una rete a 1 strato nascosto con uno strato di uscita fisso.
Questo articolo mostra che su dati linearmente separati, SGD su una rete iperparametrizzata può ancora appoggiare un classificatore che generalizza in modo dimostrabile.
L'addestramento degli agenti con i colli di bottiglia dell'informazione sulle politiche degli obiettivi promuove il trasferimento e produce un potente bonus di esplorazione
Propone la regolarizzazione delle perdite RL standard con l'informazione reciproca condizionata negativa per la ricerca della politica in un ambiente RL a più obiettivi.
Questo articolo propone il concetto di stato di decisione e propone una regolarizzazione di divergenza KL per apprendere la struttura dei compiti e usare questa informazione per incoraggiare la politica a visitare gli stati di decisione.
L'articolo propone un metodo per regolarizzare le politiche condizionate dagli obiettivi con un termine di informazione reciproca. 
Proponiamo un metodo di ottimizzazione per quando sono disponibili solo gradienti distorti - definiamo un nuovo stimatore di gradiente per questo scenario, deriviamo la distorsione e la varianza di questo stimatore e lo applichiamo a problemi di esempio.
Gli autori propongono un approccio che combina la ricerca casuale con l'informazione del gradiente surrogato e danno una discussione sul trade-off varianza-bias così come una discussione sull'ottimizzazione degli iperparametri.
 L'articolo propone un metodo per migliorare la ricerca casuale costruendo un sottospazio dei precedenti k gradienti surrogati.
Questo articolo cerca di accelerare l'evoluzione del tipo OpenAI introducendo una distribuzione non isotrofica con una matrice di covarianza nella forma I + UU^t e informazioni esterne come un gradiente surrogato per determinare U
Una GAN che usa operazioni di convoluzione dei grafi con grafici calcolati dinamicamente da caratteristiche nascoste
L'articolo propone una versione di GANs specificamente progettata per la generazione di nuvole di punti con il contributo centrale del lavoro l'operazione di upsampling.
Questo articolo propone delle GAN grafo-convoluzionali per nuvole di punti 3D irregolari che imparano il dominio e le caratteristiche allo stesso tempo.
Introduciamo un algoritmo veloce e facile da implementare che è robusto al rumore del dataset.
Il documento mira a rimuovere i potenziali esempi con rumore di etichetta scartando quelli con grandi perdite nella procedura di formazione.
Gli attacchi basati sul gradiente sulle reti neurali binarizzate non sono efficaci a causa della non differenziabilità di tali reti; il nostro algoritmo IPROP risolve questo problema utilizzando l'ottimizzazione intera
Propone un nuovo algoritmo in stile propagazione degli obiettivi per generare forti attacchi avversari sulle reti neurali binarizzate.
Questo articolo ha proposto un nuovo algoritmo di attacco basato su MILP su reti neurali binarie.
Questo articolo presenta un algoritmo per trovare attacchi avversari alle reti neurali binarie che trova iterativamente le rappresentazioni desiderate strato per strato dall'alto all'ingresso ed è più efficiente che risolvere il risolutore completo di programmazione lineare integrale mista (MILP).
La decodifica dell'ultimo token nel contesto usando la distribuzione prevista del token successivo agisce come un regolatore e migliora la modellazione del linguaggio.
Gli autori introducono l'idea della decodifica del passato allo scopo di regolarizzare la perplessità su Penn Treebank
Propone un termine di perdita aggiuntivo da usare quando si addestra un LSTM LM e mostra che aggiungendo questo termine di perdita si può raggiungere la perplessità SOTA su una serie di benchmark LM.
Suggerisce una nuova tecnica di regolarizzazione che può essere aggiunta in cima a quelle usate in AWD-LSTM di Merity et al. (2017) con poco overhead.
Proporre di osservare gli ordini impliciti nei set di dati in un'ottica di modello generativo.
Gli autori affrontano il problema dell'ordinamento implicito in un set di dati e la sfida di recuperarlo e propongono di imparare un modello senza distanza metrica che assume una catena di Markov come meccanismo generativo dei dati 
L'articolo propone Generative Markov Networks - un approccio basato sull'apprendimento profondo per modellare le sequenze e scoprire l'ordine nei set di dati.
Propone di imparare l'ordine di un campione di dati non ordinato imparando una catena di Markov.
Sintesi del programma dalla descrizione del linguaggio naturale e dagli esempi di input/output tramite la ricerca Tree-Beam sul modello Seq2Tree
Presenta un modello seq2Tree per tradurre una dichiarazione di problema in linguaggio naturale al corrispondente programma funzionale in DSL, che ha mostrato un miglioramento rispetto all'approccio di base seq2seq.
Questo articolo affronta il problema di fare la sintesi di un programma quando viene data una descrizione del problema e un piccolo numero di esempi di input-output.
L'articolo introduce una tecnica per la sintesi di programmi che coinvolge una grammatica ristretta di problemi che è cercata a raggiera usando una rete attenzionale encoder-decoder.
Questo articolo studia le proprietà di discriminazione e generalizzazione dei GAN quando l'insieme discriminatore è una classe di funzioni ristretta come le reti neurali.
Bilancia le capacità delle classi di generatori e discriminatori nelle GAN garantendo che le IPM indotte siano metriche e non pseudo metriche
Questo articolo fornisce un'analisi matematica del ruolo della dimensione dell'insieme avversario/discriminatore nelle GAN
Tutto ciò di cui avete bisogno per addestrare reti residue profonde è una buona inizializzazione; gli strati di normalizzazione non sono necessari.
Viene presentato un metodo per l'inizializzazione e la normalizzazione delle reti residuali profonde. Questo si basa sulle osservazioni dell'esplosione in avanti e all'indietro in tali reti. La performance del metodo è alla pari con i migliori risultati ottenuti da altre reti con una normalizzazione più esplicita.
Gli autori propongono un nuovo modo di inizializzare le reti residue, che è motivato dalla necessità di evitare l'esplodere/vanire dei gradienti.
Propone un nuovo metodo di inizializzazione usato per addestrare RedNets molto profonde senza usare il batch-norm.
Questo articolo mira a imparare una metrica migliore per l'apprendimento non supervisionato, come la generazione di testo, e mostra un miglioramento significativo rispetto a SeqGAN.
Descrive un approccio alla generazione di sequenze temporali attraverso l'apprendimento di valori stato-azione, dove lo stato è la sequenza generata finora, e l'azione è la scelta del valore successivo. 
Questo articolo considera il problema di migliorare la generazione di sequenze imparando metriche migliori, in particolare il problema del bias di esposizione
Decomporre il compito di apprendere un modello generativo nell'apprendimento di fattori latenti disgiunti per sottoinsiemi di dati e poi apprendere il congiunto su quei fattori latenti.  
Fattori Localmente Disentangled per il modello generativo gerarchico a variabili latenti, che può essere visto come una variante gerarchica di Adversarially Learned Inference
L'articolo indaga il potenziale dei modelli gerarchici di variabili latenti per generare immagini e sequenze di immagini e propone di addestrare diversi modelli ALI impilati uno sull'altro per creare una rappresentazione gerarchica dei dati.
L'articolo mira ad apprendere le gerarchie per l'addestramento di GAN in un programma di ottimizzazione gerarchica direttamente invece di essere progettato da un umano
Proponiamo un modello congiunto per incorporare la conoscenza visiva nelle rappresentazioni delle frasi
L'articolo propone un metodo per utilizzare i video abbinati a didascalie per migliorare l'embedding delle frasi
Questa presentazione propone un modello per l'apprendimento di rappresentazioni di frasi che sono fondate, basate su dati video associati.
Propone un metodo per migliorare le embeddings di frasi basate sul testo attraverso un quadro multimodale congiunto.
