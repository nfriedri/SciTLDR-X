Abbiamo ideato una scalatura adattativa delle perdite per migliorare l'addestramento di precisione mista che supera i risultati dello stato dell'arte.
Proposta di un metodo adattivo di scalatura delle perdite durante la backpropagation per l'addestramento di precisione mista dove il tasso di scala viene deciso automaticamente per ridurre l'underflow.
Gli autori propongono un metodo per addestrare modelli in precisione FP16 che adotta un modo più elaborato per minimizzare l'underflow in ogni strato simultaneamente e automaticamente.
Presentiamo un nuovo approccio per imparare a prevedere insiemi con permutazione e cardinalità sconosciute usando reti neurali profonde feed-forward.
Una formulazione per imparare la distribuzione su variabili di permutazione non osservabili basata su reti profonde per il problema della previsione degli insiemi.
Confrontiamo le prestazioni di riconoscimento degli oggetti su immagini sottocampionate in modo uniforme e con tre diversi schemi di foveazione.
Sviluppiamo metodi per addestrare modelli neurali profondi che sono robusti alle perturbazioni avversarie e la cui robustezza è significativamente più facile da verificare.
L'articolo presenta diversi modi per regolarizzare le reti ReLU semplici per ottimizzare la robustezza avversaria, la robustezza avversaria dimostrabile e la velocità di verifica.
Questo articolo propone metodi per addestrare reti neurali robuste che possono essere verificate più velocemente, utilizzando metodi di potatura per incoraggiare la sparsità dei pesi e la regolarizzazione per incoraggiare la stabilità ReLU.
Indagine su come BatchNorm provoca la vulnerabilità avversaria e come evitarla. 
Questo articolo affronta la vulnerabilità alle perturbazioni avversarie in BatchNorm, e propone un'alternativa chiamata RobustNorm, utilizzando il rescaling min-max invece della normalizzazione.
Questo articolo indaga la ragione dietro la vulnerabilità di BatchNorm e propone Robust Normalization, un metodo di normalizzazione che raggiunge risultati significativamente migliori sotto una varietà di metodi di attacco.
La nostra rete di imputazione variazionale-recorrente (V-RIN) prende in considerazione le caratteristiche correlate, le dinamiche temporali e utilizza ulteriormente l'incertezza per alleviare il rischio di stime distorte dei valori mancanti.
Una rete di imputazione dei dati mancanti per incorporare la correlazione, le relazioni temporali e l'incertezza dei dati per il problema della scarsità dei dati in EHRs, che produce un AUC più alto sui compiti di classificazione del tasso di mortalità.
L'articolo ha presentato un metodo che combina VAE e GRU consapevole dell'incertezza per l'imputazione sequenziale dei dati mancanti e la previsione dei risultati.
Un metodo adattivo per la quantizzazione a punto fisso delle reti neurali basato sull'analisi teorica piuttosto che sull'euristica. 
Propone un metodo per quantizzare le reti neurali che permette di quantizzare i pesi con una precisione diversa a seconda della loro importanza, tenendo conto della perdita.
L'articolo propone una tecnica per quantizzare i pesi di una rete neurale con profondità/precisione di bit variabile per ogni parametro.
Sulla base della teoria degli insiemi fuzzy, proponiamo un modello che, date solo le dimensioni delle differenze simmetriche tra coppie di multiset, impara le rappresentazioni di tali multiset e dei loro elementi.
Questo articolo propone un nuovo compito di apprendimento degli insiemi, prevedendo la dimensione della differenza simmetrica tra più insiemi, e fornisce un metodo per risolvere il compito basato sulla teoria degli insiemi fuzzy.
Questo articolo propone un metodo basato sull'apprendimento profondo per ottenere campioni credibili da agenti auto-interessati. 
Gli autori propongono un quadro di elicitazione del campione per il problema di elicitare campioni credibili dagli agenti per distribuzioni complesse, suggeriscono che i quadri neurali profondi possono essere applicati in questo quadro, e collegano l'elicitazione del campione e f-GAN.
Questo articolo studia il problema dell'elicitazione del campione, proponendo un approccio di apprendimento profondo che si basa sull'espressione duale della f-divergenza che scrive come massimo su un insieme di funzioni t.
Apprendimento da grafico a sequenza con reti neurali basate sull'attenzione
Un'architettura graph2seq che combina un codificatore di grafici che mescola componenti GGNN e GCN con un codificatore di sequenze attenzionali, e che mostra miglioramenti rispetto alle basi.
Questo lavoro propone un codificatore di grafici end-to-end a modelli di decodificatori di sequenze con un meccanismo di attenzione nel mezzo.
Un quadro di segmentazione a zero colpi per la segmentazione di parti di oggetti 3D. Modellare la segmentazione come un processo decisionale e risolvere come un problema di bandito contestuale.
Un metodo per la segmentazione di nuvole di punti 3D di oggetti in parti componenti, focalizzato sulla generalizzazione dei raggruppamenti di parti a nuove categorie di oggetti non visti durante l'addestramento, che mostra forti prestazioni rispetto alle linee di base.
Questo articolo propone un metodo per la segmentazione di parti in nuvole di punti di oggetti.
Una nuova prospettiva su come raccogliere la correlazione tra i nodi basata sulle proprietà di diffusione.
Una nuova operazione di diffusione per le reti neurali a grafo che non richiede il calcolo degli autovalori e può propagarsi esponenzialmente più velocemente rispetto alle reti neurali a grafo tradizionali.
L'articolo propone di affrontare il problema della velocità di diffusione introducendo la camminata balistica.
Proponiamo una pipeline di addestramento di supervisione debole basata sul quadro di programmazione dei dati per compiti di classificazione, in cui addestriamo un modello di classificazione basato su BERT e stabiliamo la nuova SOTA.
Gli autori propongono una combinazione di BERT e del quadro di supervisione debole per affrontare il problema della classificazione dei passaggi, ottenendo risultati migliori dello stato dell'arte completamente supervisionato.
Nei problemi reali, abbiamo trovato che le DNN spesso adattano le funzioni target dalle basse alle alte frequenze durante il processo di formazione.
Questo articolo analizza la perdita delle reti neurali nel dominio di Fourier e trova che le DNN tendono ad imparare le componenti a bassa frequenza prima di quelle ad alta frequenza.
L'articolo studia il processo di formazione delle NN attraverso l'analisi di Fourier, concludendo che le NN imparano le componenti a bassa frequenza prima di quelle ad alta frequenza.
Proponiamo un encoder-decoder multi-risoluzione, accoppiato gerarchicamente per la traduzione grafo-grafo.
Un modello gerarchico di traduzione grafo-grafico per generare grafi molecolari usando sottostrutture chimiche come elementi costitutivi che è completamente autoregressivo e impara rappresentazioni coerenti a multirisoluzione, superando i modelli precedenti.
Gli autori presentano un metodo di traduzione gerarchica da grafico a grafico per generare nuove molecole organiche.
Utilizziamo l'attenzione per limitare le reti neurali equivarianti all'insieme o alle trasformazioni co-occorrenti nei dati. 
Questo articolo combina l'attenzione con l'equivarianza di gruppo, in particolare guardando al gruppo p4m di rotazioni, traslazioni e capovolgimenti, e deriva una forma di autoattenzione che non distrugge la proprietà di equivarianza.
Gli autori propongono un meccanismo di auto-attenzione per le reti neurali roto-equivarianti che migliora le prestazioni di classificazione rispetto alle reti roto-equivarianti regolari.
Addestriamo una GAN per generare e recuperare backbone di proteine full-atom, e mostriamo che in casi selezionati possiamo recuperare le proteine generate dopo la progettazione della sequenza e il forward-folding ab initio.
Un modello generativo per la spina dorsale della proteina che utilizza una GAN, una rete di tipo autoencoder e un processo di raffinamento, e una serie di valutazioni qualitative che suggeriscono risultati positivi.
Questo articolo presenta un approccio end-to-end per la generazione di dorsali proteiche utilizzando reti generative avversarie.
Il Meta Learning per l'apprendimento di pochi colpi presuppone che i compiti di allenamento e i compiti di test siano tratti dalla stessa distribuzione. Cosa fare se non lo sono? Meta Apprendimento con adattamento del dominio a livello di compito!
Questo articolo propone un modello che combina l'adattamento non supervisionato del dominio avversario con le reti prototipiche che esegue meglio delle basi di apprendimento a pochi colpi su compiti di apprendimento a pochi colpi con spostamento del dominio.
Gli autori hanno proposto l'adattamento al meta-dominio per affrontare lo scenario del cambio di dominio nella configurazione del meta-apprendimento, dimostrando miglioramenti delle prestazioni in diversi esperimenti.
Divide, Conquer, and Combine è un nuovo schema di inferenza che può essere eseguito sui programmi probabilistici con supporto stocastico, cioè l'esistenza stessa delle variabili è stocastica.
Un algoritmo di incorporazione dei nodi che preserva la comunità e che risulta in un rilevamento più efficace delle comunità con un clustering sullo spazio incorporato
Un modello di apprendimento profondo autoregressivo per la generazione di nuvole di punti diversi.
Un approccio per la generazione di forme 3D come nuvole di punti che considera l'ordinamento lessicografico dei punti secondo le coordinate e addestra un modello per prevedere i punti in ordine.
L'articolo introduce un modello generativo per le nuvole di punti utilizzando un modello auto-regressivo simile al pixel RNN e un modello di attenzione per gestire le interazioni a lungo raggio.
Descrive una serie di tecniche di spiegabilità applicate a un semplice controller di rete neurale utilizzato per la navigazione.
Questo articolo fornisce intuizioni e spiegazioni sul problema di fornire spiegazioni per un perceptron multistrato usato come controller inverso per il movimento del rover, e idee su come spiegare un modello black-box.
Proponiamo un agente di auto-monitoraggio per il compito di navigazione con visione e lingua.
Un metodo per la navigazione in visione+lingua che tiene traccia dei progressi sull'istruzione usando un monitor di progresso e un modulo di co-grounding visivo-testuale, e ha buone prestazioni sui benchmark standard.
Questo articolo descrive un modello per la navigazione in visione e lingua con un'attenzione visiva panoramica e una perdita ausiliaria di monitoraggio del progresso, dando risultati all'avanguardia.
scoperta di eventi per rappresentare la storia dell'agente in RL
Gli autori studiano il problema di RL sotto impostazioni parzialmente osservate, e propongono una soluzione che utilizza una FFNN ma fornisce una rappresentazione della storia, superando PPO.
Questo articolo propone un nuovo modo di rappresentare la storia passata come input per un agente RL, mostrando di avere prestazioni migliori di PPO e di una variante RNN di PPO.
Mostriamo che i modelli autoregressivi possono generare immagini ad alta fedeltà. 
Un'architettura che utilizza componenti decoder, size-upscaling decoder e depth-upscaling decoder per affrontare il problema dell'apprendimento delle dipendenze a lungo raggio nelle immagini al fine di ottenere immagini ad alta fedeltà.
Questo articolo affronta il problema della generazione di immagini ad alta fedeltà, mostrando con successo campioni convincenti di Imagenet con risoluzione 128x128 per un modello di densità di verosimiglianza.
Un modello di spazio di stato gerarchico profondo in cui le transizioni di stato degli oggetti correlati sono coordinate da reti neurali a grafo.
Un modello gerarchico a variabili latenti di processi dinamici sequenziali di oggetti multipli quando ogni oggetto presenta una stocasticità significativa.
L'articolo presenta un modello di spazio di stato relazionale che simula le transizioni di stato congiunte di oggetti correlati che sono gerarchicamente coordinati in una struttura a grafo.
Introduciamo un nuovo bias induttivo che integra le strutture ad albero nelle reti neurali ricorrenti.
Questo articolo propone ON-LSTM, una nuova unità RNN che integra la struttura ad albero latente nei modelli ricorrenti e che ha buoni risultati nella modellazione del linguaggio, nel parsing non supervisionato, nella valutazione sintattica mirata e nell'inferenza logica.
I collettori degenerati derivanti dalla non identificabilità del modello rallentano l'apprendimento nelle reti profonde; le connessioni saltate aiutano a rompere le degenerazioni.
Gli autori mostrano che le singolarità di eliminazione e le singolarità di sovrapposizione impediscono l'apprendimento nelle reti neurali profonde, e dimostrano che le connessioni di salto possono ridurre la prevalenza di queste singolarità, accelerando l'apprendimento.
Il documento esamina l'uso delle connessioni di salto nelle reti profonde come un modo per alleviare le singolarità nella matrice Hessiana durante l'allenamento.
Apprendimento di rappresentazioni di stato che catturano i fattori necessari per il controllo
Un approccio all'apprendimento delle rappresentazioni nel contesto dell'apprendimento per rinforzo che distingue due fasi in termini di azioni necessarie per raggiungerle.
L'articolo presenta un metodo per imparare rappresentazioni in cui la vicinanza nella distanza euclidea rappresenta stati che sono raggiunti da politiche simili.
Studiamo il comportamento di una CNN mentre padroneggia nuovi compiti mentre conserva la padronanza per i compiti precedentemente appresi
Morty modifica le embeddings di parole pretrainate per: (a) migliorare le prestazioni complessive di embedding (per le impostazioni multi-task) o migliorare le prestazioni single-task, richiedendo solo uno sforzo minimo.
Aumentando selettivamente i punti difficili da classificare si ottiene un addestramento efficiente.
Gli autori studiano il problema dell'identificazione delle strategie di sottocampionamento per l'aumento dei dati e propongono strategie basate sull'influenza e la perdita del modello, così come il benchmarking empirico dei metodi proposti.
Gli autori propongono di usare metodi basati sull'influenza o sulla perdita per selezionare un sottoinsieme di punti da usare per aumentare i set di dati per l'addestramento di modelli in cui la perdita è additiva sui punti dei dati.
Un modello generativo profondo per le molecole organiche che genera prima i blocchi di costruzione dei reagenti prima di combinarli usando un predittore di reazione.
Un modello generativo molecolare che genera molecole attraverso un processo a due fasi che fornisce percorsi di sintesi delle molecole generate, permettendo agli utenti di esaminare l'accessibilità sintetica dei composti generati.
Migliorare la robustezza e l'efficienza energetica di una rete neurale profonda usando le rappresentazioni nascoste.
Questo articolo mira a ridurre gli errori di classificazione delle reti neurali profonde in un modo efficiente dal punto di vista energetico aggiungendo Celle ausiliarie basate su caratteristiche rilevanti dopo uno o più strati nascosti per decidere se terminare la classificazione in anticipo.
Comprendere la struttura della rappresentazione dei grafi di conoscenza usando l'intuizione di embeddings di parole.
Questo articolo cerca di capire la struttura latente che sta alla base dei metodi di incorporazione dei grafi di conoscenza, e dimostra che la capacità di un modello di rappresentare un tipo di relazione dipende dai limiti dell'architettura del modello rispetto alle condizioni di relazione.
Questo articolo propone uno studio dettagliato sulla spiegabilità dei modelli di previsione dei link (LP) utilizzando una recente interpretazione delle embeddings di parole per fornire una migliore comprensione delle prestazioni dei modelli LP.
Un nuovo meccanismo di autoattenzione per l'imputazione di serie temporali multivariate e geo-taggate.
Questo articolo propone il problema di applicare la rete di trasformazione ai dati spazio-temporali in un modo computazionalmente efficiente, e studia i modi di implementare l'attenzione 3D.
Questo articolo studia empiricamente l'efficacia dei modelli di trasformazione per l'imputazione dei dati delle serie temporali attraverso le dimensioni dell'input.
Abbiamo progettato e testato un REDNET (ResNet Encoder-Decoder) con 8 connessioni di salto per rimuovere il rumore dai documenti, compresa la sfocatura e le filigrane, ottenendo una rete profonda ad alte prestazioni per la pulizia delle immagini dei documenti. 
Identifichiamo una famiglia di tecniche di difesa e mostriamo che sia la compressione lossy deterministica che le perturbazioni randomizzate all'input portano a guadagni simili nella robustezza.
Questo articolo discute i modi di destabilizzare un dato attacco avversario, cosa rende le immagini avversarie non robuste, e se è possibile per gli attaccanti usare un modello universale di perturbazioni per rendere i loro esempi avversari robusti contro tali perturbazioni.
L'articolo studia la robustezza degli attacchi avversari alle trasformazioni del loro input.
Forniamo un metodo per il benchmark degli ottimizzatori che è consapevole del processo di sintonizzazione degli iperparametri.
Introduzione di una nuova metrica per catturare la sintonizzazione di un ottimizzatore, e un confronto empirico completo degli ottimizzatori di apprendimento profondo sotto diverse quantità di sintonizzazione dell'iperparametro. 
Questo articolo introduce una semplice misura di sintonizzabilità che permette di confrontare gli ottimizzatori sotto vincoli di risorse, scoprendo che la sintonizzazione del tasso di apprendimento degli ottimizzatori di Adam è la più facile per trovare configurazioni di iperparametri ben performanti.
Introduciamo una rete neurale profonda semi-supervisionata per approssimare la soluzione del problema di fase nella microscopia elettronica
Word2net è un nuovo metodo per l'apprendimento di rappresentazioni di reti neurali di parole che possono usare le informazioni sintattiche per imparare migliori caratteristiche semantiche.
Questo articolo estende SGNS con un cambiamento architettonico da un modello bag-of-words a un modello feedforward, e contribuisce a una nuova forma di regolarizzazione legando un sottoinsieme di strati tra diverse reti associate.
Un metodo per utilizzare la combinazione non lineare di vettori di contesto per l'apprendimento della rappresentazione vettoriale delle parole, dove l'idea principale è quella di sostituire ogni incorporazione di parole con una rete neurale.
Utilizzando una finestra di 10s di segnali fMRI, il nostro modello GCN ha identificato 21 diverse condizioni di compito dal dataset HCP con una precisione di prova del 89%.
Induzione efficiente di reti neurali profonde a basso rango tramite formazione SVD con valori singolari radi e vettori singolari ortogonali.
Questo articolo introduce un approccio alla compressione della rete incoraggiando la matrice dei pesi in ogni strato ad avere un rango basso e fattorizzando esplicitamente le matrici dei pesi in una fattorizzazione simile a SVD per il trattamento come nuovi parametri.
Proposta di parametrizzare ogni strato di una rete neurale profonda, prima dell'addestramento, con una decomposizione di matrice a basso rango, di conseguenza sostituire le convoluzioni con due convoluzioni consecutive, e poi addestrare il metodo decomposto.
Proponiamo un modello di apprendimento a pochi scatti che è fatto su misura per compiti di regressione
Questo articolo propone un nuovo metodo di apprendimento a scatti per problemi di regressione su piccoli campioni.
Un metodo che impara un modello di regressione con pochi campioni e supera gli altri metodi.
Presentiamo un nuovo approccio per rilevare i pixel fuori distribuzione nella segmentazione semantica.
Questo articolo affronta il rilevamento di fuori distribuzione per aiutare il processo di segmentazione, e propone un approccio di addestramento di un classificatore binario che distingue le patch dell'immagine da un insieme di classi conosciute da quelle di una sconosciuta.
Questo documento mira a rilevare i pixel fuori distribuzione per la segmentazione semantica, e questo lavoro utilizza i dati di altri domini per rilevare le classi indeterminate per modellare meglio l'incertezza.
Quantizzazione accurata, veloce e automatizzata della rete neurale Kernel-Wise con precisione mista usando l'apprendimento di rinforzo profondo gerarchico
Un metodo per quantizzare i pesi e le attivazioni delle reti neurali che utilizza l'apprendimento di rinforzo profondo per selezionare la larghezza di bit per i singoli kernel in uno strato e che raggiunge prestazioni migliori, o latenza, rispetto agli approcci precedenti.
Questo articolo propone di cercare automaticamente schemi di quantizzazione per ogni kernel nella rete neurale, usando la RL gerarchica per guidare la ricerca. 
Gaggle, un sistema analitico visivo interattivo per aiutare gli utenti a navigare interattivamente nello spazio dei modelli per compiti di classificazione e classificazione.
Un nuovo sistema analitico visivo che mira a permettere agli utenti non esperti di navigare in modo interattivo in uno spazio modello utilizzando un approccio basato sulla dimostrazione.
Un sistema di visual analytics che aiuta gli analisti alle prime armi a navigare nello spazio dei modelli nell'eseguire compiti di classificazione e classificazione.
Proponiamo una nuova rete di attenzione con il codificatore hybird per risolvere il problema della rappresentazione del testo della classificazione del testo cinese, specialmente i fenomeni linguistici sulle pronunce come il polifono e l'omofono.
Questo articolo propone un modello basato sull'attenzione composto dal codificatore di parole e dal codificatore Pinyin per il compito di classificazione del testo cinese, ed estende l'architettura per il codificatore di caratteri Pinyin.
Proposta di una rete di attenzione in cui sia la parola che il pinyin sono considerati per la rappresentazione cinese, con risultati migliori mostrati in diversi set di dati per la classificazione del testo.
apprendimento di imitazione multimodale da dimostrazioni non strutturate utilizzando l'intenzione di modellazione di rete neurale stocastica. 
Un nuovo approccio basato sul campionamento per l'inferenza nei modelli a variabile latente che si applica all'apprendimento dell'imitazione multimodale e funziona meglio delle reti neurali deterministiche e delle reti neurali stocastiche per un vero compito di robotica visiva.
Questo articolo mostra come apprendere diverse modalità utilizzando l'apprendimento per imitazione da dati visivi utilizzando reti neurali stocastiche, e un metodo per imparare da dimostrazioni in cui sono date diverse modalità dello stesso compito.
Un nuovo approccio per costruire spiegazioni gerarchiche per la classificazione del testo rilevando le interazioni delle caratteristiche.
Un nuovo metodo per fornire spiegazioni per le predizioni fatte dai classificatori di testo che supera le prestazioni di base sui punteggi di importanza a livello di parola, e una nuova metrica, la perdita di coesione, per valutare l'importanza a livello di span.
Un metodo di interpretazione basato sulle interazioni delle caratteristiche e sul punteggio di importanza delle caratteristiche rispetto ai contributi indipendenti delle caratteristiche.
Facciamo funzionare più velocemente gli strati convoluzionali potenziando e sopprimendo dinamicamente i canali nel calcolo delle caratteristiche.
Un metodo di potenziamento e soppressione delle caratteristiche per la potatura dinamica dei canali che predice l'importanza di ogni canale e poi usa una funzione affine per amplificare/sopprimere l'importanza del canale.
Proposta di un metodo di potatura dei canali per selezionare dinamicamente i canali durante il test.
Le reti neurali possono essere pre-definite per avere una connettività rada senza degradazione delle prestazioni.
Questo articolo esamina i modelli di connessione sparsi negli strati superiori delle reti di classificazione delle immagini convoluzionali, e introduce un'euristica per distribuire le connessioni tra finestre/gruppi e una misura chiamata scatter per costruire maschere di connettività.
Proposta di ridurre il numero di parametri appresi da una rete profonda impostando pesi di connessione sparsi negli strati di classificazione, e introduzione di un concetto di "dispersione".
Forniamo un benchmark completo, rigoroso e coerente per valutare la robustezza avversaria dei modelli di apprendimento profondo.
Questo articolo presenta una valutazione di diversi tipi di modelli di classificazione sotto vari metodi di attacco avversario.
Uno studio empirico su larga scala che confronta diverse tecniche di attacco e difesa avversaria, e l'uso della precisione rispetto al budget di perturbazione e della precisione rispetto alle curve di forza dell'attacco per valutare attacchi e difese.
Proponiamo una modifica alle reti neurali artificiali tradizionali motivata dalla biologia dei neuroni per permettere alla forma della funzione di attivazione di essere dipendente dal contesto.
Un metodo per scalare le attivazioni di uno strato di neuroni in una RNA a seconda degli ingressi a quello strato che riporta miglioramenti al di sopra delle linee di base.
Introduzione di un cambiamento architettonico per i neuroni di base in una rete neurale, e l'idea di moltiplicare l'uscita della combinazione lineare dei neuroni con un modulatore prima di alimentarla nella funzione di attivazione.
Identifichiamo il problema della dimenticanza nel fine-tuning dei modelli NLG pre-addestrati, e proponiamo la strategia mix-review per affrontarlo.
Questo articolo analizza il problema dell'oblio nel quadro del pretraining-finetuning dal punto di vista della sensibilità al contesto e del trasferimento della conoscenza, e propone una strategia di fine-tuning che supera il metodo del decadimento del peso.
Studio del problema dell'oblio nella struttura pretrain-finetune, in particolare nei compiti di generazione di risposte al dialogo, e proposta di una strategia di revisione mista per alleviare il problema dell'oblio.
Una migliore modellazione di sistemi complessi utilizza una composizione ibrida di modelli neurali/di dominio, nuove funzioni di perdita di decorrelazione e set di test estrapolativi 
Questo articolo conduce esperimenti per confrontare le previsioni estrapolative di vari modelli ibridi che compongono modelli fisici, reti neurali e modelli stocastici, e affronta la sfida della dinamica non modellata che è un collo di bottiglia.
Questo articolo presenta approcci per la combinazione di reti neurali con modelli non-NN per prevedere il comportamento di sistemi fisici complessi.
Impariamo i punteggi densi e il modello dinamico come priori dai dati di esplorazione e li usiamo per indurre una buona politica nei nuovi compiti in condizione zero-shot.
Questo articolo discute la generalizzazione dello zero shot in nuovi ambienti, e propone un approccio con risultati su Grid-World, Super Mario Bros, e 3D Robotics.
Un metodo che mira ad apprendere priori task-agnostic per la generalizzazione zero-shot, con l'idea di impiegare un approccio di modellazione in cima alla struttura RL basata sul modello.
Analizzare i meccanismi sottostanti al collasso della varianza di SVGD in dimensioni elevate.
Ripensare la generalizzazione richiede la rivisitazione di vecchie idee: approcci di meccanica statistica e comportamento di apprendimento complesso
Gli autori suggeriscono che le idee di meccanica statistica aiuteranno a capire le proprietà di generalizzazione delle reti neurali profonde, e danno un approccio che fornisce forti descrizioni qualitative dei risultati empirici riguardanti le reti neurali profonde e gli algoritmi di apprendimento.
Un insieme di idee legate alla comprensione teorica delle proprietà di generalizzazione delle reti neurali multistrato, e un'analogia qualitativa tra i comportamenti nell'apprendimento profondo e i risultati dell'analisi fisica statistica quantitativa delle reti neurali a uno o due strati.
Presentiamo una softmax doppiamente sparsa, la miscela sparsa di esperti sparsi, per migliorare l'efficienza dell'inferenza softmax attraverso lo sfruttamento della gerarchia a due livelli di sovrapposizione. 
Questo articolo propone un'approssimazione veloce al calcolo di softmax quando il numero di classi è molto grande.
Questo articolo propone una miscela di esperti sparsi che impara una gerarchia di classi a due livelli per un'inferenza softmax efficiente.
Esploriamo l'uso di dati di eye-tracking raccolti passivamente per ridurre la quantità di dati etichettati necessari durante la formazione.
Un metodo per utilizzare le informazioni sullo sguardo per ridurre la complessità del campione di un modello e lo sforzo di etichettatura necessario per ottenere una performance target, con risultati migliori nei campioni di medie dimensioni e nei compiti più difficili.
Un metodo per incorporare i segnali di sguardo nelle CNN standard per la classificazione delle immagini, aggiungendo un termine di funzione di perdita basato sulla differenza tra la Class Activation Map del modello e la mappa costruita dalle informazioni di eye tracking.
Applichiamo la correzione delle perdite alle reti neurali a grafo per addestrare un modello più robusto al rumore.
Questo articolo introduce la correzione delle perdite per le reti neurali dei grafi per affrontare il rumore simmetrico delle etichette dei grafi, concentrandosi su un compito di classificazione dei grafi.
Questo articolo propone l'uso di una perdita di correzione del rumore nel contesto delle reti neurali a grafo per affrontare le etichette rumorose.
Usiamo la co-attenzione dei grafi in un sistema di addestramento dei grafi accoppiati per la classificazione e la regressione dei grafi.
Questo articolo inietta un meccanismo di co-attenzione a più teste in GCN che permette a un farmaco di partecipare a un altro farmaco durante la previsione degli effetti collaterali.
Un metodo per estendere l'apprendimento basato sui grafi con uno strato co-attenzionale, che supera altri precedenti su un compito di classificazione a coppie di grafi.
Image captioning come una formazione GAN condizionale con nuove architetture, anche studiare due metodi di formazione GAN discreti. 
Un modello GAN migliorato per la didascalia delle immagini che propone un didascalizzatore LSTM consapevole del contesto, introduce un discriminatore co-attentivo più forte con prestazioni migliori, e usa SCST per l'addestramento GAN.
Sfruttare la curvatura per far convergere i metodi MCMC più velocemente dello stato dell'arte.
Keras per reti neurali infinite.
Proponiamo una nuova funzione di perdita che raggiunge risultati allo stato dell'arte nel rilevamento di out-of-distribution con Outlier Exposure sia su compiti di classificazione di immagini che di testo.
Questo articolo affronta i problemi del rilevamento di fuori distribuzione e della calibrazione del modello adattando la funzione di perdita della tecnica Outlier Exposure, con risultati che dimostrano un aumento delle prestazioni rispetto a OE su benchmark di visione e testo e una migliore calibrazione del modello.
Proposta di una nuova funzione di perdita per addestrare la rete con Outlier Exposure che porta a una migliore individuazione di OOD rispetto alle semplici funzioni di perdita che utilizzano la divergenza KL.
Il pre-addestramento agnostico può modellare il paesaggio degli attrattori di RNN e formare diversi bias induttivi per diversi compiti di navigazione   
Questo articolo studia le rappresentazioni interne delle reti neurali ricorrenti addestrate su compiti di navigazione, e trova che le RNN pre-addestrate a usare l'integrazione dei percorsi contengono attrattori continui 2D mentre le RNN pre-addestrate per la memoria dei punti di riferimento contengono attrattori discreti.
Questo articolo esplora come il pre-addestramento delle reti ricorrenti su diversi obiettivi di navigazione conferisce diversi benefici per la risoluzione dei compiti a valle, e mostra come il diverso pre-addestramento si manifesta come diverse strutture dinamiche nelle reti dopo il pre-addestramento.
Verifica della rete neurale per proprietà temporali e modelli di generazione di sequenze
Questo articolo estende la propagazione dei limiti di intervallo alla computazione ricorrente e ai modelli auto-regressivi, introduce ed estende la Signal Temporal Logic per specificare i vincoli temporali, e fornisce la prova che STL con la propagazione dei limiti può garantire che i modelli neurali siano conformi alle specifiche temporali.
Un modo per addestrare regressori di serie temporali in modo verificabile rispetto a un insieme di regole definite dalla logica temporale del segnale, e un lavoro di derivazione delle regole di propagazione dei vincoli per il linguaggio STL.
Proponiamo una soluzione di rete neurale universale per derivare automaticamente delle architetture NN efficaci per i dati tabulari.
Una nuova procedura di addestramento delle reti neurali, progettata per i dati tabulari, che cerca di sfruttare i cluster di caratteristiche estratte dai GBDT.
Proposta di un algoritmo ibrido di apprendimento automatico che utilizza alberi decisionali potenziati dal gradiente e reti neurali profonde, con direzione di ricerca prevista su dati tabulari.
Sistema di apprendimento probabilistico delle regole utilizzando l'inferenza sollevata
Un modello per l'apprendimento di regole probabilistiche per automatizzare il completamento di database probabilistici che usa AMIE+ e l'inferenza sollevata per aiutare l'efficienza computazionale.
Proponiamo il primo modello neurale non autoregressivo per il Dialogue State Tracking (DST), raggiungendo la precisione SOTA (49,04%) sul benchmark MultiWOZ2.1, e riducendo la latenza di inferenza di un ordine di grandezza.
Un nuovo modello per il compito DST che riduce la complessità del tempo di inferenza con un decodificatore non autoregressivo, ottiene una precisione DST competitiva e mostra miglioramenti rispetto ad altre basi.
Proposta di un modello capace di seguire gli stati di dialogo in modo non ricorsivo.
Una nuova architettura di rete per eseguire Deep 3D Zoom o close-up.
Un metodo per creare una "immagine ingrandita" per una data immagine di input, e una nuova perdita di ricostruzione della retroproiezione che permette alla rete di imparare la struttura 3D sottostante e mantenere un aspetto naturale.
Un algoritmo per sintetizzare il comportamento dello zoom 3D quando la telecamera si muove in avanti, una struttura di rete che incorpora la stima della disparità in un quadro GANs per sintetizzare nuove viste, e un nuovo compito di visione del computer proposto.
Un perfezionamento quantitativo del teorema di approssimazione universale attraverso un approccio algebrico.
Gli autori derivano le prove della proprietà di approssimazione universale in modo algebrico e affermano che i risultati sono generali per altri tipi di reti neurali e apprendenti simili.
Una nuova dimostrazione della versione di Leshno della proprietà di approssimazione universale per le reti neurali, e nuove intuizioni sulla proprietà di approssimazione universale.
Struttura modulare per la classificazione dei documenti e tecnica di aggregazione dei dati per rendere la struttura robusta a varie distorsioni e rumore e concentrarsi solo sulle parole importanti. 
Gli autori considerano l'addestramento di una classificazione di testo basata su RNN dove c'è una restrizione di risorse sulla predizione del tempo di prova, e forniscono un approccio che utilizza un meccanismo di mascheramento per ridurre le parole/frasi/sentenze utilizzate nella predizione seguito da un classificatore per gestire tali componenti.
Consentire la connessione parziale dei canali nelle super-reti per regolarizzare e accelerare la ricerca di architetture differenziabili
Un'estensione del metodo di ricerca dell'architettura neurale DARTS che affronta il suo difetto di immenso costo di memoria utilizzando un sottoinsieme casuale di canali e un metodo per normalizzare i bordi.
Questo articolo propone di migliorare DARTS in termini di efficienza di formazione, da grandi spese generali di memoria e di calcolo, e propone un DARTS parzialmente connesso con connessione parziale del canale e normalizzazione del bordo.
Gli agenti interagiscono (parlano, agiscono) e possono raggiungere obiettivi in un mondo ricco con un linguaggio diverso, colmando il divario tra le chiacchiere e il dialogo orientato agli obiettivi.
Questo articolo studia un compito di dialogo multiagente in cui l'agente di apprendimento mira a generare azioni in linguaggio naturale che elicitano una particolare azione dall'altro agente, e mostra che gli agenti RL possono raggiungere livelli più alti di completamento del compito rispetto alle basi di apprendimento per imitazione.
Questo articolo esplora l'impostazione del dialogo orientato all'obiettivo con l'apprendimento di rinforzo in un gioco di avventura con testo fantasy e osserva che gli approcci RL superano i modelli di apprendimento supervisionato.
Un nuovo metodo parzialmente indipendente dalla politica per la valutazione della politica off-policy a orizzonte infinito con più politiche di comportamento conosciute o sconosciute.
Una stima della politica mista che prende le idee dagli stimatori di orizzonte infinito di valutazione della politica off-policy e dal campionamento di importanza della regressione per il peso dell'importanza, e li estende a molte politiche e a politiche sconosciute.
Un algoritmo per risolvere l'orizzonte infinito della valutazione della politica off con politiche di comportamento multiple stimando una politica mista sotto regressione, e la prova teorica che un rapporto di politica stimato può ridurre la varianza.
Introduciamo un'architettura neurale più efficiente per l'inferenza ammortizzata, che combina flussi di normalizzazione continui e condizionali utilizzando una scelta di principio della struttura di sparsità.
Mostriamo che ENAS con ottimizzazione ES in RL è altamente scalabile, e lo usiamo per compattare le politiche delle reti neurali attraverso la condivisione dei pesi.
Gli autori costruiscono politiche di apprendimento di rinforzo con pochissimi parametri comprimendo una rete neurale feed-forward, forzandola a condividere i pesi e usando un metodo di apprendimento di rinforzo per imparare la mappatura dei pesi condivisi.
Questo articolo combina idee dai metodi ENAS e ES per l'ottimizzazione, e introduce l'architettura della rete cromatica, che partiziona i pesi della rete RL in sottogruppi legati.
Introduciamo Deep SAD, un metodo profondo per il rilevamento generale semi-supervisionato delle anomalie che sfrutta soprattutto le anomalie etichettate.
Un nuovo metodo per trovare dati anomali, quando alcune anomalie etichettate sono date, che applica la perdita derivata dalla teoria dell'informazione basata sui dati normali che hanno solitamente un'entropia più bassa dei dati anomali.
Proposta per un quadro di rilevamento delle anomalie in impostazioni in cui sono disponibili dati non etichettati, dati positivi etichettati e dati negativi etichettati, e proposta di approccio all'AD semi-supervisionato da una prospettiva teorica dell'informazione.
Questo articolo analizza le dinamiche di formazione e i punti critici della formazione della rete ReLU profonda tramite SGD nell'impostazione insegnante-studente. 
Studio della sovra-parametrizzazione nelle reti ReLU multistrato studente-insegnante, una parte teorica sui punti critici SGD per l'impostazione insegnante-studente, e una parte euristica ed empirica sulla dinamica dell'algoritmo SDG in funzione delle reti di insegnanti.
Sotto certe condizioni sulle trasformazioni lineari in entrata e in uscita, sia GD che SGD possono raggiungere la convergenza globale per l'addestramento di ResNets lineari profonde.
Gli autori studiano la convergenza della discesa del gradiente nell'addestramento di reti residuali lineari profonde, e stabiliscono una convergenza globale di GD/SGD e tassi di convergenza lineare di SG/SGD.
Studio delle proprietà di convergenza di GD e SGD su reti lineari profonde, e dimostrazione che sotto certe condizioni sulle trasformazioni di input e output e con inizializzazione zero, GD e SGD convergono ai minimi globali.
Analizziamo il processo di addestramento delle reti profonde e mostriamo che partono da un rapido apprendimento di esempi classificabili poco profondi e generalizzano lentamente a punti di dati più difficili.
Apprendimento di MRF latenti profonde con un obiettivo saddle-point derivato dall'approssimazione della funzione di partizione di Bethe.
Un metodo per l'apprendimento di MRF profonde a variabili latenti con un obiettivo di ottimizzazione che utilizza l'energia libera di Bethe, che risolve anche i vincoli sottostanti alle ottimizzazioni dell'energia libera di Bethe.
Un obiettivo per l'apprendimento di MRF a variabili latenti basato sull'energia libera di Bethe e sull'inferenza ammortizzata, diverso dall'ottimizzazione dell'ELBO standard.
Un quadro generale per la generazione di spiegazioni utilizzando la logica.
Questo articolo studia la generazione di spiegazioni da un punto di vista KR e conduce esperimenti che misurano la dimensione delle spiegazioni e il tempo di esecuzione su formule casuali e formule da un'istanza di Blocksworld.
Questo articolo fornisce una prospettiva sulle spiegazioni tra due basi di conoscenza, ed è parallelo al lavoro sulla riconciliazione dei modelli nella letteratura sulla pianificazione.
Le reti neurali profonde e strette convergeranno verso stati medi o mediani errati della funzione obiettivo a seconda della perdita con alta probabilità.
Questo articolo studia le modalità di fallimento delle reti profonde e strette, concentrandosi sui modelli più piccoli possibili per i quali si verifica il comportamento indesiderato.
Questo articolo mostra che l'addestramento delle reti neurali ReLU profonde convergerà a un classificatore costante con alta probabilità rispetto all'inizializzazione casuale se le larghezze degli strati nascosti sono troppo piccole.
Proponiamo l'addestramento MMA per massimizzare direttamente il margine dello spazio di input al fine di migliorare la robustezza avversaria principalmente rimuovendo il requisito di specificare un limite di distorsione fisso.
Un approccio di addestramento adversariale basato sul margine adattivo per addestrare DNN robuste, massimizzando il margine più breve degli ingressi al confine della decisione, che rende possibile l'addestramento adversariale con grandi perturbazioni.
Viene introdotto un metodo per l'apprendimento robusto contro gli attacchi avversari in cui il margine dello spazio di input è direttamente massimizzato e una variante softmax del max-margin.
Proponiamo un metodo per il rilevamento delle anomalie con le GAN cercando nello spazio latente del generatore delle buone rappresentazioni del campione.
Gli autori propongono di utilizzare GAN per il rilevamento delle anomalie, un metodo basato sul gradiente di discesa per aggiornare iterativamente le rappresentazioni latenti, e un nuovo aggiornamento dei parametri dei generatori.
Un approccio basato su GAN per fare il rilevamento delle anomalie per i dati di immagine dove lo spazio latente del generatore è esplorato per trovare una rappresentazione per un'immagine di prova.
Il comportamento transitorio degli algoritmi MCMC basati sul gradiente e dell'inferenza variazionale è più simile di quanto si possa pensare, mettendo in discussione l'affermazione che l'inferenza variazionale è più veloce di MCMC.
Un quadro di convoluzione dei grafi basato sulla composizione per i grafi multirelazionali.
Gli autori sviluppano GCN su grafi multi-relazionali e propongono CompGCN, che sfrutta le intuizioni delle embeddings dei grafi di conoscenza e impara le rappresentazioni dei nodi e delle relazioni per alleviare il problema dell'iper-parametrizzazione.
Questo articolo introduce una struttura GCN per i grafi multi-relazionali e generalizza diversi approcci esistenti per l'incorporazione di Knowledge Graph in una struttura.
Quantizziamo completamente il trasformatore a 8 bit e miglioriamo la qualità della traduzione rispetto al modello a precisione completa.
Un metodo di quantizzazione a 8 bit per quantizzare il modello di traduzione automatica Transformer, proponendo di usare una quantizzazione uniforme min-max durante l'inferenza e di usare i bucketing weigts prima della quantizzazione per ridurre l'errore di quantizzazione.
Un metodo per ridurre lo spazio di memoria richiesto da una tecnica di quantizzazione, focalizzato sulla riduzione per l'architettura Transformer.
Latent Embedding Optimization (LEO) è un nuovo meta-apprendista basato sul gradiente con prestazioni allo stato dell'arte sui difficili compiti di classificazione miniImageNet e tieredImageNet a 5 vie e 1 colpo.
Un nuovo quadro di meta-apprendimento che impara lo spazio latente dipendente dai dati, esegue un adattamento veloce nello spazio latente, è efficace per l'apprendimento di pochi colpi, ha un'inizializzazione dipendente dal compito per l'adattamento, e funziona bene per la distribuzione multimodale del compito.
Questo articolo propone un metodo di ottimizzazione di embedding latente per il meta-apprendimento, e sostiene che il contributo è quello di disaccoppiare le tecniche di meta-apprendimento basate sull'ottimizzazione dallo spazio ad alta densità dei parametri del modello.
I bias induttivi relazionali migliorano le capacità di generalizzazione fuori distribuzione negli agenti di apprendimento del rinforzo senza modello
Un'architettura di rete relazionale condivisa per parametrizzare la rete degli attori e dei critici, focalizzata su algoritmi di vantaggio distribuito attore-critico, che migliora le tecniche di rinforzo profondo senza modello con la conoscenza relazionale dell'ambiente in modo che gli agenti possano imparare rappresentazioni di stato interpretabili.
Un'analisi e una valutazione quantitativa e qualitativa del meccanismo di auto-attenzione combinato con la rete di relazioni nel contesto della RL senza modello.
Proponiamo un semplice modello generativo per la traduzione non supervisionata delle immagini e il rilevamento della salienza.
Definiamo un concetto di reti neurali profonde a strati, per le quali gli strati operano in parallelo, e forniamo un toolbox per progettare, formare, valutare e interagire online con queste reti.
Un toolbox accelerato dalla GPU per l'aggiornamento parallelo dei neuroni, scritto in Theano, che supporta diversi ordini di aggiornamento in reti ricorrenti e reti con connessioni che saltano gli strati. 
Un nuovo toolbox per l'apprendimento e la valutazione delle reti neurali profonde, e proposta per un cambio di paradigma dalle reti sequenziali a strati alle reti parallele a strati.
Un metodo di difesa avversaria che collega la robustezza delle reti neurali profonde con la stabilità di Lyapunov
Gli autori formulano l'addestramento delle NN come la ricerca di un controller ottimale per un sistema dinamico discreto, permettendo loro di utilizzare il metodo delle approssimazioni successive per addestrare una NN in modo da essere più robusta agli attacchi avversari.
Questo articolo usa la visione teorica di una rete neurale come un ODE discretizzato per sviluppare una teoria del controllo robusto che mira all'addestramento della rete mentre fa rispettare la robustezza.
Proponiamo uno schema di riponderazione semplice ma efficace per le GCN, supportato teoricamente dalla teoria del campo medio.
Un metodo, noto come DrGCN, per ripesare le diverse dimensioni delle rappresentazioni dei nodi nelle reti convoluzionali a grafo riducendo la varianza tra le dimensioni.
Il nostro approccio è il primo tentativo di sfruttare un modello di variabile latente sequenziale per la selezione della conoscenza nel dialogo multigiro basato sulla conoscenza. Raggiunge il nuovo stato dell'arte delle prestazioni su Wizard of Wikipedia benchmark.
Un modello di variabile latente sequenziale per la selezione della conoscenza nella generazione di dialogo che estende il modello di attenzione posteriore al problema della selezione della conoscenza latente e raggiunge prestazioni più elevate rispetto ai precedenti modelli allo stato dell'arte.
Una nuova architettura per la selezione del dialogo multigiro basato sulla conoscenza che rende lo stato dell'arte sui set di dati di riferimento rilevanti e ottiene punteggi più alti nelle valutazioni umane.
Proponiamo un metodo di meta-apprendimento che ammortizza in modo efficiente l'inferenza variazionale gerarchica attraverso gli episodi di allenamento.
Un adattamento ai modelli di tipo MAML che tiene conto dell'incertezza posteriore nelle variabili latenti specifiche del compito impiegando l'inferenza variazionale per i parametri specifici del compito in una visione gerarchica bayesiana di MAML.
Gli autori considerano il meta-apprendimento per imparare una priorità sui pesi delle reti neurali, fatto tramite inferenza variazionale ammortizzata.
Rappresentazione/distillazione della conoscenza massimizzando l'informazione reciproca tra insegnante e studente
Questo articolo combina un obiettivo contrastivo che misura l'informazione reciproca tra le rappresentazioni apprese dalle reti di insegnanti e studenti per la distillazione dei modelli, e propone un modello con un miglioramento rispetto alle alternative esistenti sui compiti di distillazione.
Le reti che imparano con connessioni di feedback e regole di plasticità locale possono essere ottimizzate per l'uso del meta apprendimento.
Le CNN con connessioni laterali biologicamente ispirate apprese in modo non supervisionato sono più robuste agli input rumorosi. 
Questo articolo ha lo scopo di sviluppare un approccio di rappresentazione del prodotto tensoriale per applicazioni di elaborazione del linguaggio naturale basate sull'apprendimento profondo.
Studiamo la robustezza certificata per le previsioni top-k attraverso lo smoothing randomizzato sotto il rumore gaussiano e deriviamo un limite di robustezza stretto nella norma L_2.
Questo articolo estende il lavoro sulla deduzione di un raggio certificato usando lo smoothing randomizzato, e mostra il raggio al quale un classificatore smoothed sotto perturbazioni gaussiane è certificato per le prime k previsioni.
Questo articolo si basa sulla tecnica di lisciatura casuale per la predizione top-1, e mira a fornire una certificazione sulle predizioni top-k.
Presentiamo priori strutturati per l'apprendimento non supervisionato di rappresentazioni disentangolate in VAEs che mitigano significativamente il trade-off tra disentanglement e perdita di ricostruzione.
Un quadro generale per utilizzare la famiglia di distribuzioni L^p-nested come priore per il vettore codice di VAE, dimostrando un MIG superiore.
Gli autori sottolineano i problemi negli attuali approcci VAE e forniscono una nuova prospettiva sul compromesso tra ricostruzione e ortogonalizzazione per VAE, beta-VAE e beta-TCVAE.
Generalizziamo i blocchi residui ai blocchi tandem, che usano mappe lineari arbitrarie invece di scorciatoie, e migliorano le prestazioni rispetto alle ResNet.
Questo articolo esegue un'analisi delle connessioni a scorciatoia nelle architetture ResNet-like, e propone di sostituire le scorciatoie di identità con una convoluzione alternativa chiamata blocco tandem.
Questo articolo studia l'effetto della sostituzione delle connessioni di salto di identità con connessioni di salto convoluzionali addestrabili in ResNet e trova che le prestazioni migliorano.
Presentiamo un nuovo quadro per adattare i metodi di tipo Adam, cioè AdamT, per includere le informazioni sulla tendenza quando si aggiornano i parametri con la dimensione del passo e i gradienti adattivi.
Un nuovo tipo di variante di Adam che usa il metodo lineare di Holt per calcolare il momento lisciato del primo ordine e del secondo ordine invece di usare la media pesata esponenziale.
Un metodo per spiegare un classificatore, generando una perturbazione visiva di un'immagine esagerando o diminuendo le caratteristiche semantiche che il classificatore associa a un'etichetta di destinazione.
Un modello che quando viene data una query in input a una black-box, mira a spiegare il risultato fornendo variazioni plausibili e progressive alla query che possono portare a un cambiamento dell'output.
Un metodo per spiegare l'output della classificazione a scatola nera delle immagini, che genera una perturbazione graduale degli output in risposta a richieste di input gradualmente perturbate.
Noi presentiamo un approccio influenzato alla costruzione di spiegazioni per il comportamento delle reti convoluzionali profonde, e mostriamo come può essere usato per rispondere a un'ampia serie di domande che non potevano essere affrontate dal lavoro precedente.
Un modo di misurare l'influenza che soddisfa certi assiomi, e una nozione di influenza che può essere utilizzata per identificare quale parte di input è più influente per l'output di un neurone in una rete neurale profonda.
Questo articolo propone di misurare l'influenza di singoli neuroni rispetto a una quantità di interesse rappresentata da un altro neurone.
Evidenziamo una tecnica con cui i sistemi di elaborazione del linguaggio naturale possono imparare una nuova parola dal contesto, permettendo loro di essere molto più flessibili.
Una tecnica per sfruttare la conoscenza precedente per imparare rappresentazioni di incorporazione per nuove parole con dati minimi.
Un'architettura di memoria che supporta il ragionamento inferenziale.
Questo articolo propone modifiche all'architettura End2End Memory Network, introduce un nuovo compito Paired Associative Inference che la maggior parte dei modelli esistenti fatica a risolvere, e mostra che l'architettura proposta risolve meglio il compito.
Un nuovo compito (paired associate inference) tratto dalla psicologia cognitiva, e la proposta di una nuova architettura di memoria con caratteristiche che permettono una migliore performance sul compito paired associate.
Le convoluzioni separabili in senso profondo migliorano la traduzione automatica neurale: più sono separabili, meglio è.
Questo articolo propone di usare strati di convoluzione separabili in profondità in un modello di traduzione automatica neurale completamente convoluzionale, e introduce un nuovo strato di convoluzione super-separabile che riduce ulteriormente il costo computazionale.
L'addestramento GAN non saturante minimizza efficacemente una f-divergenza inversa di tipo KL.
Questo articolo propone un'utile espressione della classe delle f-divergenze, indaga le proprietà teoriche delle f-divergenze popolari da strumenti di recente sviluppo, e studia le GAN con lo schema di allenamento non saturante.
Introduciamo un nuovo metodo di rappresentazione del testo che permette ai classificatori di immagini di essere applicati a problemi di classificazione del testo, e applichiamo il metodo alla disambiguazione dei nomi degli inventori.
Un metodo per mappare una coppia di informazioni testuali in un'immagine RGB 2D che può essere alimentata a reti neurali convusionali 2D (classificatori di immagini).
Gli autori considerano il problema della disambiguazione dei nomi per gli inventori di nomi di brevetti e propongono di costruire una rappresentazione di pagina immagine delle due stringhe di nomi da confrontare e di applicare un classificatore di immagini.
Abbiamo proposto il modello "Difference-Seeking Generative Adversarial Network" (DSGAN) per imparare la distribuzione di destinazione che è difficile da raccogliere dati di formazione.
Questo articolo presenta DS-GAN, che mira ad apprendere la differenza tra due distribuzioni qualsiasi i cui campioni sono difficili o impossibili da raccogliere, e mostra la sua efficacia nell'apprendimento semi-supervisionato e nei compiti di formazione avversaria.
Questo articolo considera il problema dell'apprendimento di una GAN per catturare una distribuzione di destinazione con solo pochi campioni di formazione da quella distribuzione disponibili.
Un metodo generale che migliora le prestazioni di traduzione delle immagini del quadro GAN utilizzando un discriminatore incorporato di attenzione
Un meccanismo di feedback nel framework GAN che migliora la qualità delle immagini generate nella traduzione immagine per immagine, e il cui discriminatore produce una mappa che indica dove il generatore dovrebbe concentrarsi per rendere i suoi risultati più convincenti.
Proposta di una GAN con un discriminatore basato sull'attenzione per la traduzione I2I che fornisce la probabilità di reale/falso e una mappa di attenzione che riflette la salienza per la generazione dell'immagine.
Proponiamo un nuovo set di dati per investigare il problema di entailment sotto la tabella semi-strutturata come premessa
Questo articolo propone un nuovo set di dati per la verifica dei fatti basata sulle tabelle e introduce metodi per questo compito.
Gli autori propongono il problema della verifica dei fatti con fonti di dati semi-strutturati come le tabelle, creano un nuovo set di dati e valutano i modelli di base con variazioni.
Sviluppiamo un'architettura di corrispondenza grafica profonda che raffina le corrispondenze iniziali al fine di raggiungere il consenso del vicinato.
Una struttura per rispondere alle domande di corrispondenza dei grafi che consiste in embeddings di nodi locali con una fase di raffinamento con passaggio di messaggi.
Un'architettura basata su GNN a due stadi per stabilire le corrispondenze tra due grafi che si comporta bene nei compiti del mondo reale di corrispondenza delle immagini e di allineamento delle entità dei grafi di conoscenza.
Questo articolo estende la prova di densità delle reti neurali nello spazio delle funzioni continue (o anche misurabili) su spazi euclidei alle funzioni su insiemi compatti di misure di probabilità. 
Questo articolo studia le proprietà di approssimazione di una famiglia di reti neurali progettate per affrontare problemi di apprendimento multistanza, e mostra che i risultati per le architetture standard a uno strato si estendono a questi modelli.
Questo articolo generalizza il teorema di approssimazione universale alle funzioni reali sullo spazio delle misure.
Un nuovo quadro per le spiegazioni delle previsioni dipendenti dal contesto e senza contesto
Gli autori estendono il metodo di attribuzione locale lineare LIME per interpretare i modelli a scatola nera, e propongono un metodo per discernere tra interazioni dipendenti dal contesto e senza contesto.
Un metodo che può fornire spiegazioni gerarchiche per un modello, includendo sia spiegazioni dipendenti dal contesto che libere dal contesto tramite un algoritmo di interpretazione locale.
Il finetuning dopo la quantizzazione corrisponde o supera le reti di piena precisione allo stato dell'arte sia a 8 che a 4 bit.
Questo articolo propone di migliorare le prestazioni dei modelli a bassa precisione facendo la quantizzazione sui modelli pre-addestrati, usando grandi lotti, e usando un adeguato tasso di ricottura di apprendimento con un tempo di formazione più lungo.
Un metodo per una bassa quantizzazione dei bit per permettere l'inferenza su un hardware efficiente che raggiunge la piena accuratezza su ResNet50 con pesi e attivazioni a 4 bit, basato sull'osservazione che la regolazione fine a bassa precisione introduce rumore nel gradiente.
Due metodi basati su Representational Similarity Analysis (RSA) e Tree Kernels (TK) che quantificano direttamente quanto fortemente l'informazione codificata nei modelli di attivazione neurale corrisponde all'informazione rappresentata da strutture simboliche.
Questo articolo introduce una struttura per l'apprendimento di rappresentazioni efficienti dal punto di vista dei dati attraverso il campionamento adattivo nello spazio latente.
Un metodo per la selezione sequenziale e adattiva degli esempi di formazione da presentare all'algoritmo di formazione, dove la selezione avviene nello spazio latente basato sulla scelta dei campioni nella direzione del gradiente della perdita.
Un metodo per selezionare in modo efficiente i campioni difficili durante l'addestramento delle reti neurali, ottenuto tramite un auto-encoder variazionale che codifica i campioni in uno spazio latente.
Un metodo basato sull'addestramento avversario per distinguere due insiemi complementari di variazioni in un set di dati in cui solo uno di essi è etichettato, testato su stile vs. contenuto nelle illustrazioni di anime.
Un metodo di generazione di immagini che combina GAN condizionali e VAE condizionali che genera immagini di anime ad alta fedeltà con vari stili di vari artisti. 
Proposta di un metodo per imparare rappresentazioni disgiunte di stile (artista) e contenuto negli anime.
Introduciamo una regolarizzazione di morbidezza per i kernel convoluzionali di CNN che può aiutare a migliorare la robustezza avversaria e portare a gradienti percettivamente allineati
Questo articolo propone un nuovo schema di regolarizzazione che incoraggia i kernel convoluzionali ad essere più lisci, sostenendo che ridurre la dipendenza della rete neurale dalle componenti ad alta frequenza aiuta la robustezza contro gli esempi avversari. 
Gli autori propongono un metodo per l'apprendimento di kernel convoluzionali più lisci, in particolare, un regolatore che penalizza i grandi cambiamenti tra pixel consecutivi del kernel con l'intuizione di penalizzare l'uso di componenti di input ad alta frequenza.
Indaghiamo il comportamento su grandi campioni delle stime dei valori Q e abbiamo proposto una strategia di esplorazione efficiente che si basa sulla stima delle discrepanze relative tra le stime Q. 
Addestriamo le embeddings di parole basate sull'entailment invece che sulla somiglianza, prevedendo con successo l'entailment lessicale.
L'articolo presenta un algoritmo di word embedding per l'entailment lessicale che segue il lavoro di Henderson e Popa (ACL, 2016).
Apprendimento non supervisionato per l'apprendimento di rinforzo utilizzando un curriculum automatico di auto-gioco
Una nuova formulazione per esplorare l'ambiente in modo non supervisionato per aiutare un compito specifico in seguito, dove un agente propone compiti sempre più difficili e l'agente che impara cerca di realizzarli.
Un modello di auto-gioco in cui un agente impara a proporre compiti che sono facili per lui ma difficili per un avversario, creando un bersaglio mobile di obiettivi di auto-gioco e curriculum di apprendimento. 
Sfruttare i ricchi dettagli strutturali nei dati strutturati a grafo tramite "impronte digitali strutturali" adattive
Una metodologia basata sulla struttura del grafico per aumentare il meccanismo di attenzione delle reti neurali a grafo, con l'idea principale di esplorare le interazioni tra diversi tipi di nodi del quartiere locale di un nodo radice.
Questo articolo estende l'idea di auto-attenzione nelle NN a grafo, che è tipicamente basata sulla somiglianza delle caratteristiche tra i nodi, per includere la somiglianza strutturale.
Proponiamo un algoritmo scalabile di Bayesian Reinforcement Learning che impara una correzione bayesiana su un ensemble di esperti chiaroveggenti per risolvere problemi con ricompense e dinamiche latenti complesse.
Questo articolo considera il problema dell'apprendimento di rinforzo bayesiano su processi decisionali di Markov latenti (MDP) prendendo decisioni con esperti.
In questo articolo, gli autori motivano e propongono un algoritmo di apprendimento, chiamato Bayesian Residual Policy Optimization (BRPO), per problemi di apprendimento di rinforzo bayesiani.
Dimostriamo che la discesa del gradiente raggiunge la perdita di addestramento zero con un tasso lineare su reti neurali sovra-parametrizzate.
Questo lavoro considera l'ottimizzazione di una rete ReLU a due strati iperparametrizzata con la perdita quadratica e data una serie di dati con etichette arbituali.
Questo articolo studia le reti neurali a uno strato nascosto con perdita quadrata, dove dimostrano che nell'impostazione sovra-parametrizzata, l'inizializzazione casuale e la discesa del gradiente arriva a perdita zero.
Analizzare problemi inversi con reti neurali invertibili
L'autore propone di utilizzare le reti invertibili per risolvere problemi inversi ambigui e suggerisce di non addestrare solo il modello in avanti, ma anche il modello inverso con un critico MMD.
Il documento di ricerca propone una rete invertibile con osservazioni per la probabilità posteriore di distribuzioni di input complesse con uno schema di formazione bidirezionale teoricamente valido.
Le metriche di performance sono specifiche incomplete; il fine non sempre giustifica i mezzi.
Gli autori mostrano come il meta-apprendimento rivela gli incentivi nascosti per lo spostamento della distribuzione e propongono un approccio basato sullo scambio di studenti tra gli ambienti per ridurre lo spostamento della distribuzione introdotto da loro stessi.
L'articolo generalizza l'incentivo intrinseco per il discente a vincere rendendo il compito più facile nel meta-apprendimento a una classe più ampia di problemi.
Proponiamo un approccio di rilevamento delle anomalie che combina la modellazione della classe di primo piano tramite densità locali multiple con l'addestramento adversariale.
L'articolo propone una tecnica per rendere i modelli generativi più robusti rendendoli coerenti con la densità locale.
Proponiamo una variante GAN che impara a generare nuvole di punti. Diversi studi sono stati esplorati, tra cui la stima della distanza di Wasserstein più stretta, la generazione condizionale, la generalizzazione a nuvole di punti non visti e l'immagine alla nuvola di punti.
Questo articolo propone di utilizzare GAN per generare nuvole di punti 3D e introduce un obiettivo di sandwich, facendo la media tra il limite superiore e inferiore della distanza di Wasserstein tra le distribuzioni.
Questo articolo propone un nuovo modello generativo per dati non ordinati, con una particolare applicazione alle nuvole di punti, che include un metodo di inferenza e una nuova funzione obiettivo. 
L'articolo presenta un nuovo approccio per i meccanismi attenzionali che possono beneficiare di una serie di compiti come la traduzione automatica e la didascalia delle immagini.
Questo articolo estende gli attuali modelli di attenzione dal livello di parola alla combinazione di parole adiacenti, applicando i modelli agli articoli composti da parole adiacenti fuse.
Identifichiamo un fenomeno, il lavaggio del cervello neurale, e introduciamo una perdita di plasticità del peso statisticamente giustificata per superarlo.
Questo articolo discute il fenomeno del "lavaggio del cervello neurale", che si riferisce al fatto che le prestazioni di un modello sono influenzate da un altro modello che ne condivide i parametri.
Questo articolo introduce Morpho-MNIST, una collezione di metriche di forma e perturbazioni, in un passo verso la valutazione quantitativa dell'apprendimento della rappresentazione.
Questo articolo discute il problema della valutazione e della diagnosi delle rappresentazioni apprese utilizzando un modello generativo.
Gli autori presentano un insieme di criteri per categorizzare i digisti MNISt e un insieme di perturbazioni interessanti per modificare il dataset MNIST.
esplorazione strutturata nell'apprendimento di rinforzo profondo attraverso la scoperta e il controllo dell'astrazione visiva non supervisionata
L'articolo introduce astrazioni visive che sono utilizzate per l'apprendimento di rinforzo, dove un algoritmo impara a "controllare" ogni astrazione così come a selezionare le opzioni per raggiungere il compito complessivo.
Un nuovo algoritmo a gradiente di policy progettato per affrontare problemi di ottimizzazione combinatoria black-box. L'algoritmo si basa solo su valutazioni di funzioni e restituisce soluzioni localmente ottimali con alta probabilità.
L'articolo propone un approccio per costruire obiettivi surrogati per l'applicazione dei metodi di gradiente della politica all'ottimizzazione combinatoria con lo scopo di ridurre la necessità di sintonizzazione degli iperparametri.
L'articolo propone di sostituire il termine di ricompensa nell'algoritmo del gradiente della politica con la sua distribuzione cumulativa empirica centrata. 
Stima dell'incertezza veloce e calibrata per reti neurali senza campionamento
Questo articolo propone un nuovo approccio per stimare la fiducia delle previsioni in un ambiente di regressione, aprendo la porta ad applicazioni online con stime di incertezza completamente integrate.
Questo articolo ha proposto la regressione evidenziale profonda, un metodo per l'addestramento delle reti neurali per stimare non solo l'output ma anche le prove associate a sostegno di quell'output.
Proponiamo un nuovo algoritmo che trova rapidamente i biglietti vincenti nelle reti neurali.
Questo articolo propone una nuova funzione obiettivo che può essere usata per ottimizzare congiuntamente un obiettivo di classificazione mentre incoraggia la sparsificazione in una rete che funziona con alta precisione.
Questo lavoro propone un nuovo metodo di pruning iterativo chiamato Continuous Sparsification, che pota continuamente il peso corrente fino a raggiungere il rapporto target.
Introdurre un'impostazione formale per la formazione a budget e proporre un programma di tasso di apprendimento lineare consapevole del budget
Questo lavoro presenta una tecnica per sintonizzare il tasso di apprendimento per l'addestramento delle reti neurali sotto un numero fisso di epoche.
Questo articolo ha analizzato quale programma di tasso di apprendimento dovrebbe essere usato quando il numero di iterazioni è limitato usando un concetto introdotto di BAS (Budget-Aware Schedule).
Conduciamo l'esplorazione usando ricompense intrinseche che si basano su una distanza ponderata dei vicini più vicini nello spazio rappresentazionale.
Questo articolo propone un metodo per un'esplorazione efficiente in MDP tabulari e un semplice ambiente di controllo, usando codificatori deterministici per imparare una rappresentazione a bassa dimensione della dinamica dell'ambiente.
Questo articolo propone un metodo di esplorazione efficiente per agenti RL usando una combinazione di approcci basati sul modello e senza modello con una metrica di novità.
Le prestazioni di robustezza dei modelli addestrati da PGD sono sensibili alla trasformazione semantica dei dataset di immagini, il che implica l'ingannevolezza della valutazione degli algoritmi di apprendimento robusto nella pratica.
Proponiamo il ranking policy gradient che impara il rango ottimale delle azioni per massimizzare il rendimento. Proponiamo un quadro generale di apprendimento off-policy con le proprietà di conservazione dell'ottimalità, riduzione della varianza ed efficienza del campione.
Questo articolo propone di riparametrizzare la politica utilizzando una forma di ranking per convertire il problema RL in un problema di apprendimento supervisionato.
Questo articolo presenta un nuovo punto di vista sui metodi di gradiente della politica dal punto di vista del ranking. 
Combinando la classificazione e il recupero delle immagini in un'architettura di rete neurale, otteniamo un miglioramento per entrambi i compiti.
Questo articolo propone un'incorporazione unificata per la classificazione delle immagini e il recupero delle istanze per migliorare le prestazioni di entrambi i compiti.
L'articolo propone di addestrare congiuntamente una rete neurale profonda per la classificazione delle immagini, l'istanza e il riconoscimento delle copie.
Studiamo la mappatura della relazione di iponimia di wordnet ai vettori di caratteristiche
Questo articolo studia come l'iponimia tra le parole può essere mappata nelle rappresentazioni delle caratteristiche.
Questo articolo esplora la nozione di iponimia nelle rappresentazioni vettoriali di parole e descrive un metodo per organizzare le relazioni WordNet in una struttura ad albero per definire l'iponimia.
Costruiamo un generatore di linguaggio naturale più forte addestrando in modo discriminatorio funzioni di punteggio che classificano le generazioni candidate rispetto a varie qualità di buona scrittura.
Questo articolo propone di riunire più bias induttivi che sperano di correggere le incongruenze nella decodifica delle sequenze e propone di ottimizzare per i parametri di una combinazione predefinita di vari sotto-obiettivi. 
Questo documento combina il modello linguistico RNN con diversi modelli addestrati in modo discriminatorio per migliorare la generazione del linguaggio.
Questo articolo propone di migliorare la generazione di modelli linguistici RNN utilizzando obiettivi aumentati ispirati alle massime di comunicazione di Grice.
Soluzione di bilanciamento del carico scalabile e a bassa comunicazione per sistemi multi-dispatcher a server eterogenei con forti garanzie teoriche e promettenti risultati empirici. 
Una misura quantitativa per prevedere le prestazioni dei modelli di rete neurale profonda.
L'articolo propone una nuova quantità che conta il numero di percorsi nella rete neurale che è predittiva delle prestazioni delle reti neurali con lo stesso numero di parametri.
L'articolo presenta un metodo per contare i percorsi nelle reti neurali profonde che probabilmente può essere utilizzato per misurare le prestazioni della rete.
Questo articolo presenta uno studio rigoroso del perché gli schemi di tasso di apprendimento usati praticamente (per un dato budget computazionale) offrono vantaggi significativi anche se questi schemi non sono sostenuti dalla teoria classica dell'approssimazione stocastica.
Questo articolo presenta uno studio teorico di diversi programmi di tasso di apprendimento che ha portato a limiti minimi statistici per entrambi gli schemi polinomiali e constant-and-cut.
L'articolo studia l'effetto delle scelte di learning-rate per l'ottimizzazione stocastica, concentrandosi sul least-mean-squares con stepsizes decrescenti
Presentiamo pianificatori basati su convnet che sono efficienti dal punto di vista del campione e che si generalizzano a istanze più grandi di problemi di navigazione e pathfinding.
Propone metodi che possono essere visti come modifiche delle reti di iterazione del valore (VIN), con alcuni miglioramenti volti a migliorare l'efficienza del campione e la generalizzazione a grandi dimensioni dell'ambiente.
L'articolo presenta un'estensione delle reti originali di iterazione del valore (VIN) considerando una funzione di transizione dipendente dallo stato.
imparare migliori embeddings di dominio attraverso l'apprendimento permanente e il meta-apprendimento
Presenta un metodo di apprendimento permanente per l'apprendimento di embeddings di parole.
Questo articolo propone un approccio per imparare le embeddings in nuovi domini e batte significativamente la linea di base su un compito di estrazione di aspetti. 
 proponiamo un nuovo metodo di potatura basato sulla regolarizzazione (chiamato IncReg) per assegnare in modo incrementale diversi fattori di regolarizzazione a diversi gruppi di peso in base alla loro importanza relativa.
Questo articolo propone un metodo di potatura basato sulla regolarizzazione per assegnare in modo incrementale diversi fattori di regolarizzazione a diversi gruppi di peso in base alla loro importanza relativa.
Gli schemi esistenti di slancio/accelerazione come il metodo della palla pesante e l'accelerazione di Nesterov impiegati con gradienti stocastici non migliorano rispetto alla discesa del gradiente stocastico di vaniglia, specialmente quando sono impiegati con lotti di piccole dimensioni.
Mostriamo che i compiti di pianificazione di oversubscription possono essere risolti usando A* e introduciamo nuove euristiche sensibili ai limiti per i compiti di pianificazione di oversubscription.
Presenta un approccio per risolvere in modo ottimale i compiti di pianificazione di oversubscription (OSP) utilizzando una traduzione alla pianificazione classica con funzioni di costo multiple.
L'articolo propone delle modifiche all'euristica ammissibile per renderla meglio informata in un ambiente multi-criteri dove.
Sviluppiamo metodi di meta-apprendimento per l'apprendimento adversarialmente robusto di pochi colpi.
Questo articolo presenta un metodo che migliora la robustezza dell'apprendimento a pochi colpi introducendo l'attacco di dati di query avversaria nella fase di fine-tuning dell'inner-task di un algoritmo di meta-apprendimento.
Gli autori di questo articolo propongono un nuovo approccio per la formazione di un modello robusto a pochi scatti. 
Troviamo che il pooling da solo non determina la stabilità della deformazione nelle CNN e che la morbidezza del filtro gioca un ruolo importante nel determinare la stabilità. 
Proponiamo un quadro di auto-assemblaggio per addestrare modelli di apprendimento profondo più robusti sotto dataset etichettati e rumorosi.
Questo articolo ha proposto il "filtraggio dell'etichetta auto-assemblata" per l'apprendimento con etichette rumorose, dove il rumore dell'etichetta è indipendente dall'istanza, che produce un'identificazione più accurata delle predizioni incoerenti. 
Questo articolo propone un algoritmo per l'apprendimento da dati con etichette rumorose che alterna l'aggiornamento del modello alla rimozione dei campioni che sembrano avere etichette rumorose.
Indaghiamo la potatura delle DNN prima dell'addestramento e forniamo una risposta su quale topologia dovrebbe essere usata per l'addestramento di reti sparse a priori.
Gli autori propongono di sostituire gli strati densi con strati lineari scarsamente connessi e un approccio per trovare la migliore topologia misurando quanto bene gli strati scarsi approssimano i pesi casuali delle loro controparti dense.
L'articolo propone un'architettura sparse a cascata che è una moltiplicazione di diverse matrici sparse e un modello di connettività specifico che supera altre considerazioni fornite.
Presentiamo Multitask Neural Model Search, un meta-learner che può progettare modelli per più compiti simultaneamente e trasferire l'apprendimento a compiti non visti.
Questo articolo estende la ricerca dell'architettura neurale al problema dell'apprendimento multitasking in cui un controllore di ricerca del modello condizionato dal compito viene appreso per gestire più compiti contemporaneamente.
In questo articolo, gli autori riassumono il loro lavoro sulla costruzione di una struttura, chiamata Multitask Neural Model Search controller, per la costruzione automatizzata di reti neurali attraverso più compiti contemporaneamente.
Modelliamo i processi visivi non lineari come rumore autoregressivo attraverso l'apprendimento profondo generativo.
Propone un nuovo metodo che modella il processo visivo non lineare con una versione profonda di un processo lineare (processo di Markov).
Questo articolo propone un nuovo modello generativo profondo per sequenze, in particolare sequenze di immagini e video, che utilizza una struttura lineare in una parte del modello.
Questo articolo propone una nuova rete feed-forward, chiamata PDE-Net, per imparare le PDE dai dati. 
L'articolo espone l'uso di macchinari di apprendimento profondo allo scopo di identificare sistemi dinamici specificati da PDE.
L'articolo propone un algoritmo basato su reti neurali per l'apprendimento da dati che derivano da sistemi dinamici con equazioni di governo che possono essere scritte come equazioni differenziali parziali.
Questo articolo affronta la modellazione di sistemi dinamici complessi attraverso equazioni differenziali parziali non parametriche utilizzando architetture neurali, con l'idea più importante del papier (PDE-net) per imparare sia gli operatori differenziali che la funzione che governa la PDE.
Diamo una procedura di campionamento veloce simile al flusso normalizzante per modelli di variabili latenti discreti.
Questo articolo usa un'approssimazione variazionale di filtraggio autoregressivo per la stima dei parametri nei sistemi dinamici discreti usando iterazioni a punto fisso.
Gli autori pongono una famiglia posteriore autoregressiva generale per le variabili discrete o le loro rilassamenti continui. 
Questo articolo ha due contributi principali: estende i flussi di normalizzazione alle impostazioni discrete e presenta una regola approssimativa di aggiornamento a punto fisso per le serie temporali autoregressive che può sfruttare il parallelismo della GPU. 
Proponiamo una struttura che impara a codificare la conoscenza simbolicamente e a generare programmi per ragionare sulla conoscenza codificata.
Gli autori propongono la macchina N-Gram per rispondere a domande su documenti lunghi.
Questo articolo presenta la n-gram machine, un modello che codifica le frasi in semplici rappresentazioni simboliche che possono essere interrogate in modo efficiente.
Questo articolo propone un obiettivo di meta-apprendimento basato sulla velocità di adattamento alle distribuzioni di trasferimento per scoprire una decomposizione modulare e variabili causali.
L'articolo mostra che un modello con la struttura di base corretta si adatterà più velocemente a un intervento causale rispetto a un modello con una struttura non corretta.
In questo lavoro, gli autori hanno proposto un quadro generale e sistematico dell'obiettivo di meta-trasferimento che incorpora l'apprendimento della struttura causale sotto interventi sconosciuti.
Un'altra prospettiva sulla dimenticanza catastrofica
Questo articolo introduce una struttura per combattere la dimenticanza catastrofica basata sulla modifica del termine di perdita per minimizzare i cambiamenti nella probabilità del classificatore, ottenuta tramite un'approssimazione in serie di Taylor.
Questo articolo cerca di risolvere il problema dell'apprendimento continuo concentrandosi sugli approcci di regolarizzazione, e propone una strategia L_1 per mitigare il problema.
Proponiamo un approccio per costruire modelli realistici 3D morphable facial (3DMM) che permette un flusso di lavoro intuitivo di editing degli attributi facciali selezionando i migliori set di autovettori e misure antropometriche.
Propone un modello frammentario e morfabile per le mesh dei volti umani e propone anche una mappatura tra le misure antropometriche del volto e i parametri del modello al fine di sintetizzare e modificare i volti con gli attributi desiderati. 
Questo articolo descrive un metodo di modello facciale morphable basato su parti che permette il controllo localizzato dell'utente.
Due algoritmi hanno superato altri otto su un esperimento BCI basato su EEG
Insegniamo agli agenti a negoziare usando solo l'apprendimento per rinforzo; gli agenti egoisti possono farlo, ma solo usando un canale di comunicazione affidabile, e gli agenti prosociali possono negoziare usando il cheap talk.
Gli autori descrivono una variante del gioco di negazione con la considerazione di un canale di comunicazione secondario per il cheap talk, trovando che il canale secondario migliora i risultati di negazione.
Questo articolo esplora come gli agenti possono imparare a comunicare per risolvere un compito di negoziazione e trovare che gli agenti prosociali sono in grado di imparare a mettere a terra i simboli usando la RL, ma gli agenti auto-interessati no.
Esamina i problemi di come gli agenti possono usare la comunicazione per massimizzare le loro ricompense in un semplice gioco di negoziazione.
Proponiamo un nuovo quadro di meta-apprendimento per l'inferenza trasduttiva che classifica l'intero set di test in una sola volta per alleviare il problema dei pochi dati.
Questo articolo propone di affrontare l'apprendimento di pochi colpi in modo trasduttivo imparando un modello di propagazione delle etichette in un modo end-to-end, il primo ad imparare la propagazione delle etichette per l'apprendimento trasduttivo di pochi colpi e ha prodotto risultati empirici efficaci. 
Questo articolo propone un quadro di meta-apprendimento che sfrutta i dati non etichettati imparando la propogazione delle etichette basata sul grafico in modo end-to-end.
Studia l'apprendimento di pochi ospiti in un'impostazione trasduttiva: usando il meta apprendimento per imparare a propagare le etichette dai campioni di formazione ai campioni di prova. 
Descriviamo l'uso di un sistema di pianificazione automatizzato per la progettazione della politica di osservazione e per programmare le operazioni della missione ECOSTRESS della NASA.
Questo articolo presenta un adattamento di un sistema di programmazione automatica, CLASP, per indirizzare un esperimento EO (ECOSTRESS) sulla ISS. 
La memorizzazione e la rappresentazione Hybird della conoscenza appresa può essere una ragione per gli esempi avversari.
Nuovi esperimenti e teoria per il Q-Learning basato su Adam
Questo articolo fornisce un risultato di convergenza per il tradizionale Q-learning con approssimazione di funzione lineare quando si usa un aggiornamento simile ad Adam. 
Questo articolo descrive un metodo per migliorare l'algoritmo AltQ utilizzando una combinazione di un ottimizzatore Adam e riavviando regolarmente i parametri interni dell'ottimizzatore Adam.
Una nuova rete di capsule che converge più velocemente sui nostri esperimenti di benchmark sanitario.
Presenta una variante delle reti di capsule che invece di usare il routing EM impiega un sottospazio lineare spaziato dall'autovettore dominante sulla matrice dei voti pesati della capsula precedente.
L'articolo propone un metodo di routing migliorato, che impiega strumenti di eigendecomposition per trovare l'attivazione e la posa della capsula.
Proponiamo un metodo di messa a punto distribuita dei modelli linguistici sui dispositivi degli utenti senza raccolta di dati privati
Questo articolo si occupa di migliorare i modelli linguistici su apparecchiature mobili basati su una piccola porzione di testo che l'utente ha immesso utilizzando un obiettivo interpolato linearmente tra il testo specifico dell'utente e l'inglese generale. 
Questo articolo utilizza l'analisi della perdita di Lipschitz su uno spazio di ipotesi delimitato per derivare nuovi algoritmi di tipo ERM con forti garanzie di performance che possono essere applicati al modello GP sparso non coniugato.
Proponiamo un metodo di regolarizzazione per la rete neurale e un metodo di analisi del rumore
Questo articolo propone un nuovo metodo di regolarizzazione per mitigare il problema dell'overfitting delle reti neurali profonde ruotando le caratteristiche con una matrice di rotazione casuale per ridurre il co-adattamento.
Questo articolo propone un nuovo metodo di regolarizzazione per l'addestramento delle reti neurali, che aggiunge neuroni di rumore in modo interdipendente.
Un quadro probabilistico per l'apprendimento di rinforzo multi-agente
Questo articolo propone un nuovo algoritmo chiamato Multi-Agent Soft Actor-Critic (MA-SAC) basato sull'algoritmo off-policy maximum-entropy actor critic Soft Actor-Critic (SAC)
Forniamo un rilassamento continuo all'operatore di ordinamento, consentendo un'ottimizzazione stocastica basata sul gradiente end-to-end.
L'articolo considera come ordinare un certo numero di elementi senza imparare esplicitamente e necessariamente i loro significati o valori reali e propone un metodo per eseguire l'ottimizzazione attraverso un rilassamento continuo.
Questo lavoro si basa su un'identità sum(top k) per derivare un campionatore differenziabile pathwise di matrici 'unimodal row stochastic'.
Introduce un rilassamento continuo dell'operatore di ordinamento per costruire un'ottimizzazione basata sul gradiente end-to-end e introduce un'estensione stocastica del suo metodo usando distribuzioni Placket-Luce e Monte Carlo.
Eseguiamo un apprendimento di trasferimento efficiente e flessibile nel quadro dell'ottimizzazione bayesiana attraverso funzioni di acquisizione neurale meta-apprese.
Gli autori presentano MetaBO che usa il reinforcement learning per meta-apprendere la funzione di acquisizione per l'ottimizzazione bayesiana, mostrando un'efficienza di campionamento crescente su nuovi compiti.
Gli autori propongono un'alternativa basata sul meta-apprendimento alle funzioni di acquisizione standard (AF), in cui una rete neurale preaddestrata produce valori di acquisizione in funzione di caratteristiche scelte a mano.
Le reti neurali profonde deterministiche non scartano le informazioni, ma raggruppano i loro input.
Questo articolo fornisce un metodo di principio per esaminare la frase di compressione nelle reti neurali profonde, fornendo uno stimatore teorico di entropia sonora per stimare l'informazione reciproca. 
Proponiamo obiettivi di regolarizzazione per algoritmi RL multi-agente che favoriscono la coordinazione nei compiti cooperativi.
Questo articolo propone due metodi per indirizzare gli agenti verso l'apprendimento di comportamenti coordinati e valuta entrambi rigorosamente attraverso domini multi-agente di adeguata complessità.
Questo articolo propone due metodi basati su MADDPG per incoraggiare la collaborazione tra agenti MARL decentralizzati.
Presentiamo un modello che impara rappresentazioni congiunte robuste eseguendo traduzioni cicliche gerarchiche tra modalità multiple.
Questo articolo presenta il Multimodal Cyclic Translation Network (MCTN) e lo valuta per l'analisi multimodale del sentimento.
Comprendere gli autovalori di Hessian delle reti neurali sotto la distribuzione generatrice di dati.
Questo articolo analizza lo spettro della matrice Hessiana di grandi reti neurali, con un'analisi degli autovalori max/min e la visualizzazione degli spettri utilizzando un approccio di quadratura Lanczos.
Questo articolo usa la teoria delle matrici casuali per studiare la distribuzione dello spettro dell'Hessiano empirico e dell'Hessiano vero per l'apprendimento profondo, e propone un metodo efficiente di visualizzazione dello spettro.
Un semplice trucco per migliorare i modelli di sequenza: Comporli con un modello a grafo
Questo articolo presenta un modello di riassunto strutturale con un codificatore basato su grafici esteso da RNN.
Questo lavoro combina le Reti Neurali Grafiche con un approccio sequenziale alla sintesi astrattiva, efficace su tutti i set di dati rispetto alle linee di base esterne.
Un classificatore sparso basato su un modello discriminativo a miscela gaussiana, che può anche essere incorporato in una rete neurale.
L'articolo presenta un modello di miscela gaussiana addestrato tramite argomenti di discesa del gradiente che permette di indurre la sparsità e di ridurre i parametri di strato del modello addestrabili.
Questo articolo propone un classificatore, chiamato SDGM, basato sulla miscela discriminativa gaussiana e la sua stima dei parametri sparsi.
Inizializzare i pesi utilizzando i codebook di Grassmannian, ottenere un addestramento più veloce e una migliore accuratezza
Un approccio di adattamento al dominio non supervisionato che si adatta sia a livello di pixel che di caratteristiche
Questo articolo propone un approccio di adattamento al dominio estendendo la CycleGAN con funzioni di perdita specifiche del compito e perdite imposte sia sui pixel che sulle caratteristiche. 
Questo articolo propone l'uso di CycleGANs per l'adattamento al dominio
Questo articolo fa una nuova estensione al lavoro precedente su CycleGAN accoppiandolo con approcci di adattamento adversariale, includendo una nuova caratteristica e una perdita semantica nell'obiettivo generale del CycleGAN, con chiari benefici.
Amharic Light Stemmer è progettato per migliorare le prestazioni di Amharic Sentiment Classification.
Questo articolo studia lo stemming per le lingue morfologicamente ricche con uno stemmer leggero che rimuove solo gli affissi nella misura in cui l'informazione semantica originale della parola viene mantenuta.
Questo articolo propone una tecnica di stemming della luce amarica utilizzando una cascata di trasformazioni che standardizzano la forma, rimuovono i suffissi, i prefissi e gli infissi.
Abbiamo studiato se le reti profonde semplici possiedono neuroni artificiali simili a celle di griglia durante il recupero della memoria nello spazio concettuale appreso.
Pro e contro della visione computerizzata basata sulle saccadi in una prospettiva di codifica predittiva
Presenta un quadro computazionale per il problema della visione attiva e spiega come la politica di controllo può essere appresa per ridurre l'entropia della credenza posteriore.
Studiamo teoricamente la consistenza dello spettro di Laplacian e lo usiamo come embeddding di interi grafici
Questo articolo si basa sullo spettro laplaciano di un grafico come mezzo per generare una rappresentazione da utilizzare per confrontare i grafici e classificarli.
Questo lavoro ha proposto di usare lo spettro di Graph Laplacian per imparare la rappresentazione dei grafici.
L'addestramento avversario basato su FGSM, con randomizzazione, funziona altrettanto bene dell'addestramento avversario basato su PGD: possiamo usarlo per addestrare un classificatore robusto in 6 minuti su CIFAR10, e 12 ore su ImageNet, su una singola macchina.
Questo articolo rivisita il metodo Random+FGSM per addestrare modelli robusti contro forti attacchi di evasione PGD più velocemente dei metodi precedenti.
L'affermazione principale di questo articolo è che una semplice strategia di randomizzazione più l'addestramento contraddittorio del metodo del gradiente veloce (FGSM) produce reti neurali robuste.
Proponiamo regolatori quasi ovunque differenziabili e invarianti di scala per il pruning DNN, che possono portare alla sparsità suprema attraverso l'addestramento standard SGD.
L'articolo propone un regolarizzatore invariante in scala (DeepHoyer) ispirato alla misura Hoyer per far rispettare la sparsità nelle reti neurali. 
Dimostriamo che i dati extra non etichettati non sono necessari affinché i compiti ausiliari auto-supervisionati siano utili per la classificazione delle serie temporali, e presentiamo nuovi ed efficaci compiti ausiliari.
Questo articolo propone un metodo auto-supervisionato per l'apprendimento da dati di serie temporali in ambienti sanitari attraverso la progettazione di compiti ausiliari basati sulla struttura interna dei dati per creare compiti di formazione ausiliari più etichettati.
Questo articolo propone un approccio per l'apprendimento auto-supervisionato su serie temporali.
Gli autovalori di Conjugate (aka NNGP) e Neural Tangent Kernel possono essere calcolati in forma chiusa sul cubo booleano e rivelano gli effetti degli iperparametri sul bias induttivo delle reti neurali, l'addestramento e la generalizzazione.
Questo articolo fornisce un'analisi spettrale sul kernel coniugato delle reti neurali e sul kernel tangente neurale sul cubo booleano per risolvere il motivo per cui le reti profonde sono orientate verso funzioni semplici.
Tutte le parcellizzazioni funzionali del cervello sono sbagliate, ma alcune sono utili
Imitazione dai pixel, con ricompensa scarsa o nulla, usando RL off-policy e una piccola funzione di ricompensa imparata in modo avversario.
L'articolo propone di utilizzare un "avversario minimo" nell'apprendimento generativo di imitazione avversaria in spazi visivi ad alta densità.
Questo articolo mira a risolvere il problema della stima di ricompense sparse in un ambiente di input ad alta densità.
Mostriamo strategie per identificare facilmente i campioni falsi generati con il framework Generative Adversarial Network.
Mostrare che i campioni falsi creati con le comuni implementazioni di reti generative avversarie (GAN) sono facilmente identificabili utilizzando varie tecniche statistiche. 
L'articolo propone delle statistiche per identificare i dati falsi generati usando le GAN basate su semplici statistiche marginali o specifiche formali generate automaticamente dai dati reali.
Presentiamo un quadro analitico per determinare i requisiti di bit-width di accumulazione in tutti e tre i GEMM di apprendimento profondo e verifichiamo la validità e la tenuta del nostro metodo tramite esperimenti di benchmarking.
Gli autori propongono un metodo analitico per prevedere il numero di bit di mantissa necessari per le sommatorie parziali per gli strati convoluzionali e completamente connessi
Gli autori conducono un'analisi approfondita della precisione numerica richiesta per le operazioni di accumulazione nella formazione delle reti neurali e mostrano l'impatto teorico della riduzione del numero di bit nell'accumulatore in virgola mobile.
Una nuova teoria di adattamento non supervisionato del dominio per l'apprendimento metrico a distanza e la sua applicazione al riconoscimento dei volti attraverso diverse variazioni di etnia.
Propone una nuova rete di trasferimento delle caratteristiche che ottimizza la perdita avversaria del dominio e la perdita di separazione del dominio.
Proponiamo un algoritmo di discesa del gradiente stocastico di tipo prossimale convergente per problemi di ottimizzazione non lisci non convessi vincolati
Questo articolo propone Prox-SGD, un quadro teorico per algoritmi di ottimizzazione stocastica che convergono asintoticamente alla stazionarietà per perdite lisce non convesse + vincoli/regolarizzatori convessi.
L'articolo propone un nuovo algoritmo di ottimizzazione stocastica basato sul gradiente con media del gradiente adattando la teoria degli algoritmi prossimali all'impostazione non convessa.
Diamo un limite per le NN sull'errore di uscita in caso di fallimenti casuali dei pesi usando un'espansione di Taylor nel limite continuo in cui i neuroni vicini sono simili
Questo articolo considera il problema dell'abbandono dei neuroni da una rete neurale, mostrando che se l'obiettivo è quello di diventare robusto ai neuroni abbandonati casualmente durante la valutazione, allora è sufficiente allenarsi con l'abbandono.
Questo contributo studia l'impatto delle cancellazioni di neuroni casuali sulla precisione di previsione dell'architettura addestrata, con l'applicazione all'analisi dei guasti e al contesto specifico dell'hardware neuromorfo.
Esploriamo e studiamo le sinergie tra suono e azione.
Questo articolo esplora le connessioni tra azione e suono costruendo un dataset suono-azione-visione con un tilt-bot.
Questo articolo studia il ruolo dell'audio nella percezione degli oggetti e delle azioni, così come il modo in cui le informazioni uditive possono aiutare l'apprendimento di modelli dinamici in avanti e inversi.
Proponiamo Hierarchical Complement Objective Training, un nuovo paradigma di formazione per sfruttare efficacemente la gerarchia delle categorie nello spazio di etichettatura sia nella classificazione delle immagini che nella segmentazione semantica.
Un metodo che regolarizza l'entropia della distribuzione posteriore sulle classi che può essere utile per i compiti di classificazione e segmentazione delle immagini
Il nostro articolo identifica il problema dell'approccio di condivisione dei pesi esistente nella ricerca dell'architettura neurale e propone un metodo pratico, ottenendo forti risultati.
L'autore identifica un problema con il NAS chiamato posterior fading e introduce il Posterior Convergent NAS per mitigare questo effetto
Proponiamo un nuovo approccio di formazione in due fasi basato sull'"arresto precoce" per una formazione robusta su etichette rumorose.
Il documento propone di studiare come l'arresto precoce nell'ottimizzazione aiuta a trovare esempi sicuri
Questo articolo propone un metodo di formazione in due fasi per l'apprendimento con rumore di etichetta.
Introduciamo IC3Net, una singola rete che può essere usata per addestrare gli agenti in scenari cooperativi, competitivi e misti. Mostriamo anche che gli agenti possono imparare quando comunicare usando il nostro modello.
L'autore propone una nuova architettura per l'apprendimento di rinforzo multi-agente che utilizza diversi controllori LSTM con pesi legati che trasmettono un vettore continuo l'uno all'altro
Gli autori propongono un interessante schema di gating che permette agli agenti di comunicare in un ambiente RL multi-agente. 
Presentiamo il primo modello di riassunto neurale astrattivo in grado di personalizzare i riassunti generati.
Proponiamo una struttura software basata sulle idee dell'algoritmo Learning-Compression, che permette di comprimere qualsiasi rete neurale con diversi meccanismi di compressione (pruning, quantizzazione, low-rank, ecc.).
Questo articolo presenta il progetto di una libreria software che rende più facile per l'utente comprimere le loro reti nascondendo i dettagli dei metodi di compressione.
Questo articolo propone un metodo di generazione multimodale end-to-end del volto umano dal discorso basato su un quadro di apprendimento auto-supervisionato.
Questo articolo presenta un quadro di apprendimento multimodale che collega la fase di inferenza e la fase di generazione per cercare la possibilità di generare il volto umano dalla sola voce.
Questo lavoro mira a costruire una struttura condizionale per la generazione di immagini del viso dal segnale audio. 
Viene presentato un approccio top-down per rappresentare ricorsivamente le formule proposizionali tramite reti neurali.
Questo articolo fornisce un nuovo modello a rete neurale di formule logiche che raccoglie informazioni su una data formula attraversando il suo albero di parse dall'alto verso il basso.
L'articolo persegue il percorso di una rete strutturata ad albero isomorfa all'albero di parse di una formula del calcolo proposizionale, ma passando informazioni dall'alto verso il basso piuttosto che dal basso verso l'alto.
Ape-X DQfD = DQN distribuito (molti attori + un discente + replay prioritario) con dimostrazioni che ottimizzano il ritorno non scontato di 0,999 su Atari.
L'articolo propone tre estensioni (aggiornamento di Bellman, perdita di coerenza temporale e dimostrazione esperta) a DQN per migliorare le prestazioni di apprendimento sui giochi Atari, raggiungendo prestazioni superiori ai risultati dello stato dell'arte per i giochi Atari. 
Questo articolo propone un operatore di Bellman trasformato che mira a risolvere la sensibilità alla ricompensa non presa, la robustezza al valore del fattore di sconto e il problema dell'esplorazione.
Metodo di formazione per imporre vincoli rigorosi sulle embeddings apprese durante la formazione supervisionata. Applicato alla risposta alle domande visive.
Gli autori propongono un quadro per incorporare una conoscenza preliminare semantica aggiuntiva nell'addestramento tradizionale dei modelli di apprendimento profondo per regolarizzare lo spazio di incorporazione invece dello spazio dei parametri.
L'articolo sostiene la necessità di codificare la conoscenza esterna nello strato di incorporazione linguistica di una rete neurale multimodale, come un insieme di vincoli rigidi.
Risolvere problemi inversi usando approssimazioni lisce degli algoritmi forward per addestrare i modelli inversi.
Un metodo di apprendimento profondo per la localizzazione puntiforme debolmente supervisionata che impara usando solo l'etichetta a livello di immagine. Si basa sull'entropia condizionale per localizzare le regioni rilevanti e irrilevanti con l'obiettivo di minimizzare le regioni false positive.
Questo lavoro esplora il problema del WSL usando un nuovo design di termini di regolarizzazione e un algoritmo di cancellazione ricorsivo.
Questo articolo presenta un nuovo approccio debolmente supervisionato per l'apprendimento della segmentazione degli oggetti con etichette di classe a livello di immagine.
Acquisire stati dalla regione ad alta frequenza per il controllo della ricerca in Dyna.
Gli autori propongono di fare il campionamento nel dominio delle alte frequenze per aumentare l'efficienza del campione
Questo articolo propone un nuovo modo di selezionare gli stati da cui fare le transizioni nell'algoritmo dyna.
Introduciamo un quadro di codifica delle fonti distribuite basato su Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC).
L'articolo ha proposto un autocodificatore ricorrente distribuito per la compressione delle immagini che utilizza un ConvLSTM per imparare codici binari che sono costruiti progressivamente dai residui delle informazioni precedentemente codificate
Gli autori propongono un metodo per addestrare modelli di compressione delle immagini su fonti multiple, con un codificatore separato su ogni fonte e un decodificatore condiviso. 
Le porte fanno tutto il lavoro pesante negli LSTM calcolando somme pesate in base agli elementi, e la rimozione della RNN semplice interna non degrada le prestazioni del modello.
Questo articolo propone una variante semplificata di LSTM rimuovendo la non linearità dell'elemento di contenuto e del gate di uscita
Questo articolo presenta un'analisi delle LSTMS, mostrando che hanno una forma in cui il contenuto della cella di memoria ad ogni passo è una combinazione pesata dei valori di "aggiornamento del contenuto" calcolati ad ogni passo temporale e offre una semplificazione delle LSTM che calcolano il valore con cui la cella di memoria ad ogni passo temporale in termini di una funzione deterministica dell'input piuttosto che una funzione dell'input e del contesto corrente.
L'articolo propone una nuova visione di LSTM in cui il nucleo è una somma pesata elementale e sostiene che LSTM è ridondante mantenendo solo porte d'ingresso e dimentica per calcolare i pesi
Analizzando più di 300 articoli nelle recenti conferenze sull'apprendimento automatico, abbiamo scoperto che le applicazioni di Machine Learning for Health (ML4H) sono in ritardo rispetto ad altri campi di apprendimento automatico in termini di metriche di riproducibilità.
Questo documento conduce una revisione quantitativa e qualitativa dello stato della riproducibilità per le applicazioni sanitarie ML e propone raccomandazioni per rendere la ricerca più riproducibile.
Addestriamo molte piccole reti ognuna per una specifica operazione, queste sono poi combinate per eseguire operazioni complesse
Questo articolo propone di usare le reti neurali per valutare le espressioni matematiche progettando 8 piccoli blocchi di costruzione per 8 operazioni fondamentali, ad esempio, addizione, sottrazione, ecc. e poi progettando la moltiplicazione e la divisione a più cifre usando questi piccoli blocchi.
L'articolo propone un metodo per progettare un motore di valutazione delle espressioni matematiche basato su NN.
Migliorare la qualità e la stabilità delle GAN usando un discriminatore relativistico; le GAN IPM (come WGAN-GP) sono un caso speciale.
L'articolo propone un "discriminatore relativistico", che aiuta in alcune impostazioni, anche se un po' sensibile a iperparametri, architetture e set di dati.
In questo lavoro, gli autori considerano una variazione di GAN diminuendo simultaneamente la probabilità che i dati reali siano reali per il generatore.
Una versione di MPO basata sulla funzione stato-valore che raggiunge buoni risultati in una vasta gamma di compiti nel controllo discreto e continuo.
Questo articolo presenta un algoritmo per l'apprendimento di rinforzo on-policy che può gestire sia il controllo continuo/discreto, l'apprendimento singolo/multi-task e usare sia stati che pixel a bassa dimensione.
L'articolo propone una variante online di MPO, V-MPO, che impara la funzione V e aggiorna la distribuzione non parametrica verso i vantaggi.
Proponiamo motori di esecuzione neurali (NEE), che sfruttano una maschera appresa e tracce di esecuzione supervisionate per imitare la funzionalità delle subroutine e dimostrare una forte generalizzazione.
Questo articolo indaga un problema di costruzione di un motore di esecuzione di programmi con reti neurali e propone un modello basato su trasformatori per imparare le subroutine di base e le applica in diversi algoritmi standard.
Questo articolo affronta il problema della progettazione di architetture di reti neurali che possono imparare e implementare programmi generali.
Il rilevamento bayesiano dei punti di cambiamento permette il meta-apprendimento direttamente dai dati delle serie temporali.
L'articolo considera il meta-apprendimento nell'impostazione del compito non segmentato e applica il rilevamento bayesiano online del punto di cambiamento con il meta-apprendimento.
Questo articolo spinge il meta-apprendimento verso impostazioni non segmentate, dove il quadro MOCA adotta uno schema di stima bayesiana dei punti di cambiamento per il rilevamento dei cambiamenti di attività.
Un approccio basato sull'apprendimento profondo per il rilevamento di fonemi fricativi a ritardo zero
Questo articolo applica metodi di apprendimento profondo supervisionato per rilevare la durata esatta di un fonema fricativo al fine di migliorare l'algoritmo pratico di abbassamento della frequenza.
Un meccanismo di attenzione online e a tempo lineare che esegue un'attenzione morbida su pezzi della sequenza di input localizzati in modo adattivo.
Questo articolo propone una piccola modifica all'attenzione monotonica in [1] aggiungendo un'attenzione morbida al segmento previsto dall'attenzione monotonica.
L'articolo propone un'estensione di un precedente modello di attenzione monotona (Raffel et al 2017) per partecipare a una finestra di dimensioni fisse fino alla posizione di allineamento.
Sviluppare nuove tecniche che si basano sul riordino delle patch per consentire un'analisi dettagliata della relazione tra i set di dati e le prestazioni di formazione e generalizzazione.
Produciamo agenti di apprendimento del rinforzo che generalizzano bene ad una vasta gamma di ambienti utilizzando una nuova tecnica di regolarizzazione.
L'articolo introduce la sfida delle politiche ad alta varianza nella randomizzazione del dominio per l'apprendimento di rinforzo e si concentra principalmente sul problema della randomizzazione visiva, dove i diversi domini randomizzati differiscono solo nello spazio di stato e le ricompense e le dinamiche sottostanti sono le stesse.
Per migliorare la capacità di generalizzazione degli agenti di RL profonda attraverso i compiti con diversi modelli visivi, questo articolo ha proposto una semplice tecnica di regolarizzazione per la randomizzazione del dominio.
Esploriamo l'intersezione tra le neuroscienze di rete e l'apprendimento profondo. 
Questo articolo presenta un sistema per la costruzione non supervisionata e ad alta precisione di basi di conoscenza, utilizzando un programma probabilistico per definire un processo di conversione dei fatti della base di conoscenza in testo non strutturato.
Panoramica sulla base di conoscenza esistente che è costruita con un modello probabilistico, con l'approccio di costruzione della base di conoscenza valutato contro altri approcci di base di conoscenza YAGO2, NELL, Knowledge Vault, e DeepDive.
Questo articolo usa un programma probabilistico che descrive il processo attraverso il quale i fatti che descrivono le entità possono essere realizzati nel testo e in un gran numero di pagine web, per imparare ad eseguire l'estrazione di fatti sulle persone usando un singolo fatto seme.
Nuovo metodo di estrazione del segnale nel dominio di Fourier
Migliorare la scalabilità delle reti neurali a grafo sull'apprendimento per imitazione e la previsione del movimento dello sciame
L'articolo propone un nuovo modello di serie temporali per l'apprendimento di una sequenza di grafici.
Questo lavoro considera i problemi di predizione delle sequenze in un sistema multi-agente.
Proponiamo un quadro di quantizzazione del prodotto differenziabile che può ridurre la dimensione dello strato di incorporazione in una formazione end-to-end senza costi di performance.
Questo articolo lavora sui metodi per comprimere gli strati di embedding per l'inferenza a bassa memoria, dove le embeddings compresse sono apprese insieme ai modelli specifici del compito in un modo differenziabile end-to-end.
Introduciamo un semplice e nuovo algoritmo di regressione modale che è facile da scalare a grandi problemi. 
L'articolo propone un approccio di funzione implicita per imparare le modalità di regressione multimodale.
Il presente lavoro propone un approccio parametrico per stimare il modo condizionale usando il Teorema della Funzione Implicita per distribuzioni multimodali. 
Meta-RL efficiente per il campione combinando l'inferenza variazionale delle variabili probabilistiche del compito con RL off-policy 
Questo articolo propone di usare RL off-policy durante il meta-training per migliorare notevolmente l'efficienza del campione dei metodi Meta-RL.
Questo articolo si concentra sull'identificazione di fonti web di alta qualità per la pipeline di aumento della base di conoscenza industriale.
Indaghiamo i meriti dell'impiego di reti neurali nel problema della predizione delle corrispondenze, dove si cerca di stimare la probabilità che un gruppo di oggetti M sia preferito ad un altro, sulla base di dati parziali di confronto di gruppo.
Questo articolo propone una soluzione di rete neurale profonda al problema della classificazione degli insiemi e progetta un'architettura per questo compito ispirata da precedenti algoritmi progettati manualmente.
Questo articolo fornisce una tecnica per risolvere il problema della previsione delle partite utilizzando un'architettura di apprendimento profondo.
Un nuovo approccio per mantenere matrici di peso ricorrenti ortogonali in una RNN.
Introduce uno schema per l'apprendimento della matrice dei parametri ricorrenti in una rete neurale che usa la trasformazione di Cayley e una matrice di peso scalare. 
Questo articolo suggerisce una riparametrizzazione RNN dei pesi ricorrenti con una matrice asimmetrica utilizzando la trasformazione Cayley per mantenere la matrice dei pesi ricorrenti ortogonale.
Una nuova parametrizzazione delle RNN permette di rappresentare le matrici di peso ortogonali in modo relativamente semplice.
Usiamo un unico modello per risolvere una grande varietà di compiti di analisi del linguaggio naturale formulandoli in un formato unificato di span-relation.
Questo articolo generalizza una vasta gamma di compiti di elaborazione del linguaggio naturale in un unico quadro basato sullo span e propone un'architettura generale per risolvere tutti questi problemi.
Questo lavoro presenta una formulazione unificata di vari compiti NLP a livello di frase e di token.
Presentiamo un limite inferiore variazionale per i modelli GP che può essere ottimizzato senza calcolare operazioni matriciali costose come gli inversi, mentre fornisce le stesse garanzie delle approssimazioni variazionali esistenti.
Gli autocodificatori variazionali con spazi latenti modellati come prodotti di collettori Riemanni a curvatura costante migliorano la ricostruzione delle immagini rispetto alle varianti a singolo collettore.
Questo articolo introduce una formulazione generale della nozione di VAE con uno spazio latente composto da un manifold curvo.
Questo articolo riguarda lo sviluppo di VAE in spazi non euclidei.
Introduciamo un algoritmo a scatola nera per l'ottimizzazione ripetuta dei composti utilizzando un quadro di traduzione.
Gli autori inquadrano l'ottimizzazione delle molecole come un problema sequenza-sequenza, ed estendono i metodi esistenti per migliorare le molecole, mostrando che è vantaggioso per ottimizzare il logP ma non il QED.
L'articolo si basa su modelli di traduzione esistenti sviluppati per l'ottimizzazione molecolare, facendo un uso iterativo di modelli di traduzione da sequenza a sequenza o da grafico a grafico.
Proponendo il primo quadro di watermarking per l'incorporazione e l'estrazione della firma multi-bit usando le uscite della DNN. 
Propone un metodo per il watermarking multi-bit delle reti neurali in un ambiente black-box e dimostra che le previsioni dei modelli esistenti possono portare una stringa multi-bit che può essere utilizzata in seguito per verificare la proprietà.
L'articolo propone un approccio per il watermarking del modello in cui la filigrana è una stringa di bit incorporata nel modello come parte di una procedura di messa a punto
Non sai come ottimizzare? Allora impara a ottimizzare!
Questo articolo propone un modo per addestrare i modelli di classificazione delle immagini per essere resistenti agli attacchi di perturbazione di L-infinità.
Questo articolo propone di utilizzare la struttura learning-to-learn per imparare un attaccante.
Un metodo semplice e facile da addestrare per la predizione multimodale nelle serie temporali. 
Questo articolo introduce un modello di previsione di serie temporali che impara una mappatura deterministica e allena un'altra rete per prevedere i fotogrammi futuri dato l'input e l'errore residuo della prima rete.
L'articolo propone un modello per la predizione sotto incertezza in cui si separano la predizione deterministica dei componenti e la predizione incerta dei componenti.
Questo articolo introduce e motiva simple_rl, una nuova libreria open source per realizzare esperimenti di apprendimento per rinforzo in Python 2 e 3 con un focus sulla semplicità.
Questo articolo si occupa della stabilità di una semplice ottimizzazione a gradiente $\mu$-WGAN introducendo un concetto di differenziazione a valore di misura.
Si studia la WGAN con un termine di penalità a gradiente zero centrato al quadrato per una misura generale.
Caratterizza la convergenza di Wasserstein GAN penalizzata dal gradiente.
Metodo di addestramento allo stato dell'arte per reti a pesi binarie e ternarie basato sull'ottimizzazione alternata di partizioni di peso rilassate in modo casuale
L'articolo propone un nuovo schema di allenamento per ottimizzare una rete neurale ternaria.
Gli autori propongono RPR, un modo per partizionare e quantizzare i pesi in modo casuale e addestrare i parametri rimanenti seguiti dal rilassamento a cicli alternati per addestrare i modelli quantizzati.
Sostituiamo alcuni percorsi dei gradienti nelle RNN gerarchiche con una perdita ausiliaria. Mostriamo che questo può ridurre il costo della memoria preservando le prestazioni.
L'articolo introduce un'architettura RNN gerarchica che potrebbe essere addestrata in modo più efficiente.
Il documento proposto suggerisce di disaccoppiare i diversi livelli di gerarchia in RNN utilizzando perdite ausiliarie.
Le reti neurali che fanno un buon lavoro di classificazione proiettano i punti in forme più sferiche prima di comprimerli in meno dimensioni.
Proponiamo un nuovo metodo di apprendimento per il riconoscimento profondo del suono chiamato apprendimento BC.
Gli autori hanno definito un nuovo compito di apprendimento che richiede una DNN per prevedere il rapporto di miscelazione tra i suoni di due classi diverse per aumentare il potere discriminatorio della rete finale appresa.
Propone un metodo per migliorare le prestazioni di un metodo di apprendimento generico generando campioni di formazione "tra le classi" e presenta l'intuizione di base e la necessità della tecnica proposta.
Proponiamo un metodo che infonde il livello di qualità dei dati variabili nel tempo per la previsione spazio-temporale senza etichette esplicitamente assegnate.
Introduce una nuova definizione di qualità dei dati che si basa sulla nozione di variazione locale definita in (Zhou e Scholkopf) e la estende a più fonti di dati eterogenee.
Questo lavoro ha proposto un nuovo modo di valutare la qualità delle diverse fonti di dati con il modello a grafo variabile nel tempo, con il livello di qualità usato come termine di regolarizzazione nella funzione obiettivo
Proponiamo programmi di forma 3D, una rappresentazione strutturata e compositiva delle forme. Il nostro modello impara a dedurre ed eseguire programmi di forma per spiegare le forme 3D.
Un approccio per dedurre programmi di forma dati modelli 3D, con un'architettura composta da una rete ricorrente che codifica una forma 3D e fornisce istruzioni, e un secondo modulo che rende il programma in 3D.
Questo articolo introduce una descrizione semantica di alto livello per le forme 3D, data dallo ShapeProgram.
Mostriamo che i metodi di regolarizzazione convenzionali (ad esempio, $L_2$, dropout), che sono stati ampiamente ignorati nei metodi RL, possono essere molto efficaci nell'ottimizzazione delle politiche.
Gli autori studiano una serie di metodi di ottimizzazione della politica diretta esistenti nel campo dell'apprendimento di rinforzo e forniscono un'indagine dettagliata sull'effetto dei regolamenti sulle prestazioni e sul comportamento degli agenti che seguono questi metodi.
Questo articolo fornisce uno studio sull'effetto della regolarizzazione sulle prestazioni in ambienti di formazione in metodi di ottimizzazione delle politiche in compiti di controllo continuo multipli.
Presentiamo un set di dati di risposta alle domande, FigureQA, come un primo passo verso lo sviluppo di modelli che possono riconoscere intuitivamente i modelli dalle rappresentazioni visive dei dati.
Questo articolo introduce un dataset di risposte a domande template sulle figure, che coinvolgono il ragionamento sugli elementi delle figure.
L'articolo introduce un nuovo set di dati di ragionamento visivo chiamato Figure-QA che consiste in 140K immagini di figure e 1.55M di coppie QA, che può aiutare nello sviluppo di modelli che possono estrarre informazioni utili dalle rappresentazioni visive dei dati.
Questo position paper analizza diversi tipi di autospiegazione che possono sorgere nella pianificazione e nei sistemi correlati. 
Discute diversi aspetti delle spiegazioni, in particolare nel contesto del processo decisionale sequenziale. 
Il primo approccio di deep learning a MFSR per risolvere registrazione, fusione, up-sampling in modo end-to-end.
Questo articolo propone un algoritmo di super-risoluzione end-to-end multi-frame, che si basa su una co-registrazione a coppie e su blocchi di fusione (blocchi residuali convoluzionali), incorporati in una rete di encoder-decoder 'HighRes-net' che stima l'immagine di super-risoluzione.
Questo articolo propone un quadro che include la fusione ricorsiva alla perdita di co-registrazione per risolvere il problema dei risultati di super-risoluzione e delle etichette ad alta risoluzione che non sono allineati ai pixel.
Per l'addestramento distribuito su reti ad alta latenza, usate una media distribuita approssimativa basata sul gossip invece di una media distribuita esatta come AllReduce.
Gli autori propongono di usare algoritmi di gossip come metodo generale per calcolare approssimativamente la media su un insieme di lavoratori
L'articolo dimostra la convergenza di SGP per funzioni lisce non convesse e mostra che SGP può raggiungere una velocità significativa nell'ambiente a bassa latenza senza sacrificare troppe prestazioni predittive. 
Questo articolo sviluppa un quadro di apprendimento contraddittorio per modelli di conversazione neurali con persona
Questo articolo propone un'estensione di hredGAN per imparare simultaneamente un insieme di embeddings di attributi che rappresentano la persona di ogni parlante e generare risposte basate sulla persona
Le reti neurali artificiali bio-ispirate, composte da neuroni posizionati in uno spazio bidimensionale, sono in grado di formare gruppi indipendenti per eseguire diversi compiti.
Trasformatore discreto che usa l'attenzione dura per assicurare che ogni passo dipenda solo da un contesto fisso.
Questo articolo presenta delle modifiche all'architettura standard del trasformatore con l'obiettivo di migliorare l'interpretabilità mantenendo le prestazioni nei compiti NLP.
Questo articolo propone tre trasformatori discreti: un modulo di attenzione discreto e stocastico basato su Gumbel-softmax, un trasformatore sintattico e semantico a due flussi e la regolarizzazione della sparsità.
Mostriamo l'evidenza empirica che i modelli di codifica predittiva producono rappresentazioni più correlate ai dati del cervello rispetto ai modelli supervisionati di riconoscimento delle immagini.
Un quadro generico per gestire il trasferimento e l'apprendimento multitasking usando coppie di autocodificatori con pesi specifici del compito e condivisi.
Propone un quadro generico per l'apprendimento di trasferimento end-to-end / adattamento al dominio con reti neurali profonde. 
Questo articolo propone un modello per permettere alle architetture di reti neurali profonde di condividere i parametri tra diversi set di dati, e lo applica all'apprendimento di trasferimento.
L'articolo si concentra sull'apprendimento di caratteristiche comuni da dati di domini multipli e finisce con un'architettura generale per l'apprendimento multi-task, semi-supervisionato e di trasferimento.
Proponiamo una struttura per combinare alberi decisionali e reti neurali, e dimostriamo su compiti di classificazione di immagini che gode dei benefici complementari dei due approcci, mentre affronta le limitazioni del lavoro precedente.
Gli autori hanno proposto un nuovo modello, Adaptive Neural Trees, combinando l'apprendimento della rappresentazione e l'ottimizzazione del gradiente delle reti neurali con l'apprendimento dell'architettura degli alberi di decisione
Questo articolo propone l'approccio Adaptive Neural Trees per combinare i due paradigmi di apprendimento delle reti neurali profonde e degli alberi decisionali
La traduzione di porzioni dell'input durante l'addestramento può migliorare le prestazioni multilingue.
L'articolo propone un metodo di aumento dei dati multilingue per migliorare l'inferenza linguistica e i compiti di risposta alle domande.
Questo articolo propone di aumentare i dati crosslinguali con scambi euristici usando traduzioni allineate, come fanno gli umani bilingui nel code-switching.
Proponiamo un quadro di autoencoder condizionale variazionale che mitiga il collasso posteriore negli scenari in cui il segnale di condizionamento è abbastanza forte per un decoder espressivo per generare un output plausibile da esso.
Questo articolo considera i modelli generativi fortemente condizionati, e propone una funzione obiettivo e una parametrizzazione della distribuzione variazionale tale che le variabili latenti dipendano esplicitamente dalle condizioni di input.
Questo articolo sostiene che quando il decodificatore è condizionato dalla concatenazione di variabili latenti e informazioni ausiliarie, allora il collasso posteriore è più probabile che nella VAE vanilla.
Proponiamo uno studio della stabilità di diversi algoritmi di apprendimento a pochi colpi soggetti a variazioni negli iper-parametri e negli schemi di ottimizzazione mentre controlliamo il seme casuale.
Questo articolo studia la riproducibilità per l'apprendimento di pochi colpi.
Traduciamo un limite sulla sub-ottimalità delle rappresentazioni in un obiettivo pratico di formazione nel contesto dell'apprendimento di rinforzo gerarchico.
Gli autori propongono un nuovo approccio nell'apprendimento di una rappresentazione per HRL e dichiarano un'intrigante connessione tra l'apprendimento della rappresentazione e il contenimento della sub-ottimalità che risulta in un algoritmo basato sul gradiente
Questo articolo propone un modo per gestire la sub-ottimalità nel contesto delle rappresentazioni di apprendimento che si riferiscono alla sub-ottimalità della polarità gerarchica rispetto alla ricompensa del compito.
Metareasoning in un pianificatore temporale situato
Questo articolo affronta il problema della pianificazione temporale situata, proponendo un'ulteriore semplificazione sulle strategie greedy precedentemente proposte da Shperberg.
Le prestazioni di robustezza dei modelli addestrati da PGD sono sensibili alla trasformazione semantica dei dataset di immagini, il che implica l'ingannevolezza della valutazione degli algoritmi di apprendimento robusto nella pratica.
Il documento chiarisce la differenza tra accuratezza pulita e robusta e mostra che cambiare la distribuzione marginale dei dati di input P(x) conservando la sua semantica P(y|x) influisce sulla robustezza del modello.
Questo articolo indaga l'origine della mancanza di robustezza dei classificatori alle perturbazioni degli input avversari sotto l-inf bounded perturbazioni.
Corrispondenza delle frasi tramite l'apprendimento delle strutture latenti dell'albero dei costituenti con una variante dell'algoritmo inside-outside incorporato come strato della rete neurale.
Questo articolo introduce un meccanismo di attenzione strutturato per calcolare i punteggi di allineamento tra tutti i possibili span in due frasi date
Questo articolo propone un modello di allineamenti strutturati tra le frasi come mezzo per confrontare le frasi abbinando le loro strutture latenti.
Imparare la rappresentazione di disentangle in modo non supervisionato.
Gli autori presentano un quadro in cui un codificatore automatico (E, D) è regolarizzato in modo che la sua rappresentazione latente condivida informazioni reciproche con una rappresentazione dello spazio latente generata.
Le GAN condizionali addestrate per generare campioni di dati aumentati dei loro input condizionali utilizzati per migliorare la classificazione di vaniglia e i sistemi di apprendimento one shot come le reti di corrispondenza e la distanza dei pixel
Gli autori propongono un metodo per condurre l'aumento dei dati in cui le trasformazioni di classe incrociate sono mappate in uno spazio latente a bassa dimensione utilizzando GAN condizionale
Sviluppiamo un semplice metodo di selezione delle caratteristiche basato sulla regressione eagnostico per interpretare i processi di generazione dei dati con il controllo FDR, e superiamo diverse baseline popolari su diversi set di dati simulati, medici e di immagini.
Questo articolo propone un miglioramento pratico del test di randomizzazione condizionale e una nuova statistica di test, dimostra che la f-divergenza è una scelta possibile, e mostra che la KL-divergenza annulla alcune distribuzioni condizionali.
Questo articolo affronta il problema di trovare caratteristiche utili in un input che sono dipendenti da una variabile di risposta anche quando si condizionano tutte le altre variabili di input.
Un metodo agnostico per fornire un'interpretazione dell'influenza delle caratteristiche di input sulla risposta di un modello a livello di macchina fino al livello di istanza, e statistiche di test adeguate per la selezione delle caratteristiche agnostiche del modello.
Un nuovo approccio per l'apprendimento di un modello da annotazioni rumorose in crowdsourcing.
Questo articolo propone un metodo per l'apprendimento da etichette rumorose, concentrandosi sul caso in cui i dati non sono etichettati in modo ridondante con una validazione teorica e sperimentale
Questo articolo si concentra sul problema dell'apprendimento dalla folla, dove l'aggiornamento congiunto dei pesi del classificatore e delle matrici di confusione dei lavoratori può aiutare nel problema della stima con etichette rare in crowdsourcing.
Propone un algoritmo di apprendimento supervisionato per modellare la qualità delle etichette e dei lavoratori e utilizza l'algoritmo per studiare quanta ridondanza è necessaria nel crowdsourcing e se una bassa ridondanza con abbondanti esempi di rumore porta a etichette migliori.
Nuovo modo di spiegare perché una rete neurale ha classificato male un'immagine
Questo articolo propone un metodo per spiegare gli errori di classificazione delle reti neurali. 
Mira a comprendere meglio la classificazione delle reti neurali ed esplora lo spazio latente di un autocodificatore variazionale e considera le perturbazioni dello spazio latente per ottenere la classificazione corretta.
Un metodo per la costruzione automatica di reti multi-task ramificate con una forte valutazione sperimentale su diversi dataset multi-tasking.
Questo articolo propone un nuovo quadro di apprendimento multi-task con condivisione morbida dei parametri basato su una struttura ad albero.
Questo articolo presenta un metodo per dedurre l'architettura delle reti multi-task per determinare quale parte della rete dovrebbe essere condivisa tra diversi compiti.
È possibile sostituire la matrice dei pesi in uno strato convoluzionale per addestrarlo come uno strato efficiente strutturato; con le stesse prestazioni della decomposizione a basso rango.
Questo lavoro applica i precedenti Structured Efficient Linear Layers ai conv layers e propone Structured Efficient Convolutional Layers come sostituzione dei conv layers originali.
Presentiamo SVDocNet, una rete neurale ricorrente spaziale (RNN) addestrabile end-to-end basata su U-Net per la deblurring cieca dei documenti.
Estendiamo la codifica sparsa bilineare e sfruttiamo le sequenze video per imparare i filtri dinamici.
Proponiamo un nuovo rilevatore di OOD che utilizza immagini sfocate come esempi avversari. Il nostro modello raggiunge prestazioni significative di rilevamento OOD in vari domini.
Questo articolo presenta l'idea di utilizzare immagini sfocate come esempi di regolarizzazione per migliorare le prestazioni di rilevamento di fuori distribuzione basate su Random Network Distillation.
Questo articolo affronta la distribuzione fuori dai dati sfruttando la RND applicata all'incremento dei dati, addestrando un modello per abbinare le uscite di una rete casuale con un incremento come input.
Un ottimizzatore veloce per applicazioni generali e per l'addestramento di grandi lotti.
In questo articolo, gli autori hanno fatto uno studio sull'addestramento in grandi lotti per il BERT, e hanno addestrato con successo un modello BERT in 76 minuti.
Questo articolo sviluppa una strategia di adattamento a livello che permette di addestrare i modelli BERT con grandi mini-batch da 32k contro i 512 di base.
Abbiamo analizzato il ruolo di due tassi di apprendimento nel meta-apprendimento agnostico del modello nella convergenza.
Gli autori hanno affrontato il problema dell'instabilità dell'ottimizzazione in MAML studiando i due tassi di apprendimento.
Questo articolo studia un metodo per aiutare a sintonizzare i due tassi di apprendimento utilizzati nell'algoritmo di formazione MAML.
Modello neurale indipendente dal compito per l'apprendimento di associazioni tra gruppi di parole correlate.
L'articolo ha proposto un metodo per addestrare vettori di parole specifici per la funzione, in cui ogni parola è rappresentata con tre vettori ciascuno in una categoria diversa (Soggetto-Verbo-Oggetto).
Questo articolo propone una rete neurale per imparare rappresentazioni di lavoro specifiche per una funzione e dimostra il vantaggio rispetto alle alternative.
Utilizzo del metodo di deep learning per effettuare la misurazione automatica delle immagini SEM nell'industria dei semiconduttori
Questo articolo descrive e analizza tre metodi per programmare attività a durata non fissa in presenza di risorse consumabili.
L'articolo presenta tre approcci per la programmazione a bordo di attività in un rover planetario sotto vincoli di risorse di riserva.
Descrizione della presentazione a NeurIPS2019 Disentanglement Challenge basata su autoencoder ipersferici variazionali
Un rilevamento delle anomalie che: utilizza la classificazione a trasformazione casuale per generalizzare ai dati non immagine.
Questo articolo propone un metodo profondo per il rilevamento delle anomalie che unifica la recente classificazione profonda di una classe e gli approcci di classificazione basati sulla trasformazione.
Questo articolo propone un approccio al rilevamento delle anomalie basato sulla classificazione per dati generali utilizzando la trasformazione affine y = Wx+b.
Riduciamo le distorsioni del sentimento basandoci sulla valutazione controfattuale della generazione del testo utilizzando modelli linguistici.
Questo articolo misura i bias di sentimento nei modelli linguistici come riflesso del testo generato dai modelli, e aggiunge altri termini oggettivi al solito obiettivo di modellazione linguistica per ridurre i bias.
Questo articolo propone di valutare i bias nei modelli linguistici pre-addestrati utilizzando un sistema di sentimento fisso e prova diversi modelli di prefisso.
Un metodo basato sulla somiglianza semantica e un metodo basato sulla somiglianza del sentimento per debias i modelli neurali del linguaggio addestrati da grandi insiemi di dati.
Un modello nonparametrico bayesiano di argomenti con autocodificatori variazionali che raggiunge lo stato dell'arte su benchmark pubblici in termini di perplessità, coerenza di argomenti e compiti di recupero.
Questo articolo costruisce un modello infinito di argomenti con autocodificatori variazionali combinando l'autocodificatore variazionale di Nalisnick & Smith con l'allocazione latente di Dirichlet e diverse tecniche di inferenza usate in Miao.
Presentiamo una nuova struttura di Distillazione della conoscenza che utilizza campioni di pari come insegnante
Propone un metodo per migliorare l'efficacia della distillazione della conoscenza ammorbidendo le etichette utilizzate e utilizzando un set di dati invece di un singolo campione.
Questo articolo propone di affrontare il costo computazionale extra della formazione con la distillazione della conoscenza, basandosi sulla tecnica Snapshot Distillation recentemente proposta.
imparare le sotto-politiche gerarchiche attraverso l'addestramento end-to-end su una distribuzione di compiti
Gli autori considerano il problema dell'apprendimento di un utile insieme di "politiche secondarie" che possono essere condivise tra i compiti in modo da avviare l'apprendimento su nuovi compiti tratti dalla distribuzione dei compiti. 
Questo articolo propone un nuovo metodo per indurre la struttura gerarchica temporale in un ambiente specializzato multi-task.
Modello di rete neurale convoluzionale per l'incorporazione non supervisionata dei documenti.
Introduce un nuovo modello per il compito generale di indurre rappresentazioni di documenti (embeddings) che utilizza un'architettura CNN per migliorare l'efficienza computazionale.
Questo articolo propone di usare CNN con un obiettivo simile a skip-gram come un modo veloce per produrre embeddings di documenti
Dimostriamo i limiti di generalizzazione per le reti neurali convoluzionali che tengono conto del weight-tying
Studia il potere di generalizzazione delle CNN e migliora i limiti superiori degli errori di generalizzazione, mostrando la correlazione tra l'errore di generalizzazione delle CNN apprese e il termine dominante del limite superiore.
Questo articolo presenta un limite di generalizzazione per le reti neurali convoluzionali basato sul numero di parametri, la costante Lipschitz e la distanza dei pesi finali dall'inizializzazione.
Risparmio di 2 volte nella dimensione del modello, riduzione del 28% dell'energia per MobileNets su ImageNet senza perdita di precisione utilizzando strati ibridi composti da filtri convenzionali a piena precisione e filtri ternari
Si concentra sulla quantizzazione dell'architettura MobileNets a valori ternari, abbassando lo spazio richiesto e il calcolo per rendere le reti neurali più efficienti dal punto di vista energetico.
L'articolo propone un banco di filtri ibrido a strati che quantizza solo una frazione dei filtri convoluzionali a valori ternari verso l'architettura MobileNets.
Stabiliamo un benchmark di rumore reale controllato e riveliamo diversi risultati interessanti sui dati rumorosi del mondo reale.
Questo articolo confronta 6 metodi esistenti per l'apprendimento di etichette rumorose in due impostazioni di formazione: da zero, e finetuning.
Gli autori stabiliscono un grande set di dati e un benchmark di rumore controllato del mondo reale per eseguire esperimenti controllati su dati rumorosi nell'apprendimento profondo.
Impariamo a risolvere il problema RNA Design con il reinforcement learning usando approcci di meta apprendimento e autoML.
Ha utilizzato l'ottimizzazione del gradiente di policy per generare sequenze di RNA che si ripiegano in una struttura secondaria di destinazione, ottenendo chiari miglioramenti in termini di precisione e tempo di esecuzione. 
Addestrare piccole reti batte il pruning, ma il pruning trova buone piccole reti da addestrare che sono facili da copiare.
Studiamo il problema dell'apprendimento per prevedere la diversità sottostante delle credenze presenti nei domini di apprendimento supervisionato.
Abbiamo introdotto una strategia che permette l'inpainting dei modelli su dataset di varie dimensioni
Aiuta l'inpainting dell'immagine usando GANs utilizzando un filtro di aumento comparativo e aggiungendo rumore casuale ad ogni pixel.
Troviamo prove che la minimizzazione della divergenza potrebbe non essere una caratterizzazione accurata della formazione GAN.
La presentazione mira a presentare l'evidenza empirica che la teoria della minimizzazione della divergenza è più uno strumento per comprendere il risultato dell'addestramento delle GANs che una condizione necessaria da far rispettare durante l'addestramento stesso
Questo articolo studia le GAN non saturanti e l'effetto di due approcci a gradiente penalizzato, considerando diversi esperimenti di pensiero per dimostrare le osservazioni e validarle su esperimenti di dati reali.
Un nuovo e pratico test statistico di dipendenza usando le reti neurali, con un benchmark su set di dati sintetici e reali di fMRI.
Propone una stima dell'informazione reciproca basata su reti neurali che può funzionare in modo affidabile con piccoli insiemi di dati, riducendo la complessità del campione disaccoppiando il problema dell'apprendimento della rete e il problema della stima.
Didascalia di immagini utilizzando l'incorporazione di parole bidimensionali.
LEAP combina la forza del campionamento adattivo con quella dell'apprendimento online in mini-batch e dell'apprendimento adattivo della rappresentazione per formulare una strategia rappresentativa autodidattica in un protocollo di formazione DNN end-to-end. 
Introduce un metodo per creare mini lotti per una rete di studenti utilizzando un secondo spazio di rappresentazione appreso per selezionare dinamicamente gli esempi in base alla loro "facilità e vera diversità".
Sperimenta l'accuratezza di classificazione su MNIST, FashionMNIST, e CIFAR-10 dataset per imparare una rappresentazione con la selezione di minibatch in stile di apprendimento curricolare in un framework end-to-end.
Proponiamo di costruire le macro azioni con un algoritmo genetico, che elimina la dipendenza della procedura di derivazione delle macro azioni dalle politiche passate dell'agente.
Questo articolo propone un algoritmo generico per la costruzione di macro azioni per l'apprendimento di rinforzo profondo aggiungendo una macro azione allo spazio delle azioni primitive.
Proponiamo un'estensione di LFADS in grado di dedurre i treni di spike per ricostruire le tracce di fluorescenza del calcio usando i VAE gerarchici.
Introduciamo il primo metodo di successo per addestrare la traduzione automatica neurale in modo non supervisionato, usando solo corpora monolingue
Gli autori presentano un modello di NMT non supervisionato che non richiede corpora paralleli tra le due lingue di interesse. 
Questo è un articolo sulla traduzione automatica non supervisionata che addestra un'architettura standard utilizzando le incorporazioni di parole in uno spazio di incorporamento condiviso solo con carte di parole bilingui e un encoder-decoder addestrato utilizzando dati monolingui.
Addestriamo le reti generative avversarie in modo progressivo, permettendoci di generare immagini ad alta risoluzione con alta qualità.
Introduce la crescita progressiva e una semplice funzione statistica di riepilogo minibatch senza parametri da usare nell'addestramento GAN per consentire la sintesi di immagini ad alta risoluzione.
Una CNN sferica basata su grafici che trova un interessante equilibrio di compromessi per un'ampia varietà di applicazioni.
Combina le strutture CNN esistenti basate sulla discretizzazione di una sfera come un grafico per mostrare un risultato di convergenza che è legato all'equivalenza di rotazione su una sfera.
Gli autori usano la formulazione esistente di CNN a grafo e una strategia di pooling che sfrutta le pixelature gerarchiche della sfera per imparare dalla sfera discretizzata.
Dimostriamo le relazioni di fluttuazione-dissipazione per SGD, che possono essere utilizzate per (i) impostare in modo adattivo i tassi di apprendimento e (ii) sondare le superfici di perdita.
I concetti di Paper lavorano nel formalismo a tempo discreto, usano l'equazione master ed eliminano la dipendenza da un'approssimazione localmente quadratica della funzione di perdita o da qualsiasi ipotesi gaussiana del rumore SGD. 
Gli autori derivano le relazioni di fluttuazione-dissipazione stazionarie che collegano le quantità misurabili e gli iperparametri in SGD e usano le relazioni per impostare il programma di allenamento in modo adattivo e analizzare il paesaggio della funzione di perdita.
Proponiamo un meccanismo di denoising dello stato interno di una RNN per migliorare le prestazioni di generalizzazione.
Per ambienti dettati parzialmente da processi di input esterni, deriviamo una linea di base dipendente dall'input che riduce provabilmente la varianza per i metodi di gradiente della politica e migliora le prestazioni della politica in una vasta gamma di compiti RL.
Gli autori considerano il problema dell'apprendimento in ambienti guidati dall'input, mostrano come il teorema di PG si applichi ancora per un critico consapevole dell'input, e mostrano che le linee di base dipendenti dall'input sono le migliori da usare nella congettura con quel critico.
Questo articolo introduce la nozione di linee di base dipendente dall'input nei metodi di gradiente politico in RL, e propone diversi metodi per addestrare la funzione di linea di base dipendente dall'input per aiutare a cancellare la varianza dalla perturbazione dei fattori esterni.
Aumentare lo strato superiore di una rete classificatrice con una memoria di stile le permette di essere generativa.
Questo articolo propone di addestrare una rete neurale classificatrice non solo per classificare, ma anche per ricostruire una rappresentazione del suo input, al fine di fattorizzare le informazioni di classe dall'aspetto.
L'articolo propone di addestrare un autocodificatore in modo tale che la rappresentazione dello strato intermedio consista nell'etichetta di classe dell'input e in una rappresentazione vettoriale nascosta
I modelli di routing per esempio beneficiano della diversità architettonica, ma lottano ancora per scalare a un gran numero di decisioni di routing.
Aggiunge diversità al tipo di unità architettonica disponibile per il router ad ogni decisione e scalando a reti più profonde, raggiungendo prestazioni allo stato dell'arte su Omniglot. 
Questo lavoro estende le reti di routing per utilizzare diverse architetture attraverso i moduli instradati
Presentiamo RNN per l'addestramento di modelli surrogati di PDE, in cui i vincoli di coerenza assicurano che le soluzioni siano fisicamente significative, anche quando l'addestramento utilizza domini molto più piccoli di quelli a cui viene applicato il modello addestrato.
Proponiamo l'uso di specchi ottimistici decenti per affrontare i problemi di ciclismo nell'addestramento delle GAN. Introduciamo anche l'algoritmo Optimistic Adam
Questo articolo propone l'uso della discesa a specchio ottimistica per addestrare le WGAN
L'articolo propone di usare la discesa a gradiente ottimistica per l'addestramento GAN che evita il comportamento ciclico osservato con SGD e le sue varianti e fornisce risultati promettenti nell'addestramento GAN.
Questo articolo propone una semplice modifica del gradient descent standard, sostenendo di migliorare la convergenza delle GAN e di altri problemi di ottimizzazione minimax.
Una semplice estensione della fattorizzazione matriciale generalizzata può superare gli approcci all'avanguardia per la raccomandazione.
Il lavoro presenta un quadro di fattorizzazione della matrice per imporre l'effetto dei dati storici quando si imparano le preferenze degli utenti nelle impostazioni di filtraggio collaborativo.
Un metodo che costruisce rappresentazioni di dati sequenziali e le sue dinamiche attraverso modelli generativi con un processo attivo
Combina reti neurali e distribuzioni gaussiane per creare un'architettura e un modello generativo per immagini e video che minimizza l'errore tra le immagini generate e quelle fornite.
L'articolo propone un modello di rete bayesiana, realizzato come una rete neurale, che apprende diversi dati sotto forma di un sistema dinamico lineare
Proponiamo polinomi come funzioni di attivazione.
Gli autori introducono funzioni di attivazione apprendibili che sono parametrizzate da funzioni polinomiali e mostrano risultati leggermente migliori di ReLU.
Un semplice metodo di motivazione intrinseca utilizzando l'errore del modello di dinamica in avanti nello spazio delle caratteristiche della politica.
Mostriamo che le VAE disentangled sono più robuste delle VAE vanilla agli attacchi avversari che mirano a ingannarli nella decodifica dell'input avversario verso un obiettivo scelto. Sviluppiamo poi un VAE gerarchico disentangled ancora più robusto, Seatbelt-VAE.
Gli autori propongono un nuovo modello VAE chiamato seatbelt-VAE, dimostrando di essere più robusto per gli attacchi latenti rispetto ai benchmark.
Dimostriamo che i cambiamenti di funzione nella backpropagation è equivalente a un tasso di apprendimento implicito
Un approccio di apprendimento con rinforzo al trasferimento di stile del testo
Introduce un metodo basato su RL che fa leva su un modello linguistico pre-addestrato per trasferire lo stile del testo, senza un obiettivo di dissociazione, utilizzando le generazioni di trasferimento dello stile da un altro modello.
Gli autori propongono un premio combinato composto da fluidità, contenuto e stile per il trasferimento dello stile del testo.
Mostriamo che la gerarchia semantica altamente strutturata emerge nelle rappresentazioni generative profonde come risultato per sintetizzare le scene.
La carta studia gli aspetti codificati dalle variabili latenti in ingresso nei diversi strati di StyleGAN.
L'articolo presenta un'interpretazione visivamente guidata delle attivazioni degli strati di convoluzione nel generatore di StyleGAN su layout, categoria di scena, attributi di scena e colore.
Mettiamo in comune i messaggi tra più stringhe SMILES della stessa molecola per passare le informazioni lungo tutti i percorsi attraverso il grafo molecolare, producendo rappresentazioni latenti che superano significativamente lo stato dell'arte in una varietà di compiti.
Il metodo utilizza input multipli di stringhe SMILES, la fusione delle caratteristiche in base ai caratteri attraverso queste stringhe e l'addestramento della rete attraverso target multipli di output di stringhe SMILES, creando una robusta rappresentazione latente a lunghezza fissa indipendente dalla variazione SMILES.  
Gli autori descrivono un nuovo metodo di autoencoder variazionale per le molecole che codifica le molecole come stringhe per ridurre le operazioni necessarie per condividere le informazioni tra gli atomi nella molecola.
Proponiamo un approccio semplice e generale che evita un problema di collasso di modalità in varie GAN condizionali.
L'articolo propone un termine di regolarizzazione per l'obiettivo GAN condizionale al fine di promuovere una generazione multimodale diversa e prevenire il collasso della modalità.
L'articolo propone un metodo per generare diversi output per vari framework GAN condizionali tra cui la traduzione immagine-immagine, l'image-inpainting e la predizione video, che può essere applicato a vari framework di sintesi condizionale per vari compiti. 
Dotare il modello di trasformazione di scorciatoie per lo strato di incorporazione libera la capacità del modello per imparare nuove informazioni.
Esaminiamo la relazione tra i valori di densità di probabilità e il contenuto dell'immagine nelle GAN non invertibili.
Gli autori cercano di stimare la distribuzione di probabilità dell'immagine con l'aiuto di GAN e sviluppano un'approssimazione adeguata alle PDF nello spazio latente.
Proponiamo una convoluzione spaziale rimescolata che la convoluzione regolare incorpora le informazioni dall'esterno del suo campo recettivo.
Propone la convulazione SS che utilizza informazioni al di fuori della sua RF, mostrando risultati migliori quando viene testata su più modelli CNN.
Gli autori hanno proposto una strategia di rimescolamento per gli strati di convoluzione nelle reti neurali convoluzionali.
Un metodo per modellare la distribuzione generativa di sequenze provenienti da entità connesse a grafi.
Gli autori propongono un metodo per modellare i dati sequenziali da fonti multiple interconnesse utilizzando una miscela di pool comune di HMM.
Il nostro lavoro applica il meta-apprendimento al multi-agente Reinforcement Learning per aiutare il nostro agente ad adattarsi in modo efficiente ai nuovi avversari in arrivo.
Questo articolo si concentra sull'adattamento veloce al nuovo comportamento degli altri agenti dell'ambiente utilizzando un metodo basato su MAML
L'articolo presenta un approccio all'apprendimento multi-agente basato sul quadro del meta-apprendimento agnostico per il compito di modellazione dell'avversario per la RL multi-agente.
Caratterizziamo i valori singolari della trasformazione lineare associata a uno strato convoluzionario standard 2D multicanale, permettendo il loro calcolo efficiente. 
L'articolo è dedicato al calcolo dei valori singolari degli strati convoluzionali
Deriva formule esatte per il calcolo dei valori singolari degli strati di convoluzione delle reti neurali profonde e mostra che il calcolo dei valori singolari può essere fatto molto più velocemente del calcolo dell'SVD completo della matrice di convoluzione facendo appello alle trasformazioni FFT veloci.
VariBAD apre una strada all'esplorazione approssimativa Bayes-ottimale trattabile per la RL profonda usando idee dal meta-apprendimento, dalla RL bayesiana e dall'inferenza variazionale approssimata.
Questo articolo presenta un nuovo metodo di apprendimento di rinforzo profondo che può scambiare in modo efficiente l'esplorazione e lo sfruttamento che combina meta-apprendimento, inferenza variazionale e RL bayesiana.
Mostriamo che l'apprendimento metrico può aiutare a ridurre la dimenticanza catastrofica
Questo articolo applica l'apprendimento metrico per ridurre la dimenticanza catastrofica sulle reti neurali migliorando l'espressività dello strato finale, portando a migliori risultati nell'apprendimento continuo.
Presentiamo NormCo, un modello di coerenza profonda che considera la semantica di una menzione di entità, così come la coerenza topica delle menzioni all'interno di un singolo documento per eseguire la normalizzazione delle entità delle malattie.
Utilizza un autocodificatore GRU per rappresentare il "contesto" (entità correlate di una data malattia nell'arco di una frase), risolvendo il compito BioNLP con miglioramenti significativi rispetto ai metodi più noti.
Esploriamo il ruolo dell'interazione moltiplicativa come quadro unificante per descrivere una serie di motivi architettonici di reti neurali classiche e moderne, come il gating, gli strati di attenzione, le iperreti e le convoluzioni dinamiche tra gli altri.
Presenta l'interazione moltiplicativa come caratterizzazione unificata per rappresentare i componenti di progettazione dell'architettura del modello comunemente usati, mostrando prove empiriche di prestazioni superiori in compiti come la modellazione di RL e sequenze.
L'articolo esplora diversi tipi di interazioni moltiplicative e trova modelli MI in grado di raggiungere una performance allo stato dell'arte nella modellazione del linguaggio e nei problemi di apprendimento di rinforzo.
Un'efficace struttura GAN di condizionamento del testo per la generazione di video dal testo
Questo articolo presenta un metodo basato su GAN per la generazione di video condizionati dalla descrizione del testo, con un nuovo metodo di condizionamento che genera filtri di convoluzione dal testo codificato e li usa per una convoluzione nel discriminatore.
Questo articolo propone modelli GAN condizionali per la sintesi testo-video: sviluppando filtri CNN condizionati dalle caratteristiche del testo e costruendo dataset di forme in movimento con prestazioni migliorate nella generazione di video/immagini.
SplitLBI è applicato all'apprendimento profondo per esplorare la sparsità strutturale del modello, raggiungendo prestazioni allo stato dell'arte in ImageNet-2012 e svelando un'efficace architettura di subnet.
Propone un algoritmo basato sull'ottimizzazione per trovare importanti strutture sparse di reti neurali su larga scala accoppiando l'apprendimento della matrice di peso e i vincoli di sparsità, offrendo una convergenza garantita su problemi di ottimizzazione non convessi.
Proponiamo meccanismi gated per migliorare l'ISTA appreso per la codifica sparsa, con garanzie teoriche sulla superiorità del metodo. 
Propone estensioni di LISTA che affrontano la sottostima introducendo "porte di guadagno" e includendo il momento con "porte di overshoot", mostrando tassi di convergenza migliorati.
Questo articolo si concentra sulla soluzione di problemi di codifica sparsa usando reti di tipo LISTA, proponendo una "funzione di guadagno di gating" per mitigare la debolezza dell'ipotesi "nessun falso positivo".
Presentiamo un quadro efficiente e adattivo per confrontare i classificatori di immagini per massimizzare le discrepanze tra i classificatori, al posto del confronto su set di test fissi.
Meccanismo di individuazione degli errori che confronta i classificatori di immagini campionando il loro set di test "più in disaccordo", misurando il disaccordo attraverso una distanza semantica derivata dall'ontologia WordNet.
Proponiamo una tecnica che modifica le strutture CNN per migliorare la robustezza mantenendo un'alta accuratezza del test, e solleviamo dubbi sull'adeguatezza dell'attuale definizione di esempi avversari generando esempi avversari in grado di ingannare gli umani.
Questo articolo propone una tecnica semplice per migliorare la robustezza delle reti neurali contro gli attacchi black-box.
Gli autori propongono un metodo semplice per aumentare la robustezza delle reti neurali convoluzionali contro gli esempi avversari, con risultati sorprendentemente buoni.
Proponiamo di confrontare l'apprendimento semi-supervisionato e robusto per le etichette rumorose in un contesto condiviso
Gli autori propongono una strategia basata sul mixup per l'addestramento di un modello in un ambiente formale che include i compiti di apprendimento semi-supervisionato e robusto come casi speciali.
Questo articolo dimostra sperimentalmente l'effetto benefico delle connessioni top-down nell'algoritmo Hierarchical Sparse Coding.
Questo articolo presenta uno studio che confronta le tecniche di Hierarchical Sparse Coding, mostrando che il termine top-down è vantaggioso nel ridurre l'errore predittivo e può imparare più velocemente.
Un approccio a scatola nera per spiegare le previsioni di un modello di somiglianza delle immagini.
Introduce un metodo per la spiegazione del modello di somiglianza delle immagini che identifica gli attributi che contribuiscono positivamente al punteggio di somiglianza e li accoppia con una mappa di salienza generata.
L'articolo propone un meccanismo di spiegazione che accoppia le regioni tipiche della mappa di salienza insieme agli attributi per le reti neurali profonde di similarità.
Come dovreste valutare gli attacchi avversari su seq2seq
Gli autori studiano modi di generare esempi avversari, mostrando che l'addestramento avversario con l'attacco più coerente con i criteri di conservazione del significato introdotti risulta in una migliore robustezza a questo tipo di attacco senza degradazione nell'impostazione non avversaria.
L'articolo riguarda le perturbazioni adversariali che preservano il significato nel contesto dei modelli Seq2Seq
Una tecnica di normalizzazione alternativa alla normalizzazione in batch
Introduce una tecnica di normalizzazione, che normalizza i pesi degli strati convoluzionali. 
Questo manoscritto introduce una nuova trasformazione per livelli, EquiNorm, per migliorare la normalizzazione in batch che non modifica gli ingressi ai livelli ma piuttosto i pesi dei livelli.
Rappresentare ogni entità come una distribuzione di probabilità su contesti inseriti in uno spazio terreno.
Propone di costruire embeddings di parole da un istogramma su parole di contesto, invece che come vettori di punti, che permette di misurare le distanze tra due parole in termini di trasporto ottimale tra gli istogrammi attraverso un metodo che aumenta la rappresentazione di un'entità da "punto in uno spazio vettoriale" standard a un istogramma con bidoni situati in alcuni punti di quello spazio vettoriale. 
Piccole perturbazioni avversarie dovrebbero essere attese dati i tassi di errore osservati dei modelli al di fuori della distribuzione naturale dei dati.
Questo articolo propone una visione alternativa per gli esempi avversari in spazi ad alta dimensione, considerando il "tasso di errore" in una distribuzione gaussiana centrata su ogni punto di prova.
Studia come l'apprendimento auto-supervisionato e la distillazione della conoscenza interagiscono nel contesto della costruzione di modelli compatti.
Studia l'addestramento di modelli linguistici compatti pre-addestrati attraverso la distillazione e mostra che l'uso di un insegnante per distillare un modello compatto di studente funziona meglio del pre-addestramento diretto del modello.
Questa presentazione mostra che il pre-addestramento di uno studente direttamente sulla modellazione del linguaggio mascherato è meglio della distillazione, e il meglio è combinare entrambi e distillare da quel modello di studente pre-addestrato.
Introduciamo lo schema di compressione universale delle reti neurali profonde, che è applicabile universalmente per la compressione di qualsiasi modello e può funzionare in modo quasi ottimale indipendentemente dalla loro distribuzione del peso.
Introduce una pipeline per la compressione di rete che è simile alla compressione profonda e usa la quantizzazione a reticolo randomizzata invece della classica quantizzazione vettoriale, e usa la codifica universale delle fonti (bzip2) invece della codifica Huffman.
Questo articolo cerca di affrontare preliminarmente il disentanglement teoricamente nella situazione idealistica e praticamente attraverso la prospettiva di modellazione del rumore nel caso realistico.
Studia l'importanza della modellazione del rumore nella VAE gaussiana e propone di addestrare il rumore usando il metodo Empirical-Bayes.
Modificare il modo in cui i fattori di rumore sono trattati nello sviluppo dei modelli VAE
Studiamo la regolarizzazione del decadimento del peso per diversi ottimizzatori e identifichiamo tre meccanismi distinti attraverso i quali il decadimento del peso migliora la generalizzazione.
Discute l'effetto del decadimento dei pesi sull'addestramento dei modelli di rete profonda con e senza normalizzazione dei lotti e quando si usano metodi di ottimizzazione del primo/secondo ordine e ipotizza che un tasso di apprendimento più grande abbia un effetto di regolarizzazione.
Il primo set di dati di adattamento del dominio liberamente disponibile per il rilevamento di eventi sonori.
Stimatore di informazioni reciproche basato sulla meccanica statistica non estensiva
Questo articolo cerca di stabilire nuovi limiti inferiori variazionali per l'informazione reciproca introducendo il parametro q e definendo q-algebra, mostrando che i limiti inferiori hanno una varianza minore e raggiungono valori elevati.
Mostriamo che la discesa stocastica del gradiente converge a un ottimo globale per WGAN con una rete di generatori a uno strato.
Tenta di dimostrare che lo Stochastic Gradient Descent-Ascent potrebbe convergere ad una soluzione globale per il problema min-max di WGAN.
Mostriamo empiricamente che l'addestramento avversario è efficace per rimuovere le perturbazioni universali, rende gli esempi avversari meno robusti alle trasformazioni dell'immagine, e li lascia rilevabili per un approccio di rilevamento.
Analizza l'addestramento avversario e il suo effetto sugli esempi avversari universali così come sugli esempi avversari standard (iterazione di base) e come l'addestramento avversario influisce sul rilevamento. 
Gli autori mostrano che l'addestramento contraddittorio è efficace nel proteggere contro la perturbazione "condivisa", in particolare contro la perturbazione universale, ma meno efficace per proteggere contro le perturbazioni singolari.
Introduciamo tecniche per addestrare una singola rete una volta per tutte che si adatta a molte piattaforme hardware.
Il metodo si traduce in una rete da cui si possono estrarre sottoreti per vari vincoli di risorse (latenza, memoria) che funzionano bene senza bisogno di riqualificazione.
Questo articolo cerca di affrontare il problema della ricerca delle migliori architetture per scenari di distribuzione con vincoli di risorse specializzate con un metodo NAS basato sulla previsione.
Proporre un approccio per il boosting dei modelli generativi mediante modelli a cascata di variabili nascoste
Questo articolo ha proposto un nuovo approccio di boosting in cascata per il boosting dei modelli generativi che permette ad ogni meta-modello di essere addestrato separatamente e avidamente.
Abbiamo analizzato la struttura della frase in ELMo e nei modelli di inclusione contestuale correlati. Troviamo che i modelli esistenti codificano efficientemente la sintassi e mostrano prove di dipendenze a lungo raggio, ma offrono solo piccoli miglioramenti sui compiti semantici.
Propone il metodo "edge probing" e si concentra sulla relazione tra gli span piuttosto che sulle singole parole, permettendo agli autori di guardare alla costituzione sintattica, alle dipendenze, alle etichette delle entità e all'etichettatura dei ruoli semantici.
Fornisce nuove intuizioni su ciÃ² che viene catturato dalle incorporazioni di parole contestualizzate compilando una serie di compiti di "edge probing". 
Introduciamo DPFRL, una struttura per l'apprendimento di rinforzo sotto osservazioni parziali e complesse con un filtro discriminativo a particelle completamente differenziabile
Introduce idee per l'addestramento di agenti DLR con variabili di stato latenti, modellate come una distribuzione di credenze, in modo che possano gestire ambienti parzialmente osservati.
Questo articolo introduce un metodo di principio per POMDP RL: Discriminative Particle Filter Reinforcement Learning che permette di ragionare con osservazioni parziali su più passi temporali, raggiungendo lo stato dell'arte sui benchmark.
Gli obiettivi di Monte Carlo sono analizzati utilizzando l'inferenza variazionale delle variabili ausiliarie, ottenendo una nuova analisi di CPC e NCE così come un nuovo modello generativo.
Propone una visione diversa sul miglioramento dei limiti variazionali con modelli ausiliari di variabili latenti ed esplora l'uso di questi modelli nel modello generativo.
Miglioriamo l'esecuzione di tutti gli algoritmi di discesa del gradiente esistenti.
Gli autori propongono di campionare i gradienti stocastici da una funzione monotona proporzionale alla grandezza del gradiente utilizzando LSH. 
Considera SGD su un obiettivo della forma di una somma su esempi di una perdita quadratica.
Le limitazioni dell'attuale IA sono generalmente riconosciute, ma meno persone sono consapevoli che comprendiamo abbastanza del cervello per offrire immediatamente nuove formulazioni di IA.
Usiamo la risposta alle domande per valutare quanta conoscenza dell'ambiente può essere appresa dagli agenti attraverso la predizione auto-supervisionata.
Propone l'AQ come uno strumento per investigare ciò che gli agenti imparano nel mondo, sostenendo che questo è un metodo intuitivo per gli umani che permette una complessità arbitraria.
Gli autori propongono un quadro per valutare le rappresentazioni costruite dai modelli predittivi che contengono informazioni sufficienti per rispondere alle domande sull'ambiente su cui sono addestrati, mostrando che quelli di SimCore contenevano informazioni sufficienti per il LSTM per rispondere accuratamente alle domande.
Sviluppiamo un nuovo metodo per la classificazione sbilanciata usando esempi contraddittori
Propone un nuovo obiettivo di ottimizzazione che genera campioni sintetici sovracampionando le classi di maggioranza invece di quelle di minoranza, risolvendo il problema dell'overfitting delle classi di minoranza.
Gli autori propongono di affrontare la classificazione dello squilibrio usando metodi di ricampionamento, mostrando che gli esempi avversari nella classe minoritaria aiuterebbero ad addestrare un nuovo modello che generalizza meglio.
Un'interessante applicazione della CNN negli esperimenti di fisica della materia condensata morbida.
Gli autori dimostrano che un approccio di deep learning offre un miglioramento sia all'accuratezza dell'identificazione che al tasso di identificazione dei difetti dei cristalli liquidi nematici.
Applicare un modello neurale ben noto (YOLO) per rilevare le scatole di delimitazione degli oggetti nelle immagini.
Un'analisi degli effetti della compositività e della località sull'apprendimento della rappresentazione per l'apprendimento a zero colpi.
Propone un quadro di valutazione per ZSL in cui il modello non può essere preaddestrato e, invece, i parametri del modello sono inizializzati in modo casuale per una migliore comprensione di ciò che accade in ZSL.
L'errore avversario ha una forma simile alla legge di potenza per tutti i set di dati e i modelli studiati, e l'architettura conta.
Presentiamo una formulazione della curiosità come un problema di apprendimento della rappresentazione visiva e mostriamo che permette buone rappresentazioni visive negli agenti.
Questo articolo formula l'addestramento RL basato sulla curiosità come apprendimento di un modello di rappresentazione visiva, sostenendo che concentrandosi su una migliore LR e massimizzando la perdita del modello per le scene nuove si otterrà una migliore performance complessiva.
Da una scansione RGB-D incompleta di una scena, miriamo a rilevare le istanze individuali degli oggetti che compongono la scena e a dedurre la loro geometria completa.
Propone una struttura CNN 3D end-to-end che combina caratteristiche di colore e caratteristiche 3D per prevedere la struttura 3D mancante di una scena dalle scansioni RGB-D.
Gli autori propongono una nuova rete di convoluzione 3D end-to-end che predice il completamento dell'istanza semantica 3D come scatole di delimitazione dell'oggetto, etichette di classe e geometria completa dell'oggetto.
XGAN è un modello non supervisionato per la traduzione immagine-immagine a livello di caratteristiche applicato a problemi di trasferimento di stile semantico come il compito faccia-a-cartoon, per il quale introduciamo un nuovo dataset.
Questo articolo propone un nuovo modello basato su GAN per la traduzione da immagine a immagine non accoppiata simile a DTN
I lavoratori inviano i segnali del gradiente al server, e l'aggiornamento viene deciso a maggioranza. Mostriamo che questo algoritmo è convergente, efficiente nella comunicazione e tollerante agli errori, sia in teoria che in pratica.
Presenta un'implementazione distribuita di signSGD con voto di maggioranza come aggregazione.
Correggiamo la variazione di disturbo per le incorporazioni di immagini in diversi domini, conservando solo le informazioni rilevanti.
Discute un metodo per regolare le incorporazioni di immagini al fine di separare la variazione tecnica dal segnale biologico.
Gli autori presentano un metodo per rimuovere l'informazione specifica del dominio preservando l'informazione biologica rilevante, addestrando una rete che minimizza la distanza di Wasserstein tra le distrbuzioni.
Uno stimatore di informazioni reciproche scalabile per dimensione e dimensioni del campione.
La nuova combinazione di rinforzo e apprendimento supervisionato, diminuisce drasticamente il numero di campioni richiesti per l'addestramento su video
Questo articolo propone di sfruttare i dati controllati etichettati per accelerare l'apprendimento basato sul rinforzo di una politica di controllo
Apprendimento veloce attraverso la memoria episodica verificata da un quadro biologicamente plausibile per la corteccia prefrontale-basale ganglia-hippocampus (PFC-BG) circuito
In questo lavoro, indichiamo una nuova connessione tra l'espressività delle DNN e il teorema di Sharkovsky dei sistemi dinamici, che ci permette di caratterizzare i trade-off profondità-larghezza delle reti ReLU 
Mostra come la potenza espressiva di NN dipende dalla sua profondità e larghezza, approfondendo la comprensione del beneficio delle reti profonde per rappresentare certe classi di funzioni.
Gli autori derivano condizioni di tradeoff profondità-larghezza per quando le reti relu sono in grado di rappresentare funzioni periodiche usando l'analisi dei sistemi dinamici.
Studiamo l'addestramento consapevole della quantizzazione negli spotter di parole chiave quantizzati a bit molto bassi per ridurre il costo dello spotting di parole chiave sul dispositivo.
Questa presentazione propone una combinazione di decomposizione low-rank e un approccio di quanitizzazione per comprimere i modelli DNN per l'individuazione delle parole chiave.
Un nuovo quadro di elaborazione dei segnali grafici per quantificare gli effetti delle perturbazioni sperimentali nei dati biomedici delle singole cellule.
Questo articolo introduce diversi metodi per elaborare i risultati sperimentali sulle cellule biologiche e propone un algoritmo MELD che mappa le assegnazioni di gruppo hard in assegnazioni soft, permettendo di raggruppare i gruppi di cellule rilevanti.
Proponiamo una classe di modelli utente basati sull'utilizzo di processi gaussiani applicati a uno spazio trasformato definito da regole decisionali
Proponiamo un algoritmo di ottimizzazione bayesiana Bayes-optimal per la sintonizzazione degli iperparametri sfruttando approssimazioni economiche.
Studia l'ottimizzazione degli iperparametri tramite l'ottimizzazione bayesiana, usando il quadro del Knowledge Gradient e permettendo all'ottimizzatore bayesiano di sintonizzare la fedeltà rispetto al costo.
Verifichiamo in modo efficiente la robustezza dei modelli neurali profondi con oltre 100.000 ReLU, certificando più campioni dello stato dell'arte e trovando più esempi avversari di un forte attacco di primo ordine.
Esegue un attento studio degli approcci di programmazione lineare integrale mista per verificare la robustezza delle reti neurali alle perturbazioni avversarie e propone tre miglioramenti alle formulazioni MILP della verifica delle reti neurali.
Un insieme di metodi per ottenere la stima dell'incertezza di qualsiasi modello dato senza ri-progettazione, ri-addestramento o messa a punto.
Descrive diversi approcci per misurare l'incertezza nelle reti neurali arbitrarie quando c'è un'assenza di distorsione durante la formazione.
Operazione di ordine superiore proposta per l'apprendimento del contesto
Propone un nuovo blocco di convoluzione 3D che convolge l'input video con il suo contesto, basato sull'ipotesi che il contesto rilevante sia presente intorno all'oggetto dell'immagine.
I modelli basati sulla coerenza per l'apprendimento semi-supervisionato non convergono verso un singolo punto, ma continuano ad esplorare un insieme vario di soluzioni plausibili sul perimetro di una regione piatta. La media dei pesi aiuta a migliorare le prestazioni di generalizzazione.
L'articolo propone di applicare lo Stochastic Weight Averaging al contesto dell'apprendimento semi-supervisionato, sostenendo che i modelli MT/Pi semi-supervisionati sono particolarmente adatti a SWA e propone SWA veloce per accelerare l'addestramento.
Abbiamo convertito con successo un popolare rilevatore RPN in un tracker ben performante dal punto di vista della funzione di perdita.
Un'architettura neurale per segnare e classificare i candidati alla riparazione del programma per eseguire la riparazione semantica del programma in modo statico senza accesso ai test di unità.
Presenta un'architettura di rete neurale composta dalle parti share, specialize e compete per riparare il codice in quattro casi.
È possibile co-progettare l'accuratezza, la robustezza e l'efficienza dei modelli per raggiungere il loro triplo successo? Sì!
Sfrutta l'input-adaptive multiple early-exits per il campo dell'attacco e della difesa avversaria, riducendo la complessità media dell'inferenza senza entrare in conflitto con l'ipotesi di maggiore capacità.
Mostriamo che le singole unità nelle rappresentazioni CNN apprese in compiti NLP sono selettivamente reattive a specifici concetti del linguaggio naturale.
Utilizza le unità grammaticali del linguaggio naturale che conservano i significati per dimostrare che le unità delle CNN profonde apprese in compiti NLP potrebbero agire come un rilevatore di concetti del linguaggio naturale.
Si tratta di un documento prevalentemente teorico che descrive le sfide nel separare i fattori di variazione, utilizzando autoencoder e GAN.
Questo articolo considera la separazione dei fattori di variazione nelle immagini, mostra che in generale, senza ulteriori presupposti, non si possono distinguere due diversi fattori di variazione, e suggerisce una nuova architettura AE+GAN per cercare di distinguere i fattori di variazione.
Questo articolo studia le sfide della separazione dei fattori indipendenti di variazione sotto dati debolmente etichettati e introduce il termine ambiguità di riferimento per la mappatura dei punti dati.
imparare a classificare con diverse incorporazioni e attenzioni
Propone di usare l'attenzione per combinare rappresentazioni di input multipli sia per la query che per i risultati della ricerca nel compito di imparare a classificare.
Abbiamo sviluppato un algoritmo che prende come input registrazioni di attività neurale e restituisce cluster di neuroni per tipo di cellule e modelli di attività neurale vincolati da questi cluster.
Supervisioniamo le reti neurali a grafo per imitare le uscite intermedie e a gradini degli algoritmi a grafo classici, recuperando intuizioni molto favorevoli.
Suggerisce l'addestramento di reti neurali per imitare gli algoritmi del grafico imparando primitive e subroutine piuttosto che l'output finale.
Descriviamo un'architettura per generare diverse ipotesi di obiettivi intermedi durante i compiti di manipolazione robotica.
Valuta la qualità di un modello predittivo generativo proposto per generare piani di esecuzione dei robot.
Questo articolo propone un metodo per imparare una funzione di transizione di alto livello utile per la pianificazione dei compiti.
Questo articolo fornisce un'analisi innovativa degli algoritmi di gradiente adattivi per risolvere problemi di min-max non concavi come GAN, e spiega il motivo per cui i metodi di gradiente adattivi superano le loro controparti non adattive attraverso studi empirici.
Sviluppa algoritmi per la soluzione di disuguaglianze variazionali in ambiente stocastico, proponendo una variazione del metodo dell'extragradiente.
Impariamo traiettorie sohpisticate di un oggetto puramente da pixel con un set di dati video giocattolo utilizzando una struttura VAE con un processo Gaussiano precedente.
Indaghiamo le basi neurali del richiamo dei sogni usando una rete neurale convoluzionale e tecniche di visualizzazione delle caratteristiche, come tSNE e la backpropagation guidata.
Questo articolo propone una nuova formulazione e un nuovo protocollo di comunicazione per problemi di controllo multi-agente in rete
Riguarda le N-MARL dove gli agenti aggiornano la loro politica basandosi solo sui messaggi dei nodi vicini, mostrando che l'introduzione di un fattore di sconto spaziale stabilizza l'apprendimento.
Mean field VB usa il doppio dei parametri; leghiamo i parametri di varianza in mean field VB senza alcuna perdita in ELBO, guadagnando velocità e gradienti di varianza più bassi.
Sfruttiamo efficacemente alcune parole chiave come supervisione debole per addestrare le reti neurali per l'estrazione degli aspetti.
Discute una variante di distillazione della conoscenza che usa un "insegnante" basato su un classificatore di bag-of-words con parole seme e uno "studente" che è una rete neurale basata sull'embedding.
Le connessioni di feedback orizzontali e top-down sono responsabili delle strategie complementari di raggruppamento percettivo nei sistemi di visione biologica e ricorrente.
Utilizzando le reti neurali come modello computazionale del cervello, esamina l'efficienza di diverse strategie per risolvere due sfide visive.
Introduciamo GAN-TTS, una Generative Adversarial Network per Text-to-Speech, che raggiunge il Mean Opinion Score (MOS) 4.2.
Risolve la sfida delle GAN nella sintesi delle forme d'onda grezze e comincia a colmare il divario di prestazioni esistente tra i modelli autoregressivi e le GAN per gli audio grezzi.
proponiamo un algoritmo di apprendimento per potare la rete applicando penalità di sparsità della struttura
Questo articolo introduce un approccio al pruning durante l'addestramento di una rete usando lasso e penalità LBI divise
Introduciamo l'apprendimento continuo non supervisionato (UCL) e un'architettura neuro-ispirata che risolve il problema UCL.
Propone l'uso di gerarchie di moduli STAM per risolvere il problema UCL, fornendo la prova che le rappresentazioni che i moduli imparano sono adatte alla classificazione di pochi colpi.
Nuovo metodo di estrazione del segnale nel dominio di Fourier
Contribuisce a una versione convoluzionaria a valori complessi della Feature-Wise Linear Modulation che permette l'ottimizzazione dei parametri e progetta una perdita che tiene conto di magnitudine e fase.
Presentiamo una nuova struttura per imparare la rappresentazione separata del contenuto e dello stile in modo completamente non supervisionato. 
Proporre un modello basato sul framework autoencoder per distinguere la rappresentazione di un oggetto, i risultati mostrano che il modello può produrre rappresentazioni che catturano il contenuto e lo stile.
Sviluppiamo una strategia di stima CATE che sfrutta alcune delle intriganti proprietà delle reti neurali. 
Mostra i miglioramenti di X-learner modellando la funzione di risposta al trattamento, la funzione di risposta al controllo e la mappatura dall'effetto di trattamento imputato all'effetto di trattamento medio condizionato, come reti neurali.
Gli autori propongono l'Y-learner per stimare l'effetto di trattamento medio condizionato (CATE), che aggiorna simultaneamente i parametri delle funzioni di risultato e lo stimatore CATE.
Esecuzione del firmware indipendente dal dispositivo
Un'architettura per dati tabulari, che emula i rami degli alberi decisionali e usa una densa connettività residua 
Questo articolo propone la foresta neurale profonda, un algoritmo che si rivolge ai dati tabulari e integra i punti forti del gradient boosting degli alberi di decisione.
Una nuova architettura di rete neurale che imita il funzionamento delle foreste decisionali per affrontare il problema generale dell'addestramento di modelli profondi per i dati tabulari e mostrare un'efficacia alla pari con GBDT.
YellowFin è un ottimizzatore basato su SGD con adattabilità sia al momento che al tasso di apprendimento.
Propone un metodo per sintonizzare automaticamente il parametro momentum nei metodi SGD momentum, che raggiunge risultati migliori e una velocità di convergenza veloce rispetto all'algoritmo Adam di ultima generazione.
Attacchi avversari sullo spazio latente degli autocodificatori variazionali per cambiare il significato semantico degli input
Questo articolo riguarda la sicurezza e l'apprendimento automatico e propone un attacco man-in-middle che altera la codifica VAE dei dati di input in modo che l'output decodificato sia classificato male.
Uno studio empirico che esamina l'efficacia di diverse combinazioni encoder-decoder per il compito di analizzare le dipendenze
Analizza empiricamente vari codificatori, decodificatori e le loro dipendenze per l'analisi delle dipendenze basata su grafici.
Insegnante che allena i meta-apprendenti come gli umani
Introduciamo un approccio di embedding space per vincolare la distribuzione di probabilità dell'output delle reti neurali.
Questo articolo introduce un metodo per eseguire l'apprendimento semi-supervisionato con reti neurali profonde, e il modello raggiunge una precisione relativamente alta, data una piccola dimensione di formazione.
Questo articolo incorpora la distribuzione delle etichette nell'apprendimento del modello quando è disponibile un numero limitato di istanze di formazione, e propone due tecniche per gestire il problema della distribuzione delle etichette di uscita che è erroneamente distorta.
Introduciamo un nuovo tipo di rappresentazione profonda contestualizzata delle parole che migliora significativamente lo stato dell'arte per una serie di compiti NLP impegnativi.
Questo lavoro introduce una nuova funzione di perdita per l'addestramento robusto di DNN di localizzazione temporale in presenza di etichette non allineate.
Una nuova perdita per l'addestramento di modelli che prevedono dove si verificano gli eventi in una sequenza di addestramento con etichette rumorose, confrontando l'etichetta lisciata e la sequenza di predizione.
Introduciamo la nozione di decomposizioni tensoriali miste e la usiamo per dimostrare che l'interconnessione di reti convoluzionali dilatate aumenta la loro potenza espressiva.
Questo articolo convalida teoricamente che l'interconnessione di reti con diverse dilatazioni può portare all'efficienza espressiva utilizzando la decomposizione tensoriale mista.
Gli autori studiano le reti di convoluzione dilatate e mostrano che l'intreccio di due reti di convoluzione dilatate A e B in varie fasi è più efficiente dal punto di vista espressivo che non intrecciarle.
Mostra che l'assunzione strutturale della WaveNet di un singolo albero binario perfetto ostacola le sue prestazioni e che le architetture simili alla WaveNet con strutture ad albero miste più complesse funzionano meglio.
l'apprendimento multi-task funziona 
Questo articolo presenta una rete neurale multi-task per la classificazione su dataset simili a MNIST
Forniamo una rivisitazione basata sull'ottimizzazione della nozione di esempi avversari, e sviluppiamo metodi che producono modelli che sono adversarialmente robusti contro una vasta gamma di avversari.
Studia una formulazione minimax dell'apprendimento delle reti profonde per aumentare la loro robustezza, utilizzando la discesa del gradiente proiettata come principale avversario. 
Questo articolo propone di cercare di rendere le reti neurali resistenti alle perdite avversarie attraverso il quadro dei problemi del punto di sella. 
Molti insiemi di dati di classificazione dei grafici hanno duplicati, sollevando così domande sulle capacità di generalizzazione e sull'equo confronto dei modelli. 
Gli autori discutono il bias di isomorfismo nei dataset di grafi, l'effetto di overfitting nell'apprendimento delle reti quando le caratteristiche di isomorfismo dei grafi sono incorporate nel modello, teoricamente analogo agli effetti di perdita dei dati.
Introduciamo una nozione di funzioni di valore estrapolate in modo conservativo, che portano provatamente a politiche che possono auto-correggersi per rimanere vicine agli stati dimostrativi, e le impariamo con una nuova tecnica di campionamento negativo.
Un algoritmo chiamato iterazione del valore con campionamento negativo per affrontare il problema dello spostamento delle covariate nell'apprendimento per imitazione.
I modelli di mondo strutturati addestrati in modo contrastivo (C-SWMs) imparano rappresentazioni di stato orientate agli oggetti e un modello relazionale di un ambiente a partire da input di pixel grezzi.
Gli autori superano il problema dell'utilizzo di perdite basate sui pixel nella costruzione e nell'apprendimento di modelli di mondo strutturati utilizzando uno spazio latente contrastivo.
Metodi non supervisionati per trovare, analizzare e controllare i neuroni importanti nella NMT
Questo lavoro propone di trovare neuroni "significativi" nei modelli di traduzione automatica neurale classificando in base alla correlazione tra coppie di modelli, diverse epoche o diversi set di dati, e propone un meccanismo di controllo dei modelli.
Presentiamo una softmax doppiamente sparsa, la miscela sparsa di esperti sparsi, per migliorare l'efficienza dell'inferenza softmax attraverso lo sfruttamento della gerarchia a due livelli di sovrapposizione. 
L'articolo propone la nuova implementazione dell'algoritmo Softmax con due livelli gerarchici di sparsità che accelera l'operazione nella modellazione del linguaggio.
Questo articolo presenta prove empiriche che supportano la scoperta di un indicatore di generalizzazione: l'evoluzione durante l'addestramento della distanza coseno tra il vettore di peso di ogni strato e la sua inizializzazione.
I modelli di codice sorgente che combinano caratteristiche globali e strutturali imparano rappresentazioni più potenti dei programmi.
Un nuovo metodo per modellare il codice sorgente per il compito di riparazione dei bug usando un modello a sandwich come [RNN GNN RNN] che migliora significativamente la localizzazione e la precisione della riparazione.
Incremental-RNNs risolve il problema del gradiente esplosivo/vaneggiante aggiornando i vettori di stato in base alla differenza tra lo stato precedente e quello previsto da una ODE.
Gli autori affrontano il problema della propagazione del segnale nelle reti neurali ricorrenti costruendo un sistema di attrazione per la transizione del segnale e controllando se converge ad un equilibrio. 
Forniamo prove contro le affermazioni classiche sul tradeoff bias-varianza e proponiamo una nuova decomposizione per la varianza.
Abbiamo proposto un nuovo quadro di classificazione delle immagini di deep learning che può sia classificare accuratamente le immagini che proteggere la privacy degli utenti.
Questo articolo propone un quadro che conserva le informazioni private nell'immagine e non compromette l'usabilità dell'immagine.
Questo lavoro attuale suggerisce l'uso di reti avversarie per offuscare le immagini e quindi permettere di raccoglierle senza preoccupazioni di privacy per utilizzarle per l'addestramento di modelli di apprendimento automatico.
un modello 2vec per i grafi delle transazioni di criptovaluta
L'articolo propone di usare un autoencoder, networkX e node2Vec per prevedere se un indirizzo Bitcoin diventerà vuoto dopo un anno, ma i risultati sono peggiori di una linea di base esistente.
Prova di convergenza del metodo stocastico dei sub-gradienti e variazioni su problemi minimax convessi-concavi
Un'analisi del subgradiente stocastico simultaneo, del gradiente simultaneo con ottimismo e del gradiente simultaneo con ancoraggio nel contesto dei giochi concavi convessi minmax.
Questo articolo analizza la dinamica della discesa stocastica del gradiente quando applicata a giochi convessi-concavi, così come la GD con ottimismo e un nuovo algoritmo GD ancorato che converge sotto ipotesi più deboli di SGD o SGD con ottimismo.
Proponiamo un quadro algoritmico per programmare costellazioni di piccoli veicoli spaziali con capacità di ri-orientamento 3-DOF, collegati in rete con collegamenti inter-sat.
Questo articolo propone un modulo di comunicazione per ottimizzare il programma di comunicazione per il problema delle costellazioni di veicoli spaziali, e confronta l'algoritmo in impostazioni distribuite e centralizzate.
Abbiamo proposto un nuovo algoritmo di campionamento d'importanza compresso kernelizzato.
Studiamo la struttura della regressione ridge in un quadro asintotico ad alta dimensione, e otteniamo intuizioni sulla convalida incrociata e sullo sketch.
Uno studio teorico della regressione ridge sfruttando una nuova caratterizzazione asintotica dello stimatore di regressione ridge.
Analizziamo il panorama delle perdite delle reti neurali con attenzione e spieghiamo perché l'attenzione è utile nell'addestramento delle reti neurali per ottenere buone prestazioni.
Questo articolo dimostra dal punto di vista teorico che le reti di attenzione possono generalizzare meglio delle basi di non-attenzione per l'attenzione fissa (monostrato e multistrato) e l'autoattenzione nell'impostazione a singolo strato.
Modello a miscela per la dissociazione neurale
Abbiamo sviluppato stime robuste dell'informazione reciproca per le DNN e le abbiamo usate per osservare la compressione nelle reti con funzioni di attivazione non saturanti
Questo articolo ha studiato la credenza popolare che le reti neurali profonde fanno la compressione dell'informazione per compiti supervisionati
Questo articolo propone un metodo per la stima dell'informazione reciproca per reti con funzioni di attivazione non limitate e l'uso della regolarizzazione L2 per indurre una maggiore compressione.
Presentiamo il TimbreTron, una pipeline per il trasferimento del timbro di alta qualità su forme d'onda musicali utilizzando il trasferimento in stile dominio CQT.
Un metodo per convertire le registrazioni di uno specifico strumento musicale in un altro applicando CycleGAN, sviluppato per il trasferimento dello stile delle immagini, per trasferire gli spettrogrammi.
Gli autori usano tecniche/strumenti multipli per permettere il trasferimento neurale del timbro (conversione della musica da uno strumento all'altro) senza esempi di allenamento accoppiati. 
Descrive un modello per il trasferimento del timbro musicale con i risultati che indicano che il sistema proposto è efficace per il trasferimento dell'altezza e del tempo, così come l'adattamento del timbro.
L'articolo presenta Deep Rewiring, un algoritmo che può essere utilizzato per addestrare reti neurali profonde quando la connettività della rete è fortemente limitata durante la formazione.
Un approccio per implementare l'apprendimento profondo direttamente su grafi sparsamente connessi, permettendo alle reti di essere addestrate in modo efficiente online e per un apprendimento veloce e flessibile.
Gli autori forniscono un semplice algoritmo in grado di allenarsi con una memoria limitata
I metodi di potatura esistenti falliscono quando applicati alle GAN che affrontano compiti complessi, quindi presentiamo un metodo semplice e robusto per potare i generatori che funziona bene per un'ampia varietà di reti e compiti.
Gli autori propongono una modifica al metodo classico di distillazione per il compito di comprimere una rete per affrontare il fallimento delle soluzioni precedenti quando applicate alle reti generative avversarie.
troviamo che il 99,9% dello scambio di gradiente in SGD distribuito è ridondante; riduciamo la larghezza di banda di comunicazione di due ordini di grandezza senza perdere precisione. 
Questo documento propone un ulteriore miglioramento rispetto al gradient dropping per migliorare l'efficienza della comunicazione
Proponiamo la rete Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) che condiziona il processo di traduzione su un'immagine esemplare nel dominio di destinazione.
Discute un nucleo mancante e la necessità di modelli di traduzione I2I.
L'articolo esplora l'idea che un'immagine ha due componenti e applica un modello di attenzione in cui le maschere di caratteristiche che guidano il processo di traduzione non richiedono etichette semantiche
Imporre una struttura a grafo sugli strati delle reti neurali per una migliore interpretabilità visiva.
Un nuovo regolatore per imporre la struttura del grafico sugli strati nascosti di una rete neurale per migliorare l'interpretabilità delle rappresentazioni nascoste.
Evidenzia il contributo del regolatore spettrale del grafico all'interpretabilità delle reti neurali.
Mostriamo che i modelli Energy-Based, quando addestrati sul residuo di un modello linguistico auto-regressivo, possono essere utilizzati in modo efficace ed efficiente per generare testo. 
Un modello proposto basato sull'energia residua (EBM) per la generazione di testo che opera a livello di frase, e può quindi sfruttare il BERT, e raggiunge una perplessità inferiore ed è preferito dalla valutazione umana.
studio sistematico di modelli di riconoscimento di immagini su larga scala basati sulla cache, concentrandosi in particolare sulle loro proprietà di robustezza
Questo articolo ha proposto di usare la cache di memoria per migliorare la robustezza contro gli esempi di immagini avversarie, e ha concluso che l'uso di una grande cache continua non è superiore all'attenzione dura.
L'articolo descrive un quadro flessibile per costruire CNN che sono equivarianti a una grande classe di gruppi di trasformazioni.
Un quadro per costruire CNN di gruppo con un gruppo di Lie arbitrario G, che mostra superiorità rispetto a una CNN nella classificazione dei tumori e nella localizzazione dei punti di riferimento. 
Un benchmark di nove schemi rappresentativi di pooling globale rivela alcuni risultati interessanti.
Per compiti di classificazione a grana fine, questo articolo ha convalidato che maxpooling incoraggia mappe di caratteristiche più spartane e supera avgpooling. 
L'autosupervisione migliora il riconoscimento di pochi scatti su set di dati piccoli e impegnativi senza fare affidamento su dati extra; i dati extra aiutano solo quando provengono dallo stesso dominio o da uno simile.
Uno studio empirico di diversi metodi di apprendimento auto-supervisionato (SSL), mostrando che SSL aiuta di più quando il dataset è più difficile, che il dominio conta per l'addestramento, e un metodo per scegliere campioni da un dataset non etichettato. 
Creiamo modelli astratti di ambienti dall'esperienza e li usiamo per imparare più velocemente nuovi compiti.
Una metodologia che utilizza l'idea degli omomorfismi MDP per trasformare un MDP complesso con uno spazio di stato continuo in uno più semplice.
Espandiamo la Network Dissection per includere l'interpretazione delle azioni ed esaminiamo i percorsi delle caratteristiche interpretabili per capire la gerarchia concettuale usata per classificare un'azione.
Proponiamo un nuovo modello per rappresentare le note e le loro proprietà, che può migliorare la generazione automatica della melodia.
Questo articolo propone un modello generativo di melodia simbolica (MIDI) nella musica popolare occidentale che codifica congiuntamente i simboli delle note con informazioni di tempo e durata per formare "parole" musicali.
L'articolo propone di facilitare la generazione di melodia rappresentando le note come "parole", rappresentando tutte le proprietà della nota e permettendo così la generazione di "frasi" musicali.
Un metodo che fa crescere automaticamente gli strati nelle reti neurali per scoprire la profondità ottimale.
Un quadro per interlacciare l'addestramento di una rete meno profonda e l'aggiunta di nuovi strati che fornisce intuizioni sul paradigma delle "reti in crescita".
Esplorazione dell'apprendimento della rappresentazione nel dominio per set di dati di telerilevamento.
Questo articolo ha fornito diversi set di dati standardizzati per il telerilevamento e ha dimostrato che la rappresentazione nel dominio potrebbe produrre migliori risultati di base per il telerilevamento rispetto alla messa a punto su ImageNet o all'apprendimento da zero.
Evitare di generare risposte una parola alla volta utilizzando una supervisione debole per addestrare un classificatore a scegliere una risposta completa.
Un modo per generare risposte per il dialogo medico usando un classificatore per selezionare da risposte curate da esperti in base al contesto della conversazione.
CNN addestrate con SGD ad ampiezza finita contro CNN completamente bayesiane ad ampiezza infinita. Chi vince?
L'articolo stabilisce una connessione tra la rete neurale convoluzionale bayesiana a canale infinito e i processi gaussiani.
Scaliamo l'inferenza bayesiana alla classificazione ImageNet e raggiungiamo risultati competitivi di accuratezza e calibrazione dell'incertezza.
Un algoritmo MCMC adattivo per la classificazione delle immagini che regola dinamicamente il momentum e il rumore applicato ad ogni aggiornamento dei parametri, ed è robusto all'overfitting e fornisce una misura di incertezza con le previsioni. 
Uno studio empirico sulle immagini false rivela che la texture è un importante indizio che le immagini false attuali differiscono dalle immagini reali. Il nostro modello migliorato che cattura le statistiche globali delle texture mostra migliori prestazioni di rilevamento delle immagini false cross-GAN.
L'articolo propone un modo per migliorare le prestazioni del modello per il rilevamento di volti falsi in immagini generate da una GAN per essere più generalizzabile in base alle informazioni sulla texture.
La distanza Wasserstein è difficile da minimizzare con la discesa stocastica del gradiente, mentre la distanza Cramer può essere ottimizzata facilmente e funziona altrettanto bene.
Il manoscritto propone di usare la distanza di Cramer come perdita quando si ottimizza una funzione obiettivo usando la discesa del gradiente stocastico perché ha gradienti campione imparziali.
Il contributo dell'articolo è legato ai criteri di performance, in particolare alla metrica di Wasserstein/Mallows
Impariamo la freccia del tempo per gli MDP e la usiamo per misurare la raggiungibilità, rilevare gli effetti collaterali e ottenere un segnale di ricompensa della curiosità. 
Questo lavoro propone l'h-potenziale come soluzione a un obiettivo che misura l'asimmetria stato-transizione in un MDP.
Abbiamo formulato SGD come un problema di filtraggio bayesiano, e mostriamo che questo dà origine a RMSprop, Adam, AdamW, NAG e altre caratteristiche dei metodi adattativi allo stato dell'arte
L'articolo analizza la discesa del gradiente stocastico attraverso il filtraggio bayesiano come quadro per analizzare i metodi adattivi.
Gli autori tentano di unificare i metodi di gradiente adattivo esistenti nel quadro del filtraggio bayesiano con la priorità dinamica
Introduciamo l'idea dell'apprendimento avversario nell'aumento automatico dei dati per migliorare la generalizzazione di una rete di targe.
Una tecnica chiamata Adversarial AutoAugment che impara dinamicamente buone politiche di incremento dei dati durante l'addestramento usando un approccio adversariale.
Lo studio introduce due approcci per migliorare la generalizzazione del meta-apprendimento del primo ordine e presenta una valutazione empirica sulla classificazione di immagini a pochi scatti.
L'articolo presenta uno studio empirico dell'algoritmo Reptile di meta-apprendimento del primo ordine, investigando una tecnica di regolarizzazione proposta e reti più profonde
Questo articolo propone di usare la fattorizzazione della matrice al momento dell'addestramento per la traduzione automatica neurale, che può ridurre la dimensione del modello e diminuire il tempo di addestramento senza danneggiare le prestazioni.
Questo articolo propone di comprimere i modelli usando la fattorizzazione della matrice durante l'addestramento per le reti neurali profonde di traduzione automatica.
Diversi metodi di analisi del BERT suggeriscono conclusioni diverse (ma compatibili) in un caso di studio su NPI.
In questo lavoro, presentiamo V1Net - una nuova rete neurale ricorrente che modella le connessioni orizzontali corticali che danno origine a robuste rappresentazioni visive attraverso il raggruppamento percettivo.
Gli autori propongono di modificare una variante convoluzionale di LSTM per includere connessioni orizzontali ispirate alle interazioni note nella corteccia visiva.
Proponiamo un collegamento tra l'equivarianza di permutazione e la generalizzazione compositiva, e forniamo modelli linguistici equivarianti
Questo lavoro si concentra sull'apprendimento di rappresentazioni e funzioni localmente equivarianti su parole di input/output ai fini del compito SCAN.
L'articolo propone un algoritmo per aumentare la flessibilità del posteriore variazionale nelle reti neurali bayesiane attraverso l'ottimizzazione iterativa.
Un metodo per addestrare distribuzioni posteriori variazionali flessibili, applicato alle reti neurali bayesiane per eseguire l'inferenza di variazione (VI) sui pesi.
Nuovo quadro all'avanguardia per il restauro delle immagini
L'articolo propone un'architettura di rete neurale convoluzionale che include blocchi per meccanismi di attenzione locale e non locale, che sono ritenuti responsabili del raggiungimento di risultati eccellenti in quattro applicazioni di restauro delle immagini.
Questo articolo propone una rete di attenzione residua non locale per il restauro delle immagini
Approccio ibrido all'acquisizione di modelli che compensa la mancanza di dati disponibili con la conoscenza specifica del dominio fornita da esperti
Un approccio di acquisizione del dominio che considera l'uso di una rappresentazione diversa per il modello di dominio parziale utilizzando relazioni mutex schematiche al posto delle condizioni pre/post.
Rilasciamo un set di dati costruito a partire dai dati ECG a singolo cavo di 11.000 pazienti a cui è stato prescritto l'uso del dispositivo {DEVICENAME}(TM).
Questo articolo descrive un dataset ECG su larga scala che gli autori intendono pubblicare e fornisce un'analisi e una visualizzazione non supervisionata del dataset.
Un nuovo Context-Gated Convolution che incorpora informazioni globali sul contesto nelle CNN modulando esplicitamente i kernel di convoluzione, e quindi cattura modelli locali più rappresentativi ed estrae caratteristiche discriminanti.
Questo articolo usa il contesto globale per modulare i pesi degli strati convoluzionali e aiutare le CNN a catturare più caratteristiche discriminanti con alte prestazioni e meno parametri rispetto alla modulazione della mappa delle caratteristiche.
Analizziamo il trade-off tra il rumore di quantizzazione e la distorsione di clipping nelle reti a bassa precisione, e mostriamo miglioramenti marcati rispetto agli schemi di quantizzazione standard che normalmente evitano il clipping
Deriva una formula per trovare i valori minimi e massimi di ritaglio per la quantizzazione uniforme che minimizzano l'errore quadratico risultante dalla quantizzazione, per una distribuzione Laplace o Gaussiana sul valore pre-quantizzato.
Proponiamo un nuovo metodo di normalizzazione per gestire i casi di piccoli lotti.
Un metodo per affrontare il problema della piccola dimensione dei lotti di BN che applica l'operazione di media mobile senza troppo overhead e riduce il numero di statistiche di BN per una migliore stabilità.
ReLU MLP prova di separazione della profondità con argomenti gemoterici
Una prova che le reti più profonde hanno bisogno di meno unità di quelle meno profonde per una famiglia di problemi. 
Un nuovo algoritmo di apprendimento few-shot basato su GAN sintetizzando caratteristiche diverse e discriminanti
Un metodo di meta-apprendimento che impara un modello generativo che può aumentare il set di supporto di un apprendista a pochi scatti che ottimizza una combinazione di perdite.
Dimostriamo come la struttura nei set di dati abbia un impatto sulle reti neurali e introduciamo un modello generativo per set di dati sintetici che riproduce questo impatto.
L'articolo studia come diverse impostazioni della struttura dei dati influenzano l'apprendimento delle reti neurali e come imitare il comportamento su set di dati reali quando si impara su uno sintetico.
Addestriamo reti neurali profonde basate su matrici diagonali e circolanti, e dimostriamo che questo tipo di reti sono sia compatte che accurate in applicazioni del mondo reale.
Gli autori forniscono un'analisi teorica della potenza espressiva delle reti neurali diagonali circolanti (DCNN) e propongono uno schema di inizializzazione per le DCNN profonde.
Proponiamo di sfruttare la distillazione dei modelli per imparare spiegazioni additive globali sotto forma di forme di caratteristiche (che sono più espressive delle attribuzioni di caratteristiche) per modelli come le reti neurali addestrate su dati tabulari.
Questo articolo incorpora i modelli additivi generalizzati (GAM) con la distillazione dei modelli per fornire spiegazioni globali delle reti neurali.
Una struttura di apprendimento multi-task su larga scala con diversi obiettivi di formazione per imparare rappresentazioni di frasi di lunghezza fissa
Questo articolo riguarda l'apprendimento di embeddings di frasi combinando diversi segnali di addestramento: skip-thought, previsione della traduzione, classificazione delle relazioni di implicazione e previsione del parsing dei costituenti.
Proponiamo un annotatore neurale di bias per confrontare i modelli sulla loro robustezza ai set di dati di testo distorti.
Un metodo per generare set di dati distorti per l'NLP, basandosi su un autocodificatore condizionato regolarizzato dall'avversario (CARA).
Proponiamo di supervisionare i modelli di argomento in stile VAE regolando in modo intelligente il priore su una base per documento. Troviamo che un posteriore logit-normale fornisce le migliori prestazioni.
Un metodo flessibile di supervisione debole di un modello di argomento per ottenere un migliore allineamento con l'intuizione dell'utente.
Prima analisi completa del piano di informazione delle reti neurali profonde su larga scala utilizzando l'entropia basata sulla matrice e i kernel tensori.
Gli autori propongono uno stimatore basato su tensor-kernel per la stima dell'informazione reciproca tra strati ad alta densità in una rete neurale.
Proponiamo un quadro modulare che può eseguire compiti specificati da programmi e raggiungere una generalizzazione a zero per compiti più complessi.
Questo articolo studia l'addestramento di agenti RL con istruzioni e decomposizioni di compiti formalizzati come programmi, proponendo un modello per un agente guidato dal programma che interpreta un programma e propone sotto-obiettivi a un modulo di azione.
Dimostriamo che la discesa del gradiente inizializzato in modo casuale (stocastico) impara un filtro convoluzionale in tempo polinomiale.
Studia il problema dell'apprendimento di un singolo filtro convoluzionale usando SGD e mostra che sotto certe condizioni, SGD impara un singolo filtro convoluzionale.
Questo articolo estende l'ipotesi di distribuzione gaussiana a un'ipotesi di levigatezza angolare più generale, che copre una famiglia più ampia di distribuzioni di input
Il primo metodo di aumento dei dati appositamente progettato per migliorare la robustezza generale di DNN senza alcuna ipotesi sugli algoritmi di attacco.
Propone un metodo di addestramento di aumento dei dati per guadagnare la robustezza del modello contro le perturbazioni avversarie, aumentando i campioni casuali in modo uniforme da una sfera a raggio fisso centrata sui dati di formazione. 
Utilizzo di Wasserstein-GANs per generare attività neurale realistica e per rilevare le caratteristiche più rilevanti presenti nei modelli di popolazione neurale.
Un metodo per simulare treni di spike da popolazioni di neuroni che corrispondono a dati empirici utilizzando una GAN semi-convoluzionale.
L'articolo propone di usare le GAN per sintetizzare modelli realistici di attività neurale
Gli stimatori di gradiente doppiamente riparametrizzati forniscono una riduzione imparziale della varianza che porta a migliori prestazioni.
L'autore ha trovato sperimentalmente che lo stimatore del lavoro esistente (STL) è distorto e propone di ridurre la distorsione per migliorare lo stimatore del gradiente dell'ELBO.
Gradientless Descent è un algoritmo senza gradiente provatamente efficiente che è monotono-invariante e veloce per l'ottimizzazione ad alta densità di ordine zero.
Questo articolo propone algoritmi GradientLess Descent (GLD) stabili che non si basano sulla stima del gradiente.
Proponiamo una nuova classe di modelli generativi visivi: i predittori condizionati dall'obiettivo. Mostriamo sperimentalmente che il condizionamento sull'obiettivo permette di ridurre l'incertezza e produrre previsioni su orizzonti molto più lunghi.
Questo articolo riformula il problema della predizione video come interpolazione invece di estrapolazione, condizionando la predizione sul fotogramma iniziale e finale (obiettivo), ottenendo predizioni di qualità superiore.
Proponiamo una struttura profonda di Multi Instance Learning basata su reti neurali ricorrenti che utilizza funzioni di pooling e meccanismi di attenzione per i compiti di annotazione dei concetti.
L'articolo affronta la classificazione dei dati delle serie temporali mediche e propone di modellare la relazione temporale tra le istanze di ogni serie utilizzando un'architettura di rete neurale ricorrente. 
Propone una nuova formulazione di Multiple Instance Learning (MIL) chiamata Relation MIL (RMIL), e discute una serie di sue varianti con LSTM, Bi-LSTM, S2S, ecc. ed esplora l'integrazione di RMIL con vari meccanismi di attenzione, e dimostra il suo utilizzo nella previsione di concetti medici da dati di serie temporali. 
Gli strati di incorporazione sono fattorizzati con la decomposizione Tensor Train per ridurre la loro impronta di memoria.
Questo articolo propone un modello di decomposizione tensoriale a basso rango per parametrizzare la matrice di incorporazione nel Natural Language Processing (NLP), che comprime la rete e talvolta aumenta la precisione del test.
Regolarizzazione del decadimento dei pesi nei metodi a gradiente adattivo come Adam
Propone l'idea di disaccoppiare il decadimento del peso dal numero di passi del processo di ottimizzazione.
L'articolo presenta un modo alternativo per implementare il decadimento del peso in Adam con risultati empirici mostrati
Studia i problemi di decadimento del peso nelle varianti SGD e propone il metodo di disaccoppiamento tra il decadimento del peso e l'aggiornamento basato sul gradiente.
Apprendimento distributivo permanente attraverso un'architettura studente-insegnante accoppiata con un regolatore posteriore del modello incrociato.
Autocodificatori profondi per imparare una buona rappresentazione per i dati geometrici delle nuvole di punti 3D; Modelli generativi per le nuvole di punti.
Approcci per apprendere modelli generativi di tipo GAN usando l'architettura PointNet e il latent-space GAN.
Proponiamo un nuovo metodo per sopprimere la vulnerabilità dello spazio delle caratteristiche latenti per ottenere reti robuste e compatte.
Questo articolo propone un metodo di "adversarial neural pruning" per addestrare una maschera di potatura e una nuova perdita di soppressione della vulnerabilità per migliorare la precisione e la robustezza avversaria.
Abbiamo proposto due modifiche VAE che tengono conto degli esempi di dati negativi, e li abbiamo utilizzati per il rilevamento semi-supervisionato delle anomalie.
Gli articoli propongono due metodi di approcci simili a VAE per il rilevamento semi-supervisionato delle novità, MML-VAE e DP-VAE.
La nuova comprensione delle dinamiche di formazione e le metriche di durezza della memorizzazione portano all'apprendimento efficiente e dimostrabile del curriculum.
Questo articolo formula DIH come un problema di apprendimento del curriculum che può utilizzare più efficacemente i dati per addestrare le DNN, e deriva la teoria sul limite di approssimazione.
Storia degli sviluppi paralleli delle leggi di aggiornamento e dei concetti tra controllo adattivo e ottimizzazione nell'apprendimento automatico.
Convoluzione ricorrente per la compressione del modello e un trucco per addestrarlo, cioè l'apprendimento di strati BN indipendenti su passi.
L'autore modifica la rete neurale di convoluzione ricorrente (RCNN) con la normalizzazione indipendente dei lotti, con i risultati sperimentali su RCNN compatibili con l'architettura della rete neurale ResNet quando contiene lo stesso numero di strati.
I campi recettivi dinamici con struttura gaussiana spaziale sono accurati ed efficienti.
Questo articolo propone un operatore di convoluzione strutturato per modellare le deformazioni delle regioni locali di un'immagine, che ha ridotto significativamente il numero di parametri.
Un trucco sui campioni avversari in modo che le etichette mal classificate siano impercettibili nello spazio delle etichette agli osservatori umani
Un metodo per costruire attacchi avversari che sono meno rilevabili dall'uomo senza costi nello spazio dell'immagine cambiando la classe di destinazione per essere simile alla classe originale dell'immagine.
Questo documento presenta la classificazione del tipo di rumore/posizione di vari rumori d'impatto generati in un edificio, che è un serio problema di conflitto nei complessi di appartamenti
Questo lavoro descrive l'uso delle reti neurali convoluzionali in una nuova area di applicazione della classificazione del tipo di rumore degli edifici e della posizione del rumore. 
Le reti neurali ricorrenti imparano ad aumentare e ridurre la dimensionalità della loro rappresentazione interna in un modo che corrisponde al compito, a seconda della dinamica della rete iniziale.
Invece dei rigidi allineamenti di distribuzione nei tradizionali obiettivi di adattamento del dominio profondo, che fallisce quando la distribuzione delle etichette di destinazione si sposta, proponiamo di ottimizzare un obiettivo rilassato con nuove analisi, nuovi algoritmi e convalida sperimentale.
Questo articolo suggerisce metriche rilassate per l'adattamento al dominio che danno nuovi limiti teorici sull'errore di destinazione.
esploriamo il compito della generazione di sommario-articolo e proponiamo uno schema di generazione gerarchico insieme a un quadro di apprendimento di rinforzo congiunto end-to-end per addestrare il modello gerarchico.
Per affrontare il problema della degenerazione nella generazione di sommario-articolo, questo articolo propone un approccio di generazione gerarchica che genera prima uno schizzo intermedio dell'articolo e poi l'articolo completo.
Proponiamo una regolarizzazione controfattuale per difendersi dagli spostamenti di dominio avversi che si verificano attraverso spostamenti nella distribuzione delle "caratteristiche di stile" latenti delle immagini.
L'articolo discute i modi per difendersi dai cambiamenti di dominio avversari con la regolarizzazione controfattuale imparando un classificatore che è invariante ai cambiamenti superficiali (o caratteristiche di "stile") nelle immagini.
Questo documento mira a una classificazione robusta delle immagini contro i cambiamenti di dominio avversari e l'obiettivo è raggiunto evitando di usare le caratteristiche di stile mutevoli.
Proponiamo un meta-apprendista per adattarsi rapidamente su più compiti anche un passo in un'impostazione di pochi scatti.
Questo articolo propone un metodo per meta-apprendere un modulo di correzione del gradiente in cui il precondizionamento è parametrizzato da una rete neurale, e costruisce un processo di aggiornamento del gradiente in due fasi durante l'adattamento. 
I modelli di risposta alle domande che modellano la distribuzione congiunta di domande e risposte possono imparare di più dei modelli discriminativi
Questo articolo propone un approccio generativo all'AQ testuale e visuale, dove viene appresa una distribuzione congiunta sullo spazio delle domande e delle risposte dato il contesto, che cattura relazioni più complesse.
Questo articolo introduce un modello generativo per la risposta alle domande e propone di modellare p(q,a|c), fattorizzato come p(a|c) * p(q|a,c). 
Gli autori propongono un modello generativo di AQ, che ottimizza congiuntamente la distribuzione delle domande e delle risposte date da un documento/contesto. 
Viene proposta una nuova funzione di attivazione chiamata Displaced Rectifier Linear Unit. Ha dimostrato di migliorare le prestazioni di formazione e inferenza delle reti neurali convoluzionali normalizzate in batch.
Il documento confronta e suggerisce contro l'uso della normalizzazione dei lotti dopo l'uso di unità lineari raddrizzatrici
Questo articolo propone una funzione di attivazione, chiamata ReLU spostata, per migliorare le prestazioni delle CNN che usano la normalizzazione batch.
Costruiamo reti neurali convoluzionali scale-equivariant nella forma più generale con efficienza computazionale e robustezza di deformazione dimostrata.
Gli autori propongono un'architettura CNN che è teoricamente equivariante a scalature e traslazioni isotrope aggiungendo una dimensione di scala extra ai tensori di attivazione.
Abbiamo diagnosticato reti neurali profonde per l'elaborazione di nuvole di punti 3D per esplorare l'utilità di diverse architetture di rete. 
L'articolo studia diverse architetture di reti neurali per l'elaborazione di nuvole di punti 3D e propone metriche per la robustezza avversaria, la robustezza rotazionale e la coerenza di vicinato.
L'utilizzo della struttura delle distribuzioni migliora l'inferenza variazionale semi-implicita
Apprendimento auto-imitazione di traiettorie diverse con politica condizionata dalla traiettoria
Questo articolo affronta compiti di esplorazione difficili applicando l'auto-imitazione a una diversa selezione di traiettorie dall'esperienza passata, per guidare un'esplorazione più efficiente in problemi a ricompensa sparsa, ottenendo risultati SOTA.
Un metodo che addestra reti neurali di grande capacità con una precisione significativamente migliorata e un costo computazionale dinamico inferiore
Un metodo per addestrare una rete con grande capacità, di cui solo parti sono usate al momento dell'inferenza dipendente dall'input, usando una selezione condizionale a grana fine e un nuovo metodo di regolarizzazione, "batch shaping".
Presentiamo un'architettura differenziabile end-to-end che impara a mappare i pixel ai predicati, e la valutiamo su una serie di semplici compiti di ragionamento relazionale
Un'architettura di rete basata sul modulo di auto-attenzione a più teste per imparare una nuova forma di rappresentazioni relazionali, che migliora l'efficienza dei dati e la capacità di generalizzazione sull'apprendimento del curriculum.
Usiamo le reti neurali per proiettare le informazioni superficiali per l'inferenza del linguaggio naturale, definendo e identificando le informazioni superficiali dalla prospettiva della logica del primo ordine.
Questo articolo cerca di ridurre le informazioni superficiali nell'inferenza del linguaggio naturale per prevenire l'overfitting, e introduce una rete neurale a grafo per modellare la relazione tra premessa e ipotesi. 
Un approccio per trattare l'inferenza del linguaggio naturale usando la logica del primo ordine e per infondere i modelli NLI con informazioni logiche per essere più robusti nell'inferenza.
Algoritmo per l'addestramento di classificatori individualmente equi utilizzando la robustezza avversaria
Questo articolo propone una nuova definizione di equità algoritmica e un algoritmo per trovare in modo dimostrabile un modello ML che soddisfi il vincolo di equità.
La semina e l'aumento sono tutto ciò che serve per classificare le cifre in qualsiasi lingua?
Questo articolo presenta nuovi set di dati per cinque lingue e propone un nuovo quadro (SAT) per la generazione di set di dati di immagini di font per la classificazione universale delle cifre.
Il successo di MAML si basa sul riutilizzo delle caratteristiche dalla meta-inizializzazione, che produce anche una semplificazione naturale dell'algoritmo, con il ciclo interno rimosso per il corpo della rete, così come altri approfondimenti sulla testa e sul corpo.
L'articolo trova che il riutilizzo delle caratteristiche è il fattore dominante nel successo di MAML, e propone nuovi algoritmi che spendono molto meno calcolo di MAML.
Forniamo un algoritmo indipendente dal metodo per decidere quando addestrare in modo incrementale rispetto all'addestramento completo e fornisce un significativo aumento di velocità rispetto all'addestramento completo ed evita la dimenticanza catastrofica
Questo articolo propone un approccio per decidere quando incrementare o riqualificare completamente un modello nell'impostazione dello sviluppo iterativo del modello nei compiti di riempimento degli slot.
Sviluppiamo un quadro teorico per caratterizzare quali compiti di ragionamento una rete neurale può imparare bene.
L'articolo propone una misura di classi di allineamento algoritmico che misura quanto le reti neurali sono "vicine" agli algoritmi conosciuti, dimostrando il legame tra diverse classi di algoritmi conosciuti e le architetture delle reti neurali.
Esploriamo le interazioni cellula-cellula attraverso i contesti dell'ambiente tumorale osservati in immagini altamente multiplexed, attraverso la sintesi delle immagini utilizzando una nuova architettura GAN di attenzione.
Un nuovo metodo per modellare i dati generati dal multiplexed ion beam imaging by time-of-flight (MIBI-TOF) imparando la mappatura many-to-many tra i tipi di cellule e i livelli di espressione dei marcatori di proteine.
Un approccio a due stadi che consiste nella selezione della frase seguita dalla selezione dello span può essere reso più robusto agli attacchi avversari rispetto a un modello a stadio singolo addestrato sul contesto completo.
Questo articolo esamina un modello esistente e scopre che un metodo di AQ addestrato in due fasi non è più robusto agli attacchi avversari rispetto ad altri metodi.
Verifica di un modello di guidatore umano basato su un'architettura cognitiva e sintesi di un ADAS corretto per costruzione da esso.
Un nuovo approccio ibrido di apprendimento profondo fornisce la migliore soluzione a un problema di dati limitati (che è importante per la conservazione della lingua hawaiana)
Studiamo quantitativamente il rilevamento di out-of-distribution nell'impostazione few-shot, stabiliamo risultati di base con ProtoNet, MAML, ABML, e li miglioriamo.
L'articolo propone due nuovi punteggi di fiducia che sono più adatti per il rilevamento di fuori distribuzione della classificazione di pochi colpi e mostra che un approccio basato sulla metrica della distanza migliora le prestazioni.
Questo articolo introduce la distillazione progressiva della conoscenza per l'apprendimento di modelli generativi che sono orientati al compito di riconoscimento
Questo articolo dimostra l'apprendimento curricolare facile e difficile per addestrare un modello generativo per migliorare la classificazione di pochi scatti.
Proponiamo un nuovo metodo per migliorare la trasferibilità degli esempi avversari utilizzando il gradiente ridotto al rumore.
Questo articolo postula che una perturbazione avversaria consiste in una componente specifica del modello e in una componente specifica dei dati, e che l'amplificazione di quest'ultima è più adatta per gli attacchi avversari.
Questo articolo si concentra sul miglioramento della trasferibilità degli esempi contraddittori da un modello a un altro modello.
Presentiamo il flusso iterativo di decomposizione CP a due passi per accelerare efficacemente le reti neurali convoluzionali esistenti (CNN).
L'articolo propone un nuovo flusso di lavoro per l'accelerazione e la compressione delle CNN e propone anche un modo per determinare il rango target di ogni strato dato l'accelerazione globale target. 
Questo articolo affronta il problema dell'apprendimento di un'operazione di filtro tensore di basso rango per gli strati di filtraggio nelle reti neurali profonde (DNN). 
Limiti superiori basati su LP sulla costante di Lipschitz delle reti neurali
Gli autori studiano il problema della stima della costante di Lipschitz di una rete neurale profonda con funzione di attivazione ELO, formulandolo come un problema di ottimizzazione polinomiale.
Affrontiamo la classificazione multi-dominio a pochi scatti costruendo modelli multipli per rappresentare questa complessa distribuzione di compiti in modo collettivo e semplificando l'adattamento specifico del compito come un problema di selezione da questi modelli pre-addestrati.
Questo articolo affronta la classificazione a pochi scatti con molti domini diversi costruendo un pool di modelli di incorporazione per catturare caratteristiche invarianti e specifiche del dominio senza un aumento significativo del numero di parametri.
Rimozione su base neurale degli artefatti di inchiostro dei documenti (sottolineature, sbavature, ecc.) senza dati di allenamento annotati manualmente
Proponiamo un attacco black-box query-efficiente che utilizza l'ottimizzazione bayesiana in combinazione con la selezione del modello bayesiano per ottimizzare la perturbazione avversaria e il grado ottimale di riduzione della dimensione dello spazio di ricerca. 
Gli autori propongono di utilizzare l'ottimizzazione bayesiana con un surrogato GP per la generazione di immagini avversarie, sfruttando la struttura additiva e utilizzando la selezione bayesiana del modello per determinare una riduzione ottimale della dimensionalità.
Proponiamo un modello per imparare rappresentazioni multimodali fattorizzate che sono discriminative, generative e interpretabili.
Questo articolo presenta il "modello di fattorizzazione multimodale" che fattorizza le rappresentazioni in fattori discriminativi multimodali condivisi e fattori generativi specifici della modalità. 
Sviluppiamo un algoritmo gerarchico e actor-critic per il trasferimento compositivo attraverso la condivisione di componenti di policy e dimostriamo la specializzazione dei componenti e i relativi benefici diretti nei domini multitask, così come il suo adattamento per compiti singoli.
Una combinazione di diverse tecniche di apprendimento per l'acquisizione della struttura e l'apprendimento con dati asimmetrici, utilizzati per addestrare una politica HRL.
Gli autori introducono una struttura di politica gerarchica per l'uso nell'apprendimento di rinforzo sia a compito singolo che multitask, e valutano l'utilità della struttura su compiti robotici complessi.
Contiamo empiricamente il numero di regioni lineari delle reti di raddrizzatori e raffiniamo i limiti superiori e inferiori.
Questo articolo presenta dei limiti migliorati per il conteggio del numero di regioni lineari nelle reti ReLU.
Analizziamo le proprietà di memorizzazione tramite un convnet del set di allenamento e proponiamo diversi casi d'uso in cui possiamo estrarre alcune informazioni sul set di allenamento. 
Illumina le proprietà di generalizzazione/memorizzazione delle Convnet grandi e profonde e cerca di sviluppare procedure relative all'identificazione se un input di una ConvNet addestrata è stato effettivamente utilizzato per addestrare la rete.
Le GAN possono in linea di principio imparare distribuzioni in modo efficiente dal punto di vista del campione, se la classe discriminante è compatta e ha un forte potere di distinzione rispetto alla particolare classe generatrice.
Propone la nozione di approssimabilità ristretta e fornisce un limite di complessità del campione, polinomiale nella dimensione, che è utile nello studio della mancanza di diversità nelle GAN.
Analizza che la metrica di probabilità integrale può essere una buona approssimazione della distanza di Wasserstein sotto alcune ipotesi lievi.
Nella fase iniziale della formazione delle reti neurali profonde esiste un "punto di pareggio" che determina le proprietà dell'intera traiettoria di ottimizzazione.
Questo lavoro analizza l'ottimizzazione delle reti neurali profonde considerando come la dimensione del batch e gli iper-parametri del passo modificano le traiettorie di apprendimento.
Proponiamo HWGCN per mescolare le informazioni di vicinato pertinenti a diversi ordini per imparare meglio le rappresentazioni dei nodi.
Gli autori propongono una variante di GCN, HWGCN, per considerare la convoluzione oltre i vicini a 1 passo, che è paragonabile ai metodi all'avanguardia.
Introduciamo una nuova misura di planarità ai minimi locali della superficie di perdita delle reti neurali profonde che è invariante rispetto alle riparametrizzazioni a livello e colleghiamo la planarità alla robustezza delle caratteristiche e alla generalizzazione.
Gli autori propongono una nozione di robustezza delle caratteristiche che è invariante rispetto al ridimensionamento del peso e discutono la relazione della nozione con la generalizzazione.
Questo articolo definisce una nozione di feature-robustness e la combina con la rappresentatività epsilon di una funzione per descrivere una connessione tra flatness dei minimi e generalizzazione nelle reti neurali profonde.
Proponiamo di sparsificare le preattivazioni delle porte e il flusso di informazioni in LSTM per renderle costanti e aumentare il livello di sparsità dei neuroni
Questo articolo ha proposto un metodo di sparsificazione per le reti neurali ricorrenti eliminando i neuroni con preattivazioni zero per ottenere reti compatte.
Introduciamo un approccio di rete neurale per assistere i solutori di equazioni differenziali parziali.
Gli autori mirano a migliorare la precisione dei solutori numerici addestrando una rete neurale su dati di riferimento simulati che correggono il solutore numerico.
un metodo di apprendimento confederato che allena il modello da dati medici separati orizzontalmente e verticalmente 
Un metodo di apprendimento automatico "confederato" che impara attraverso le divisioni nei dati medici separati sia orizzontalmente che verticalmente.
Questo articolo propone l'attivazione quantizzata stocastica che risolve i problemi di overfitting nell'addestramento avversario FGSM e raggiunge rapidamente la robustezza paragonabile all'addestramento multi-step.
L'articolo propone un modello per migliorare l'addestramento avversario introducendo perturbazioni casuali nelle attivazioni di uno degli strati nascosti
Studiamo la struttura del rumore nel cervello e scopriamo che può aiutare la generalizzazione spostando le rappresentazioni lungo le variazioni degli stimoli in classe.
Presentiamo un nuovo design, cioè Self-Ensembling with Category-agnostic Clusters, per l'adattamento al dominio sia a set chiusi che a set aperti.
Un nuovo approccio all'adattamento del dominio open set, dove le categorie del dominio di origine sono contenute nelle categorie del dominio di destinazione al fine di filtrare le categorie anomale e consentire l'adattamento all'interno delle classi condivise.
Mostriamo come imparare le decomposizioni spettrali degli operatori lineari con l'apprendimento profondo, e lo usiamo per l'apprendimento non supervisionato senza un modello generativo.
Gli autori propongono di utilizzare un quadro di apprendimento profondo per risolvere il calcolo degli autovettori più grandi.
Questo articolo presenta una struttura per imparare le autofunzioni attraverso un processo stocastico e propone di affrontare la sfida del calcolo delle autofunzioni in un contesto su larga scala approssimando poi usando un processo di ottimizzazione stocastica a due fasi.
Applicazione dell'algoritmo Riemannian SGD (RSGD) per l'addestramento di Tensor-Train RNNs per ridurre ulteriormente i parametri del modello.
L'articolo propone di utilizzare l'algoritmo del gradiente stocastico Riemanniano per l'apprendimento di tensori di basso rango nelle reti profonde.
Propone un algoritmo per l'ottimizzazione delle reti neurali parametrizzate dalla decomposizione Tensor Train basata sull'ottimizzazione Riemanniana e sull'adattamento di rango, e progetta un'architettura TT LSTM bidirezionale.
Proponiamo un nuovo algoritmo che impara politiche che soddisfano i vincoli, e forniamo un'analisi teorica e una dimostrazione empirica nel contesto dell'apprendimento di rinforzo con vincoli.
Questo articolo introduce un algoritmo di ottimizzazione delle politiche vincolate che utilizza un processo di ottimizzazione a due fasi, dove le politiche che non soddisfano il vincolo possono essere proiettate indietro nell'insieme dei vincoli.
Proponiamo una rappresentazione basata sul gradiente per caratterizzare le informazioni che le reti profonde non hanno imparato.
Gli autori presentano la creazione di rappresentazioni basate sui gradienti rispetto ai pesi per integrare le informazioni mancanti dal dataset di formazione per le reti profonde.
Introduciamo una struttura per la riduzione degli artefatti nelle immagini mediche "Zero-Shot", che sfrutta la potenza dell'apprendimento profondo, ma senza utilizzare reti generali pre-addestrate o qualsiasi riferimento di immagine pulita. 
Applichiamo il concetto di collo di bottiglia informativo all'attribuzione.
L'articolo propone un nuovo metodo basato sulla perturbazione per il calcolo delle mappe di attribuzione/salienza per classificatori di immagini basati su reti neurali profonde, iniettando rumore artigianale in uno strato iniziale della rete.
Mostriamo che le RNN possono essere sfrondate per indurre la sparsità dei blocchi che migliora la velocità per le operazioni sparse sull'hardware esistente.
Gli autori propongono un approccio di pruning di sparsità a blocchi per comprimere le RNN, usando il gruppo LASSO per promuovere la sparsità e per potare, ma con un programma molto specializzato per quanto riguarda il peso di potatura e la potatura.
Proponiamo un miglioramento delle reti di iterazione del valore, con applicazioni alla pianificazione del percorso del rover planetario.
Questo articolo apprende una funzione di ricompensa basata sulle traiettorie degli esperti utilizzando un modulo di iterazione del valore per rendere il passo di pianificazione differenziabile
Un nuovo strato di attenzione che combina l'autoattenzione e i sottolivelli feed-forward delle reti Transformer.
Questo articolo propone una modifica al modello Transformer incorporando l'attenzione sui vettori di memoria "persistenti" nello strato di auto-attenzione, ottenendo prestazioni alla pari con i modelli esistenti e utilizzando meno parametri.
Troviamo efficacemente un sottoinsieme di immagini che hanno attivazioni più alte del previsto per alcuni sottoinsiemi di nodi.  Queste immagini appaiono più anomale e più facili da rilevare se viste come un gruppo. 
L'articolo ha proposto uno schema per rilevare la presenza di input anomali basato su un approccio di "subset scanning" per rilevare attivazioni anomale nella rete di apprendimento profondo.
I modelli ricorrenti stabili possono essere approssimati da reti feed-forward e si comportano empiricamente bene come i modelli instabili su compiti di riferimento.
Studia la stabilità delle RNN e l'indagine della normalizzazione spettrale alle previsioni sequenziali.
Ha studiato il ruolo della condivisione dei pesi nelle reti neurali usando le funzioni hash, ha scoperto che una funzione hash equilibrata e deterministica aiuta le prestazioni della rete.
Proporre ArbNets per studiare la condivisione del peso in modo più sistematico definendo la funzione di condivisione del peso come una funzione hash.
 Introduciamo un sistema di apprendimento statistico relazionale che prende in prestito idee dalla logica di Markov ma impara una rappresentazione implicita delle regole come una rete neurale.
L'articolo fornisce un'estensione alle reti logiche di Markov rimuovendo la loro dipendenza da regole logiche di primo ordine predefinite per modellare più domini nei compiti di completamento delle basi di conoscenza.
Un metodo scalabile per l'apprendimento di un priore espressivo sulle reti neurali in compiti multipli.
L'articolo presenta un metodo per addestrare un modello probabilistico per il Multitasks Transfer Learning introducendo una variabile latente per compito per catturare la comunanza nelle istanze del compito.
Il lavoro propone un approccio variazionale al meta-apprendimento che impiega variabili latenti corrispondenti a set di dati specifici del compito.
Mira ad apprendere una priorità sulle reti neurali per compiti multipli. 
MODELLI DI SPAZIO DI STATO DISSOCIATI
L'articolo presenta un modello generativo dello spazio di stato che utilizza una variabile latente globale E per catturare le informazioni specifiche dell'ambiente.
Apprendimento della divergenza di Bregman per l'apprendimento di pochi colpi. 
Introduciamo una struttura di rete che può modificare la sua struttura durante l'addestramento e mostriamo che può convergere a vari archetipi di rete ML come MLP e LCN. 
L'aumento dei dati guidato dal dominio fornisce un metodo robusto e stabile di generalizzazione del dominio
Questo articolo propone un approccio di generalizzazione del dominio attraverso l'aumento dei dati dipendenti dal dominio
Gli autori introducono il metodo CrossGrad, che allena sia un compito di classificazione delle etichette che un compito di classificazione del dominio.
Studiamo un'alternativa ai tradizionali approcci di modellazione delle immagini pixel, e proponiamo un modello generativo per le immagini vettoriali.
Questo articolo introduce un'architettura di rete neurale per la generazione di disegni ispirata all'autocodificatore variazionale.
Forniamo uno studio che cerca di vedere come il recente adattamento del tasso di apprendimento online estende la conclusione fatta da Wilson et al. 2018 sui metodi di gradiente adattivi, insieme al confronto e all'analisi della sensibilità.
Riporta i risultati dei test di diversi metodi relativi alla regolazione delle dimensioni, tra cui SGD alla vaniglia, SGD con momento Neserov e ADAM e confronta questi metodi con ipergradiente e senza. 
Indaghiamo il comportamento su grandi campioni delle stime dei valori Q e abbiamo proposto una strategia di esplorazione efficiente che si basa sulla stima delle discrepanze relative tra le stime Q. 
Questo articolo presenta un algoritmo di pura esplorazione per l'apprendimento di rinforzo basato su un'analisi asintotica dei valori Q e la loro convergenza alla distribuzione limite centrale, superando gli algoritmi di esplorazione di riferimento.
Addestriamo una rete di traduzione da immagine a immagine che prende come input l'immagine sorgente e un campione da una distribuzione prioritaria per generare un campione dalla distribuzione target
Questo articolo formalizza il problema della traduzione non supervisionata e propone una struttura GAN aumentata che utilizza l'informazione reciproca per evitare il caso degenerato
Questo articolo formula il problema della traduzione non supervisionata di immagini da uno a molti e affronta il problema minimizzando l'informazione reciproca. 
Imparare ad estrarre punti chiave distinguibili da un compito per procura, rifiuto di outlier.
Questo articolo è dedicato all'apprendimento auto-supervisionato di caratteristiche locali utilizzando Neural Guided RANSAC come un ulteriore fornitore di perdite ausiliario per migliorare l'interpolazione dei descrittori.
Proponiamo una formulazione di motivazione intrinseca che è adatta come bias di esplorazione in compiti sinergici a più agenti con ricompense sparse, incoraggiando gli agenti a influenzare il mondo in modi che non sarebbero stati raggiunti se avessero agito individualmente.
L'articolo si concentra sull'uso della motivazione intrinseca per migliorare il processo di esplorazione degli agenti di apprendimento per rinforzo in compiti che richiedono la presenza di più agenti.
Un algoritmo di inferenza probabilistica guidato da una rete neurale per modelli strutturati a grafo
Questo articolo introduce la policy message passing, una rete neurale a grafo con un meccanismo di inferenza che assegna messaggi ai bordi in modo ricorrente, indicando prestazioni competitive su compiti di ragionamento visivo.
Mostriamo come è possibile aumentare le prestazioni in una rete multitask sintonizzando una funzione di perdita adattativa multitask che viene appresa attraverso il bilanciamento diretto dei gradienti della rete.
Questo lavoro propone uno schema di aggiornamento dinamico dei pesi che aggiorna i pesi per le diverse perdite dei compiti durante l'allenamento, facendo uso dei rapporti di perdita dei diversi compiti.
Le DNN per la segmentazione delle immagini possono implementare soluzioni per il problema dell'insidie, ma solo alcune reti ricorrenti potrebbero impararle con un tipo specifico di supervisione.
Questo articolo introduce l'interiorità per studiare la segmentazione semantica nell'era del deep learning, e i risultati possono aiutare i modelli a generalizzare meglio.
Dato un modello pre-addestrato, abbiamo esplorato i gradienti per campione dei parametri del modello rispetto a una perdita specifica del compito, e costruito un modello lineare che combina i gradienti dei parametri del modello e l'attivazione del modello.
Questo articolo propone di utilizzare i gradienti di strati specifici di reti convoluzionali come caratteristiche in un modello linearizzato per l'apprendimento di trasferimento e l'adattamento veloce.
Addestriamo il nostro modello di ricostruzione facciale con perdita avversaria in modo semi-supervisionato su lotti ibridi di immagini facciali non etichettate ed etichettate per sfruttare il valore di grandi quantità di immagini facciali non etichettate da collezioni fotografiche non vincolate.
Questo articolo propone un processo di addestramento semi-supervisionato e contraddittorio per ottenere rappresentazioni dissociate non lineari da un'immagine del viso con funzioni di perdita, raggiungendo prestazioni allo stato dell'arte nella ricostruzione del viso.
Questo articolo presenta ConceptFlow che modella esplicitamente il flusso di conversazione nel grafo di conoscenza commonsense per una migliore generazione di conversazioni.
L'articolo propone un sistema per generare una risposta a turno singolo a un enunciato in un ambiente di dialogo a dominio aperto usando la diffiusione nei vicini dei concetti fondati.
Esaminiamo l'ipotesi che l'entropia degli spazi di soluzione per i vincoli sui pesi sinaptici (la "flessibilità" del vincolo) potrebbe servire come funzione di costo per lo sviluppo dei circuiti neurali.
Gli insiemi infiniti di reti neurali infinitamente larghe sono una famiglia di modelli interessante dal punto di vista della teoria dell'informazione.
Conduciamo uno studio comparativo dell'allineamento interlinguistico rispetto ai metodi di formazione congiunta e uniamo questi due paradigmi precedentemente esclusivi in un nuovo quadro. 
Questo articolo confronta gli approcci all'induzione del lessico bilingue e mostra quale metodo si comporta meglio nel lessico, nell'induzione e nei compiti di NER e MT.
Combinazione di tecniche di compressione del modello ortogonale per ottenere una riduzione significativa della dimensione del modello e del numero di flop richiesti durante l'inferenza.
Questo articolo propone di combinare la decomposizione di Tucker con la potatura dei filtri.
Introduce JAUNE: una metodologia per sostituire il punteggio BLEU e ROUGE con valutatori multidimensionali e basati su modelli per la valutazione delle sintesi
Questo articolo propone una nuova metrica JAUNE per la valutazione della traduzione automatica e dei sistemi di riassunto del testo, mostrando che il loro modello corrisponde meglio alle etichette di somiglianza della verità di terra rispetto a BLEU.
nuovo formalismo GNN + esperimenti estesi; dimostrando che le differenze tra GGNN/GCN/GAT sono minori di quanto si pensasse
L'articolo propone una nuova architettura Graph Neural Network che utilizza la Feature-wise Linear Modulation per condizionare il passaggio dei messaggi tra i nodi di origine e di destinazione in base alla rappresentazione dei nodi di destinazione.
Questo articolo propone un nuovo quadro di decomposizione della matrice per l'incorporazione e il raggruppamento simultaneo dei dati di rete attribuiti.
Questo articolo propone un algoritmo per eseguire insieme l'embedding e il clustering della rete di attributi.
Proponiamo una tecnica di rendering appreso guidato dalle immagini che combina i vantaggi del rendering basato sulle immagini e della sintesi delle immagini basata su GAN, considerando gli effetti dipendenti dalla vista.
Questa presentazione propone un metodo per gestire gli effetti dipendenti dalla vista nel rendering neurale, che migliora la robustezza dei metodi di rendering neurale esistenti.
Le GAN sono valutate su set di dati sintetici
Proponiamo un metodo di adattamento della dimensione del passo efficiente ed efficace per i metodi del gradiente.
Un nuovo adattamento della dimensione del passo nei metodi del gradiente del primo ordine che stabilisce un nuovo problema di ottimizzazione con l'espansione del primo ordine della funzione di perdita e la regolarizzazione, dove la dimensione del passo è trattata come variabile.
Abbiamo sparato che un'ampia classe di collettori può essere generata da reti ReLU e sigmoidi con precisione arbitraria.
Questo articolo fornisce alcune garanzie di base su quando i manifold possono essere scritti come l'immagine di una mappa approssimata da una rete neurale, e cuce insieme teoremi dalla geometria dei manifold e risultati standard di approssimazione universale.
Questo articolo mostra teoricamente che i modelli generativi basati sulle reti neurali possono approssimare i collettori di dati, e dimostra che, sotto blande ipotesi, le reti neurali possono mappare uno spazio latente su un insieme vicino al collettore di dati dato entro una piccola distanza Hausdorff.
Progettiamo algoritmi di apprendimento di rinforzo basati su modelli con garanzie teoriche e raggiungiamo risultati allo stato dell'arte sui compiti di benchmark Mujuco quando sono ammessi un milione o meno di campioni.
L'articolo ha proposto un quadro per progettare algoritmi RL basati su modelli e OFU che raggiungono prestazioni SOTA sui compiti MuJoCo.
Presentiamo ulteriori tecniche per utilizzare la distillazione della conoscenza per comprimere U-net di oltre 1000x.
Gli autori hanno introdotto una strategia di distillazione modificata per comprimere un'architettura U-net di oltre 1000x mantenendo una precisione vicina alla U-net originale.
Questo articolo fornisce un approccio per affrontare la dimenticanza catastrofica attraverso stime di curvatura senza Hessiano
L'articolo propone un metodo approssimativo di Laplace per l'addestramento delle reti neurali nell'impostazione di apprendimento continuo con una bassa complessità spaziale.
Metodo per migliorare le prestazioni dei modelli semplici dato un modello complesso (accurato).
L'articolo propone un mezzo per migliorare le previsioni di un modello a bassa capacità che mostra vantaggi rispetto agli approcci esistenti.
Un algoritmo semplice e pratico per l'apprendimento di un kernel massimizzante i margini di traduzione invariante o sfericamente simmetrico dai dati di formazione, utilizzando strumenti dell'analisi di Fourier e della minimizzazione del rimpianto.
L'articolo propone di imparare un kernal personalizzato invariante alla traduzione o alla rotazione nella rappresentazione di Fourier per massimizzare il margine di SVM.
Gli autori propongono un interessante algoritmo per apprendere insieme la l1-SVM e il kernel rappresentato da Fourier
Gli autori considerano l'apprendimento diretto di rappresentazioni di Fourier di kernel invarianti di spostamento/traslazione per applicazioni di apprendimento automatico con l'allineamento del kernel ai dati come funzione obiettivo da ottimizzare.
Programmazione probabilistica che supporta nativamente l'inferenza causale e controfattuale
Inferenza di un modello di gioco di campo medio (MFG) del comportamento di grandi popolazioni attraverso una sintesi di MFG e processi decisionali di Markov.
Gli autori trattano l'inferenza nei modelli di comportamento collettivo usando l'apprendimento di rinforzo inverso per imparare le funzioni di ricompensa degli agenti nel modello.
Mescoliamo modelli generativi profondi con una supervisione debole programmatica per generare traiettorie coordinate di multi-agenti di qualità significativamente più alta rispetto alle linee di base precedenti.
Propone modelli generativi sequenziali multi-agente.
L'articolo propone l'addestramento di modelli generativi che producono traiettorie multi-agente usando funzioni euristiche che etichettano le variabili che altrimenti sarebbero latenti nei dati di addestramento
Imparare a classificare le curve di apprendimento al fine di fermare precocemente i lavori di formazione non promettenti. Novità: l'uso della perdita di classifica a coppie per modellare direttamente la probabilità di migliorare e trasferire l'apprendimento attraverso i set di dati per ridurre i dati di formazione richiesti.
L'articolo propone un metodo per classificare le curve di apprendimento delle reti neurali che può modellare le curve di apprendimento su diversi insiemi di dati, ottenendo una maggiore velocità nei compiti di classificazione delle immagini.
Mostriamo che, nelle impostazioni di apprendimento continuo, la dimenticanza catastrofica può essere evitata applicando la RL off-policy a una miscela di esperienze nuove e di replay, con una perdita di clonazione comportamentale.
Propone una particolare variante di riproduzione dell'esperienza con clonazione del comportamento come metodo per l'apprendimento continuo.
Presentiamo un metodo che impara a integrare informazioni temporali e informazioni visive ambigue nel contesto di agenti che interagiscono.
Gli autori propongono Graph VRNN che modella l'interazione di più agenti distribuendo una VRNN per ogni agente
Questo articolo presenta un'architettura basata su una rete neurale a grafo che è addestrata per localizzare e modellare le interazioni degli agenti in un ambiente direttamente dai pixel e mostra il vantaggio del modello per seguire i compiti e prevedere le posizioni degli agenti.
Consideriamo un modello semplificato di rete neurale convoluzionale profonda. Mostriamo che tutti gli strati di questa rete possono essere appresi approssimativamente con una corretta applicazione della decomposizione tensoriale.
Fornisce garanzie teoriche per l'apprendimento di reti neurali convoluzionali profonde usando la decomposizione tensoriale rank-one.
Questo articolo propone un metodo di apprendimento per un caso ristretto di reti convoluzionali profonde, dove gli strati sono limitati al caso non sovrapposto e hanno solo un canale di uscita per strato
Analizza il problema dell'apprendimento di una classe molto speciale di CNN: ogni strato consiste in un singolo filtro, applicato a patch non sovrapposte dell'input.
Le reti neurali feedforward che possono avere pesi potati dopo l'addestramento potrebbero avere gli stessi pesi potati prima dell'addestramento
Mostra che esistono sottoreti sparse che possono essere addestrate da zero con buone prestazioni di generalizzazione e propone una NNs non potata e inizializzata in modo casuale che contiene sottoreti che possono essere addestrate da zero con un'accuratezza di generalizzazione simile.
L'articolo esamina l'ipotesi che le reti neurali inizializzate in modo casuale contengono sottoreti che convergono altrettanto velocemente o più velocemente e possono raggiungere la stessa o migliore accuratezza di classificazione
In questo articolo evidenziamo la difficoltà di addestrare reti neurali sparse facendo esperimenti di interpolazione nel paesaggio energetico 
La simmetria dello spazio dei pesi nei paesaggi delle reti neurali dà luogo a un gran numero di selle e sottospazi piatti ad alta densità.
L'articolo ha presentato un metodo a bassa perdita per studiare la funzione di perdita rispetto ai parametri in una rete neurale dal punto di vista della simmetria spazio-peso.
teoria della propagazione del segnale applicata a surrogati continui di reti binarie; inizializzazione contro intuitiva; trucco di riparametrizzazione non utile
Gli autori studiano le dinamiche di allenamento delle reti neurali binarie quando si usano surrogati continui, studiano quali proprietà dovrebbero avere le reti all'inizializzazione per allenarsi al meglio e forniscono consigli concreti sui pesi stocastici all'inizializzazione.
Un'esplorazione approfondita delle reti binarie stocastiche, dei surrogati continui e delle loro dinamiche di allenamento, con approfondimenti su come inizializzare i pesi per ottenere le migliori prestazioni.
Proponiamo un approccio all'apprendimento semi-supervisionato di parser di dipendenze semantiche basato sul framework CRF autoencoder.
Questo articolo si concentra sul parsing semi-supervisionato delle dipendenze semantiche usando il CRF-autoencoder per addestrare il modello in uno stile semi-supervisionato, indicando l'efficacia su compiti di dati etichettati con poche risorse.
DeFINE utilizza una rete profonda, gerarchica e rada con nuove connessioni saltanti per imparare in modo efficiente le embeddings delle parole. 
Questo articolo descrive un nuovo metodo per l'apprendimento di rappresentazioni profonde a livello di parola in modo efficiente utilizzando una struttura gerarchica con skip-connections per l'uso di strati di input e output a bassa dimensione.
Riproduciamo con successo e diamo osservazioni sul confronto con le linee di base di un approccio di meta-apprendimento per la classificazione di pochi scatti che funziona tramite backpropagating attraverso la soluzione di un solutore in forma chiusa.
La riallocazione dinamica dei parametri permette il successo dell'addestramento diretto di reti sparse compatte, e gioca un ruolo indispensabile anche quando conosciamo la rete sparse ottimale a-priori
3 spinte che servono come trampolini di lancio per l'apprendimento esperienziale dei robot del modulo di visione
Indaga sulle prestazioni dei classificatori di immagini e dei rilevatori di oggetti esistenti. 
Tutti i nostri modelli acustici basati su CNNs, tranne i primi due strati, hanno dimostrato un certo grado di specificità della lingua, ma l'addestramento di congelamento ha permesso un trasferimento di successo tra le lingue.
L'articolo misura la trasferibilità delle caratteristiche per ogni strato nei modelli acustici basati su CNN attraverso le lingue, concludendo che gli AM addestrati con la tecnica "freeze training" hanno superato gli altri modelli trasferiti.
Collegare i gradienti di politica entropica della regione di Wasserstein-trust e l'equazione del calore.
L'articolo esplora le connessioni tra l'apprendimento per rinforzo e la teoria del trasporto ottimale quadratico
Gli autori hanno studiato il gradiente della politica con il cambiamento delle politiche limitato da una regione di fiducia della distanza di Wasserstein nell'impostazione del bandito a più braccia, mostrando che nel limite dei piccoli passi, la dinamica della politica è governata dall'equazione del calore (equazione di Fokker-Planck).
La capacità discriminativa di softmax per l'apprendimento di vettori di caratteristiche di oggetti è efficacemente migliorata dalla virtuosità della normalizzazione isotropica sulla distribuzione globale dei punti dati.
Adattiamo Q-learning con bonus UCB-exploration a MDP a orizzonte infinito con ricompense scontate senza accedere a un modello generativo, e migliora il risultato precedentemente conosciuto.
Questo articolo ha considerato un algoritmo di apprendimento Q con una politica di esplorazione UCB per MDP ad orizzonte infinito.
Le perturbazioni possono essere usate per addestrare i pesi di feedback per imparare in reti neurali completamente connesse e convoluzionali
Questo articolo propone un metodo che affronta il problema del "trasporto dei pesi" stimando i pesi per il backward pass usando uno stimatore basato sul rumore 
Identifichiamo alcuni modelli universali (vale a dire, tenendo attraverso le architetture) nel comportamento di diverse perdite surrogate (CE, MSE, perdita 0-1) durante l'addestramento delle reti neurali e presentiamo prove empiriche di supporto.
