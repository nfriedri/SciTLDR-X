Ideamos un escalado de pérdidas adaptativo para mejorar el entrenamiento de precisión mixta que supera los resultados del estado del arte.
Propuesta de un método adaptativo de escalado de pérdidas durante la retropropagación para el entrenamiento de precisión mixta en el que la tasa de escalado se decide automáticamente para reducir el desbordamiento.
Los autores proponen un método para entrenar modelos en precisión FP16 que adopta una forma más elaborada para minimizar el desbordamiento en cada capa de forma simultánea y automática.
Presentamos un enfoque novedoso para aprender a predecir conjuntos con permutación y cardinalidad desconocidas utilizando redes neuronales profundas feed-forward.
Una formulación para aprender la distribución sobre variables de permutación inobservables basada en redes profundas para el problema de predicción de conjuntos.
Comparamos el rendimiento del reconocimiento de objetos en imágenes que se muestrean uniformemente y con tres esquemas de foveación diferentes.
Desarrollamos métodos para entrenar modelos neuronales profundos que son robustos a las perturbaciones adversas y cuya robustez es significativamente más fácil de verificar.
El artículo presenta varias formas de regularizar las redes ReLU simples para optimizar la robustez adversarial, la robustez adversarial demostrable y la velocidad de verificación.
Este trabajo propone métodos para entrenar redes neuronales robustas que puedan verificarse más rápidamente, utilizando métodos de poda para fomentar la dispersión de pesos y la regularización para fomentar la estabilidad de ReLU.
Investigación de cómo BatchNorm provoca una vulnerabilidad adversa y cómo evitarla. 
Este trabajo aborda la vulnerabilidad a las perturbaciones adversas en BatchNorm, y propone una alternativa llamada RobustNorm, que utiliza el reescalado min-max en lugar de la normalización.
Este artículo investiga la razón de la vulnerabilidad de BatchNorm y propone la Normalización Robusta, un método de normalización que logra resultados significativamente mejores bajo una variedad de métodos de ataque.
Nuestra red de imputación variacional-recurrente (V-RIN) tiene en cuenta las características correlacionadas, la dinámica temporal y utiliza además la incertidumbre para aliviar el riesgo de estimaciones sesgadas de los valores perdidos.
Una red de imputación de datos perdidos para incorporar la correlación, las relaciones temporales y la incertidumbre de los datos para el problema de la escasez de datos en las HCE, que produce una mayor AUC en las tareas de clasificación de la tasa de mortalidad.
El artículo presenta un método que combina la VAE y la GRU consciente de la incertidumbre para la imputación secuencial de datos perdidos y la predicción de resultados.
Un método adaptativo para la cuantificación en punto fijo de redes neuronales basado en el análisis teórico y no en la heurística. 
Propone un método de cuantificación de redes neuronales que permite cuantificar los pesos con diferente precisión en función de su importancia, teniendo en cuenta la pérdida.
El artículo propone una técnica para cuantificar los pesos de una red neuronal con profundidad/precisión de bits que varía en función de los parámetros.
Basándonos en la teoría de conjuntos difusos, proponemos un modelo que, dados sólo los tamaños de las diferencias simétricas entre pares de conjuntos múltiples, aprende representaciones de dichos conjuntos múltiples y de sus elementos.
Este trabajo propone una nueva tarea de aprendizaje de conjuntos, la predicción del tamaño de la diferencia simétrica entre conjuntos múltiples, y da un método para resolver la tarea basado en la teoría de conjuntos difusos.
Este trabajo propone un método asistido por el aprendizaje profundo para obtener muestras creíbles de agentes interesados. 
Los autores proponen un marco de elicitación de muestras para el problema de elicitar muestras creíbles de los agentes para distribuciones complejas, sugieren que los marcos neuronales profundos pueden aplicarse en este marco, y conectan la elicitación de muestras y f-GAN.
Este trabajo estudia el problema de elicitación de muestras, proponiendo un enfoque de aprendizaje profundo que se basa en la expresión dual de la divergencia f que escribe como un máximo sobre un conjunto de funciones t.
Aprendizaje de gráficos a secuencias con redes neuronales basadas en la atención
Una arquitectura graph2seq que combina un codificador de grafos que mezcla componentes GGNN y GCN con un codificador de secuencias atencionales, y que muestra mejoras sobre las líneas de base.
Este trabajo propone un modelo de codificador de grafos de extremo a extremo a decodificador de secuencias con un mecanismo de atención intermedio.
Un marco de segmentación de cero disparos para la segmentación de partes de objetos 3D. Modelar la segmentación como un proceso de toma de decisiones y resolverlo como un problema de bandido contextual.
Un método para segmentar nubes de puntos 3D de objetos en partes componentes, centrado en la generalización de agrupaciones de partes a nuevas categorías de objetos no vistas durante el entrenamiento, que muestra un fuerte rendimiento en relación con las líneas de base.
Este trabajo propone un método para la segmentación de partes en nubes de puntos de objetos.
Una nueva perspectiva para recoger la correlación entre nodos basada en las propiedades de difusión.
Una nueva operación de difusión para redes neuronales de grafos que no requiere el cálculo de valores propios y puede propagarse exponencialmente más rápido en comparación con las redes neuronales de grafos tradicionales.
El documento propone hacer frente al problema de la velocidad de difusión introduciendo el paseo balístico.
Proponemos una línea de entrenamiento de supervisión débil basada en el marco de programación de datos para tareas de clasificación, en la que entrenamos un modelo de clasificación basado en BERT y establecemos la nueva SOTA.
Los autores proponen una combinación de BERT y el marco de supervisión débil para abordar el problema de la clasificación de pasajes, obteniendo resultados mejores que el estado del arte totalmente supervisado.
En los problemas reales, descubrimos que las DNN suelen ajustarse a las funciones objetivo desde las frecuencias bajas hasta las altas durante el proceso de entrenamiento.
Este trabajo analiza la pérdida de las redes neuronales en el dominio de Fourier y descubre que las DNN tienden a aprender los componentes de baja frecuencia antes que los de alta frecuencia.
El artículo estudia el proceso de entrenamiento de las NN mediante el análisis de Fourier, concluyendo que las NN aprenden los componentes de baja frecuencia antes que los de alta frecuencia.
Proponemos un codificador-decodificador multirresolución acoplado jerárquicamente para la traducción de grafos a grafos.
Un modelo jerárquico de traducción de grafos a grafos para generar grafos moleculares utilizando subestructuras químicas como bloques de construcción que es totalmente autorregresivo y aprende representaciones multirresolución coherentes, superando los modelos anteriores.
Los autores presentan un método de traducción jerárquica de grafos a gráficos para generar nuevas moléculas orgánicas.
Utilizamos la atención para restringir las redes neuronales equivariantes al conjunto o transformaciones co-ocurrentes en los datos. 
Este trabajo combina la atención con la equidistancia de grupo, específicamente observando el grupo p4m de rotaciones, traslaciones y volteos, y deriva una forma de autoatención que no destruye la propiedad de equidistancia.
Los autores proponen un mecanismo de auto-atención para las redes neuronales de rotación-equivariante que mejora el rendimiento de la clasificación respecto a las redes de rotación-equivariante normales.
Entrenamos un GAN para generar y recuperar columnas vertebrales de proteínas de átomo completo, y mostramos que en algunos casos podemos recuperar las proteínas generadas tras el diseño de la secuencia y el plegado ab initio.
Un modelo generativo para la columna vertebral de las proteínas que utiliza un GAN, una red tipo autoencoder y un proceso de refinamiento, y un conjunto de evaluaciones cualitativas que sugieren resultados positivos.
Este artículo presenta un enfoque integral para generar esqueletos de proteínas utilizando redes generativas adversarias.
El metaaprendizaje para Few Shot learning asume que las tareas de entrenamiento y las tareas de prueba se extraen de la misma distribución. ¿Qué hacer si no es así? Metaaprendizaje con adaptación de dominio a nivel de tarea.
Este trabajo propone un modelo que combina la adaptación de dominio adversarial no supervisada con redes prototípicas que se comporta mejor que las líneas de base de aprendizaje de pocos disparos en tareas de aprendizaje de pocos disparos con cambio de dominio.
Los autores propusieron la adaptación del meta-dominio para abordar el escenario de cambio de dominio en la configuración del meta-aprendizaje, demostrando mejoras de rendimiento en varios experimentos.
Divide, Conquista y Combina es un nuevo esquema de inferencia que se puede realizar sobre los programas probabilísticos con soporte estocástico, es decir, la propia existencia de las variables es estocástica.
Un algoritmo de incrustación de nodos que preserva la comunidad y que da como resultado una detección más eficaz de las comunidades con una agrupación en el espacio incrustado
Un modelo de aprendizaje profundo autorregresivo para generar diversas nubes de puntos.
Un enfoque para generar formas 3D como nubes de puntos que considera la ordenación lexicográfica de los puntos según las coordenadas y entrena un modelo para predecir los puntos en orden.
El artículo introduce un modelo generativo para nubes de puntos que utiliza un modelo autorregresivo tipo RNN de píxeles y un modelo de atención para manejar las interacciones de mayor alcance.
Describe una serie de técnicas de explicabilidad aplicadas a un sencillo controlador de red neuronal utilizado para la navegación.
Este artículo proporciona ideas y explicaciones para el problema de proporcionar explicaciones para un perceptrón multicapa utilizado como controlador inverso para el movimiento del rover, e ideas sobre cómo explicar un modelo de caja negra.
Proponemos un agente de autocontrol para la tarea de navegación de visión y lenguaje.
Un método de navegación de visión+lenguaje que hace un seguimiento del progreso de la instrucción mediante un monitor de progreso y un módulo de co-conducción visual-textual, y que rinde bien en los puntos de referencia estándar.
Este artículo describe un modelo de navegación de visión y lenguaje con una atención visual panorámica y una pérdida auxiliar de seguimiento del progreso, dando resultados del estado del arte.
descubrimiento de eventos para representar el historial del agente en RL
Los autores estudian el problema de RL bajo escenarios parcialmente observados, y proponen una solución que utiliza una FFNN pero que proporciona una representación de la historia, superando a la PPO.
Este trabajo propone una nueva forma de representar la historia pasada como entrada a un agente RL, mostrando un mejor rendimiento que la PPO y una variante RNN de la PPO.
Demostramos que los modelos autorregresivos pueden generar imágenes de alta fidelidad. 
Una arquitectura que utiliza componentes de decodificador, decodificador de tamaño y decodificador de profundidad para abordar el problema del aprendizaje de las dependencias de largo alcance en las imágenes con el fin de obtener imágenes de alta fidelidad.
Este trabajo aborda el problema de la generación a imágenes de alta fidelidad, mostrando con éxito muestras convincentes de Imagenet con una resolución de 128x128 para un modelo de densidad de probabilidad.
Un modelo jerárquico profundo de espacio de estados en el que las transiciones de estado de los objetos correlacionados se coordinan mediante redes neuronales gráficas.
Un modelo jerárquico de variables latentes de procesos dinámicos secuenciales de múltiples objetos cuando cada objeto presenta una estocasticidad significativa.
El artículo presenta un modelo relacional de espacio de estados que simula las transiciones de estado conjuntas de objetos correlacionados que están coordinados jerárquicamente en una estructura de grafos.
Introducimos un nuevo sesgo inductivo que integra las estructuras de árbol en las redes neuronales recurrentes.
Este trabajo propone ON-LSTM, una nueva unidad RNN que integra la estructura de árbol latente en modelos recurrentes y que tiene buenos resultados en el modelado del lenguaje, el análisis sintáctico no supervisado, la evaluación sintáctica dirigida y la inferencia lógica.
Los colectores degenerados que surgen de la no identificabilidad del modelo ralentizan el aprendizaje en las redes profundas; las conexiones de salto ayudan a romper las degeneraciones.
Los autores muestran que las singularidades de eliminación y solapamiento impiden el aprendizaje en las redes neuronales profundas, y demuestran que las conexiones de salto pueden reducir la prevalencia de estas singularidades, acelerando el aprendizaje.
El artículo examina el uso de conexiones de salto en las redes profundas como forma de aliviar las singularidades en la matriz hessiana durante el entrenamiento.
Aprendizaje de representaciones de estado que captan los factores necesarios para el control
Un enfoque del aprendizaje de representación en el contexto del aprendizaje por refuerzo que distingue dos etapas funcionalmente en términos de las acciones que se necesitan para alcanzarlas.
El artículo presenta un método para aprender representaciones en las que la proximidad en la distancia euclidiana representa estados que son alcanzados por políticas similares.
Estudiamos el comportamiento de una CNN a medida que domina nuevas tareas, a la vez que preserva el dominio de las tareas previamente aprendidas
Morty refunde las incrustaciones de palabras precontenidas para: (a) mejorar el rendimiento general de la incrustación (para configuraciones multitarea) o mejorar el rendimiento de una sola tarea, requiriendo sólo un esfuerzo mínimo.
El aumento selectivo de los puntos difíciles de clasificar da lugar a un entrenamiento eficaz.
Los autores estudian el problema de la identificación de las estrategias de submuestreo para el aumento de datos y proponen estrategias basadas en la influencia y la pérdida del modelo, así como la evaluación comparativa empírica de los métodos propuestos.
Los autores proponen utilizar métodos basados en la influencia o en las pérdidas para seleccionar un subconjunto de puntos que se utilizarán para aumentar los conjuntos de datos para el entrenamiento de modelos en los que la pérdida es aditiva sobre los puntos de datos.
Un modelo generativo profundo para moléculas orgánicas que primero genera bloques de construcción de reactivos antes de combinarlos utilizando un predictor de reacciones.
Un modelo molecular generativo que genera moléculas a través de un proceso de dos pasos que proporciona rutas de síntesis de las moléculas generadas, permitiendo a los usuarios examinar la accesibilidad sintética de los compuestos generados.
Mejorar la robustez y la eficiencia energética de una red neuronal profunda utilizando las representaciones ocultas.
Este trabajo tiene como objetivo reducir las clasificaciones erróneas de las redes neuronales profundas de una manera energéticamente eficiente mediante la adición de Células Auxiliares basadas en características relevantes después de una o más capas ocultas para decidir si se termina la clasificación antes de tiempo.
Comprensión de la estructura de la representación de los gráficos de conocimiento utilizando la visión de las incrustaciones de palabras.
Este trabajo intenta comprender la estructura latente que subyace en los métodos de incrustación de grafos de conocimiento, y demuestra que la capacidad de un modelo para representar un tipo de relación depende de las limitaciones de la arquitectura del modelo con respecto a las condiciones de relación.
Este artículo propone un estudio detallado sobre la explicabilidad de los modelos de predicción de enlaces (LP) utilizando una interpretación reciente de las incrustaciones de palabras para proporcionar una mejor comprensión del rendimiento de los modelos LP.
Un nuevo mecanismo de autoatención para la imputación de series temporales multivariadas y geoetiquetadas.
Este trabajo propone el problema de aplicar la red de transformación a los datos espacio-temporales de una manera computacionalmente eficiente, e investiga formas de implementar la atención 3D.
Este trabajo estudia empíricamente la eficacia de los modelos de transformación para la imputación de datos de series temporales a través de las dimensiones de la entrada.
Diseñamos y probamos una REDNET (ResNet Encoder-Decoder) con 8 conexiones de salto para eliminar el ruido de los documentos, incluyendo el desenfoque y las marcas de agua, dando como resultado una red profunda de alto rendimiento para la limpieza de imágenes de documentos. 
Identificamos una familia de técnicas de defensa y mostramos que tanto la compresión determinista con pérdidas como las perturbaciones aleatorias de la entrada conducen a ganancias similares en la robustez.
Este artículo analiza las formas de desestabilizar un determinado ataque adversario, qué hace que las imágenes adversarias no sean robustas, y si es posible que los atacantes utilicen un modelo universal de perturbaciones para hacer que sus ejemplos adversarios sean robustos contra dichas perturbaciones.
El artículo estudia la robustez de los ataques adversarios a las transformaciones de su entrada.
Proporcionamos un método para evaluar los optimizadores que tiene en cuenta el proceso de ajuste de los hiperparámetros.
Introducción de una nueva métrica para capturar la capacidad de ajuste de un optimizador, y una comparación empírica exhaustiva de optimizadores de aprendizaje profundo bajo diferentes cantidades de ajuste de hiperparámetros. 
Este trabajo introduce una medida simple de sintonización que permite comparar optimizadores bajo restricciones de recursos, encontrando que la sintonización de la tasa de aprendizaje de los optimizadores Adam es la más fácil de encontrar configuraciones de hiperparámetros de buen rendimiento.
Introducimos una red neuronal profunda semi-supervisada para aproximar la solución del problema de fase en microscopía electrónica
Word2net es un método novedoso para el aprendizaje de representaciones de palabras en redes neuronales que puede utilizar la información sintáctica para aprender mejores características semánticas.
Este trabajo amplía el SGNS con un cambio de arquitectura, pasando de un modelo de bolsa de palabras a un modelo feedforward, y aporta una nueva forma de regularización al vincular un subconjunto de capas entre diferentes redes asociadas.
Un método para utilizar la combinación no lineal de vectores de contexto para el aprendizaje de la representación vectorial de las palabras, donde la idea principal es sustituir cada incrustación de palabras por una red neuronal.
Utilizando una ventana de 10s de señales fMRI, nuestro modelo GCN identificó 21 condiciones de tarea diferentes del conjunto de datos HCP con una precisión de prueba del 89%.
Inducción eficiente de redes neuronales profundas de bajo rango mediante el entrenamiento SVD con valores singulares dispersos y vectores singulares ortogonales.
Este trabajo introduce un enfoque para la compresión de redes fomentando que la matriz de pesos de cada capa tenga un rango bajo y factorizando explícitamente las matrices de pesos en una factorización tipo SVD para su tratamiento como nuevos parámetros.
Propuesta para parametrizar cada capa de una red neuronal profunda, antes del entrenamiento, con una descomposición matricial de bajo rango, sustituir en consecuencia las convoluciones por dos convoluciones consecutivas, y entrenar después el método descompuesto.
Proponemos un modelo de aprendizaje de pocos disparos que se adapta específicamente a las tareas de regresión
Este trabajo propone un novedoso método de aprendizaje de disparos para problemas de regresión de muestras pequeñas.
Un método que aprende un modelo de regresión con unas pocas muestras y supera a otros métodos.
Presentamos un enfoque novedoso para detectar los píxeles fuera de distribución en la segmentación semántica.
Este trabajo aborda la detección de la falta de distribución para ayudar al proceso de segmentación, y propone un enfoque de entrenamiento de un clasificador binario que distingue los parches de imagen de un conjunto de clases conocido de los de uno desconocido.
El objetivo de este trabajo es detectar píxeles fuera de distribución para la segmentación semántica, y este trabajo utiliza datos de otros dominios para detectar clases indeterminadas para modelar mejor la incertidumbre.
Cuantización precisa, rápida y automatizada de redes neuronales con precisión mixta utilizando el aprendizaje profundo por refuerzo jerárquico
Un método para cuantificar los pesos y las activaciones de las redes neuronales que utiliza el aprendizaje de refuerzo profundo para seleccionar el ancho de bits de los núcleos individuales en una capa y que consigue un mejor rendimiento, o latencia, que los enfoques anteriores.
Este trabajo propone buscar automáticamente esquemas de cuantificación para cada núcleo de la red neuronal, utilizando RL jerárquico para guiar la búsqueda. 
Gaggle, un sistema de análisis visual interactivo para ayudar a los usuarios a navegar de forma interactiva por el espacio de los modelos para las tareas de clasificación y ordenación.
Un nuevo sistema de análisis visual cuyo objetivo es permitir a los usuarios no expertos navegar de forma interactiva por un espacio de modelos mediante un enfoque basado en la demostración.
Un sistema de análisis visual que ayuda a los analistas principiantes a navegar por el espacio de los modelos en la realización de tareas de clasificación y ordenación.
Proponemos una novedosa red de atención con el codificador hybird para resolver el problema de la representación del texto en la clasificación de textos chinos, especialmente los fenómenos lingüísticos sobre las pronunciaciones, como el polifonía y el homofonía.
Este trabajo propone un modelo basado en la atención que consiste en el codificador de palabras y el codificador Pinyin para la tarea de clasificación de textos chinos, y amplía la arquitectura para el codificador de caracteres Pinyin.
Propuesta de una red de atención en la que se consideran tanto la palabra como el pinyin para la representación del chino, con resultados mejorados mostrados en varios conjuntos de datos para la clasificación de textos.
Aprendizaje de imitación multimodal a partir de demostraciones no estructuradas mediante la intención de modelado de redes neuronales estocásticas. 
Un nuevo enfoque basado en el muestreo para la inferencia en modelos de variables latentes que se aplica al aprendizaje de imitación multimodal y funciona mejor que las redes neuronales deterministas y las redes neuronales estocásticas para una tarea real de robótica visual.
Este trabajo muestra cómo aprender varias modalidades mediante el aprendizaje por imitación a partir de datos visuales utilizando redes neuronales estocásticas, y un método para aprender a partir de demostraciones en las que se dan varias modalidades de la misma tarea.
Un nuevo enfoque para construir explicaciones jerárquicas para la clasificación de textos mediante la detección de interacciones de características.
Un nuevo método para proporcionar explicaciones a las predicciones realizadas por los clasificadores de texto que superan las líneas de base en las puntuaciones de importancia a nivel de palabra, y una nueva métrica, la pérdida de cohesión, para evaluar la importancia a nivel de tramo.
Un método de interpretación basado en las interacciones de los rasgos y la puntuación de la importancia de los rasgos en comparación con las contribuciones de los rasgos independientes.
Hacemos que las capas convolucionales funcionen más rápido potenciando y suprimiendo dinámicamente los canales en el cálculo de las características.
Un método de refuerzo y supresión de características para la poda dinámica de canales que predice la importancia de cada canal y luego utiliza una función afín para amplificar/suprimir la importancia del canal.
Propuesta de un método de poda de canales para seleccionar dinámicamente los canales durante las pruebas.
Las redes neuronales pueden ser predefinidas para tener una conectividad dispersa sin que el rendimiento se vea afectado.
Este trabajo examina los patrones de conexión dispersa en las capas superiores de las redes convolucionales de clasificación de imágenes, e introduce una heurística para distribuir las conexiones entre las ventanas/grupos y una medida llamada dispersión para construir máscaras de conectividad.
Propuesta para reducir el número de parámetros aprendidos por una red profunda estableciendo pesos de conexión dispersos en las capas de clasificación, e introducción de un concepto de "dispersión".
Proporcionamos un punto de referencia completo, riguroso y coherente para evaluar la robustez adversarial de los modelos de aprendizaje profundo.
Este artículo presenta una evaluación de diferentes tipos de modelos de clasificación bajo varios métodos de ataque adversario.
Un estudio empírico a gran escala en el que se comparan diferentes técnicas de ataque y defensa adversarial, y se utilizan curvas de precisión frente a presupuesto de perturbación y de precisión frente a fuerza de ataque para evaluar los ataques y las defensas.
Proponemos una modificación de las redes neuronales artificiales tradicionales motivada por la biología de las neuronas para permitir que la forma de la función de activación dependa del contexto.
Un método para escalar las activaciones de una capa de neuronas en una RNA en función de las entradas a esa capa que reporta mejoras por encima de las líneas de base.
Introducción de un cambio de arquitectura para las neuronas básicas de una red neuronal, y la idea de multiplicar la salida de la combinación lineal de las neuronas por un modulador antes de alimentar la función de activación.
Identificamos el problema del olvido en el ajuste fino de los modelos NLG preentrenados y proponemos la estrategia de revisión mixta para abordarlo.
Este artículo analiza el problema del olvido en el marco del preentrenamiento-ajuste desde la perspectiva de la sensibilidad al contexto y la transferencia de conocimientos, y propone una estrategia de ajuste fino que supera el método de decaimiento del peso.
Estudio del problema del olvido en el marco del preentrenamiento-ajuste, específicamente en tareas de generación de respuestas de diálogo, y propuesta de una estrategia de revisión de la mezcla para aliviar el problema del olvido.
La mejora del modelado de sistemas complejos utiliza la composición de modelos neurales/dominio híbridos, nuevas funciones de pérdida por descorrelación y conjuntos de pruebas extrapolativas 
Este trabajo realiza experimentos para comparar las predicciones extrapolativas de varios modelos híbridos que componen modelos físicos, redes neuronales y modelos estocásticos, y aborda el reto de que la dinámica no modelada sea un cuello de botella.
Este artículo presenta enfoques para combinar redes neuronales con modelos no NN para predecir el comportamiento de sistemas físicos complejos.
Aprendemos las puntuaciones densas y el modelo de dinámica como priores a partir de los datos de exploración y los utilizamos para inducir una buena política en las nuevas tareas en condición de disparo cero.
Este artículo analiza la generalización del tiro cero en nuevos entornos y propone un enfoque con resultados en Grid-World, Super Mario Bros y 3D Robotics.
Un método que tiene como objetivo aprender priores agnósticos de la tarea para la generalización de cero disparos, con la idea de emplear un enfoque de modelado en la parte superior del marco RL basado en el modelo.
Analizar los mecanismos subyacentes del colapso de la varianza del SVGD en altas dimensiones.
Repensar la generalización requiere revisar viejas ideas: enfoques de mecánica estadística y comportamiento de aprendizaje complejo
Los autores sugieren que las ideas de la mecánica estadística ayudarán a comprender las propiedades de generalización de las redes neuronales profundas, y ofrecen un enfoque que proporciona sólidas descripciones cualitativas de los resultados empíricos relativos a las redes neuronales profundas y los algoritmos de aprendizaje.
Un conjunto de ideas relacionadas con la comprensión teórica de las propiedades de generalización de las redes neuronales multicapa, y una analogía cualitativa entre los comportamientos en el aprendizaje profundo y los resultados del análisis físico estadístico cuantitativo de las redes neuronales de una y dos capas.
Presentamos el softmax doblemente disperso, la mezcla dispersa de expertos dispersos, para mejorar la eficiencia de la inferencia del softmax mediante la explotación de la jerarquía de superposición de dos niveles. 
Este trabajo propone una aproximación rápida al cálculo de softmax cuando el número de clases es muy grande.
Este trabajo propone una mezcla dispersa de expertos dispersos que aprende una jerarquía de clases de dos niveles para una inferencia softmax eficiente.
Exploramos el uso de datos de seguimiento ocular recogidos de forma pasiva para reducir la cantidad de datos etiquetados necesarios durante el entrenamiento.
Un método para utilizar la información de la mirada para reducir la complejidad de la muestra de un modelo y el esfuerzo de etiquetado necesario para obtener un rendimiento objetivo, con resultados mejorados en muestras de tamaño medio y tareas más difíciles.
Un método para incorporar las señales de la mirada a las CNNs estándar para la clasificación de imágenes, añadiendo un término de función de pérdida basado en la diferencia entre el Mapa de Activación de Clases del modelo y el mapa construido a partir de la información de seguimiento ocular.
Aplicamos la corrección de pérdidas a las redes neuronales gráficas para entrenar un modelo más robusto al ruido.
Este trabajo introduce la corrección de pérdidas para que las redes neuronales de grafos se enfrenten al ruido simétrico de las etiquetas de los grafos, centrándose en una tarea de clasificación de grafos.
Este trabajo propone el uso de una pérdida de corrección de ruido en el contexto de las redes neuronales de grafos para hacer frente a las etiquetas ruidosas.
Utilizamos la coatención de gráficos en un sistema de entrenamiento de gráficos emparejados para la clasificación y regresión de gráficos.
Este trabajo inyecta un mecanismo de coatención multicabezal en GCN que permite que un fármaco atienda a otro durante la predicción del efecto secundario del fármaco.
Un método para ampliar el aprendizaje basado en grafos con una capa coatencional, que supera a otros anteriores en una tarea de clasificación de grafos por pares.
El subtitulado de imágenes como un entrenamiento GAN condicional con arquitecturas novedosas, también estudian dos métodos de entrenamiento GAN discretos. 
Un modelo GAN mejorado para el subtitulado de imágenes que propone un subtitulador LSTM consciente del contexto, introduce un discriminador co-atento más fuerte con mejor rendimiento y utiliza SCST para el entrenamiento del GAN.
Explotar la curvatura para que los métodos MCMC converjan más rápido que el estado del arte.
Keras para redes neuronales infinitas.
Proponemos una nueva función de pérdida que logra resultados de vanguardia en la detección de fuera de la distribución con la exposición de valores atípicos tanto en tareas de clasificación de imágenes como de textos.
Este artículo aborda los problemas de la detección de los valores fuera de distribución y la calibración del modelo mediante la adaptación de la función de pérdida de la técnica Outlier Exposure, con resultados que demuestran un mayor rendimiento con respecto a OE en puntos de referencia de visión y texto y una mejor calibración del modelo.
Propuesta de una nueva función de pérdida para entrenar la red con exposición a valores atípicos que conduce a una mejor detección de OOD en comparación con las funciones de pérdida simples que utilizan la divergencia KL.
El preentrenamiento agnóstico a la tarea puede moldear el paisaje de atractores de la RNN y formar diversos sesgos inductivos para diferentes tareas de navegación   
Este trabajo estudia las representaciones internas de las redes neuronales recurrentes entrenadas en tareas de navegación, y encuentra que las RNN preentrenadas para utilizar la integración de trayectorias contienen atractores continuos 2D mientras que las RNN preentrenadas para la memoria de puntos de referencia contienen atractores discretos.
Este trabajo explora cómo el preentrenamiento de redes recurrentes en diferentes objetivos de navegación confiere diferentes beneficios para la resolución de tareas posteriores, y muestra cómo el diferente preentrenamiento se manifiesta como diferentes estructuras dinámicas en las redes después del preentrenamiento.
Verificación de redes neuronales para propiedades temporales y modelos de generación de secuencias
Este artículo amplía la propagación de límites de intervalo a la computación recurrente y a los modelos autorregresivos, introduce y amplía la Lógica Temporal de Señales para especificar restricciones temporales, y proporciona una prueba de que la LTS con propagación de límites puede garantizar que los modelos neuronales se ajusten a la especificación temporal.
Una forma de entrenar regresores de series temporales de forma verificable con respecto a un conjunto de reglas definidas por la lógica temporal de la señal, y trabajar en la derivación de reglas de propagación de límites para el lenguaje STL.
Proponemos una solución universal de redes neuronales para derivar automáticamente arquitecturas de NN eficaces para datos tabulares.
Un nuevo procedimiento de entrenamiento de redes neuronales, diseñado para datos tabulares, que busca aprovechar los clusters de características extraídos de los GBDT.
Propuesta de un algoritmo de aprendizaje automático híbrido que utiliza árboles de decisión con impulso de gradiente y redes neuronales profundas, con una dirección de investigación prevista en datos tabulares.
Sistema de aprendizaje de reglas probabilísticas mediante la inferencia elevada
Un modelo de aprendizaje de reglas probabilísticas para automatizar la cumplimentación de bases de datos probabilísticas que utiliza AMIE+ y la inferencia levantada para ayudar a la eficiencia computacional.
Proponemos el primer modelo neuronal no autorregresivo para el seguimiento del estado del diálogo (DST), logrando la precisión SOTA (49,04%) en el benchmark MultiWOZ2.1, y reduciendo la latencia de la inferencia en un orden de magnitud.
Un nuevo modelo para la tarea DST que reduce la complejidad del tiempo de inferencia con un decodificador no autorregresivo, obtiene una precisión DST competitiva y muestra mejoras sobre otras líneas de base.
Propuesta de un modelo capaz de seguir los estados del diálogo de forma no recursiva.
Una novedosa arquitectura de red para realizar Zoom 3D profundo o primeros planos.
Un método para crear una "imagen ampliada" para una imagen de entrada dada, y una novedosa pérdida de reconstrucción por retroproyección que permite a la red aprender la estructura 3D subyacente y mantener una apariencia natural.
Un algoritmo para sintetizar el comportamiento del zoom 3D cuando la cámara se mueve hacia delante, una estructura de red que incorpora la estimación de la disparidad en un marco de GANs para sintetizar nuevas vistas, y una propuesta de nueva tarea de visión por ordenador.
Un refinamiento cuantitativo del teorema de aproximación universal mediante un enfoque algebraico.
Los autores derivan las pruebas de la propiedad de aproximación universal algebraicamente y afirman que los resultados son generales para otros tipos de redes neuronales y aprendices similares.
Una nueva prueba de la versión de Leshno de la propiedad de aproximación universal para redes neuronales, y nuevos conocimientos sobre la propiedad de aproximación universal.
Marco modular para la clasificación de documentos y técnica de agregación de datos para hacer el marco robusto a varias distorsiones, y el ruido y centrarse sólo en las palabras importantes. 
Los autores consideran el entrenamiento de una clasificación de texto basada en RNN cuando hay una restricción de recursos en la predicción en tiempo de prueba, y proporcionan un enfoque que utiliza un mecanismo de enmascaramiento para reducir las palabras/frases/sentencias utilizadas en la predicción, seguido de un clasificador para manejar esos componentes.
Permitir la conexión parcial de canales en superredes para regularizar y acelerar la búsqueda de arquitecturas diferenciables
Una extensión del método de búsqueda de arquitecturas neuronales DARTS que aborda su defecto de inmenso coste de memoria utilizando un subconjunto aleatorio de canales y un método para normalizar los bordes.
Este trabajo propone mejorar el DARTS en términos de eficiencia de entrenamiento, a partir de una gran sobrecarga de memoria y computación, y propone un DARTS parcialmente conectado con conexión parcial de canales y normalización de bordes.
Los agentes interactúan (hablan, actúan) y pueden alcanzar objetivos en un mundo rico con un lenguaje diverso, salvando la distancia entre la cháchara y el diálogo orientado a objetivos.
Este trabajo estudia una tarea de diálogo multiagente en la que el agente que aprende tiene como objetivo generar acciones en lenguaje natural que provoquen una acción concreta del otro agente, y muestra que los agentes RL pueden alcanzar niveles de finalización de la tarea más altos que los de aprendizaje por imitación.
Este artículo explora la configuración de diálogos orientados a objetivos con aprendizaje por refuerzo en un juego de aventuras de texto de fantasía y observa que los enfoques de RL superan a los modelos de aprendizaje supervisado.
Un nuevo método parcialmente agnóstico para la evaluación de políticas fuera de horizonte infinito con múltiples políticas de comportamiento conocidas o desconocidas.
Una política de mezcla estimada que toma ideas de los estimadores de horizonte infinito de evaluación de políticas y del muestreo de importancia de regresión para la ponderación de la importancia, y las extiende a muchas políticas y a políticas desconocidas.
Un algoritmo para resolver la evaluación de políticas fuera del horizonte infinito con políticas de comportamiento múltiple mediante la estimación de una política mixta bajo regresión, y la prueba teórica de que una relación de política estimada puede reducir la varianza.
Introducimos una arquitectura neuronal más eficiente para la inferencia amortizada, que combina flujos normalizadores continuos y condicionales utilizando una elección de principio de la estructura de dispersión.
Demostramos que ENAS con optimización ES en RL es altamente escalable, y lo utilizamos para compactar las políticas de las redes neuronales mediante el reparto de pesos.
Los autores construyen políticas de aprendizaje por refuerzo con muy pocos parámetros comprimiendo una red neuronal feed-forward, forzándola a compartir pesos, y utilizando un método de aprendizaje por refuerzo para aprender el mapeo de los pesos compartidos.
Este trabajo combina ideas de los métodos ENAS y ES para la optimización, e introduce la arquitectura de red cromática, que particiona los pesos de la red RL en subgrupos vinculados.
Presentamos Deep SAD, un método profundo para la detección general de anomalías semisupervisada que aprovecha especialmente las anomalías etiquetadas.
Un nuevo método para encontrar datos anómalos, cuando se dan algunas anomalías etiquetadas, que aplica la pérdida derivada de la teoría de la información basada en que los datos normales suelen tener menor entropía que los anómalos.
Propuesta de un marco de detección de anomalías en entornos en los que se dispone de datos no etiquetados, de datos positivos etiquetados y de datos negativos etiquetados, y propuesta para abordar la EA semisupervisada desde una perspectiva de la teoría de la información.
Este trabajo analiza la dinámica de entrenamiento y los puntos críticos del entrenamiento de una red ReLU profunda mediante SGD en el entorno profesor-alumno. 
Estudio de la sobreparametrización en redes ReLU multicapa alumno-profesor, una parte teórica sobre los puntos críticos del SGD para el entorno profesor-alumno, y una parte heurística y empírica sobre la dinámica del algoritmo del SGD en función de las redes de profesores.
Bajo ciertas condiciones en las transformaciones lineales de entrada y salida, tanto GD como SGD pueden lograr la convergencia global para el entrenamiento de ResNets lineales profundas.
Los autores estudian la convergencia del descenso de gradiente en el entrenamiento de redes residuales lineales profundas, y establecen una convergencia global de GD/SGD y tasas de convergencia lineal de SG/SGD.
Estudio de las propiedades de convergencia de GD y SGD en redes lineales profundas, y prueba de que bajo ciertas condiciones en las transformaciones de entrada y salida y con inicialización cero, GD y SGD convergen a mínimos globales.
Analizamos el proceso de entrenamiento de las redes profundas y mostramos que parten de un aprendizaje rápido de ejemplos clasificables poco profundos y generalizan lentamente a puntos de datos más difíciles.
Aprendizaje de MRFs de variables latentes profundas con un objetivo de punto de silla derivado de la aproximación de la función de partición de Bethe.
Un método para el aprendizaje de MRF profundo de variables latentes con un objetivo de optimización que utiliza la energía libre de Bethe, que también resuelve las restricciones subyacentes de las optimizaciones de energía libre de Bethe.
Un objetivo para el aprendizaje de MRFs de variables latentes basado en la energía libre de Bethe y la inferencia amortizada, diferente de la optimización del ELBO estándar.
Un marco general para la generación de explicaciones mediante la lógica.
Este artículo investiga la generación de explicaciones desde el punto de vista del KR y lleva a cabo experimentos que miden el tamaño de las explicaciones y el tiempo de ejecución en fórmulas aleatorias y fórmulas de una instancia de Blocksworld.
Este documento ofrece una perspectiva sobre las explicaciones entre dos bases de conocimiento, y es paralelo a los trabajos sobre reconciliación de modelos en la literatura de planificación.
Las redes neuronales profundas y estrechas convergerán a estados medios o medianos erróneos de la función objetivo dependiendo de la pérdida con alta probabilidad.
Este trabajo estudia los modos de fallo de las redes profundas y estrechas, centrándose en los modelos más pequeños posibles para los que se produce el comportamiento no deseado.
Este trabajo muestra que el entrenamiento de las redes neuronales profundas ReLU convergerá a un clasificador constante con alta probabilidad sobre la inicialización aleatoria si las anchuras de las capas ocultas son demasiado pequeñas.
Proponemos el entrenamiento MMA para maximizar directamente el margen del espacio de entrada con el fin de mejorar la robustez del adversario, principalmente al eliminar el requisito de especificar un límite de distorsión fijo.
Un enfoque de entrenamiento adversarial basado en el margen adaptativo para entrenar DNNs robustas, maximizando el margen más corto de las entradas a la frontera de decisión, que hace posible el entrenamiento adversarial con grandes perturbaciones.
Se introduce un método de aprendizaje robusto contra ataques adversarios en el que se maximiza directamente el margen del espacio de entrada y una variante softmax del margen máximo.
Proponemos un método para la detección de anomalías con GANs buscando en el espacio latente del generador buenas representaciones de la muestra.
Los autores proponen el uso de GAN para la detección de anomalías, un método basado en el gradiente de ascenso para actualizar iterativamente las representaciones latentes, y una novedosa actualización de los parámetros de los generadores.
Un enfoque basado en GAN para hacer la detección de anomalías para los datos de la imagen donde el espacio latente del generador se explora para encontrar una representación para una imagen de prueba.
El comportamiento transitorio de los algoritmos MCMC basados en el gradiente y la inferencia variacional es más similar de lo que se podría pensar, lo que pone en duda la afirmación de que la inferencia variacional es más rápida que MCMC.
Un marco convolucional de grafos basado en la composición para grafos multirrelacionales.
Los autores desarrollan GCN en grafos multirrelacionales y proponen CompGCN, que aprovecha los conocimientos de las incrustaciones de grafos de conocimiento y aprende representaciones de nodos y relaciones para aliviar el problema de la sobreparametrización.
Este artículo introduce un marco GCN para grafos multirrelacionales y generaliza varios enfoques existentes para la incrustación de grafos de conocimiento en un marco.
Cuantizamos totalmente el Transformer a 8 bits y mejoramos la calidad de la traducción en comparación con el modelo de precisión total.
Un método de cuantificación de 8 bits para cuantificar el modelo de traducción automática Transformer, en el que se propone utilizar una cuantificación uniforme min-max durante la inferencia y pesos en cubos antes de la cuantificación para reducir el error de cuantificación.
Un método para reducir el espacio de memoria requerido mediante una técnica de cuantificación, enfocado a reducirlo para la arquitectura Transformer.
Latent Embedding Optimization (LEO) es un novedoso meta-aprendizaje basado en el gradiente con un rendimiento de vanguardia en las desafiantes tareas de clasificación de miniImageNet y tieredImageNet de 1 y 5 disparos.
Un nuevo marco de meta-aprendizaje que aprende el espacio latente dependiente de los datos, realiza una rápida adaptación en el espacio latente, es eficaz para el aprendizaje de pocos disparos, tiene una inicialización dependiente de la tarea para la adaptación, y funciona bien para la distribución de tareas multimodales.
Este artículo propone un método de optimización de incrustación latente para el meta-aprendizaje, y afirma que la contribución es desacoplar las técnicas de meta-aprendizaje basadas en la optimización del espacio de alta dimensión de los parámetros del modelo.
Los sesgos inductivos relacionales mejoran la capacidad de generalización fuera de la distribución en agentes de aprendizaje por refuerzo sin modelo
Una arquitectura de red relacional compartida para parametrizar la red de actores y críticos, centrada en algoritmos de actor-crítico de ventaja distribuida, que mejora las técnicas de refuerzo profundo sin modelo con conocimiento relacional sobre el entorno para que los agentes puedan aprender representaciones de estado interpretables.
Un análisis cuantitativo y cualitativo y una evaluación del mecanismo de auto-atención combinado con la red de relaciones en el contexto de la RL sin modelo.
Proponemos un modelo generativo simple para la traducción de imágenes sin supervisión y la detección de saliencia.
Definimos un concepto de redes neuronales profundas paralelas al modelo de capas, para las que las capas operan en paralelo, y proporcionamos una caja de herramientas para diseñar, entrenar, evaluar e interactuar en línea con estas redes.
Una caja de herramientas acelerada por la GPU para la actualización paralela de neuronas, escrita en Theano, que soporta diferentes órdenes de actualización en redes recurrentes y redes con conexiones que se saltan capas. 
Una nueva caja de herramientas para el aprendizaje y la evaluación de redes neuronales profundas, y una propuesta de cambio de paradigma de las redes secuenciales por capas a las redes paralelas por capas.
Un método de defensa adversarial que une la robustez de las redes neuronales profundas con la estabilidad de Lyapunov
Los autores formulan el entrenamiento de las NN como la búsqueda de un controlador óptimo para un sistema dinámico discreto, lo que les permite utilizar el método de aproximaciones sucesivas para entrenar una NN de forma que sea más robusta a los ataques de los adversarios.
Este trabajo utiliza la visión teórica de una red neuronal como una EDO discretizada para desarrollar una teoría de control robusto dirigida a entrenar la red mientras se refuerza la robustez.
Proponemos un esquema de reponderación sencillo pero eficaz para los GCN, apoyado teóricamente por la teoría del campo medio.
Un método, conocido como DrGCN, para reponderar las diferentes dimensiones de las representaciones de los nodos en las redes convolucionales de grafos reduciendo la varianza entre dimensiones.
Nuestro enfoque es el primer intento de aprovechar un modelo de variable latente secuencial para la selección de conocimientos en el diálogo basado en el conocimiento de varias vueltas. Consigue el nuevo rendimiento del estado del arte en la prueba de referencia Wizard of Wikipedia.
Un modelo secuencial de variables latentes para la selección de conocimiento en la generación de diálogos que extiende el modelo de atención posterior al problema de selección de conocimiento latente y logra rendimientos más altos que los modelos anteriores del estado del arte.
Una arquitectura novedosa para la selección de diálogos multivuelta basados en el conocimiento, que se sitúa en el estado del arte en conjuntos de datos de referencia relevantes, y que obtiene una mayor puntuación en las evaluaciones humanas.
Proponemos un método de meta-aprendizaje que amortiza eficientemente la inferencia variacional jerárquica a través de los episodios de entrenamiento.
Una adaptación a los modelos de tipo MAML que da cuenta de la incertidumbre posterior en las variables latentes específicas de la tarea mediante el empleo de la inferencia variacional para los parámetros específicos de la tarea en una visión jerárquica bayesiana de MAML.
Los autores consideran el meta-aprendizaje para aprender una prioridad sobre los pesos de las redes neuronales, realizado a través de la inferencia variacional amortizada.
Representación/destilación del conocimiento maximizando la información mutua entre el profesor y el alumno
Este trabajo combina un objetivo contrastivo que mide la información mutua entre las representaciones aprendidas por las redes del profesor y del alumno para la destilación de modelos, y propone un modelo que mejora las alternativas existentes en las tareas de destilación.
Las redes que aprenden con conexiones de retroalimentación y reglas de plasticidad local pueden ser optimizadas para utilizar el meta aprendizaje.
Las CNN con conexiones laterales de inspiración biológica aprendidas de forma no supervisada son más robustas a las entradas ruidosas. 
Este trabajo pretende desarrollar un enfoque de representación de productos tensoriales para aplicaciones de procesamiento del lenguaje natural basadas en el aprendizaje profundo.
Estudiamos la robustez certificada para las predicciones top-k a través del suavizado aleatorio bajo ruido gaussiano y derivamos un límite de robustez ajustado en la norma L_2.
Este artículo amplía el trabajo sobre la deducción de un radio certificado utilizando el suavizado aleatorio, y muestra el radio en el que se certifica un clasificador suavizado bajo perturbaciones gaussianas para las k mejores predicciones.
Este artículo se basa en la técnica de suavizado aleatorio para la predicción top-1, y pretende proporcionar una certificación sobre las predicciones top-k.
Presentamos priores estructurados para el aprendizaje no supervisado de representaciones desentrañadas en VAE que mitigan significativamente el compromiso entre el desentrañamiento y la pérdida de reconstrucción.
Un marco general para utilizar la familia de distribuciones anidadas L^p como la prioridad para el vector de código de VAE, demostrando un mayor MIG.
Los autores señalan problemas en los enfoques actuales de VAE y proporcionan una nueva perspectiva sobre el compromiso entre la reconstrucción y la ortogonalización para VAE, beta-VAE y beta-TCVAE.
Generalizamos los bloques residuales a los bloques en tándem, que utilizan mapas lineales arbitrarios en lugar de atajos, y mejoran el rendimiento respecto a las ResNets.
Este trabajo realiza un análisis de las conexiones de atajo en las arquitecturas tipo ResNet, y propone sustituir los atajos de identidad por una alternativa convolucional denominada bloque tándem.
Este artículo investiga el efecto de sustituir las conexiones de salto de identidad por conexiones de salto convolucionales entrenables en ResNet y descubre que el rendimiento mejora.
Presentamos un nuevo marco para adaptar los métodos de tipo Adam, concretamente AdamT, para incluir la información de la tendencia cuando se actualizan los parámetros con el tamaño de paso adaptativo y los gradientes.
Un nuevo tipo de variante de Adam que utiliza el método lineal de Holt para calcular el impulso de primer y segundo orden suavizado en lugar de utilizar la media ponderada exponencial.
Un método para explicar un clasificador, generando una perturbación visual de una imagen exagerando o disminuyendo las características semánticas que el clasificador asocia con una etiqueta objetivo.
Un modelo que, cuando se le da una entrada de consulta a una caja negra, pretende explicar el resultado proporcionando variaciones plausibles y progresivas a la consulta que pueden dar lugar a un cambio en el resultado.
Un método para explicar la salida de la clasificación de imágenes en caja negra, que genera una perturbación gradual de las salidas en respuesta a consultas de entrada gradualmente perturbadas.
Presentamos un enfoque orientado a la influencia para construir explicaciones sobre el comportamiento de las redes convolucionales profundas, y mostramos cómo se puede utilizar para responder a un amplio conjunto de preguntas que no pudieron ser abordadas por trabajos anteriores.
Una forma de medir la influencia que satisface ciertos axiomas, y una noción de influencia que puede utilizarse para identificar qué parte de la entrada es más influyente para la salida de una neurona en una red neuronal profunda.
Este trabajo propone medir la influencia de las neuronas individuales con respecto a una cantidad de interés representada por otra neurona.
Destacamos una técnica por la que los sistemas de procesamiento del lenguaje natural pueden aprender una nueva palabra a partir del contexto, lo que les permite ser mucho más flexibles.
Una técnica para explotar el conocimiento previo para aprender representaciones de incrustación para nuevas palabras con datos mínimos.
Una arquitectura de memoria que soporta el razonamiento inferencial.
Este artículo propone cambios en la arquitectura de la red de memoria End2End, introduce una nueva tarea de inferencia asociativa emparejada que la mayoría de los modelos existentes tienen dificultades para resolver, y muestra que su arquitectura propuesta resuelve mejor la tarea.
Una nueva tarea (inferencia de asociación por pares) extraída de la psicología cognitiva, y la propuesta de una nueva arquitectura de memoria con características que permiten un mejor rendimiento en la tarea de asociación por pares.
Las convoluciones separables en profundidad mejoran la traducción automática neural: cuanto más separables, mejor.
Este trabajo propone el uso de capas de convolución separables en profundidad en un modelo de traducción automática neuronal totalmente convolucional, e introduce una nueva capa de convolución súper separable que reduce aún más el coste computacional.
El entrenamiento GAN no saturado minimiza eficazmente una divergencia f inversa tipo KL.
Este trabajo propone una expresión útil de la clase de divergencias f, investiga las propiedades teóricas de las divergencias f populares a partir de herramientas recientemente desarrolladas, e investiga los GAN con el esquema de entrenamiento no saturado.
Introducimos un novedoso método de representación de textos que permite aplicar clasificadores de imágenes a problemas de clasificación de textos, y aplicamos el método a la desambiguación de nombres de inventores.
Un método para mapear un par de información textual en una imagen RGB 2D que puede ser alimentada a redes neuronales convoucionales 2D (clasificadores de imágenes).
Los autores consideran el problema de la desambiguación de los nombres de los inventores de patentes y proponen construir una representación de páginas de imágenes de las dos cadenas de nombres para compararlas y aplicar un clasificador de imágenes.
Hemos propuesto el modelo "Difference-Seeking Generative Adversarial Network" (DSGAN) para aprender la distribución del objetivo, que es difícil de recopilar en los datos de entrenamiento.
Este trabajo presenta DS-GAN, cuyo objetivo es aprender la diferencia entre dos distribuciones cualesquiera cuyas muestras son difíciles o imposibles de recoger, y muestra su eficacia en tareas de aprendizaje semi-supervisado y de entrenamiento adversarial.
Este trabajo considera el problema de aprender un GAN para capturar una distribución objetivo con sólo muy pocas muestras de entrenamiento de esa distribución disponibles.
Un método general que mejora el rendimiento de la traducción de imágenes del marco GAN mediante el uso de un discriminador incrustado en la atención
Un mecanismo de retroalimentación en el marco del GAN que mejora la calidad de las imágenes generadas en la traducción de imagen a imagen, y cuyo discriminador emite un mapa que indica dónde debe centrarse el generador para que sus resultados sean más convincentes.
Propuesta de un GAN con un discriminador basado en la atención para la traducción I2I que proporciona la probabilidad de real/falsa y un mapa de atención que refleja la saliencia para la generación de imágenes.
Proponemos un nuevo conjunto de datos para investigar el problema de vinculación bajo la tabla semiestructurada como premisa
Este artículo propone un nuevo conjunto de datos para la verificación de hechos basados en tablas e introduce métodos para la tarea.
Los autores plantean el problema de la verificación de hechos con fuentes de datos semiestructuradas, como las tablas, crean un nuevo conjunto de datos y evalúan modelos de referencia con variaciones.
Desarrollamos una arquitectura de emparejamiento de grafos en profundidad que refina las correspondencias iniciales para alcanzar un consenso de vecindad.
Un marco para responder a preguntas de correspondencia de grafos que consiste en incrustaciones de nodos locales con un paso de refinamiento de paso de mensajes.
Una arquitectura basada en GNN de dos etapas para establecer correspondencias entre dos grafos que rinde bien en tareas del mundo real de correspondencia de imágenes y alineación de entidades de grafos de conocimiento.
Este trabajo extiende la prueba de la densidad de las redes neuronales en el espacio de las funciones continuas (o incluso medibles) en los espacios euclidianos a las funciones en conjuntos compactos de medidas de probabilidad. 
Este trabajo investiga las propiedades de aproximación de una familia de redes neuronales diseñadas para abordar problemas de aprendizaje multi-instancia, y muestra que los resultados para arquitecturas estándar de una capa se extienden a estos modelos.
Este trabajo generaliza el teorema de aproximación universal a funciones reales en el espacio de medidas.
Un nuevo marco para explicaciones de predicciones dependientes y libres de contexto
Los autores amplían el método de atribución local lineal LIME para interpretar los modelos de caja negra, y proponen un método para discernir entre las interacciones dependientes del contexto y las libres de él.
Un método que puede proporcionar explicaciones jerárquicas para un modelo, incluyendo explicaciones dependientes y libres de contexto mediante un algoritmo de interpretación local.
El ajuste después de la cuantificación iguala o supera a las redes de precisión total del estado de la técnica tanto en la cuantificación de 8 como de 4 bits.
Este trabajo propone mejorar el rendimiento de los modelos de baja precisión realizando la cuantificación en los modelos preentrenados, utilizando lotes de gran tamaño y empleando un recocido de velocidad de aprendizaje adecuado con un tiempo de entrenamiento más largo.
Un método de cuantificación de bits bajo para permitir la inferencia en un hardware eficiente que logra una precisión total en ResNet50 con pesos y activaciones de 4 bits, basado en las observaciones de que el ajuste fino a baja precisión introduce ruido en el gradiente.
Dos métodos basados en el Análisis de Similitud de Representación (RSA) y los Núcleos de Árbol (TK) que cuantifican directamente la intensidad de la información codificada en los patrones de activación neural que corresponde a la información representada por las estructuras simbólicas.
Este trabajo introduce un marco para el aprendizaje de la representación eficiente de los datos mediante el muestreo adaptativo en el espacio latente.
Un método para la selección secuencial y adaptativa de los ejemplos de entrenamiento que se presentarán al algoritmo de entrenamiento, donde la selección ocurre en el espacio latente basado en la elección de muestras en la dirección del gradiente de la pérdida.
Un método para seleccionar eficazmente las muestras duras durante el entrenamiento de las redes neuronales, conseguido mediante un autocodificador variacional que codifica las muestras en un espacio latente.
Un método basado en el entrenamiento adversarial para desentrañar dos conjuntos complementarios de variaciones en un conjunto de datos donde sólo uno de ellos está etiquetado, probado en el estilo frente al contenido en las ilustraciones de anime.
Un método de generación de imágenes que combina GANs condicionales y VAEs condicionales que genera imágenes de anime de alta fidelidad con varios estilos de varios artistas. 
Propuesta de un método para el aprendizaje de representaciones disociadas de estilo (artista) y contenido en el anime.
Introducimos una regularización de suavidad para los núcleos convolucionales de las CNN que puede ayudar a mejorar la robustez de los adversarios y conducir a gradientes perceptualmente alineados
Este trabajo propone un nuevo esquema de regularización que anima a los núcleos convolucionales a ser más suaves, argumentando que la reducción de la dependencia de las redes neuronales en los componentes de alta frecuencia ayuda a la robustez contra los ejemplos adversos. 
Los autores proponen un método para el aprendizaje de kernels convolucionales más suaves, concretamente, un regularizador que penaliza los grandes cambios entre píxeles consecutivos del kernel con la intuición de penalizar el uso de componentes de entrada de alta frecuencia.
Investigamos los comportamientos en muestras grandes de las estimaciones del valor Q y propusimos una estrategia de exploración eficiente que se basa en la estimación de las discrepancias relativas entre las estimaciones de Q. 
Entrenamos incrustaciones de palabras basadas en la vinculación en lugar de la similitud, prediciendo con éxito la vinculación léxica.
El artículo presenta un algoritmo de incrustación de palabras para la vinculación léxica que sigue el trabajo de Henderson y Popa (ACL, 2016).
Aprendizaje no supervisado para el aprendizaje por refuerzo mediante un plan de estudios automático de autoaprendizaje
Una nueva formulación para explorar el entorno de forma no supervisada para ayudar a una tarea específica más adelante, en la que un agente propone tareas cada vez más difíciles y el agente que aprende intenta realizarlas.
Un modelo de autojuego en el que un agente aprende a proponer tareas que son fáciles para él pero difíciles para un oponente, creando un objetivo móvil de objetivos de autojuego y un plan de aprendizaje. 
Explotación de los ricos detalles estructurales de los datos estructurados en grafos mediante "huellas estructurales" adaptativas
Una metodología basada en la estructura de grafos para aumentar el mecanismo de atención de las redes neuronales de grafos, con la idea principal de explorar las interacciones entre diferentes tipos de nodos de la vecindad local de un nodo raíz.
Este trabajo amplía la idea de la autoatención en las NN de grafos, que suele basarse en la similitud de características entre nodos, para incluir la similitud estructural.
Proponemos un algoritmo de aprendizaje de refuerzo bayesiano escalable que aprende una corrección bayesiana sobre un conjunto de expertos clarividentes para resolver problemas con recompensas y dinámicas latentes complejas.
Este trabajo considera el problema del Aprendizaje de Refuerzo Bayesiano sobre Procesos de Decisión de Markov (MDPs) latentes tomando decisiones con expertos.
En este trabajo, los autores motivan y proponen un algoritmo de aprendizaje, llamado Bayesian Residual Policy Optimization (BRPO), para problemas de aprendizaje de refuerzo bayesiano.
Demostramos que el descenso de gradiente consigue una pérdida de entrenamiento nula con una tasa lineal en redes neuronales sobreparametrizadas.
Este trabajo considera la optimización de una red ReLU sobreparametrizada de dos capas con la pérdida al cuadrado y dado un conjunto de datos con etiquetas arbitrarias.
Este trabajo estudia las redes neuronales de una capa oculta con pérdida cuadrada, donde demuestran que en un entorno sobreparametrizado, la inicialización aleatoria y el descenso de gradiente llegan a la pérdida cero.
Analizar problemas inversos con redes neuronales invertibles
El autor propone utilizar redes invertibles para resolver problemas inversos ambiguos y sugiere no sólo entrenar el modelo hacia delante, sino también el modelo inverso con un crítico MMD.
El trabajo de investigación propone una red invertible con observaciones para la probabilidad posterior de distribuciones de entrada complejas con un esquema de entrenamiento bidireccional teóricamente válido.
Los parámetros de rendimiento son especificaciones incompletas; el fin no siempre justifica los medios.
Los autores muestran cómo el meta-aprendizaje revela los incentivos ocultos para el cambio distributivo y proponen un enfoque basado en el intercambio de alumnos entre entornos para reducir el cambio distributivo autointroducido.
El documento generaliza el incentivo inherente para que el alumno gane facilitando la tarea en el metaaprendizaje a una clase más amplia de problemas.
Proponemos un enfoque de detección de anomalías que combina el modelado de la clase de primer plano a través de múltiples densidades locales con el entrenamiento adversarial.
El artículo propone una técnica para hacer más robustos los modelos generativos haciéndolos consistentes con la densidad local.
Proponemos una variante de GAN que aprende a generar nubes de puntos. Se han explorado diferentes estudios, incluyendo la estimación de la distancia Wasserstein más ajustada, la generación condicional, la generalización a nubes de puntos no vistas y la imagen a nube de puntos.
Este trabajo propone el uso de GAN para generar nubes de puntos 3D e introduce un objetivo de sándwich, promediando el límite superior e inferior de la distancia de Wasserstein entre las distribuciones.
Este trabajo propone un nuevo modelo generativo para datos desordenados, con una aplicación particular a las nubes de puntos, que incluye un método de inferencia y una función objetivo novedosa. 
El artículo presenta un novedoso enfoque de los mecanismos atencionales que puede beneficiar a una serie de tareas como la traducción automática y el subtitulado de imágenes.
Este trabajo amplía los actuales modelos de atención desde el nivel de la palabra a la combinación de palabras adyacentes, aplicando los modelos a los elementos formados por palabras adyacentes fusionadas.
Identificamos un fenómeno, el lavado de cerebro neural, e introducimos una pérdida de plasticidad de peso estadísticamente justificada para superarlo.
En este artículo se analiza el fenómeno del "lavado de cerebro neuronal", que se refiere a que el rendimiento de un modelo se ve afectado a través de otro modelo que comparte sus parámetros.
Este artículo presenta Morpho-MNIST, una colección de métricas de forma y perturbaciones, en un paso hacia la evaluación cuantitativa del aprendizaje de la representación.
En este artículo se aborda el problema de la evaluación y el diagnóstico de las representaciones aprendidas mediante un modelo generativo.
Los autores presentan un conjunto de criterios para categorizar los digistos MNISt y un conjunto de perturbaciones interesantes para modificar el conjunto de datos MNIST.
exploración estructurada en el aprendizaje profundo por refuerzo mediante el descubrimiento y control de la abstracción visual no supervisada
El artículo presenta abstracciones visuales que se utilizan para el aprendizaje por refuerzo, donde un algoritmo aprende a "controlar" cada abstracción, así como a seleccionar las opciones para lograr la tarea global.
Un nuevo algoritmo de gradiente de políticas diseñado para abordar problemas de optimización combinatoria de caja negra. El algoritmo se basa únicamente en evaluaciones de funciones y devuelve soluciones localmente óptimas con alta probabilidad.
El artículo propone un enfoque para construir objetivos sustitutos para la aplicación de métodos de gradiente de política a la optimización combinatoria con el objetivo de reducir la necesidad de ajuste de hiperparámetros.
El documento propone sustituir el término de recompensa en el algoritmo de gradiente de la política por su distribución acumulativa empírica centrada. 
Estimación rápida y calibrada de la incertidumbre para redes neuronales sin muestreo
Este trabajo propone un enfoque novedoso para estimar la confianza de las predicciones en un entorno de regresión, abriendo la puerta a aplicaciones en línea con estimaciones de incertidumbre totalmente integradas.
En este trabajo se propone la regresión evidencial profunda, un método para entrenar redes neuronales que no sólo estiman la salida, sino también la evidencia asociada en apoyo de esa salida.
Proponemos un nuevo algoritmo que encuentra rápidamente los billetes ganadores en las redes neuronales.
Este trabajo propone una función objetivo novedosa que puede utilizarse para optimizar conjuntamente un objetivo de clasificación al tiempo que se fomenta la sparsificación en una red que rinde con alta precisión.
En este trabajo se propone un nuevo método de poda iterativo denominado Sparsification continuo, que poda continuamente el peso actual hasta alcanzar el ratio objetivo.
Introducir una configuración formal para la formación presupuestada y proponer un programa de tasa de aprendizaje lineal que tenga en cuenta el presupuesto
Este trabajo presenta una técnica para sintonizar la tasa de aprendizaje para el entrenamiento de Redes Neuronales cuando están bajo un número fijo de épocas.
En este trabajo se analiza qué programa de tasa de aprendizaje debe utilizarse cuando el número de iteraciones es limitado utilizando un concepto introducido de BAS (Budget-Aware Schedule).
Llevamos a cabo la exploración utilizando recompensas intrínsecas que se basan en una distancia ponderada de los vecinos más cercanos en el espacio de representación.
Este trabajo propone un método para la exploración eficiente en MDPs tabulares así como un entorno de control simple, utilizando codificadores deterministas para aprender una representación de baja dimensión de la dinámica del entorno.
Este trabajo propone un método de exploración eficiente de muestras para el agente RL utilizando una combinación de enfoques basados en modelos y libres de modelos con una métrica de novedad.
El rendimiento de la robustez de los modelos entrenados por PGD es sensible a la transformación que preserva la semántica de los conjuntos de datos de imágenes, lo que implica la dificultad de evaluar los algoritmos de aprendizaje robusto en la práctica.
Proponemos el gradiente de la política de clasificación que aprende el rango óptimo de las acciones para maximizar el rendimiento. Proponemos un marco general de aprendizaje fuera de política con las propiedades de preservación de la optimidad, reducción de la varianza y eficiencia de la muestra.
Este trabajo propone reparametrizar la política utilizando una forma de clasificación para convertir el problema de RL en un problema de aprendizaje supervisado.
Este artículo presenta una nueva visión de los métodos de gradiente político desde la perspectiva de la clasificación. 
Combinando la clasificación y la recuperación de imágenes en una arquitectura de red neuronal, obtenemos una mejora para ambas tareas.
Este trabajo propone una incrustación unificada para la clasificación de imágenes y la recuperación de instancias para mejorar el rendimiento de ambas tareas.
El artículo propone entrenar de forma conjunta una red neuronal profunda para la clasificación de imágenes, la instancia y el reconocimiento de copias.
Investigamos la relación de hiponimia de la red de palabras con los vectores de características
Este trabajo estudia cómo la hiponimia entre las palabras puede asignarse a representaciones de rasgos.
Este artículo explora la noción de hiponimia en las representaciones de vectores de palabras y describe un método para organizar las relaciones de WordNet en una estructura de árbol para definir la hiponimia.
Construimos un generador de lenguaje natural más potente mediante el entrenamiento discriminatorio de funciones de puntuación que clasifican las generaciones candidatas con respecto a varias cualidades de la buena escritura.
Este trabajo propone reunir múltiples sesgos inductivos que esperan corregir las inconsistencias en la decodificación de secuencias y propone optimizar para los parámetros de una combinación predefinida de varios subobjetivos. 
Este trabajo combina el modelo de lenguaje RNN con varios modelos entrenados discriminatoriamente para mejorar la generación de lenguaje.
Este trabajo propone mejorar la generación de modelos lingüísticos RNN utilizando objetivos aumentados inspirados en las máximas de comunicación de Grice.
Solución de equilibrio de carga escalable y de baja comunicación para sistemas multiservidores heterogéneos con fuertes garantías teóricas y resultados empíricos prometedores. 
Una medida cuantitativa para predecir el rendimiento de los modelos de redes neuronales profundas.
El artículo propone una cantidad novedosa que cuenta el número de caminos en la red neuronal que es predictiva del rendimiento de las redes neuronales con el mismo número de parámetros.
El artículo presenta un método para contar trayectorias en redes neuronales profundas que podría utilizarse para medir el rendimiento de la red.
Este trabajo presenta un estudio riguroso de por qué los esquemas de tasa de aprendizaje utilizados en la práctica (para un presupuesto computacional dado) ofrecen ventajas significativas a pesar de que estos esquemas no son defendidos por la teoría clásica de la Aproximación Estocástica.
Este trabajo presenta un estudio teórico de diferentes esquemas de tasa de aprendizaje que dio como resultado límites inferiores estadísticos minimax para los esquemas polinómicos y de corte constante.
El artículo estudia el efecto de la elección de la tasa de aprendizaje para la optimización estocástica, centrándose en los mínimos cuadrados medios con tamaños de paso decrecientes
Presentamos planificadores basados en convnets que son eficientes en cuanto a las muestras y que se generalizan a instancias más grandes de problemas de navegación y búsqueda de caminos.
Propone métodos, que pueden ser vistos como modificaciones de las Redes de Iteración de Valor (RIV), con algunas mejoras destinadas a mejorar la eficiencia de la muestra y la generalización a grandes tamaños de entorno.
El artículo presenta una extensión de las redes de iteración de valores originales (VIN) al considerar una función de transición dependiente del estado.
aprender a integrar mejor los dominios mediante el aprendizaje permanente y el metaaprendizaje
Presenta un método de aprendizaje permanente para aprender incrustaciones de palabras.
Este trabajo propone un enfoque para aprender incrustaciones en nuevos dominios y supera significativamente la línea de base en una tarea de extracción de aspectos. 
 proponemos un nuevo método de poda basado en la regularización (denominado IncReg) para asignar de forma incremental diferentes factores de regularización a distintos grupos de pesos en función de su importancia relativa.
Este trabajo propone un método de poda basado en la regularización para asignar de forma incremental diferentes factores de regularización a distintos grupos de pesos en función de su importancia relativa.
Los esquemas de impulso/aceleración existentes, como el método de la bola pesada y la aceleración de Nesterov, empleados con gradientes estocásticos, no mejoran con respecto al descenso de gradiente estocástico vainilla, especialmente cuando se emplean con tamaños de lote pequeños.
Demostramos que las tareas de planificación de la sobresuscripción pueden resolverse utilizando A* e introducimos una nueva heurística sensible a los límites para las tareas de planificación de la sobresuscripción.
Presenta un enfoque para resolver de forma óptima las tareas de planificación de sobre-suscripción (OSP) utilizando una traslación a la planificación clásica con múltiples funciones de coste.
El documento propone modificaciones a la heurística admisible para que esté mejor informada en un entorno multicriterio en el que.
Desarrollamos métodos de meta-aprendizaje para el aprendizaje adversarialmente robusto de pocos disparos.
Este trabajo presenta un método que mejora la robustez del aprendizaje de pocos disparos mediante la introducción de un ataque de datos de consulta adversarios en la fase de ajuste interno de un algoritmo de meta-aprendizaje.
Los autores de este artículo proponen un enfoque novedoso para entrenar un modelo robusto de pocos disparos. 
Encontramos que la agrupación por sí sola no determina la estabilidad de la deformación en las CNN y que la suavidad del filtro juega un papel importante en la determinación de la estabilidad. 
Proponemos un marco de autoensamblaje para entrenar modelos de aprendizaje profundo más robustos bajo conjuntos de datos etiquetados ruidosos.
Este artículo propone un "filtrado de etiquetas autoensamblado" para el aprendizaje con etiquetas ruidosas en el que el ruido de la etiqueta es independiente de la instancia, lo que produce una identificación más precisa de las predicciones inconsistentes. 
Este trabajo propone un algoritmo para el aprendizaje a partir de datos con etiquetas ruidosas que alterna entre la actualización del modelo y la eliminación de las muestras que parecen tener etiquetas ruidosas.
Investigamos la poda de las DNNs antes del entrenamiento y proporcionamos una respuesta a qué topología debe utilizarse para entrenar redes a priori dispersas.
Los autores proponen sustituir las capas densas por capas lineales escasamente conectadas y un enfoque para encontrar la mejor topología midiendo lo bien que las capas escasas se aproximan a los pesos aleatorios de sus homólogas densas.
El artículo propone una arquitectura en cascada dispersa que es una multiplicación de varias matrices dispersas y un patrón de conectividad específico que supera otras consideraciones proporcionadas.
Presentamos Multitask Neural Model Search, un meta-aprendizaje que puede diseñar modelos para múltiples tareas simultáneamente y transferir el aprendizaje a tareas no vistas.
Este trabajo amplía la búsqueda de arquitecturas neuronales al problema del aprendizaje multitarea en el que se aprende un controlador de búsqueda de modelos condicionado por la tarea para manejar múltiples tareas simultáneamente.
En este artículo, los autores resumen su trabajo sobre la construcción de un marco, llamado controlador de búsqueda de modelos neuronales multitarea, para la construcción automatizada de redes neuronales en múltiples tareas simultáneamente.
Modelamos los procesos visuales no lineales como ruido autorregresivo mediante el aprendizaje profundo generativo.
Propone un nuevo método que modela el proceso visual no lineal con una versión profunda de un proceso lineal (proceso de Markov).
Este trabajo propone un nuevo modelo generativo profundo para secuencias, especialmente de imágenes y vídeo, que utiliza una estructura lineal en parte del modelo.
Este trabajo propone una nueva red feed-forward, llamada PDE-Net, para aprender PDEs a partir de datos. 
El artículo expone el uso de maquinaria de aprendizaje profundo con el fin de identificar sistemas dinámicos especificados por PDEs.
El trabajo propone un algoritmo basado en redes neuronales para el aprendizaje a partir de datos que surgen de sistemas dinámicos con ecuaciones de gobierno que pueden escribirse como ecuaciones diferenciales parciales.
Este trabajo aborda la modelización de sistemas dinámicos complejos mediante Ecuaciones Diferenciales Parciales no paramétricas utilizando arquitecturas neuronales, con la idea más importante del papier (PDE-net) para aprender tanto los operadores diferenciales como la función que gobierna la PDE.
Proporcionamos un procedimiento de muestreo rápido similar al flujo normalizador para modelos de variables latentes discretas.
Este trabajo utiliza una aproximación variacional de filtrado autorregresivo para la estimación de parámetros en sistemas dinámicos discretos mediante el uso de iteraciones de punto fijo.
Los autores plantean una familia posterior autorregresiva general para variables discretas o sus relajaciones continuas. 
Este trabajo tiene dos contribuciones principales: extiende los flujos de normalización a entornos discretos y presenta una regla de actualización aproximada de punto fijo para series temporales autorregresivas que puede explotar el paralelismo de la GPU. 
Proponemos un marco que aprende a codificar el conocimiento simbólicamente y a generar programas para razonar sobre el conocimiento codificado.
Los autores proponen la máquina N-Gram para responder a preguntas sobre documentos largos.
Este artículo presenta la máquina de n-gramas, un modelo que codifica las oraciones en representaciones simbólicas simples que pueden ser consultadas de manera eficiente.
Este trabajo propone un objetivo de meta-aprendizaje basado en la velocidad de adaptación a las distribuciones de transferencia para descubrir una descomposición modular y variables causales.
El documento muestra que un modelo con la estructura subyacente correcta se adaptará más rápidamente a una intervención causal que un modelo con la estructura incorrecta.
En este trabajo, los autores propusieron un marco general y sistemático de objetivo de metatransferencia que incorpora el aprendizaje de la estructura causal bajo intervenciones desconocidas.
Otra perspectiva del olvido catastrófico
Este artículo introduce un marco para combatir el olvido catastrófico basado en el cambio del término de pérdida para minimizar los cambios en la probabilidad del clasificador, obtenido mediante una aproximación de series de Taylor.
Este artículo trata de resolver el problema del aprendizaje continuo centrándose en los enfoques de regularización, y propone una estrategia L_1 para mitigar el problema.
Proponemos un enfoque para construir modelos morfológicos faciales 3D realistas (3DMM) que permite un flujo de trabajo intuitivo de edición de atributos faciales mediante la selección de los mejores conjuntos de vectores propios y medidas antropométricas.
Propone un modelo morfable a trozos para mallas de rostros humanos y también propone un mapeo entre las medidas antropométricas del rostro y los parámetros del modelo para sintetizar y editar rostros con los atributos deseados. 
Este artículo describe un método de modelo facial morfable basado en partes que permite el control localizado del usuario.
Dos algoritmos superan a otros ocho en un experimento de BCI basado en EEG
Enseñamos a los agentes a negociar utilizando sólo el aprendizaje por refuerzo; los agentes egoístas pueden hacerlo, pero sólo utilizando un canal de comunicación de confianza, y los agentes prosociales pueden negociar utilizando conversaciones baratas.
Los autores describen una variante del juego de la negociación con la consideración de un canal de comunicación secundario para la charla barata, encontrando que el canal secundario mejora los resultados de la negociación.
Este trabajo explora cómo los agentes pueden aprender a comunicarse para resolver una tarea de negociación y encuentra que los agentes prosociales son capaces de aprender a fundamentar los símbolos utilizando la RL, pero los agentes interesados no.
Examina los problemas de cómo los agentes pueden utilizar la comunicación para maximizar sus recompensas en un simple juego de negociación.
Proponemos un novedoso marco de meta-aprendizaje para la inferencia transductiva que clasifica todo el conjunto de pruebas a la vez para aliviar el problema de los pocos datos.
Este trabajo propone abordar el aprendizaje de pocos disparos de forma transductiva mediante el aprendizaje de un modelo de propagación de etiquetas de forma integral, siendo el primero en aprender la propagación de etiquetas para el aprendizaje transductivo de pocos disparos y produciendo resultados empíricos efectivos. 
Este trabajo propone un marco de meta-aprendizaje que aprovecha los datos no etiquetados mediante el aprendizaje de la propogación de etiquetas basada en grafos de manera integral.
Estudia el aprendizaje de unos pocos anfitriones en un entorno transductivo: utilizando el metaaprendizaje para aprender a propagar las etiquetas de las muestras de entrenamiento a las muestras de prueba. 
Describimos el uso de un sistema de programación automatizado para el diseño de políticas de observación y para programar las operaciones de la misión ECOSTRESS de la NASA.
Este artículo presenta una adaptación de un sistema de programación automatizada, CLASP, para orientar un experimento EO (ECOSTRESS) en la ISS. 
El almacenamiento y la representación del conocimiento aprendido pueden ser una razón para los ejemplos adversos.
Nuevos experimentos y teoría para el aprendizaje Q basado en Adam
Este trabajo proporciona un resultado de convergencia para el aprendizaje Q tradicional con aproximación de funciones lineales cuando se utiliza una actualización tipo Adam. 
Este artículo describe un método para mejorar el algoritmo AltQ utilizando una combinación de un optimizador Adam y reiniciando regularmente los parámetros internos del optimizador Adam.
Una nueva red de cápsulas que converge más rápido en nuestros experimentos de referencia en el ámbito de la sanidad.
Presenta una variante de las redes de cápsulas que en lugar de utilizar el enrutamiento EM emplea un subespacio lineal abarcado por el vector propio dominante en la matriz de votos ponderados de la cápsula anterior.
El artículo propone un método de enrutamiento mejorado, que emplea herramientas de eigendecomposición para encontrar la activación y la pose de la cápsula.
Proponemos un método de ajuste fino distribuido de los modelos lingüísticos en los dispositivos de los usuarios sin recopilar datos privados
Este trabajo trata de mejorar los modelos lingüísticos en los equipos móviles basándose en una pequeña porción de texto que el usuario ha introducido, empleando unos objetivos linealmente interpolados entre el texto específico del usuario y el inglés general. 
Este trabajo utiliza el análisis de la pérdida de Lipschitz en un espacio de hipótesis acotado para derivar nuevos algoritmos de tipo ERM con fuertes garantías de rendimiento que pueden aplicarse al modelo GP disperso no conjugado.
Proponemos un método de regularización para redes neuronales y un método de análisis del ruido
Este trabajo propone un nuevo método de regularización para mitigar el problema de sobreajuste de las redes neuronales profundas mediante la rotación de las características con una matriz de rotación aleatoria para reducir la coadaptación.
Este trabajo propone un novedoso método de regularización para el entrenamiento de redes neuronales, que añade neuronas de ruido de forma interdependiente.
Un marco probabilístico para el aprendizaje por refuerzo de múltiples agentes
Este trabajo propone un nuevo algoritmo llamado Multi-Agent Soft Actor-Critic (MA-SAC) basado en el algoritmo de crítica de actores de máxima entropía fuera de política Soft Actor-Critic (SAC)
Proporcionamos una relajación continua al operador de ordenación, lo que permite una optimización estocástica de extremo a extremo basada en el gradiente.
El artículo considera cómo ordenar una serie de elementos sin conocer necesariamente sus significados o valores reales y propone un método para realizar la optimización mediante una relajación continua.
Este trabajo se basa en una identidad sum(top k) para derivar un muestreador diferenciable por trayectorias de matrices "estocásticas de fila unimodal".
Introduce una relajación continua del operador de ordenación para construir una optimización basada en el gradiente de extremo a extremo e introduce una extensión estocástica de su método utilizando distribuciones Placket-Luce y Monte Carlo.
Realizamos un aprendizaje de transferencia eficiente y flexible en el marco de la optimización bayesiana mediante funciones de adquisición neuronal meta-aprendidas.
Los autores presentan MetaBO, que utiliza el aprendizaje por refuerzo para meta-aprender la función de adquisición para la optimización bayesiana, mostrando una eficiencia de muestra creciente en nuevas tareas.
Los autores proponen una alternativa basada en el metaaprendizaje a las funciones de adquisición (AF) estándar, por la que una red neuronal preentrenada emite valores de adquisición en función de características elegidas a mano.
Las redes neuronales profundas deterministas no descartan información, pero sí agrupan sus entradas.
Este trabajo proporciona una forma de principio para examinar la frase de compresión en las redes neuronales profundas proporcionando un estimador de entropía de sonido teórico para estimar la información mutua. 
Proponemos objetivos de regularización para algoritmos de RL multiagente que fomentan la coordinación en tareas cooperativas.
Este artículo propone dos métodos para orientar a los agentes hacia el aprendizaje de comportamientos coordinados y evalúa ambos de forma rigurosa en dominios multiagente de complejidad adecuada.
Este trabajo propone dos métodos basados en MADDPG para fomentar la colaboración entre agentes MARL descentralizados.
Presentamos un modelo que aprende representaciones conjuntas robustas realizando traducciones cíclicas jerárquicas entre múltiples modalidades.
Este artículo presenta la Red de Traducción Cíclica Multimodal (MCTN) y la evalúa para el análisis de sentimientos multimodales.
Comprensión de los valores propios hessianos de la red neuronal bajo la distribución generadora de datos.
Este trabajo analiza el espectro de la matriz hessiana de grandes redes neuronales, con un análisis de los valores propios máximos y mínimos y la visualización de los espectros mediante un enfoque de cuadratura de Lanczos.
Este trabajo utiliza la teoría de las matrices aleatorias para estudiar la distribución del espectro del hessiano empírico y del hessiano verdadero para el aprendizaje profundo, y propone un método eficiente de visualización del espectro.
Un sencillo truco para mejorar los modelos de secuencia: Componerlos con un modelo de grafos
Este trabajo presenta un modelo de resumen estructural con un codificador basado en grafos ampliado a partir de RNN.
Este trabajo combina las redes neuronales gráficas con un enfoque secuencial de resumen abstracto, eficaz en todos los conjuntos de datos en comparación con las líneas de base externas.
Un clasificador disperso basado en un modelo de mezcla gaussiana discriminativa, que también puede integrarse en una red neuronal.
El artículo presenta un modelo de mezcla gaussiana entrenado mediante argumentos de descenso de gradiente que permite inducir la dispersión y reducir los parámetros de la capa del modelo entrenable.
Este trabajo propone un clasificador, denominado SDGM, basado en la mezcla gaussiana discriminativa y su estimación de parámetros dispersos.
Inicialice los pesos utilizando libros de códigos de Grassmann, obtenga un entrenamiento más rápido y una mayor precisión
Un enfoque de adaptación de dominio no supervisado que se adapta tanto a nivel de píxeles como de características
Este trabajo propone un enfoque de adaptación de dominio mediante la ampliación del CycleGAN con funciones de pérdida específicas para la tarea y la pérdida impuesta tanto sobre los píxeles como sobre las características. 
Este trabajo propone el uso de CycleGANs para la adaptación de dominios
Este trabajo realiza una novedosa extensión del trabajo anterior sobre CycleGAN al acoplarlo con enfoques de adaptación adversarial, incluyendo una nueva característica y pérdida semántica en el objetivo general del CycleGAN, con claros beneficios.
Amharic Light Stemmer está diseñado para mejorar el rendimiento de la clasificación de sentimientos en amárico.
Este trabajo estudia el stemming para lenguas morfológicamente ricas con un stemmer ligero que sólo elimina afijos en la medida en que se mantiene la información semántica original de la palabra.
Este artículo propone una técnica para la derivación de la luz en amárico utilizando una cascada de transformaciones que estandarizan la forma, eliminan sufijos, prefijos e infijos.
Investigamos si las redes profundas simples poseen neuronas artificiales similares a las de las células de la red, mientras que la recuperación de la memoria en el espacio conceptual aprendido.
Ventajas y desventajas de la visión por ordenador basada en las sacádicas bajo una perspectiva de codificación predictiva
Presenta un marco computacional para el problema de la visión activa y explica cómo se puede aprender la política de control para reducir la entropía de la creencia posterior.
Estudiamos teóricamente la consistencia del espectro laplaciano y lo utilizamos como incrustación del gráfico completo
Este trabajo se centra en el espectro laplaciano de un grafo como medio para generar una representación que sirva para comparar grafos y clasificarlos.
En este trabajo se propone utilizar el espectro laplaciano de los gráficos para aprender su representación.
El entrenamiento adversario basado en FGSM, con aleatorización, funciona tan bien como el entrenamiento adversario basado en PGD: podemos utilizarlo para entrenar un clasificador robusto en 6 minutos en CIFAR10, y 12 horas en ImageNet, en una sola máquina.
Este trabajo revisa el método Random+FGSM para entrenar modelos robustos contra ataques de evasión de PGD fuertes más rápidamente que los métodos anteriores.
La principal afirmación de este trabajo es que una estrategia simple de aleatorización más el método de signo de gradiente rápido (FGSM) de entrenamiento adversario produce redes neuronales robustas.
Proponemos regularizadores diferenciables en casi todas partes e invariantes de escala para la poda de las DNN, que pueden conducir a una dispersión suprema a través del entrenamiento estándar de SGD.
El artículo propone un regularizador invariante de escala (DeepHoyer) inspirado en la medida Hoyer para reforzar la dispersión en las redes neuronales. 
Demostramos que no se necesitan datos adicionales sin etiquetar para que las tareas auxiliares autosupervisadas sean útiles para la clasificación de series temporales, y presentamos tareas auxiliares nuevas y eficaces.
Este trabajo propone un método autosupervisado para el aprendizaje a partir de datos de series temporales en entornos sanitarios mediante el diseño de tareas auxiliares basadas en la estructura interna de los datos para crear tareas auxiliares de entrenamiento más etiquetadas.
Este trabajo propone un enfoque para el aprendizaje autosupervisado de series temporales.
Los valores propios del Conjugado (también conocido como NNGP) y del Núcleo Tangente Neural pueden ser calculados en forma cerrada sobre el cubo booleano y revelan los efectos de los hiperparámetros sobre el sesgo inductivo de la red neural, el entrenamiento y la generalización.
Este artículo ofrece un análisis espectral sobre el núcleo conjugado de las redes neuronales y el núcleo tangente neuronal en el cubo booleano para resolver por qué las redes profundas están sesgadas hacia las funciones simples.
Todas las parcelaciones funcionales del cerebro son erróneas, pero algunas son útiles
Imitación a partir de píxeles, con recompensa escasa o nula, utilizando RL sin política y una pequeña función de recompensa aprendida de forma adversa.
El artículo propone utilizar un "adversario mínimo" en el aprendizaje generativo de imitación adversarial bajo espacios visuales de alta dimensión.
Este trabajo tiene como objetivo resolver el problema de la estimación de recompensas dispersas en un entorno de entrada de alta dimensión.
Mostramos estrategias para identificar fácilmente las muestras falsas generadas con el marco de la Red Adversarial Generativa.
Demostrar que las muestras falsas creadas con implementaciones comunes de redes generativas adversariales (GAN) se identifican fácilmente utilizando varias técnicas estadísticas. 
El artículo propone estadísticas para identificar datos falsos generados mediante GANs basadas en simples estadísticas marginales o especificaciones formales generadas automáticamente a partir de datos reales.
Presentamos un marco analítico para determinar los requisitos de ancho de bits de acumulación en los tres GEMM de entrenamiento de aprendizaje profundo y verificamos la validez y el rigor de nuestro método mediante experimentos de evaluación comparativa.
Los autores proponen un método analítico para predecir el número de bits de mantisa necesarios para las sumas parciales de las capas convolucionales y totalmente conectadas
Los autores realizan un análisis exhaustivo de la precisión numérica necesaria para las operaciones de acumulación en el entrenamiento de redes neuronales y muestran el impacto teórico de la reducción del número de bits en el acumulador de coma flotante.
Una nueva teoría de adaptación de dominio no supervisada para el aprendizaje métrico a distancia y su aplicación al reconocimiento de rostros a través de diversas variaciones étnicas.
Propone una novedosa red de transferencia de características que optimiza la pérdida por adversidad de dominio y la pérdida por separación de dominio.
Proponemos un algoritmo de descenso de gradiente estocástico de tipo proximal convergente para problemas de optimización restringidos no lisos y no convexos
Este trabajo propone Prox-SGD, un marco teórico para los algoritmos de optimización estocástica que muestran que convergen asintóticamente a la estacionariedad para pérdidas suaves no convexas + restricción/regularizador convexo.
El artículo propone un nuevo algoritmo de optimización estocástica basado en el gradiente con el promedio del gradiente mediante la adaptación de la teoría de los algoritmos proximales al entorno no convexo.
Damos una cota para las NNs sobre el error de salida en caso de fallos aleatorios de peso utilizando una expansión de Taylor en el límite continuo donde las neuronas cercanas son similares
Este trabajo considera el problema de la eliminación de neuronas de una red neuronal, mostrando que si el objetivo es ser robusto a las neuronas eliminadas aleatoriamente durante la evaluación, entonces es suficiente con entrenar con la eliminación.
Esta contribución estudia el impacto de las supresiones de neuronas aleatorias en la precisión de la predicción de la arquitectura entrenada, con la aplicación al análisis de fallos y el contexto específico del hardware neuromórfico.
Exploramos y estudiamos las sinergias entre el sonido y la acción.
Este artículo explora las conexiones entre la acción y el sonido construyendo un conjunto de datos de sonido-acción-visión con un robot basculante.
Este trabajo estudia el papel del audio en la percepción de objetos y acciones, así como el modo en que la información auditiva puede ayudar al aprendizaje de modelos de dinámica directa e inversa.
Proponemos el Entrenamiento Objetivo de Complemento Jerárquico, un nuevo paradigma de entrenamiento para aprovechar eficazmente la jerarquía de categorías en el espacio de etiquetado tanto en la clasificación de imágenes como en la segmentación semántica.
Un método que regulariza la entropía de la distribución posterior sobre las clases que puede ser útil para las tareas de clasificación y segmentación de imágenes
Nuestro trabajo identifica el problema del enfoque de reparto de pesos existente en la búsqueda de arquitecturas neuronales y propone un método práctico, logrando resultados sólidos.
El autor identifica un problema con el NAS llamado desvanecimiento posterior e introduce el NAS Convergente Posterior para mitigar este efecto
Proponemos un novedoso enfoque de entrenamiento en dos fases basado en la "detención temprana" para un entrenamiento robusto con etiquetas ruidosas.
El artículo propone estudiar cómo la detención temprana en la optimización ayuda a encontrar ejemplos seguros
Este trabajo propone un método de entrenamiento en dos fases para el aprendizaje con ruido de etiqueta.
Presentamos IC3Net, una red única que puede utilizarse para entrenar a los agentes en escenarios cooperativos, competitivos y mixtos. También demostramos que los agentes pueden aprender cuándo comunicarse utilizando nuestro modelo.
El autor propone una nueva arquitectura para el aprendizaje por refuerzo multiagente que utiliza varios controladores LSTM con pesos ligados que se transmiten un vector continuo entre sí
Los autores proponen un interesante esquema de puertas que permite a los agentes comunicarse en un entorno RL multiagente. 
Presentamos el primer modelo de resumen neural capaz de personalizar los resúmenes generados.
Proponemos un marco de software basado en ideas del algoritmo de Aprendizaje-Compresión , que permite comprimir cualquier red neuronal mediante diferentes mecanismos de compresión (poda, cuantificación, bajo rango, etc.).
Este trabajo presenta el diseño de una biblioteca de software que facilita al usuario la compresión de sus redes ocultando los detalles de los métodos de compresión.
Este trabajo propone un método de generación multimodal de extremo a extremo de rostros humanos a partir del habla, basado en un marco de aprendizaje autosupervisado.
Este trabajo presenta un marco de aprendizaje multimodal que vincula la etapa de inferencia y la etapa de generación para buscar la posibilidad de generar el rostro humano a partir únicamente de la voz.
Este trabajo pretende construir un marco de generación de imágenes faciales condicionales a partir de la señal de audio. 
Se presenta un enfoque descendente de cómo representar recursivamente fórmulas proposicionales mediante redes neuronales.
Este artículo proporciona un nuevo modelo de red neuronal de fórmulas lógicas que recoge información sobre una fórmula dada recorriendo su árbol de análisis sintáctico de arriba abajo.
El artículo sigue el camino de una red estructurada en forma de árbol que es isomorfa al árbol de análisis sintáctico de una fórmula de cálculo proposicional, pero pasando la información de arriba abajo en lugar de abajo arriba.
Ape-X DQfD = DQN distribuido (muchos actores + un aprendiz + repetición priorizada) con demostraciones que optimizan el rendimiento sin recorte de 0,999 en Atari.
El artículo propone tres extensiones (actualización de Bellman, pérdida de consistencia temporal y demostración experta) a DQN para mejorar el rendimiento del aprendizaje en los juegos Atari, logrando un rendimiento superior a los resultados del estado del arte para los juegos Atari. 
Este trabajo propone un operador de Bellman transformado que pretende resolver la sensibilidad a la recompensa no recortada, la robustez al valor del factor de descuento y el problema de exploración.
Método de entrenamiento para imponer restricciones estrictas a las incrustaciones aprendidas durante el entrenamiento supervisado. Aplicado a la respuesta de preguntas visuales.
Los autores proponen un marco para incorporar conocimiento previo semántico adicional al entrenamiento tradicional de los modelos de aprendizaje profundo para regularizar el espacio de incrustación en lugar del espacio de parámetros.
El artículo defiende la codificación del conocimiento externo en la capa de incrustación lingüística de una red neuronal multimodal, como un conjunto de restricciones duras.
Resolver los problemas inversos utilizando aproximaciones suaves de los algoritmos de avance para entrenar los modelos inversos.
Un método de aprendizaje profundo para la localización puntual débilmente supervisada que aprende utilizando únicamente la etiqueta a nivel de imagen. Se basa en la entropía condicional para localizar regiones relevantes e irrelevantes con el objetivo de minimizar las regiones falsas positivas.
Este trabajo explora el problema de WSL utilizando un novedoso diseño de términos de regularización y un algoritmo de borrado recursivo.
Este artículo presenta un nuevo enfoque débilmente supervisado para el aprendizaje de la segmentación de objetos con etiquetas de clase a nivel de imagen.
Adquirir estados de la región de alta frecuencia para el control de la búsqueda en Dyna.
Los autores proponen realizar el muestreo en el dominio de la alta frecuencia para aumentar la eficiencia de la muestra
Este trabajo propone una nueva forma de seleccionar los estados a partir de los cuales se realizan las transiciones en el algoritmo dyna.
Presentamos un marco de codificación de fuentes distribuidas basado en datos y en un autocodificador distribuido recurrente para la compresión de imágenes escalable (DRASIC).
El artículo propone un autocodificador recurrente distribuido para la compresión de imágenes que utiliza un ConvLSTM para aprender códigos binarios que se construyen progresivamente a partir de los residuos de la información previamente codificada
Los autores proponen un método para entrenar modelos de compresión de imágenes en múltiples fuentes, con un codificador separado en cada fuente, y un decodificador compartido. 
Las puertas hacen todo el trabajo pesado en las LSTMs calculando sumas ponderadas por elementos, y la eliminación de la RNN simple interna no degrada el rendimiento del modelo.
Este trabajo propone una variante simplificada de LSTM eliminando la no linealidad del elemento de contenido y la puerta de salida
Este trabajo presenta un análisis de las LSTMS, mostrando que tienen una forma en la que el contenido de la celda de memoria en cada paso es una combinación ponderada de los valores de "actualización de contenido" calculados en cada paso de tiempo y ofrece una simplificación de las LSTM que calculan el valor por el que la celda de memoria en cada paso de tiempo en términos de una función determinista de la entrada en lugar de una función de la entrada y el contexto actual.
El artículo propone una nueva visión de la LSTM en la que el núcleo es una suma ponderada por elementos y argumenta que la LSTM es redundante al mantener sólo puertas de entrada y olvido para calcular los pesos
Al analizar más de 300 artículos en conferencias recientes sobre aprendizaje automático, descubrimos que las aplicaciones de Machine Learning for Health (ML4H) están por detrás de otros campos de aprendizaje automático en términos de métricas de reproducibilidad.
Este artículo realiza una revisión cuantitativa y cualitativa del estado de la reproducibilidad de las aplicaciones sanitarias de LD y propone recomendaciones para que la investigación sea más reproducible.
Entrenamos muchas redes pequeñas, cada una para una operación específica, que luego se combinan para realizar operaciones complejas
Este trabajo propone utilizar redes neuronales para evaluar las expresiones matemáticas mediante el diseño de 8 pequeños bloques de construcción para 8 operaciones fundamentales, por ejemplo, la suma, la resta, etc. y, a continuación, diseñar la multiplicación y la división de varios dígitos utilizando estos pequeños bloques.
El artículo propone un método para diseñar un motor de evaluación de expresiones matemáticas basado en NN.
Mejorar la calidad y la estabilidad de los GAN utilizando un discriminador relativista; los GAN IPM (como el WGAN-GP) son un caso especial.
El artículo propone un "discriminador relativista", que ayuda en algunos escenarios, aunque es un poco sensible a los hiperparámetros, las arquitecturas y los conjuntos de datos.
En este trabajo, los autores consideran una variación de GAN al disminuir simultáneamente la probabilidad de que los datos reales sean reales para el generador.
Una versión de MPO basada en la función estado-valor que consigue buenos resultados en una amplia gama de tareas de control discreto y continuo.
Este trabajo presenta un algoritmo para el aprendizaje de refuerzo sobre políticas que puede manejar tanto el control continuo/discreto, el aprendizaje de una/varias tareas y utilizar tanto estados como píxeles de baja dimensión.
El artículo propone una variante online de MPO, V-MPO, que aprende la función V y actualiza la distribución no paramétrica hacia las ventajas.
Proponemos motores de ejecución neural (NEE), que aprovechan una máscara aprendida y trazas de ejecución supervisadas para imitar la funcionalidad de las subrutinas y demostrar una fuerte generalización.
Este trabajo investiga un problema de construcción de un motor de ejecución de programas con redes neuronales y propone un modelo basado en transformadores para aprender subrutinas básicas y las aplica en varios algoritmos estándar.
Este trabajo aborda el problema de diseñar arquitecturas de redes neuronales que puedan aprender e implementar programas generales.
La detección bayesiana de puntos de cambio permite el metaaprendizaje directamente a partir de los datos de las series temporales.
El artículo considera el meta-aprendizaje en el entorno de la tarea no segmentada y aplica la detección de puntos de cambio en línea bayesiana con meta-aprendizaje.
Este trabajo impulsa el meta-aprendizaje hacia entornos sin segmentación de tareas, donde el marco MOCA adopta un esquema de estimación bayesiana de puntos de cambio para la detección de cambios en las tareas.
Un enfoque basado en el aprendizaje profundo para la detección de fonemas fricativos con retardo cero
En este trabajo se emplean métodos de aprendizaje profundo supervisado para detectar la duración exacta de un fonema fricativo con el fin de mejorar el práctico algoritmo de reducción de frecuencia.
Un mecanismo de atención online y en tiempo lineal que realiza una atención suave sobre trozos de la secuencia de entrada localizados de forma adaptativa.
Este trabajo propone una pequeña modificación de la atención monotónica en [1] añadiendo una atención suave al segmento predicho por la atención monotónica.
El artículo propone una extensión de un modelo de atención monótona anterior (Raffel et al 2017) para atender a una ventana de tamaño fijo hasta la posición de alineación.
Desarrollar nuevas técnicas que se basen en la reordenación de parches para permitir un análisis detallado de la relación del conjunto de datos con los rendimientos de entrenamiento y generalización.
Producimos agentes de aprendizaje por refuerzo que generalizan bien en una amplia gama de entornos utilizando una novedosa técnica de regularización.
El artículo presenta el reto de las políticas de alta varianza en la aleatorización de dominios para el aprendizaje por refuerzo y se centra principalmente en el problema de la aleatorización visual, donde los diferentes dominios aleatorizados difieren sólo en el espacio de estados y las recompensas y dinámicas subyacentes son las mismas.
Para mejorar la capacidad de generalización de los agentes RL profundos a través de las tareas con diferentes patrones visuales, este trabajo propuso una técnica de regularización simple para la aleatorización del dominio.
Exploramos la intersección entre las neurociencias de red y el aprendizaje profundo. 
Este trabajo presenta un sistema de construcción de bases de conocimiento no supervisado y de alta precisión que utiliza un programa probabilístico para definir un proceso de conversión de hechos de la base de conocimiento en texto no estructurado.
Visión general sobre la base de conocimiento existente que se construye con un modelo probabilístico, con el enfoque de construcción de la base de conocimiento evaluado frente a otros enfoques de la base de conocimiento YAGO2, NELL, Knowledge Vault, y DeepDive.
Este trabajo utiliza un programa probabilístico que describe el proceso por el cual los hechos que describen entidades pueden ser realizados en texto y en un gran número de páginas web, para aprender a realizar la extracción de hechos sobre personas utilizando un único hecho semilla.
Nuevo método de extracción de señales en el dominio de Fourier
Mejora de la escalabilidad de las redes neuronales gráficas en el aprendizaje por imitación y la predicción del movimiento del enjambre
El artículo propone un nuevo modelo de series temporales para el aprendizaje de una secuencia de gráficos.
Este trabajo considera los problemas de predicción de secuencias en un sistema multiagente.
Proponemos un marco de cuantificación de producto diferenciable que puede reducir el tamaño de la capa de incrustación en un entrenamiento de extremo a extremo sin coste de rendimiento.
Este artículo trabaja en métodos para comprimir las capas de incrustación para la inferencia de baja memoria, donde las incrustaciones comprimidas se aprenden junto con los modelos específicos de la tarea de una manera diferenciable de extremo a extremo.
Introducimos un algoritmo de regresión modal simple y novedoso que es fácil de escalar a problemas grandes. 
El artículo propone un enfoque de función implícita para el aprendizaje de los modos de regresión multimodal.
El presente trabajo propone un enfoque paramétrico para estimar la moda condicional utilizando el Teorema de la Función Implícita para distribuciones multimodales. 
Meta-RL eficiente combinando la inferencia variacional de las variables probabilísticas de la tarea con la RL off-policy 
Este trabajo propone el uso de RL fuera de la política durante el tiempo de meta-entrenamiento para mejorar en gran medida la eficiencia de la muestra de los métodos de Meta-RL.
Este artículo se centra en la identificación de fuentes web de alta calidad para la canalización del aumento de la base de conocimientos industriales.
Investigamos las ventajas de emplear redes neuronales en el problema de predicción de coincidencias, en el que se busca estimar la probabilidad de que un grupo de M artículos sea preferido a otro, basándose en datos parciales de comparación de grupos.
Este trabajo propone una solución de red neuronal profunda para el problema de clasificación de conjuntos y diseña una arquitectura para esta tarea inspirada en algoritmos anteriores diseñados manualmente.
Este trabajo proporciona una técnica para resolver el problema de la predicción de partidos utilizando una arquitectura de aprendizaje profundo.
Un nuevo enfoque para mantener matrices de pesos recurrentes ortogonales en una RNN.
Introduce un esquema para el aprendizaje de la matriz de parámetros recurrente en una red neuronal que utiliza la transformada de Cayley y una matriz de pesos de escala. 
Este trabajo sugiere una reparametrización de la RNN de los pesos recurrentes con una matriz sesgada-simétrica utilizando la transformada de Cayley para mantener la matriz de pesos recurrentes ortogonal.
La nueva parametrización de las RNN permite representar con relativa facilidad las matrices de pesos ortogonales.
Utilizamos un único modelo para resolver una gran variedad de tareas de análisis del lenguaje natural formulándolas en un formato unificado de relación de tramos.
Este trabajo generaliza una amplia gama de tareas de procesamiento del lenguaje natural como un único marco basado en el span y propone una arquitectura general para resolver todos estos problemas.
Este trabajo presenta una formulación unificada de varias tareas de PNL a nivel de frase y de token.
Presentamos una cota inferior variacional para los modelos de GP que puede optimizarse sin calcular operaciones matriciales costosas como las inversas, al tiempo que proporciona las mismas garantías que las aproximaciones variacionales existentes.
Los autocodificadores variacionales con espacios latentes modelados como productos de variedades riemannianas de curvatura constante mejoran la reconstrucción de imágenes con respecto a las variantes de un solo manifold.
Este trabajo introduce una formulación general de la noción de un VAE con un espacio latente compuesto por una colector curvo.
Este artículo trata sobre el desarrollo de VAE en espacios no euclidianos.
Introducimos un algoritmo de caja negra para la optimización repetida de compuestos utilizando un marco de traducción.
Los autores enmarcan la optimización de la molécula como un problema de secuencia a secuencia, y amplían los métodos existentes para mejorar las moléculas, mostrando que es beneficioso para optimizar el logP pero no el QED.
El artículo se basa en los modelos de traducción existentes desarrollados para la optimización molecular, haciendo un uso iterativo de los modelos de traducción secuencia a secuencia o grafo a grafo.
Proponer el primer marco de marcado de agua para la incrustación y extracción de firmas multibits utilizando las salidas de la DNN. 
Propone un método para la marca de agua de múltiples bits de las redes neuronales en un entorno de caja negra y demuestra que las predicciones de los modelos existentes pueden llevar una cadena de múltiples bits que puede utilizarse posteriormente para verificar la propiedad.
El artículo propone un enfoque para la marca de agua del modelo donde la marca de agua es una cadena de bits incrustada en el modelo como parte de un procedimiento de ajuste fino
¿No sabe cómo optimizar? Entonces, ¡aprende a optimizar!
Este trabajo propone una forma de entrenar modelos de clasificación de imágenes para que sean resistentes a los ataques de perturbación L-infinita.
Este trabajo propone utilizar el marco de aprendizaje para aprender un atacante.
Un método sencillo y fácil de entrenar para la predicción multimodal en series temporales. 
Este trabajo introduce un modelo de predicción de series temporales que aprende un mapeo determinista y entrena otra red para predecir tramas futuras dada la entrada y el error residual de la primera red.
El documento propone un modelo de predicción bajo incertidumbre en el que se separan la predicción de componentes deterministas y la predicción de componentes inciertos.
Este artículo presenta y motiva simple_rl, una nueva biblioteca de código abierto para llevar a cabo experimentos de aprendizaje por refuerzo en Python 2 y 3 con un enfoque en la simplicidad.
Este trabajo trata de la estabilidad de la optimización de gradiente simple $\mu$-WGAN mediante la introducción de un concepto de diferenciación valorada por la medida.
Se estudia el WGAN con un término de penalización de gradiente cuadrado centrado en cero con respecto a una medida general.
Caracteriza la convergencia del GAN Wasserstein penalizado por gradiente.
Método de entrenamiento de última generación para redes de pesos binarios y ternarios basado en la optimización alternada de particiones de pesos relajadas al azar
El artículo propone un nuevo esquema de entrenamiento para optimizar una red neuronal ternaria.
Los autores proponen la RPR, una forma de dividir y cuantificar aleatoriamente los pesos y entrenar los parámetros restantes, seguida de la relajación en ciclos alternativos para entrenar los modelos cuantificados.
Sustituimos algunas trayectorias de gradientes en las RNN jerárquicas por una pérdida auxiliar. Demostramos que esto puede reducir el coste de la memoria mientras se preserva el rendimiento.
El artículo introduce una arquitectura RNN jerárquica que podría ser entrenada con mayor eficiencia de memoria.
El artículo propuesto sugiere desacoplar las diferentes capas de jerarquía en la RNN utilizando pérdidas auxiliares.
Las redes neuronales que hacen un buen trabajo de clasificación proyectan los puntos en formas más esféricas antes de comprimirlos en menos dimensiones.
Proponemos un novedoso método de aprendizaje para el reconocimiento de sonidos profundos denominado aprendizaje BC.
Los autores definieron una nueva tarea de aprendizaje que requiere que una DNN prediga la relación de mezcla entre sonidos de dos clases diferentes para aumentar el poder discriminatorio de la red final aprendida.
Propone un método para mejorar el rendimiento de un método de aprendizaje genérico mediante la generación de muestras de entrenamiento "entre clases" y presenta la intuición básica y la necesidad de la técnica propuesta.
Proponemos un método que infiere el nivel de calidad de los datos que varía en el tiempo para la previsión espacio-temporal sin asignar explícitamente etiquetas.
Introduce una nueva definición de calidad de datos que se basa en la noción de variación local definida en (Zhou y Scholkopf) y la amplía a múltiples fuentes de datos heterogéneas.
Este trabajo propone una nueva forma de evaluar la calidad de las diferentes fuentes de datos con el modelo de grafos variables en el tiempo, con el nivel de calidad utilizado como término de regularización en la función objetivo
Proponemos programas de formas 3D, una representación de formas estructurada y compositiva. Nuestro modelo aprende a inferir y ejecutar programas de formas para explicar las formas 3D.
Un enfoque para inferir programas de forma dados modelos 3D, con una arquitectura que consiste en una red recurrente que codifica una forma 3D y emite instrucciones, y un segundo módulo que renderiza el programa en 3D.
Este artículo introduce una descripción semántica de alto nivel para las formas 3D, dada por el ShapeProgram.
Demostramos que los métodos de regularización convencionales (por ejemplo, $L_2$, dropout), que han sido ampliamente ignorados en los métodos de RL, pueden ser muy eficaces en la optimización de políticas.
Los autores estudian un conjunto de métodos de optimización de políticas directas existentes en el campo del aprendizaje por refuerzo y proporcionan una investigación detallada sobre el efecto de las normas en el rendimiento y el comportamiento de los agentes que siguen estos métodos.
Este trabajo proporciona un estudio sobre el efecto de la regularización en el rendimiento en entornos de entrenamiento en métodos de optimización de políticas en tareas de control continuo múltiple.
Presentamos un conjunto de datos de respuesta a preguntas, FigureQA, como un primer paso hacia el desarrollo de modelos que puedan reconocer intuitivamente patrones a partir de representaciones visuales de datos.
Este artículo presenta un conjunto de datos de respuestas a preguntas sobre figuras, que implican un razonamiento sobre los elementos de la figura.
El artículo presenta un nuevo conjunto de datos de razonamiento visual llamado Figure-QA, que consta de 140.000 imágenes de figuras y 1,55 millones de pares de QA, que puede ayudar a desarrollar modelos que puedan extraer información útil de las representaciones visuales de los datos.
Este documento de posición analiza los diferentes tipos de autoexplicación que pueden surgir en los sistemas de planificación y afines. 
Discute diferentes aspectos de las explicaciones, particularmente en el contexto de la toma de decisiones secuenciales. 
El primer enfoque de aprendizaje profundo para MFSR que resuelve el registro, la fusión y el muestreo ascendente de manera integral.
Este trabajo propone un algoritmo de superresolución de extremo a extremo, que se basa en un co-registro por pares y bloques de fusión (bloques residuales convolucionales), incrustados en una red de codificación-decodificación 'HighRes-net' que estima la imagen de superresolución.
Este trabajo propone un marco que incluye la fusión recursiva a la pérdida de co-registro para resolver el problema de que los resultados de la súper-resolución y las etiquetas de alta resolución no estén alineadas por píxeles.
Para el entrenamiento distribuido en redes de alta latencia, utilice el promedio distribuido aproximado basado en chismes en lugar del promedio distribuido exacto como AllReduce.
Los autores proponen el uso de algoritmos de cotilleo como método general de cálculo de la media aproximada sobre un conjunto de trabajadores aproximadamente
El artículo demuestra la convergencia del SGP para funciones suaves no convexas y muestra que el SGP puede lograr un aumento significativo de la velocidad en el entorno de baja latencia sin sacrificar demasiado el rendimiento predictivo. 
Este artículo desarrolla un marco de aprendizaje adversarial para modelos de conversación neuronal con persona
Este trabajo propone una extensión de hredGAN para aprender simultáneamente un conjunto de incrustaciones de atributos que representan la persona de cada hablante y generar respuestas basadas en la persona
Las redes neuronales artificiales bioinspiradas, formadas por neuronas situadas en un espacio bidimensional, son capaces de formar grupos independientes para realizar diferentes tareas.
Transformador discreto que utiliza la atención dura para garantizar que cada paso sólo depende de un contexto fijo.
Este artículo presenta modificaciones a la arquitectura estándar de los transformadores con el objetivo de mejorar la interpretabilidad y mantener el rendimiento en las tareas de PNL.
Este trabajo propone tres transformadores discretos: un módulo de atención discreto y estocástico basado en Gumbel-softmax, un transformador sintáctico y semántico de dos flujos y una regularización de la dispersión.
Mostramos pruebas empíricas de que los modelos de codificación predictiva producen representaciones más correlacionadas con los datos cerebrales que los modelos de reconocimiento de imágenes supervisados.
Un marco genérico para manejar la transferencia y el aprendizaje multitarea utilizando pares de autocodificadores con pesos específicos de la tarea y compartidos.
Propone un marco genérico para el aprendizaje de transferencia de extremo a extremo / adaptación de dominio con redes neuronales profundas. 
Este artículo propone un modelo para permitir que las arquitecturas de redes neuronales profundas compartan parámetros entre diferentes conjuntos de datos, y lo aplica al aprendizaje por transferencia.
El artículo se centra en el aprendizaje de características comunes a partir de datos de múltiples dominios y termina con una arquitectura general para el aprendizaje multitarea, semisupervisado y de transferencia
Proponemos un marco para combinar árboles de decisión y redes neuronales, y demostramos en tareas de clasificación de imágenes que goza de las ventajas complementarias de los dos enfoques, al tiempo que aborda las limitaciones de los trabajos anteriores.
Los autores propusieron un nuevo modelo, los árboles neuronales adaptativos, combinando el aprendizaje de la representación y la optimización del gradiente de las redes neuronales con el aprendizaje de la arquitectura de los árboles de decisión
Este trabajo propone el enfoque de los árboles neuronales adaptativos para combinar los dos paradigmas de aprendizaje de las redes neuronales profundas y los árboles de decisión
La traducción de partes de la entrada durante el entrenamiento puede mejorar el rendimiento multilingüe.
El artículo propone un método de aumento de datos multilingües para mejorar las tareas de inferencia lingüística y respuesta a preguntas.
Este trabajo propone aumentar los datos multilingües con intercambios heurísticos utilizando traducciones alineadas, como hacen los humanos bilingües en el cambio de código.
Proponemos un marco de autoencoder variacional condicional que mitiga el colapso posterior en escenarios donde la señal de condicionamiento es lo suficientemente fuerte como para que un decodificador expresivo genere una salida plausible a partir de ella.
Este trabajo considera modelos generativos fuertemente condicionados, y propone una función objetivo y una parametrización de la distribución variacional tal que las variables latentes dependen explícitamente de las condiciones de entrada.
Este trabajo argumenta que cuando el decodificador está condicionado por la concatenación de variables latentes e información auxiliar, entonces el colapso posterior es más probable que en el VAE vainilla.
Proponemos un estudio de la estabilidad de varios algoritmos de aprendizaje de pocos disparos sujetos a variaciones en los hiperparámetros y esquemas de optimización mientras se controla la semilla aleatoria.
Este trabajo estudia la reproducibilidad para el aprendizaje de pocos disparos.
Traducimos un límite sobre la subóptima de las representaciones a un objetivo práctico de entrenamiento en el contexto del aprendizaje jerárquico por refuerzo.
Los autores proponen un enfoque novedoso en el aprendizaje de una representación para el HRL y establecen una conexión intrigante entre el aprendizaje de la representación y la limitación de la suboptimidad que da lugar a un algoritmo basado en el gradiente
Este trabajo propone una forma de manejar la sub-optimidad en el contexto de las representaciones de aprendizaje que se refieren a la sub-optimidad de la política jerárquica con respecto a la recompensa de la tarea.
El metarrelato en un planificador temporal situado
Este trabajo aborda el problema de la planificación temporal situada, proponiendo una simplificación adicional sobre las estrategias codiciosas previamente propuestas por Shperberg.
El rendimiento de la robustez de los modelos entrenados por PGD es sensible a la transformación que preserva la semántica de los conjuntos de datos de imágenes, lo que implica la dificultad de evaluar los algoritmos de aprendizaje robusto en la práctica.
El artículo aclara la diferencia entre precisión limpia y robusta y muestra que cambiar la distribución marginal de los datos de entrada P(x) conservando su semántica P(y|x) afecta a la robustez del modelo.
Este trabajo investiga el origen de la falta de robustez de los clasificadores a las perturbaciones de las entradas adversarias bajo perturbaciones acotadas l-inf.
Emparejamiento de frases mediante el aprendizaje de las estructuras del árbol de constituyentes latentes con una variante del algoritmo inside-outside incrustado como capa de red neuronal.
Este trabajo introduce un mecanismo de atención estructurada para calcular las puntuaciones de alineación entre todos los tramos posibles en dos frases dadas
Este trabajo propone un modelo de alineaciones estructuradas entre oraciones como medio para comparar oraciones mediante el cotejo de sus estructuras latentes.
Aprender a desentrañar la representación de forma no supervisada.
Los autores presentan un marco en el que un autocodificador (E, D) se regulariza de manera que su representación latente comparta información mutua con una representación de espacio latente generada.
GANs condicionales entrenados para generar muestras de datos aumentados de sus entradas condicionales utilizadas para mejorar la clasificación de vainilla y los sistemas de aprendizaje de una sola vez, como las redes de coincidencia y la distancia de píxeles.
Los autores proponen un método para llevar a cabo el aumento de datos en el que las transformaciones de clase cruzada se mapean a un espacio latente de baja dimensión utilizando GAN condicional
Desarrollamos un sencillo método de selección de características basado en la regresión para interpretar los procesos de generación de datos con control de FDR, y superamos varias líneas de base populares en varios conjuntos de datos simulados, médicos y de imágenes.
Este trabajo propone una mejora práctica de la prueba de aleatoriedad condicional y un nuevo estadístico de prueba, demuestra que la divergencia f es una opción posible y que la divergencia KL anula algunas distribuciones condicionales.
Este trabajo aborda el problema de encontrar características útiles en una entrada que dependen de una variable de respuesta, incluso cuando se condiciona a todas las demás variables de entrada.
Un método agnóstico al modelo para proporcionar una interpretación sobre la influencia de las características de entrada en la respuesta de un modelo a nivel de máquina hasta el nivel de instancia, y estadísticas de prueba adecuadas para la selección de características agnósticas al modelo.
Un nuevo enfoque para el aprendizaje de un modelo a partir de anotaciones ruidosas de crowdsourcing.
Este artículo propone un método de aprendizaje a partir de etiquetas ruidosas, centrándose en el caso de que los datos no estén etiquetados de forma redundante con validación teórica y experimental
Este trabajo se centra en el problema de aprendizaje desde las multitudes, donde la actualización conjunta de los pesos del clasificador y las matrices de confusión de los trabajadores puede ayudar en el problema de estimación con etiquetas raras de las multitudes.
Propone un algoritmo de aprendizaje supervisado para modelar la calidad de las etiquetas y de los trabajadores y utiliza el algoritmo para estudiar cuánta redundancia se requiere en el crowdsourcing y si una baja redundancia con abundantes ejemplos de ruido conduce a mejores etiquetas.
Nueva forma de explicar por qué una red neuronal ha clasificado mal una imagen
Este trabajo propone un método para explicar los errores de clasificación de las redes neuronales. 
Pretende comprender mejor la clasificación de las redes neuronales y explora el espacio latente de un autocodificador variacional y considera las perturbaciones del espacio latente para obtener la clasificación correcta.
Un método para la construcción automatizada de redes multitarea ramificadas con una fuerte evaluación experimental en diversos conjuntos de datos multitarea.
Este trabajo propone un novedoso marco de aprendizaje multitarea con parámetros suaves basado en una estructura en forma de árbol.
Este trabajo presenta un método para inferir la arquitectura de las redes multitarea para determinar qué parte de la red debe ser compartida entre las diferentes tareas.
Es posible sustituir la matriz de pesos en una capa convolucional para entrenarla como una capa estructurada eficiente; funcionando tan bien como la descomposición de bajo rango.
Este trabajo aplica las anteriores capas lineales estructuradas eficientes a las capas convolucionales y propone las capas convolucionales estructuradas eficientes como sustitución de las capas convolucionales originales.
Presentamos SVDocNet, una red neuronal recurrente espacial (RNN) basada en U-Net, entrenable de principio a fin, para el desenfoque ciego de documentos.
Ampliamos la codificación dispersa bilineal y aprovechamos las secuencias de vídeo para aprender filtros dinámicos.
Proponemos un nuevo detector de OOD que emplea imágenes borrosas como ejemplos adversos. Nuestro modelo logra un rendimiento significativo en la detección de OOD en varios dominios.
Este trabajo presenta la idea de utilizar imágenes borrosas como ejemplos regularizadores para mejorar el rendimiento de la detección de fuera de la distribución basada en la destilación de redes aleatorias.
Este trabajo aborda la distribución fuera de datos aprovechando la RND aplicada a los aumentos de datos mediante el entrenamiento de un modelo para hacer coincidir las salidas de una red aleatoria con un aumento como entrada.
Un optimizador rápido para aplicaciones generales y entrenamiento de grandes lotes.
En este trabajo, los autores realizaron un estudio sobre el entrenamiento de grandes lotes para el BERT, y entrenaron con éxito un modelo BERT en 76 minutos.
Este trabajo desarrolla una estrategia de adaptación por capas que permite entrenar modelos BERT con minilotes grandes de 32k frente a la línea base de 512.
Analizamos el papel de dos tasas de aprendizaje en el meta-aprendizaje agnóstico del modelo en la convergencia.
Los autores abordaron el problema de la inestabilidad de la optimización en MAML investigando las dos tasas de aprendizaje.
Este trabajo estudia un método para ayudar a afinar las dos tasas de aprendizaje utilizadas en el algoritmo de entrenamiento MAML.
Modelo neuronal independiente de la tarea para el aprendizaje de asociaciones entre grupos de palabras interrelacionadas.
El artículo propone un método para el entrenamiento de vectores de palabras con funciones específicas, en el que cada palabra se representa con tres vectores, cada uno de ellos en una categoría diferente (Sujeto-Verbo-Objeto).
Este trabajo propone una red neuronal para aprender representaciones de trabajo específicas de la función y demuestra la ventaja sobre las alternativas.
Uso del método de aprendizaje profundo para realizar la medición automática de imágenes SEM en la industria de los semiconductores
Este documento describe y analiza tres métodos para programar actividades de duración no fija en presencia de recursos consuntivos.
Este artículo presenta tres enfoques para la programación de actividades a bordo de un vehículo planetario con limitaciones de recursos.
Descripción de la presentación a NeurIPS2019 Reto de desentrañamiento basado en autocodificadores variacionales hiperesféricos
Una detección de anomalías que: utiliza la clasificación de transformación aleatoria para generalizar a los datos que no son imágenes.
Este trabajo propone un método profundo para la detección de anomalías que unifica los recientes enfoques de clasificación profunda de una clase y de clasificación basada en la transformación.
Este trabajo propone un enfoque para la detección de anomalías basada en la clasificación para datos generales utilizando la transformación afín y = Wx+b.
Reducimos los sesgos de sentimiento basándonos en la evaluación contrafactual de la generación de textos mediante modelos lingüísticos.
Este trabajo mide el sesgo del sentimiento en los modelos lingüísticos, tal y como se refleja en el texto generado por los modelos, y añade otros términos objetivos al objetivo habitual de los modelos lingüísticos para reducir el sesgo.
Este artículo propone evaluar el sesgo de los modelos lingüísticos preformados utilizando un sistema de sentimiento fijo y prueba varias plantillas de prefijos diferentes.
Un método basado en la similitud semántica y un método basado en la similitud de sentimiento para debias los modelos neurales del lenguaje entrenados a partir de grandes conjuntos de datos.
Un modelo tópico bayesiano no paramétrico con autocodificadores variacionales que alcanza el estado de la técnica en los puntos de referencia públicos en términos de perplejidad, coherencia tópica y tareas de recuperación.
Este trabajo construye un modelo temático infinito con autocodificadores variacionales combinando el autocodificador variacional de Nalisnick y Smith con la asignación latente de Dirichlet y varias técnicas de inferencia utilizadas en Miao.
Presentamos un novedoso marco de Destilación de Conocimientos que utiliza muestras de compañeros como profesor
Propone un método para mejorar la eficacia de la destilación de conocimientos suavizando las etiquetas utilizadas y empleando un conjunto de datos en lugar de una sola muestra.
Este trabajo propone abordar el coste computacional adicional del entrenamiento con la destilación de conocimientos, basándose en la técnica de destilación de instantáneas recientemente propuesta.
aprender subpolíticas jerárquicas mediante un entrenamiento de extremo a extremo sobre una distribución de tareas
Los autores consideran el problema de aprender un conjunto útil de "sub-políticas" que puedan ser compartidas entre las tareas para iniciar el aprendizaje en nuevas tareas extraídas de la distribución de tareas. 
Este trabajo propone un método novedoso para inducir la estructura jerárquica temporal en un entorno especializado de tareas múltiples.
Modelo de red neuronal convolucional para la incrustación no supervisada de documentos.
Introduce un nuevo modelo para la tarea general de inducir representaciones de documentos (embeddings) que utiliza una arquitectura CNN para mejorar la eficiencia computacional.
Este artículo propone el uso de CNNs con un objetivo similar al de los "skip-grams" como una forma rápida de producir incrustaciones de documentos
Demostramos límites de generalización para las redes neuronales convolucionales que tienen en cuenta la vinculación de pesos
Estudia el poder de generalización de las CNN y mejora los límites superiores de los errores de generalización, mostrando la correlación entre el error de generalización de las CNN aprendidas y el término dominante del límite superior.
Este trabajo presenta un límite de generalización para las redes neuronales convolucionales basado en el número de parámetros, la constante de Lipschitz y la distancia de los pesos finales desde la inicialización.
Ahorro de 2 veces en el tamaño del modelo y reducción del 28% de la energía para MobileNets en ImageNet sin pérdida de precisión utilizando capas híbridas compuestas por filtros convencionales de precisión total y filtros ternarios
Se centra en cuantificar la arquitectura de MobileNets a valores ternarios, reduciendo el espacio y el cálculo necesarios para que las redes neuronales sean más eficientes energéticamente.
El artículo propone un banco de filtros híbrido por capas que sólo cuantifica una fracción de los filtros convolucionales a valores ternarios hacia la arquitectura MobileNets.
Establecemos un punto de referencia de ruido real controlado y revelamos varias conclusiones interesantes sobre los datos ruidosos del mundo real.
Este artículo compara 6 métodos existentes de aprendizaje de etiquetas ruidosas en dos escenarios de entrenamiento: desde cero y con ajuste fino.
Los autores establecen un gran conjunto de datos y un punto de referencia de ruido controlado del mundo real para realizar experimentos controlados con datos ruidosos en el aprendizaje profundo.
Aprendemos a resolver el problema de diseño de ARN con aprendizaje por refuerzo utilizando enfoques de meta aprendizaje y autoML.
Utilización de la optimización del gradiente de la política para generar secuencias de ARN que se pliegan en una estructura secundaria objetivo, lo que resulta en claras mejoras de precisión y tiempo de ejecución. 
El entrenamiento de redes pequeñas es mejor que la poda, pero la poda encuentra buenas redes pequeñas para entrenar que son fáciles de copiar.
Estudiamos el problema de aprender a predecir la diversidad subyacente de creencias presente en los dominios de aprendizaje supervisado.
Introducimos una estrategia que permite repintar modelos en conjuntos de datos de distintos tamaños
Ayudar a pintar imágenes mediante GANs utilizando un filtro de aumento comparativo y añadiendo ruido aleatorio a cada píxel.
Encontramos pruebas de que la minimización de la divergencia puede no ser una caracterización precisa del entrenamiento de GAN.
El objetivo de este trabajo es presentar pruebas empíricas de que la teoría de la minimización de la divergencia es más una herramienta para entender el resultado del entrenamiento de los GANs que una condición necesaria que debe cumplirse durante el propio entrenamiento
Este trabajo estudia los GANs no saturados y el efecto de dos enfoques de gradiente penalizado, considerando varios experimentos de pensamiento para demostrar las observaciones y validarlas en experimentos de datos reales.
Un nuevo y práctico test estadístico de dependencia mediante redes neuronales, evaluado en conjuntos de datos de RMN sintéticos y reales.
Propone una estimación de la información mutua basada en redes neuronales que puede funcionar de forma fiable con conjuntos de datos pequeños, reduciendo la complejidad de la muestra al desacoplar el problema de aprendizaje de la red y el problema de estimación.
Subtitulado de imágenes mediante incrustación bidimensional de palabras.
LEAP combina la fuerza del muestreo adaptativo con la del aprendizaje en línea por mini lotes y el aprendizaje de representación adaptativo para formular una estrategia representativa de autocontrol en un protocolo de entrenamiento de DNN de extremo a extremo. 
Introduce un método para crear mini lotes para una red de estudiantes utilizando un segundo espacio de representación aprendido para seleccionar dinámicamente los ejemplos por su "facilidad y verdadera diversidad".
Experimenta la precisión de la clasificación en los conjuntos de datos MNIST, FashionMNIST y CIFAR-10 para aprender una representación con la selección de minilotes del estilo de aprendizaje del currículo en un marco de trabajo de extremo a extremo.
Proponemos construir macroacciones mediante un algoritmo genético, que elimina la dependencia del procedimiento de derivación de macroacciones de las políticas pasadas del agente.
Este trabajo propone un algoritmo genérico para construir macro acciones para el aprendizaje profundo por refuerzo, añadiendo una macro acción al espacio de acciones primitivas.
Proponemos una extensión de LFADS capaz de inferir trenes de espigas para reconstruir las trazas de fluorescencia de calcio utilizando VAE jerárquicas.
Presentamos el primer método exitoso para entrenar la traducción automática neural de forma no supervisada, utilizando únicamente corpus monolingües
Los autores presentan un modelo de NMT no supervisado que no requiere corpus paralelos entre las dos lenguas de interés. 
Este es un trabajo sobre la MT no supervisada que entrena una arquitectura estándar utilizando incrustaciones de palabras en un espacio de incrustación compartido sólo con documentos de palabras bilingües y un codificador-decodificador entrenado utilizando datos monolingües.
Entrenamos las redes generativas adversariales de forma progresiva, lo que nos permite generar imágenes de alta resolución con gran calidad.
Introduce el crecimiento progresivo y una sencilla función estadística de resumen de minilotes sin parámetros para su uso en el entrenamiento de GAN para permitir la síntesis de imágenes de alta resolución.
Una CNN esférica basada en gráficos que logra un interesante equilibrio de compensaciones para una amplia variedad de aplicaciones.
Combina marcos de CNN existentes basados en la discretización de una esfera como un gráfico para mostrar un resultado de convergencia que está relacionado con la equivalencia de rotación en una esfera.
Los autores utilizan la formulación existente de la CNN gráfica y una estrategia de agrupación que explota las pixelaciones jerárquicas de la esfera para aprender de la esfera discretizada.
Demostramos relaciones de fluctuación-disipación para el SGD, que pueden utilizarse para (i) establecer de forma adaptativa las tasas de aprendizaje y (ii) sondear las superficies de pérdida.
Los conceptos de Paper funcionan en el formalismo de tiempo discreto, utilizan la ecuación maestra y eliminan la dependencia de una aproximación localmente cuadrática de la función de pérdida o de cualquier suposición gaussiana del ruido SGD. 
Los autores derivan las relaciones estacionarias de fluctuación-disipación que vinculan las cantidades medibles y los hiperparámetros en SGD y utilizan las relaciones para establecer el programa de entrenamiento de forma adaptativa y analizar el panorama de la función de pérdida.
Proponemos un mecanismo para denotar el estado interno de una RNN para mejorar el rendimiento de la generalización.
Para entornos dictados parcialmente por procesos de entrada externos, derivamos una línea de base dependiente de la entrada que reduce de forma demostrable la varianza de los métodos de gradiente de política y mejora el rendimiento de la política en una amplia gama de tareas de RL.
Los autores consideran el problema del aprendizaje en entornos dependientes de la entrada, muestran cómo el teorema PG sigue siendo válido para un crítico consciente de la entrada, y muestran que las líneas de base dependientes de la entrada son las mejores para utilizar en la conjetura con ese crítico.
Este artículo introduce la noción de líneas de base dependientes de la entrada en los métodos de gradiente de política en RL, y propone diferentes métodos para entrenar la función de línea de base dependiente de la entrada para ayudar a despejar la varianza de la perturbación del factor externo.
Aumentar la capa superior de una red clasificadora con una memoria de estilo le permite ser generativa.
Este trabajo propone entrenar una red neuronal clasificadora no sólo para clasificar, sino también para reconstruir una representación de su entrada, con el fin de factorizar la información de clase a partir de la apariencia .
El artículo propone entrenar un autocodificador de manera que la representación de la capa intermedia consista en la etiqueta de clase de la entrada y una representación de vector oculto
Los modelos de enrutamiento por ejemplo se benefician de la diversidad arquitectónica, pero siguen teniendo dificultades para escalar a un gran número de decisiones de enrutamiento.
Añade diversidad al tipo de unidad arquitectónica disponible para el router en cada decisión y escala a redes más profundas, logrando un rendimiento de vanguardia en Omniglot. 
Este trabajo amplía las redes de enrutamiento para utilizar diversas arquitecturas a través de módulos enrutados
Presentamos RNNs para el entrenamiento de modelos sustitutos de PDEs, donde las restricciones de consistencia aseguran que las soluciones son físicamente significativas, incluso cuando el entrenamiento utiliza dominios mucho más pequeños que el modelo entrenado se aplica.
Proponemos el uso del espejo optimista decente para abordar los problemas de ciclismo en el entrenamiento de GANs. También introducimos el algoritmo Optimistic Adam
Este trabajo propone el uso del descenso en espejo optimista para entrenar WGANs
El artículo propone utilizar el descenso de gradiente optimista para el entrenamiento de GAN que evita el comportamiento cíclico observado con SGD y sus variantes y proporciona resultados prometedores en el entrenamiento de GAN.
Este trabajo propone una sencilla modificación del descenso de gradiente estándar, que pretende mejorar la convergencia de los GAN y otros problemas de optimización minimax.
Una simple extensión de la factorización matricial generalizada puede superar a los enfoques más avanzados para la recomendación.
El trabajo presenta un marco de factorización matricial para reforzar el efecto de los datos históricos cuando se aprenden las preferencias de los usuarios en entornos de filtrado colaborativo.
Un método que construye representaciones de datos secuenciales y su dinámica mediante modelos generativos con un proceso activo
Combina redes neuronales y distribuciones gaussianas para crear una arquitectura y un modelo generativo para imágenes y vídeo que minimice el error entre las imágenes generadas y las suministradas.
El artículo propone un modelo de red bayesiana, realizado como una red neuronal, que aprende diferentes datos en forma de un sistema dinámico lineal
Proponemos el polinomio como función de activación.
Los autores introducen funciones de activación aprendibles parametrizadas por funciones polinómicas y muestran resultados ligeramente mejores que ReLU.
Un sencillo método de motivación intrínseca que utiliza el error del modelo de dinámica hacia delante en el espacio de características de la política.
Demostramos que los VAE desentrañados son más robustos que los VAE de vainilla frente a los ataques adversarios que pretenden engañarlos para que decodifiquen la entrada adversaria a un objetivo elegido. A continuación, desarrollamos un VAE jerárquico desentrañado aún más robusto, el Seatbelt-VAE.
Los autores proponen un nuevo modelo de VAE llamado seatbelt-VAE, que demuestra ser más robusto para el ataque latente que los puntos de referencia.
Demostramos que los cambios de función en la retropropagación equivalen a una tasa de aprendizaje implícita
Un enfoque de aprendizaje por refuerzo para la transferencia de estilo de texto
Introduce un método basado en RL que aprovecha un modelo lingüístico preentrenado para transferir el estilo del texto, sin un objetivo de desentrañamiento, mientras se utilizan generaciones de transferencia de estilo de otro modelo.
Los autores proponen una recompensa combinada compuesta por fluidez, contenido y estilo para la transferencia de estilo de los textos.
Demostramos que la jerarquía semántica altamente estructurada emerge en las representaciones generativas profundas como resultado para sintetizar escenas.
El artículo investiga los aspectos codificados por las variables latentes introducidas en las diferentes capas de StyleGAN.
El artículo presenta una interpretación guiada visualmente de las activaciones de las capas de convolución en el generador de StyleGAN sobre el diseño, la categoría de la escena, los atributos de la escena y el color.
Agrupamos los mensajes entre múltiples cadenas SMILES de la misma molécula para pasar la información a lo largo de todos los caminos a través del grafo molecular, produciendo representaciones latentes que superan significativamente el estado del arte en una variedad de tareas.
El método utiliza múltiples entradas de cadenas SMILES, la fusión de características en función de los caracteres de esas cadenas y el entrenamiento de la red a través de múltiples objetivos de salida de cadenas SMILES, creando una representación latente robusta de longitud fija independiente de la variación de SMILES.  
Los autores describen un novedoso método de tipo autoencoder variacional para moléculas que codifican las moléculas como cadenas para reducir las operaciones necesarias para compartir información entre los átomos de la molécula.
Proponemos un enfoque sencillo y general que evita el problema del colapso de los modos en varias GAN condicionales.
El artículo propone un término de regularización para el objetivo del GAN condicional con el fin de promover la generación multimodal diversa y evitar el colapso de los modos.
El artículo propone un método para generar diversas salidas para varios marcos GAN condicionales, incluyendo la traducción de imagen a imagen, el pintado de imágenes y la predicción de vídeo, que puede aplicarse a varios marcos de síntesis condicional para diversas tareas. 
Equipar el modelo transformador con accesos directos a la capa de incrustación libera la capacidad del modelo para aprender información novedosa.
Examinamos la relación entre los valores de densidad de probabilidad y el contenido de la imagen en GANs no invertibles.
Los autores tratan de estimar la distribución de probabilidad de la imagen con la ayuda de GAN y desarrollan una aproximación adecuada a las PDF en el espacio latente.
Proponemos la convolución barajada espacialmente que la convolución regular incorpora la información de fuera de su campo receptivo.
Propone la convulación SS que utiliza información fuera de su RF, mostrando resultados mejorados cuando se prueba en múltiples modelos CNN.
Los autores propusieron una estrategia de barrido para las capas de convolución en las redes neuronales convolucionales.
Un método para modelar la distribución generativa de secuencias procedentes de entidades conectadas a un grafo.
Los autores proponen un método para modelar datos secuenciales procedentes de múltiples fuentes interconectadas utilizando una mezcla de pool común de HMM's.
Nuestro trabajo aplica el meta-aprendizaje al aprendizaje por refuerzo multiagente para ayudar a nuestro agente a adaptarse eficazmente a los nuevos adversarios que se avecinan.
Este trabajo se centra en la adaptación rápida a los nuevos comportamientos de los demás agentes del entorno mediante un método basado en MAML
El artículo presenta un enfoque del aprendizaje multiagente basado en el marco del metaaprendizaje agnóstico para la tarea de modelado de adversarios para la RL multiagente.
Caracterizamos los valores singulares de la transformación lineal asociada a una capa convolucional multicanal 2D estándar, lo que permite su cálculo eficiente. 
El artículo está dedicado al cálculo de los valores singulares de las capas convolucionales
Deriva fórmulas exactas para calcular los valores singulares de las capas convolucionales de las redes neuronales profundas y muestra que el cálculo de los valores singulares puede hacerse mucho más rápido que el cálculo de la SVD completa de la matriz de convolución apelando a transformaciones FFT rápidas.
VariBAD abre un camino hacia la exploración aproximada óptima de Bayes para la RL profunda utilizando ideas del meta-aprendizaje, la RL bayesiana y la inferencia variacional aproximada.
Este artículo presenta un nuevo método de aprendizaje profundo por refuerzo que puede compensar eficientemente la exploración y la explotación que combina el meta-aprendizaje, la inferencia variacional y la RL bayesiana.
Demostramos que el aprendizaje métrico puede ayudar a reducir el olvido catastrófico
Este trabajo aplica el aprendizaje métrico para reducir el olvido catastrófico en las redes neuronales mejorando la expresividad de la capa final, lo que conduce a mejores resultados en el aprendizaje continuo.
Presentamos NormCo, un modelo de coherencia profunda que tiene en cuenta la semántica de una mención de entidad, así como la coherencia tópica de las menciones dentro de un mismo documento para realizar la normalización de entidades de enfermedad.
Utiliza un autocodificador GRU para representar el "contexto" (las entidades relacionadas con una determinada enfermedad en el ámbito de una frase), resolviendo la tarea BioNLP con mejoras significativas sobre los métodos más conocidos.
Exploramos el papel de la interacción multiplicativa como marco unificador para describir una serie de motivos arquitectónicos de redes neuronales clásicas y modernas, como la compuerta, las capas de atención, las hiperredes y las convoluciones dinámicas, entre otros.
Presenta la interacción multiplicativa como una caracterización unificada para representar componentes de diseño de arquitectura de modelos comúnmente utilizados, mostrando pruebas empíricas de un rendimiento superior en tareas como la RL y el modelado de secuencias.
El artículo explora diferentes tipos de interacciones multiplicativas y encuentra modelos de MI capaces de alcanzar un rendimiento de vanguardia en problemas de modelado de lenguaje y aprendizaje por refuerzo.
Un marco de trabajo GAN de condicionamiento de texto eficaz para generar vídeos a partir de texto
Este trabajo presenta un método basado en GAN para la generación de vídeo condicionado a la descripción del texto, con un nuevo método de condicionamiento que genera filtros de convolución a partir del texto codificado, y los utiliza para una convolución en el discriminador.
Este artículo propone modelos GAN condicionales para la síntesis de texto a vídeo: desarrollo de filtros CNN condicionados por las características del texto y construcción de un conjunto de datos de formas en movimiento con un rendimiento mejorado en la generación de vídeo/imágenes.
SplitLBI se aplica al aprendizaje profundo para explorar la dispersión estructural del modelo, logrando un rendimiento de vanguardia en ImageNet-2012 y desvelando una arquitectura de subred eficaz.
Propone un algoritmo basado en la optimización para encontrar estructuras dispersas importantes de redes neuronales a gran escala, acoplando el aprendizaje de la matriz de pesos y las restricciones de dispersidad, ofreciendo una convergencia garantizada en problemas de optimización no convexos.
Proponemos mecanismos de gated para mejorar la ISTA aprendida para la codificación dispersa, con garantías teóricas sobre la superioridad del método. 
Propone extensiones de LISTA que abordan la subestimación mediante la introducción de "puertas de ganancia" y la inclusión del impulso con "puertas de rebasamiento", mostrando una mejora en las tasas de convergencia.
Este trabajo se centra en la resolución de problemas de codificación dispersa utilizando redes de tipo LISTA, proponiendo una "función de compuerta de ganancia" para mitigar la debilidad de la suposición de "ningún falso positivo".
Presentamos un marco eficiente y adaptativo para comparar clasificadores de imágenes con el fin de maximizar las discrepancias entre los clasificadores, en lugar de comparar en conjuntos de prueba fijos.
Mecanismo de detección de errores que compara los clasificadores de imágenes mediante el muestreo de su conjunto de pruebas "más discordantes", midiendo el desacuerdo a través de una distancia consciente de la semántica derivada de la ontología WordNet.
Proponemos una técnica que modifica las estructuras de las CNN para mejorar su robustez manteniendo una alta precisión en las pruebas, y planteamos la duda de si la definición actual de los ejemplos adversarios es adecuada al generar ejemplos adversarios capaces de engañar a los humanos.
Este trabajo propone una técnica sencilla para mejorar la robustez de las redes neuronales frente a los ataques de caja negra.
Los autores proponen un método sencillo para aumentar la robustez de las redes neuronales convolucionales frente a ejemplos adversos, con resultados sorprendentemente buenos.
Proponemos comparar el aprendizaje semisupervisado y robusto a la etiqueta ruidosa bajo un escenario compartido
Los autores proponen una estrategia basada en la mezcla para el entrenamiento de un modelo en un entorno formal que incluye las tareas de aprendizaje semisupervisado y robusto como casos especiales.
Este artículo demuestra experimentalmente el efecto beneficioso de las conexiones descendentes en el algoritmo de codificación jerárquica dispersa.
Este artículo presenta un estudio que compara técnicas para la codificación jerárquica dispersa, mostrando que el término top-down es beneficioso para reducir el error de predicción y puede aprender más rápido.
Un enfoque de caja negra para explicar las predicciones de un modelo de similitud de imágenes.
Introduce un método para la explicación del modelo de similitud de imágenes que identifica los atributos que contribuyen positivamente a la puntuación de similitud y los empareja con un mapa de saliencia generado.
El artículo propone un mecanismo de explicación que empareja las regiones típicas del mapa de saliencia junto con los atributos para las redes neuronales profundas de coincidencia de similitud.
Cómo se deben evaluar los ataques adversarios a seq2seq
Los autores investigan formas de generar ejemplos adversarios, mostrando que el entrenamiento adversario con el ataque más consistente con los criterios de preservación del significado introducidos resulta en una robustez mejorada frente a este tipo de ataque sin degradación en el escenario no adversario.
El artículo trata de las perturbaciones adversarias que preservan el significado en el contexto de los modelos Seq2Seq
Una técnica de normalización alternativa a la normalización por lotes
Introduce una técnica de normalización, que normaliza los pesos de las capas convolucionales. 
Este manuscrito introduce una nueva transformación por capas, EquiNorm, para mejorar la normalización por lotes que no modifica las entradas de las capas sino los pesos de las mismas.
Representar cada entidad como una distribución de probabilidad sobre contextos incrustados en un espacio de tierra.
Propone construir incrustaciones de palabras a partir de un histograma sobre las palabras del contexto, en lugar de como vectores puntuales, lo que permite medir las distancias entre dos palabras en términos de transporte óptimo entre los histogramas a través de un método que aumenta la representación de una entidad a partir de un "punto en un espacio vectorial" estándar a un histograma con bins situados en algunos puntos de ese espacio vectorial. 
Deberían esperarse pequeñas perturbaciones adversas dadas las tasas de error observadas de los modelos fuera de la distribución natural de los datos.
Este trabajo propone una visión alternativa para los ejemplos adversariales en espacios de alta dimensión considerando la "tasa de error" en una distribución gaussiana centrada en cada punto de prueba.
Estudia cómo interactúan el aprendizaje autosupervisado y la destilación de conocimientos en el contexto de la construcción de modelos compactos.
Investiga el entrenamiento de modelos lingüísticos compactos preentrenados a través de la destilación y muestra que el uso de un profesor para destilar un modelo compacto de estudiante funciona mejor que el preentrenamiento directo del modelo.
Esta presentación muestra que el preentrenamiento de un estudiante directamente en el modelado del lenguaje enmascarado es mejor que la destilación, y lo mejor es combinar ambos y destilar a partir de ese modelo de estudiante preentrenado.
Introducimos el esquema universal de compresión de redes neuronales profundas, que es aplicable universalmente para la compresión de cualquier modelo y puede funcionar de forma casi óptima independientemente de su distribución de pesos.
Presenta un canal para la compresión de redes que es similar a la compresión profunda y utiliza la cuantificación reticular aleatoria en lugar de la clásica cuantificación vectorial, y utiliza la codificación universal de fuentes (bzip2) en lugar de la codificación Huffman.
Este artículo trata de abordar de forma preliminar el desentrañamiento de forma teórica en la situación idealista y de forma práctica a través de la perspectiva del modelado del ruido en el caso realista.
Estudia la importancia de la modelización del ruido en la VAE gaussiana y propone entrenar el ruido utilizando el método Empirical-Bayes.
Modificación del tratamiento de los factores de ruido en la elaboración de los modelos VAE
Investigamos la regularización de la caída del peso para diferentes optimizadores e identificamos tres mecanismos distintos por los que la caída del peso mejora la generalización.
Discute el efecto del decaimiento de los pesos en el entrenamiento de los modelos de redes profundas con y sin normalización de lotes y cuando se utilizan métodos de optimización de primer/segundo orden y plantea la hipótesis de que una mayor tasa de aprendizaje tiene un efecto de regularización.
El primer conjunto de datos de adaptación de dominio disponible de forma gratuita para la detección de eventos sonoros.
Estimador de información mutua basado en la mecánica estadística no extensiva
Este trabajo trata de establecer nuevos límites inferiores variacionales para la información mutua introduciendo el parámetro q y definiendo el álgebra q, mostrando que los límites inferiores tienen menor varianza y alcanzan valores altos.
Demostramos que el ascenso por gradiente estocástico converge a un óptimo global para WGAN con una red generadora de una capa.
Intenta demostrar que el Descenso Gradiente Estocástico-Ascenso podría converger a una solución global para el problema min-max de WGAN.
Demostramos empíricamente que el entrenamiento adversarial es eficaz para eliminar las perturbaciones universales, hace que los ejemplos adversariales sean menos robustos a las transformaciones de la imagen y los deja detectables para un enfoque de detección.
Analiza el entrenamiento adversario y su efecto en los ejemplos adversarios universales así como en los ejemplos adversarios estándar (iteración básica) y cómo el entrenamiento adversario afecta a la detección. 
Los autores muestran que el entrenamiento adversarial es eficaz para proteger contra la perturbación adversarial "compartida", en particular contra la perturbación universal, pero menos eficaz para proteger contra las perturbaciones singulares.
Introducimos técnicas para entrenar una única red que se adapte a muchas plataformas de hardware.
El método da como resultado una red de la que se pueden extraer subredes para diversas limitaciones de recursos (latencia, memoria) que funcionan bien sin necesidad de reentrenamiento.
Este trabajo trata de abordar el problema de la búsqueda de las mejores arquitecturas para escenarios de despliegue con restricciones de recursos especializados con un método NAS basado en la predicción.
Proponer un enfoque para potenciar los modelos generativos mediante modelos de variables ocultas en cascada
Este trabajo propone un novedoso enfoque de boosting en cascada para potenciar los modelos generativos que permite entrenar cada metamodelo por separado y con avidez.
Buscamos la estructura de las frases en ELMo y en modelos de incrustación contextual relacionados. Descubrimos que los modelos existentes codifican eficazmente la sintaxis y muestran evidencias de dependencias de largo alcance, pero sólo ofrecen pequeñas mejoras en tareas semánticas.
Propone el método de "sondeo de bordes" y se centra en la relación entre los espacios en lugar de las palabras individuales, lo que permite a los autores observar la constitución sintáctica, las dependencias, las etiquetas de entidades y el etiquetado de roles semánticos.
Proporciona nuevos conocimientos sobre lo que se capta de las incrustaciones de palabras contextualizadas mediante la recopilación de un conjunto de tareas de "sondeo de bordes". 
Introducimos DPFRL, un marco para el aprendizaje por refuerzo bajo observaciones parciales y complejas con un filtro de partículas discriminativo totalmente diferenciable
Introduce ideas para entrenar a los agentes DLR con variables de estado latentes, modeladas como una distribución de creencias, para que puedan manejar entornos parcialmente observados.
Este trabajo introduce un método de principios para POMDP RL: Aprendizaje por Refuerzo con Filtro de Partículas Discriminativo que permite razonar con observaciones parciales a lo largo de múltiples pasos de tiempo, logrando el estado del arte en los benchmarks.
Los objetivos de Monte Carlo se analizan mediante la inferencia variacional de variables auxiliares, lo que da lugar a un nuevo análisis de CPC y NCE, así como a un nuevo modelo generativo.
Propone un punto de vista diferente sobre la mejora de los límites variacionales con modelos de variables latentes auxiliares y explora el uso de esos modelos en el modelo generativo.
Mejoramos la ejecución de todos los algoritmos de descenso de gradiente existentes.
Los autores proponen el muestreo de gradientes estocásticos a partir de una función monótona proporcional a las magnitudes de los gradientes mediante el uso de LSH. 
Considera el SGD sobre un objetivo de la forma de una suma sobre ejemplos de una pérdida cuadrática.
En general, se reconocen las limitaciones de la IA actual, pero menos gente es consciente de que entendemos lo suficiente sobre el cerebro como para ofrecer inmediatamente formulaciones novedosas de IA.
Utilizamos la respuesta a preguntas para evaluar cuánto conocimiento sobre el entorno pueden aprender los agentes mediante la predicción autosupervisada.
Propone la GC como herramienta para investigar lo que los agentes aprenden en el mundo, argumentando que es un método intuitivo para los humanos que permite una complejidad arbitraria.
Los autores proponen un marco para evaluar las representaciones construidas por los modelos predictivos que contienen suficiente información para responder a las preguntas sobre el entorno en el que se entrenan, mostrando que las de SimCore contenían suficiente información para que el LSTM respondiera a las preguntas con precisión.
Desarrollamos un nuevo método de clasificación desequilibrada utilizando ejemplos adversarios
Propone un nuevo objetivo de optimización que genera muestras sintéticas mediante el sobremuestreo de las clases mayoritarias en lugar de las minoritarias, resolviendo el problema del sobreajuste de las clases minoritarias.
Los autores proponen abordar la clasificación de desequilibrio utilizando métodos de remuestreo, mostrando que los ejemplos adversos en la clase minoritaria ayudarían a entrenar un nuevo modelo que generalice mejor.
Una interesante aplicación de la CNN en experimentos de física de la materia condensada blanda.
Los autores demuestran que un enfoque de aprendizaje profundo ofrece una mejora tanto en la precisión de la identificación como en la velocidad a la que se pueden identificar los defectos de los cristales líquidos nemáticos.
Aplicar un modelo neuronal bien conocido (YOLO) para detectar cajas delimitadoras de objetos en imágenes.
Un análisis de los efectos de la composicionalidad y la localidad en el aprendizaje de la representación para el aprendizaje de tiro cero.
Propone un marco de evaluación para ZSL en el que no se permite preentrenar el modelo y, en su lugar, los parámetros del modelo se inicializan aleatoriamente para comprender mejor lo que ocurre en ZSL.
El error adversarial tiene una forma de ley de potencia similar para todos los conjuntos de datos y modelos estudiados, y la arquitectura es importante.
Presentamos una formulación de la curiosidad como un problema de aprendizaje de representación visual y mostramos que permite obtener buenas representaciones visuales en los agentes.
Este artículo formula el entrenamiento de la RL basada en la curiosidad como el aprendizaje de un modelo de representación visual, argumentando que centrándose en una mejor LR y maximizando la pérdida del modelo para las escenas novedosas se obtendrá un mejor rendimiento general.
A partir de un escaneo RGB-D incompleto de una escena, pretendemos detectar las instancias individuales de los objetos que la componen e inferir su geometría completa.
Propone una estructura CNN 3D de extremo a extremo que combina características de color y características 3D para predecir la estructura 3D que falta en una escena a partir de escaneos RGB-D.
Los autores proponen una novedosa red convolucional 3D de extremo a extremo que predice la finalización de instancias semánticas 3D como cuadros delimitadores de objetos, etiquetas de clase y geometría completa de objetos.
XGAN es un modelo no supervisado para la traducción de imagen a imagen a nivel de rasgo aplicado a problemas de transferencia de estilo semántico como la tarea de cara a caricatura, para la que presentamos un nuevo conjunto de datos.
Este artículo propone un nuevo modelo basado en GAN para la traducción de imagen a imagen no emparejada, similar a la DTN
Los trabajadores envían señales de gradiente al servidor, y la actualización se decide por mayoría. Demostramos que este algoritmo es convergente, eficiente en la comunicación y tolerante a los fallos, tanto en la teoría como en la práctica.
Presenta una implementación distribuida de signSGD con voto mayoritario como agregación.
Corregimos las variaciones molestas de las incrustaciones de imágenes en diferentes dominios, preservando sólo la información relevante.
Discute un método para ajustar las incrustaciones de imágenes con el fin de separar la variación técnica de la señal biológica.
Los autores presentan un método para eliminar la información específica del dominio, preservando al mismo tiempo la información biológica relevante, mediante el entrenamiento de una red que minimiza la distancia de Wasserstein entre las distribuciones.
Un estimador de información mutua escalable en tamaño de muestra y dimensiones.
La nueva combinación de refuerzo y aprendizaje supervisado, disminuye drásticamente el número de muestras necesarias para el entrenamiento en vídeo
Este trabajo propone aprovechar los datos controlados etiquetados para acelerar el aprendizaje basado en el refuerzo de una política de control
Aprendizaje rápido a través de la memoria episódica verificado por un marco biológicamente plausible para el circuito corteza prefrontal-ganglios basales-hipocampo (PFC-BG)
En este trabajo, señalamos una nueva conexión entre la expresividad de las DNNs y el Teorema de Sharkovsky de los sistemas dinámicos, que nos permite caracterizar las compensaciones profundidad-anchura de las redes ReLU 
Muestra cómo el poder expresivo de las NN depende de su profundidad y anchura, lo que permite comprender mejor las ventajas de las redes profundas para representar determinadas clases de funciones.
Los autores derivan las condiciones de compromiso profundidad-anchura para cuando las redes relu son capaces de representar funciones periódicas utilizando el análisis de sistemas dinámicos.
Investigamos el entrenamiento consciente de la cuantificación en los localizadores de palabras clave con cuantificación de bits muy baja para reducir el coste de la localización de palabras clave en el dispositivo.
Esta presentación propone una combinación de descomposición de bajo rango y un enfoque de cuanitización para comprimir modelos DNN para la detección de palabras clave.
Un nuevo marco de procesamiento de señales gráficas para cuantificar los efectos de las perturbaciones experimentales en los datos biomédicos de una sola célula.
Este artículo presenta varios métodos para procesar los resultados experimentales en células biológicas y propone un algoritmo MELD que mapea las asignaciones de grupos duros a asignaciones blandas, lo que permite agrupar grupos relevantes de células.
Proponemos una clase de modelos de usuario basados en el uso de procesos gaussianos aplicados a un espacio transformado definido por reglas de decisión
Proponemos un algoritmo de optimización bayesiana óptimo para el ajuste de hiperparámetros explotando aproximaciones baratas.
Estudia la optimización de hiperparámetros mediante la optimización bayesiana, utilizando el marco del Gradiente de Conocimiento y permitiendo que el optimizador bayesiano ajuste la fidelidad contra el coste.
Verificamos eficientemente la robustez de los modelos neuronales profundos con más de 100.000 ReLUs, certificando más muestras que el estado del arte y encontrando más ejemplos adversarios que un ataque fuerte de primer orden.
Realiza un cuidadoso estudio de los enfoques de programación lineal entera mixta para verificar la robustez de las redes neuronales frente a las perturbaciones adversas y propone tres mejoras en las formulaciones MILP de verificación de redes neuronales.
Un conjunto de métodos para obtener la estimación de la incertidumbre de cualquier modelo dado sin necesidad de rediseñarlo, reentrenarlo o ponerlo a punto.
Describe varios enfoques para medir la incertidumbre en redes neuronales arbitrarias cuando no hay distorsión durante el entrenamiento.
Propuesta de operación de orden superior para el aprendizaje contextual
Propone un nuevo bloque convolucional 3D que convoluciona la entrada de vídeo con su contexto, basándose en el supuesto de que el contexto relevante está presente alrededor del objeto de la imagen.
Los modelos basados en la consistencia para el aprendizaje semisupervisado no convergen a un único punto, sino que continúan explorando un conjunto diverso de soluciones plausibles en el perímetro de una región plana. El promedio de pesos ayuda a mejorar el rendimiento de la generalización.
El artículo propone aplicar el Stochastic Weight Averaging al contexto del aprendizaje semisupervisado, argumentando que los modelos de MT/Pi semisupervisados son especialmente aptos para el SWA y propone un SWA rápido para acelerar el entrenamiento.
Convertimos con éxito un popular detector RPN en un rastreador de buen rendimiento desde el punto de vista de la función de pérdida.
Una arquitectura neuronal para puntuar y clasificar los candidatos a la reparación de programas para realizar la reparación semántica de programas de forma estática sin acceso a las pruebas unitarias.
Presenta una arquitectura de red neuronal compuesta por las partes de compartir, especializar y competir para reparar el código en cuatro casos.
¿Es posible codiseñar la precisión, la robustez y la eficiencia de los modelos para lograr su triple victoria? Sí.
Aprovecha las salidas tempranas múltiples adaptables a la entrada para el campo del ataque y la defensa adversarial, reduciendo la complejidad media de la inferencia sin entrar en conflicto con el supuesto de mayor capacidad.
Demostramos que las unidades individuales de las representaciones de la CNN aprendidas en tareas de PNL responden selectivamente a conceptos específicos del lenguaje natural.
Utiliza unidades gramaticales del lenguaje natural que conservan los significados para demostrar que las unidades de las CNN profundas aprendidas en tareas de PNL podrían actuar como detector de conceptos del lenguaje natural.
Se trata de un artículo principalmente teórico que describe los retos de desentrañar los factores de variación, utilizando autocodificadores y GAN.
Este trabajo considera la posibilidad de desentrañar los factores de variación en las imágenes, muestra que en general, sin más suposiciones, no se pueden distinguir dos factores de variación diferentes, y sugiere una novedosa arquitectura AE+GAN para intentar desentrañar los factores de variación.
Este artículo estudia los retos de desentrañar factores de variación independientes bajo datos débilmente etiquetados e introduce el término de ambigüedad de referencia para el mapeo de puntos de datos.
aprender a clasificar con varias incrustaciones y atenciones
Propone utilizar la atención para combinar múltiples representaciones de entrada tanto para la consulta como para los resultados de la búsqueda en la tarea de aprender a clasificar.
Desarrollamos un algoritmo que toma como entrada grabaciones de actividad neuronal y devuelve grupos de neuronas por tipo de célula y modelos de actividad neuronal restringidos por estos grupos.
Supervisamos las redes neuronales de grafos para que imiten las salidas intermedias y escalonadas de los algoritmos clásicos de grafos, recuperando así conocimientos muy favorables.
Sugiere el entrenamiento de redes neuronales para imitar los algoritmos gráficos mediante el aprendizaje de primitivas y subrutinas en lugar de la salida final.
Describimos una arquitectura para generar diversas hipótesis de objetivos intermedios durante las tareas de manipulación robótica.
Evalúa la calidad de un modelo predictivo generativo propuesto para generar planes de ejecución de robots.
Este trabajo propone un método para el aprendizaje de una función de transición de alto nivel que es útil para la planificación de tareas.
Este artículo proporciona un análisis novedoso de los algoritmos de gradiente adaptativo para resolver problemas min-max no convexos como GANs, y explica la razón por la que los métodos de gradiente adaptativo superan a sus homólogos no adaptativos mediante estudios empíricos.
Desarrolla algoritmos para la solución de desigualdades variacionales en el entorno estocástico, proponiendo una variación del método del extragradiente.
Aprendemos trayectorias sohpisticadas de un objeto puramente a partir de píxeles con un conjunto de datos de vídeo de juguete utilizando una estructura VAE con un proceso gaussiano a priori.
Investigamos las bases neuronales del recuerdo de los sueños utilizando técnicas de redes neuronales convolucionales y de visualización de rasgos, como la tSNE y la retropropagación guiada.
Este trabajo propone una nueva formulación y un nuevo protocolo de comunicación para problemas de control multiagente en red
Se ocupa de los N-MARL en los que los agentes actualizan su política basándose sólo en los mensajes de los nodos vecinos, mostrando que la introducción de un factor de descuento espacial estabiliza el aprendizaje.
El VB de campo medio utiliza el doble de parámetros; empatamos los parámetros de varianza en el VB de campo medio sin ninguna pérdida de ELBO, ganando velocidad y gradientes de varianza más bajos.
Aprovechamos eficazmente unas pocas palabras clave como supervisión débil para el entrenamiento de redes neuronales para la extracción de aspectos.
Discute una variante de la destilación de conocimientos que utiliza un "maestro" basado en un clasificador de bolsa de palabras con palabras semilla y un "alumno" que es una red neuronal basada en la incrustación.
Las conexiones de retroalimentación horizontales y descendentes son responsables de las estrategias de agrupación perceptiva complementarias en los sistemas de visión biológica y recurrente.
Utilizando redes neuronales como modelo computacional del cerebro, examina la eficacia de diferentes estrategias para resolver dos retos visuales.
Presentamos GAN-TTS, una red adversarial generativa para la conversión de texto en voz, que alcanza una puntuación media de opinión (MOS) de 4,2.
Resuelve el desafío de los GAN en la síntesis de formas de onda en bruto y comienza a cerrar la brecha de rendimiento existente entre los modelos autorregresivos y los GAN para los audios en bruto.
proponemos un algoritmo de aprendizaje para podar la red mediante la aplicación de penalizaciones por dispersión de la estructura
Este artículo presenta un enfoque para la poda mientras se entrena una red utilizando penalizaciones de lazo y LBI dividido
Introducimos el aprendizaje continuo no supervisado (UCL) y una arquitectura neuroinspirada que resuelve el problema del UCL.
Propone el uso de jerarquías de módulos STAM para resolver el problema UCL, aportando pruebas de que las representaciones que aprenden los módulos son adecuadas para la clasificación de pocos disparos.
Nuevo método de extracción de señales en el dominio de Fourier
Aporta una versión convolucional de valor complejo de la Modulación Lineal de Características que permite optimizar los parámetros y diseña una pérdida que tiene en cuenta la magnitud y la fase.
Presentamos un marco novedoso para aprender la representación desenredada del contenido y el estilo de forma completamente no supervisada. 
Proponer un modelo basado en el marco del autoencoder para desentrañar la representación de un objeto, los resultados muestran que el modelo puede producir representaciones que capturan el contenido y el estilo.
Desarrollamos una estrategia de estimación de CATE que aprovecha algunas de las intrigantes propiedades de las redes neuronales. 
Muestra mejoras en X-learner al modelar la función de respuesta al tratamiento, la función de respuesta al control y el mapeo del efecto del tratamiento imputado al efecto del tratamiento medio condicional, como redes neuronales.
Los autores proponen el Y-learner para estimar el efecto medio condicional del tratamiento (CATE), que actualiza simultáneamente los parámetros de las funciones de resultado y el estimador CATE.
Ejecución de firmware independiente del dispositivo
Una arquitectura para datos tabulares que emula las ramas de los árboles de decisión y utiliza una conectividad residual densa 
Este trabajo propone el bosque neural profundo, un algoritmo que se dirige a los datos tabulares e integra los puntos fuertes del gradient boosting de los árboles de decisión.
Una novedosa arquitectura de red neuronal que imita el funcionamiento de los bosques de decisión para abordar el problema general del entrenamiento de modelos profundos para datos tabulares y que muestra una eficacia equivalente a la de GBDT.
YellowFin es un optimizador basado en SGD con adaptabilidad de impulso y tasa de aprendizaje.
Propone un método para sintonizar automáticamente el parámetro de impulso en los métodos SGD de impulso, que consigue mejores resultados y una velocidad de convergencia más rápida que el algoritmo Adam de última generación.
Ataques adversarios al espacio latente de los autocodificadores variacionales para cambiar el significado semántico de las entradas
Este artículo se refiere a la seguridad y al aprendizaje automático y propone un ataque de intermediario que altera la codificación VAE de los datos de entrada para que la salida decodificada sea mal clasificada.
Un estudio empírico que examina la eficacia de diferentes combinaciones de codificador-decodificador para la tarea de análisis sintáctico de dependencias
Analiza empíricamente varios codificadores, decodificadores y sus dependencias para el análisis sintáctico de dependencias basado en grafos.
Profesor que entrena a los meta-aprendices como a los humanos
Introducimos un enfoque de espacio de incrustación para restringir la distribución de la probabilidad de salida de la red neuronal.
Este trabajo introduce un método para realizar un aprendizaje semi-supervisado con redes neuronales profundas, y el modelo logra una precisión relativamente alta, dado un tamaño de entrenamiento pequeño.
Este artículo incorpora la distribución de etiquetas en el aprendizaje de modelos cuando se dispone de un número limitado de instancias de entrenamiento, y propone dos técnicas para tratar el problema de que la distribución de etiquetas de salida esté erróneamente sesgada.
Introducimos un nuevo tipo de representación profunda de palabras contextualizadas que mejora significativamente el estado del arte para una serie de tareas desafiantes de PNL.
Este trabajo introduce una nueva función de pérdida para el entrenamiento robusto de DNN de localización temporal en presencia de etiquetas desalineadas.
Una nueva pérdida para los modelos de entrenamiento que predicen dónde ocurren los eventos en una secuencia de entrenamiento con etiquetas ruidosas, comparando la etiqueta suavizada y la secuencia de predicción.
Introducimos la noción de descomposiciones tensoriales mixtas y la utilizamos para demostrar que la interconexión de redes convolucionales dilatadas aumenta su capacidad de expresión.
Este trabajo valida teóricamente que la interconexión de redes con diferentes dilataciones puede conducir a la eficiencia expresiva utilizando la descomposición tensorial mixta.
Los autores estudian las redes convolucionales dilatadas y demuestran que entrelazar dos redes convolucionales dilatadas A y B en varias etapas es más eficiente desde el punto de vista expresivo que no entrelazarlas.
Demuestra que la suposición estructural de WaveNet de un único árbol binario perfecto obstaculiza su rendimiento y que las arquitecturas similares a WaveNet con estructuras de árbol mixtas más complejas rinden mejor.
el aprendizaje multitarea funciona 
Este artículo presenta una red neuronal multitarea para la clasificación en conjuntos de datos similares a los de MNIST
Proporcionamos una revisión basada en principios y optimización de la noción de ejemplos adversarios, y desarrollamos métodos que producen modelos que son adversariamente robustos contra una amplia gama de adversarios.
Investiga una formulación minimax del aprendizaje de redes profundas para aumentar su robustez, utilizando el descenso de gradiente proyectado como principal adversario. 
Este trabajo propone estudiar cómo hacer que las redes neuronales sean resistentes a las pérdidas adversas a través del marco de los problemas de punto de silla. 
Muchos conjuntos de datos de clasificación de gráficos tienen duplicados, lo que plantea dudas sobre la capacidad de generalización y la comparación justa de los modelos. 
Los autores discuten el sesgo de isomorfismo en los conjuntos de datos de grafos, el efecto de sobreajuste en el aprendizaje de redes cuando se incorporan características de isomorfismo de grafos dentro del modelo, teóricamente análogo a los efectos de fuga de datos.
Introducimos una noción de funciones de valor extrapoladas de forma conservadora, que conducen de forma demostrable a políticas que pueden autocorregirse para mantenerse cerca de los estados de demostración, y las aprendemos con una novedosa técnica de muestreo negativo.
Un algoritmo llamado iteración de valores con muestreo negativo para abordar el problema de desplazamiento de covariables en el aprendizaje por imitación.
Los modelos estructurados del mundo entrenados de forma contrastiva (C-SWM) aprenden representaciones de estado orientadas a objetos y un modelo relacional de un entorno a partir de la entrada de píxeles en bruto.
Los autores superan el problema de utilizar pérdidas basadas en píxeles en la construcción y el aprendizaje de modelos de mundo estructurado mediante el uso de un espacio latente contrastivo.
Métodos no supervisados para encontrar, analizar y controlar las neuronas importantes en la NMT
Este trabajo propone encontrar neuronas "significativas" en los modelos de traducción automática neuronal mediante una clasificación basada en la correlación entre pares de modelos, diferentes épocas o diferentes conjuntos de datos, y propone un mecanismo de control para los modelos.
Presentamos el softmax doblemente disperso, la mezcla dispersa de expertos dispersos, para mejorar la eficiencia de la inferencia del softmax mediante la explotación de la jerarquía de superposición de dos niveles. 
El artículo propone la implementación del nuevo algoritmo Softmax con dos niveles jerárquicos de sparsity que acelera la operación en el modelado del lenguaje.
Este trabajo presenta evidencia empírica que apoya el descubrimiento de un indicador de generalización: la evolución a lo largo del entrenamiento de la distancia coseno entre el vector de pesos de cada capa y su inicialización.
Los modelos de código fuente que combinan características globales y estructurales aprenden representaciones más potentes de los programas.
Un nuevo método para modelar el código fuente para la tarea de reparación de errores utilizando un modelo de sándwich como [RNN GNN RNN] que mejora significativamente la localización y la precisión de la reparación.
Las RNN incrementales resuelven el problema de la explosión/gradiente evanescente actualizando los vectores de estado en función de la diferencia entre el estado anterior y el predicho por una ODE.
Los autores abordan el problema de la propagación de la señal en las redes neuronales recurrentes construyendo un sistema atractor para la transición de la señal y comprobando si converge a un equilibrio. 
Proporcionamos pruebas en contra de las afirmaciones clásicas sobre el equilibrio entre sesgo y varianza y proponemos una nueva descomposición de la varianza.
Proponemos un novedoso marco de clasificación de imágenes de aprendizaje profundo que puede clasificar con precisión las imágenes y proteger la privacidad de los usuarios.
Este trabajo propone un marco que preserva la información privada de la imagen y no compromete la usabilidad de la misma.
Este trabajo actual sugiere el uso de redes adversariales para ofuscar las imágenes y permitir así recogerlas sin preocupaciones de privacidad para utilizarlas en el entrenamiento de modelos de aprendizaje automático.
un modelo 2vec para los gráficos de transacciones de criptomonedas
El documento propone utilizar un autocodificador, networkX y node2Vec para predecir si una dirección de Bitcoin quedará vacía después de un año, pero los resultados son peores que los de una línea de base existente.
Prueba de convergencia del método de subgradientes estocásticos y variaciones en problemas minimax convexos-cóncavos
Un análisis del subgradiente estocástico simultáneo, del gradiente simultáneo con optimismo y del gradiente simultáneo con anclaje en el contexto de los juegos cóncavos minmax.
Este trabajo analiza la dinámica del descenso de gradiente estocástico cuando se aplica a juegos convexos-cóncavos, así como el GD con optimismo y un nuevo algoritmo de GD anclado que converge bajo supuestos más débiles que el SGD o el SGD con optimismo.
Proponemos un marco algorítmico para programar constelaciones de pequeñas naves espaciales con capacidad de reorientación en 3DOF, conectadas en red con enlaces inter-sat.
Este trabajo propone un módulo de comunicación para optimizar la programación de la comunicación para el problema de las constelaciones de naves espaciales, y compara el algoritmo en entornos distribuidos y centralizados.
Proponemos un novedoso algoritmo de muestreo de importancia comprimido y kernelizado.
Estudiamos la estructura de la regresión de cresta en un marco asintótico de alta dimensión, y obtenemos conocimientos sobre la validación cruzada y el esbozo.
Un estudio teórico de la regresión de cresta explotando una nueva caracterización asintótica del estimador de regresión de cresta.
Analizamos el panorama de pérdidas de las redes neuronales con atención y explicamos por qué la atención es útil en el entrenamiento de las redes neuronales para conseguir un buen rendimiento.
Este trabajo demuestra, desde la perspectiva teórica, que las redes de atención pueden generalizar mejor que las líneas de base sin atención para la atención fija (monocapa y multicapa) y la autoatención en el entorno de una sola capa.
Modelo de mezcla para el desentrañamiento neuronal
Desarrollamos estimaciones robustas de información mutua para DNNs y las utilizamos para observar la compresión en redes con funciones de activación no saturantes
En este trabajo se ha estudiado la creencia popular de que las redes neuronales profundas realizan una compresión de información para tareas supervisadas
Este trabajo propone un método para la estimación de la información mutua para redes con funciones de activación no limitadas y el uso de la regularización L2 para inducir una mayor compresión.
Presentamos el TimbreTron, una línea de producción para realizar una transferencia tímbrica de alta calidad en formas de onda musicales utilizando la transferencia en el dominio CQT.
Un método para convertir grabaciones de un instrumento musical específico a otro aplicando CycleGAN, desarrollado para la transferencia de estilos de imagen, para transferir espectrogramas.
Los autores utilizan múltiples técnicas/herramientas para permitir la transferencia tímbrica neural (convertir la música de un instrumento a otro) sin ejemplos de entrenamiento emparejados. 
Describe un modelo de transferencia tímbrica musical cuyos resultados indican que el sistema propuesto es eficaz para la transferencia de tono y tempo, así como para la adaptación tímbrica.
El artículo presenta Deep Rewiring, un algoritmo que puede utilizarse para entrenar redes neuronales profundas cuando la conectividad de la red está muy limitada durante el entrenamiento.
Un enfoque para implementar el aprendizaje profundo directamente en grafos escasamente conectados, lo que permite que las redes se entrenen eficientemente en línea y para un aprendizaje rápido y flexible.
Los autores proporcionan un algoritmo sencillo capaz de entrenar con una memoria limitada
Los métodos de poda existentes fallan cuando se aplican a GANs que abordan tareas complejas, por lo que presentamos un método simple y robusto para podar generadores que funciona bien para una amplia variedad de redes y tareas.
Los autores proponen una modificación del método clásico de destilación para la tarea de comprimir una red con el fin de abordar el fracaso de las soluciones anteriores cuando se aplican a las redes generativas adversariales.
descubrimos que el 99,9% del intercambio de gradientes en el SGD distribuido es redundante; reducimos el ancho de banda de comunicación en dos órdenes de magnitud sin perder precisión. 
Este trabajo propone una mejora adicional sobre el gradient dropping para mejorar la eficiencia de la comunicación
Proponemos la red de Traducción de Imagen a Imagen Guiada por Ejemplar y Semánticamente Consistente (EGSC-IT) que condiciona el proceso de traducción a una imagen ejemplar en el dominio de destino.
Discute un fallo de base y la necesidad de modelos de traducción I2I.
El artículo explora la idea de que una imagen tiene dos componentes y aplica un modelo de atención en el que las máscaras de características que dirigen el proceso de traducción no requieren etiquetas semánticas
Imposición de una estructura de grafos en las capas de las redes neuronales para mejorar la interpretabilidad visual.
Un novedoso regularizador para imponer la estructura del grafo a las capas ocultas de una red neuronal para mejorar la interpretabilidad de las representaciones ocultas.
Destaca la contribución del regularizador espectral de gráficos a la interpretabilidad de las redes neuronales.
Demostramos que los modelos basados en la energía, cuando se entrenan con el residuo de un modelo lingüístico autorregresivo, pueden utilizarse de forma eficaz y eficiente para generar texto. 
Una propuesta de modelo basado en la energía residual (EBM) para la generación de texto que opera a nivel de frase, y por lo tanto puede aprovechar el BERT, y logra una menor perplejidad y es preferido por la evaluación humana.
estudio sistemático de los modelos de reconocimiento de imágenes a gran escala basados en la memoria caché, centrándose especialmente en sus propiedades de robustez
Este trabajo propuso utilizar la memoria caché para mejorar la robustez frente a los ejemplos de imágenes adversas, y concluyó que el uso de una gran memoria caché continua no es superior a la atención dura.
El artículo describe un marco flexible para la construcción de CNNs que son equivariantes a una gran clase de grupos de transformaciones.
Un marco para construir CNN de grupo con un grupo de Lie arbitrario G, que muestra superioridad sobre una CNN en la clasificación de tumores y la localización de puntos de referencia. 
Un análisis comparativo de nueve sistemas de agrupación global representativos revela algunos resultados interesantes.
Para las tareas de clasificación de grano fino, este trabajo validó que el maxpooling fomentaría mapas de características más escasos que el avgpooling y lo superaría. 
La autosupervisión mejora el reconocimiento de pocos disparos en conjuntos de datos pequeños y desafiantes sin depender de datos adicionales; los datos adicionales sólo ayudan cuando son del mismo dominio o de uno similar.
Un estudio empírico de diferentes métodos de aprendizaje autosupervisado (SSL), que muestra que SSL ayuda más cuando el conjunto de datos es más difícil, que el dominio importa para el entrenamiento, y un método para elegir muestras de un conjunto de datos sin etiquetar. 
Creamos modelos abstractos de entornos a partir de la experiencia y los utilizamos para aprender nuevas tareas más rápidamente.
Una metodología que utiliza la idea de los homomorfismos MDP para transformar un MDP complejo con un espacio de estados continuo en uno más simple.
Ampliamos la disección de redes para incluir la interpretación de las acciones y examinamos las rutas de los rasgos interpretables para comprender la jerarquía conceptual utilizada para clasificar una acción.
Proponemos un nuevo modelo para representar las notas y sus propiedades, que puede mejorar la generación automática de melodías.
Este trabajo propone un modelo generativo de melodía simbólica (MIDI) en la música popular occidental que codifica conjuntamente los símbolos de las notas con la información de tiempo y duración para formar "palabras" musicales.
El trabajo propone facilitar la generación de melodías representando las notas como "palabras", representando todas las propiedades de la nota y permitiendo así la generación de "frases" musicales.
Un método que hace crecer automáticamente las capas en las redes neuronales para descubrir la profundidad óptima.
Un marco para intercalar el entrenamiento de una red poco profunda y la adición de nuevas capas que proporciona información sobre el paradigma de las "redes en crecimiento".
Exploración del aprendizaje de representación en el dominio para conjuntos de datos de teledetección.
Este trabajo proporcionó varios conjuntos de datos estandarizados de teledetección y demostró que la representación en el dominio podría producir mejores resultados de referencia para la teledetección en comparación con el ajuste fino en ImageNet o el aprendizaje desde cero.
Evite generar respuestas de una en una utilizando una supervisión débil para entrenar a un clasificador para que elija una respuesta completa.
Una forma de generar respuestas para el diálogo médico utilizando un clasificador para seleccionar entre las respuestas seleccionadas por los expertos en función del contexto de la conversación.
CNN entrenadas con SGD de ancho finito frente a CNN totalmente bayesianas de ancho infinito. ¿Quién gana?
El artículo establece una conexión entre la red neuronal convolucional bayesiana de canal infinito y los procesos gaussianos.
Escalamos la Inferencia Bayesiana a la clasificación de ImageNet y logramos resultados competitivos de precisión y calibración de la incertidumbre.
Un algoritmo MCMC de ruido adaptativo para la clasificación de imágenes que ajusta dinámicamente el impulso y el ruido aplicado a cada actualización de los parámetros, y es robusto al sobreajuste y proporciona una medida de incertidumbre con predicciones. 
Un estudio empírico sobre imágenes falsas revela que la textura es una pista importante que diferencia las imágenes falsas actuales de las reales. Nuestro modelo mejorado, que captura las estadísticas globales de textura, muestra un mejor rendimiento en la detección de imágenes falsas entre GAN.
El artículo propone una forma de mejorar el rendimiento del modelo para la detección de caras falsas en imágenes generadas por un GAN para que sea más generalizable basándose en la información de la textura.
La distancia Wasserstein es difícil de minimizar con el descenso de gradiente estocástico, mientras que la distancia Cramer puede optimizarse fácilmente y funciona igual de bien.
El manuscrito propone utilizar la distancia de Cramer para que actúe como pérdida al optimizar una función objetivo utilizando el descenso de gradiente estocástico porque tiene gradientes de muestra insesgados.
La contribución del artículo está relacionada con los criterios de rendimiento, en particular con la métrica Wasserstein/Mallows
Aprendemos la flecha del tiempo para los MDP y la utilizamos para medir la alcanzabilidad, detectar los efectos secundarios y obtener una señal de recompensa por curiosidad. 
Este trabajo propone el potencial h como solución a un objetivo que mide la asimetría estado-transición en un MDP.
Formulamos el SGD como un problema de filtrado bayesiano, y mostramos que esto da lugar a RMSprop, Adam, AdamW, NAG y otras características de los métodos adaptativos más avanzados
El documento analiza el descenso de gradiente estocástico a través del filtrado bayesiano como marco para analizar los métodos adaptativos.
Los autores intentan unificar los métodos de gradiente adaptativo existentes en el marco del filtrado bayesiano con la prioridad dinámica
Introducimos la idea del aprendizaje adversarial en el aumento automático de datos para mejorar la generalización de una red targe.
Una técnica llamada Adversarial AutoAugment que aprende dinámicamente buenas políticas de aumento de datos durante el entrenamiento utilizando un enfoque adversarial.
El estudio introduce dos enfoques para mejorar la generalización del meta-aprendizaje de primer orden y presenta una evaluación empírica sobre la clasificación de imágenes de pocas tomas.
El artículo presenta un estudio empírico del algoritmo Reptile de meta-aprendizaje de primer orden, investigando una técnica de regularización propuesta y redes más profundas
Este trabajo propone utilizar la factorización matricial en el momento del entrenamiento para la traducción automática neural, lo que puede reducir el tamaño del modelo y disminuir el tiempo de entrenamiento sin perjudicar el rendimiento.
Este trabajo propone comprimir los modelos utilizando la factorización matricial durante el entrenamiento para las redes neuronales profundas de traducción automática.
Diferentes métodos para analizar el BERT sugieren conclusiones diferentes (pero compatibles) en un estudio de caso sobre los NPI.
En este trabajo presentamos V1Net, una nueva red neuronal recurrente que modela las conexiones horizontales corticales que dan lugar a representaciones visuales robustas a través de la agrupación perceptiva.
Los autores proponen modificar una variante convolucional de LSTM para incluir conexiones horizontales inspiradas en interacciones conocidas en la corteza visual.
Proponemos un vínculo entre la equivocidad de las permutaciones y la generalización composicional, y proporcionamos modelos lingüísticos equivariantes
Este trabajo se centra en el aprendizaje de representaciones y funciones localmente equivariantes sobre las palabras de entrada/salida para los fines de la tarea SCAN.
El artículo propone un algoritmo para aumentar la flexibilidad de la variante posterior en las redes neuronales bayesianas mediante la optimización iterativa.
Un método para entrenar distribuciones posteriores variacionales flexibles, aplicado a redes neuronales bayesianas para realizar la inferencia de variación (VI) sobre los pesos.
Nuevo marco de vanguardia para la restauración de imágenes
El artículo propone una arquitectura de red neuronal convolucional que incluye bloques para los mecanismos de atención local y no local, que, según se afirma, son los responsables de lograr excelentes resultados en cuatro aplicaciones de restauración de imágenes.
Este trabajo propone una red de atención no local residual para la restauración de imágenes
Enfoque híbrido para la adquisición de modelos que compensa la falta de datos disponibles con el conocimiento específico del dominio proporcionado por los expertos
Un enfoque de adquisición de dominios que considera el uso de una representación diferente para el modelo de dominio parcial mediante el uso de relaciones mutex esquemáticas en lugar de condiciones pre/post.
Publicamos un conjunto de datos construido a partir de datos de ECG de una sola derivación de 11.000 pacientes a los que se les prescribió el uso del dispositivo {DEVICENAME}(TM).
Este artículo describe un conjunto de datos de ECG a gran escala que los autores pretenden publicar y proporciona un análisis no supervisado y una visualización del conjunto de datos.
Una novedosa convolución controlada por el contexto que incorpora la información del contexto global en las CNNs mediante la modulación explícita de los núcleos de convolución, y por lo tanto captura patrones locales más representativos y extrae características discriminativas.
Este trabajo utiliza el contexto global para modular los pesos de las capas convolucionales y ayudar a las CNN a capturar más características discriminativas con un alto rendimiento y menos parámetros que la modulación del mapa de características.
Analizamos el equilibrio entre el ruido de cuantificación y la distorsión de recorte en redes de baja precisión, y mostramos notables mejoras respecto a los esquemas de cuantificación estándar que normalmente evitan el recorte
Deriva una fórmula para encontrar los valores de recorte mínimo y máximo para la cuantificación uniforme que minimizan el error cuadrado resultante de la cuantificación, para una distribución de Laplace o de Gauss sobre el valor precuantificado.
Proponemos un nuevo método de normalización para tratar los casos de lotes pequeños.
Un método para tratar el problema del tamaño pequeño de los lotes de BN que aplica la operación de media móvil sin demasiada sobrecarga y reduce el número de estadísticas de BN para mejorar la estabilidad.
Prueba de separación en profundidad de ReLU MLP con argumentos gemotéricos
Una prueba de que las redes más profundas necesitan menos unidades que las menos profundas para una familia de problemas. 
Un nuevo algoritmo de aprendizaje de pocos disparos basado en GAN mediante la síntesis de características diversas y discriminativas
Un método de meta-aprendizaje que aprende un modelo generativo que puede aumentar el conjunto de soporte de un aprendiz de pocos disparos que optimiza una combinación de pérdidas.
Demostramos cómo la estructura de los conjuntos de datos afecta a las redes neuronales e introducimos un modelo generativo para conjuntos de datos sintéticos que reproduce este impacto.
El artículo estudia cómo afectan las diferentes configuraciones de la estructura de datos al aprendizaje de las redes neuronales y cómo imitar el comportamiento en conjuntos de datos reales cuando se aprende en uno sintético.
Entrenamos redes neuronales profundas basadas en matrices diagonales y circulantes, y demostramos que este tipo de redes son compactas y precisas en aplicaciones del mundo real.
Los autores proporcionan un análisis teórico de la potencia expresiva de las redes neuronales diagonales circulantes (DCNN) y proponen un esquema de inicialización para las DCNN profundas.
Proponemos aprovechar la destilación de modelos para aprender explicaciones aditivas globales en forma de formas de rasgos (que son más expresivas que las atribuciones de rasgos) para modelos como las redes neuronales entrenadas en datos tabulares.
Este trabajo incorpora modelos aditivos generalizados (GAM) con destilación de modelos para proporcionar explicaciones globales de las redes neuronales.
Un marco de aprendizaje multitarea a gran escala con diversos objetivos de entrenamiento para aprender representaciones de frases de longitud fija
Este artículo trata sobre el aprendizaje de incrustaciones de oraciones mediante la combinación de varias señales de entrenamiento: omisión de pensamiento, predicción de la traducción, clasificación de las relaciones de vinculación y predicción del análisis sintáctico del constituyente.
Proponemos un anotador neuronal de sesgos para evaluar la robustez de los modelos ante conjuntos de datos de texto sesgados.
Un método para generar conjuntos de datos sesgados para la PNL, basado en un autoencoder condicional regularizado adversamente (CARA).
Proponemos supervisar los modelos temáticos de tipo VAE ajustando de forma inteligente la prioridad por documento. Encontramos que una posterior logit-normal proporciona el mejor rendimiento.
Un método flexible de supervisión débil de un modelo temático para lograr una mejor alineación con la intuición del usuario.
Primer análisis exhaustivo del plano de información de las redes neuronales profundas a gran escala utilizando la entropía basada en la matriz y los núcleos tensoriales.
Los autores proponen un estimador basado en tensor-kernel para la estimación de la información mutua entre capas de alta dimensión en una red neuronal.
Proponemos un marco modular que puede cumplir con las tareas especificadas por los programas y lograr la generalización de cero a las tareas más complejas.
Este trabajo investiga el entrenamiento de agentes RL con instrucciones y descomposiciones de tareas formalizadas como programas, proponiendo un modelo para un agente guiado por programas que interpreta un programa y propone submetas a un módulo de acción.
Demostramos que el descenso de gradiente inicializado aleatoriamente (estocástico) aprende un filtro convolucional en tiempo polinomial.
Estudia el problema del aprendizaje de un único filtro convolucional utilizando SGD y muestra que bajo ciertas condiciones, SGD aprende un único filtro convolucional.
Este artículo amplía la hipótesis de la distribución gaussiana a una hipótesis de suavidad angular más general, que abarca una familia más amplia de distribuciones de entrada
El primer método de aumento de datos especialmente diseñado para mejorar la robustez general de las DNN sin ninguna hipótesis sobre los algoritmos de ataque.
Propone un método de entrenamiento de aumento de datos para ganar robustez en el modelo frente a perturbaciones adversas, mediante el aumento de muestras aleatorias uniformes de una esfera de radio fijo centrada en los datos de entrenamiento. 
Uso de Wasserstein-GANs para generar actividad neuronal realista y detectar las características más relevantes presentes en los patrones de la población neuronal.
Un método para simular trenes de espigas de poblaciones de neuronas que coinciden con los datos empíricos utilizando un GAN semiconvolucional.
El artículo propone utilizar GANs para sintetizar patrones de actividad neuronal realistas
Los estimadores de gradiente doblemente reparametrizados proporcionan una reducción insesgada de la varianza que conduce a un mejor rendimiento.
El autor encontró experimentalmente que el estimador del trabajo existente (STL) está sesgado y propone reducir el sesgo para mejorar el estimador del gradiente del ELBO.
El Descenso sin Gradiente es un algoritmo sin gradiente probadamente eficiente que es monotono-invariante y rápido para la optimización de orden cero de alta dimensión.
Este trabajo propone algoritmos estables de Descenso sin Gradiente (GLD) que no dependen de la estimación del gradiente.
Proponemos una nueva clase de modelos generativos visuales: los predictores condicionados por la meta. Demostramos experimentalmente que el condicionamiento a la meta permite reducir la incertidumbre y producir predicciones en horizontes mucho más largos.
Este trabajo reformula el problema de la predicción de vídeo como interpolación en lugar de extrapolación, condicionando la predicción al fotograma inicial y final (meta), lo que da lugar a predicciones de mayor calidad.
Proponemos un marco de aprendizaje profundo multiinstitucional basado en redes neuronales recurrentes que utiliza funciones de agrupación y mecanismos de atención para las tareas de anotación de conceptos.
El artículo aborda la clasificación de datos de series temporales médicas y propone modelar la relación temporal entre las instancias de cada serie utilizando una arquitectura de red neuronal recurrente. 
Propone una nueva formulación de Aprendizaje de Instancias Múltiples (MIL) llamada Relation MIL (RMIL), y discute un número de sus variantes con LSTM, Bi-LSTM, S2S, etc. y explora la integración de RMIL con varios mecanismos de atención, y demuestra su uso en la predicción de conceptos médicos a partir de datos de series temporales. 
Las capas de incrustación se factorizan con la descomposición del tren tensorial para reducir su huella en la memoria.
Este trabajo propone un modelo de descomposición tensorial de bajo rango para parametrizar la matriz de incrustación en el Procesamiento del Lenguaje Natural (PLN), que comprime la red y a veces aumenta la precisión de las pruebas.
Fijación de la regularización del decaimiento del peso en los métodos de gradiente adaptativo como Adam
Propone la idea de desvincular el decaimiento del peso del número de pasos del proceso de optimización.
El documento presenta una forma alternativa de aplicar el decaimiento del peso en Adam con resultados empíricos demostrados
Investiga los problemas de decaimiento del peso que se dan en las variantes de SGD y propone el método de desacoplamiento entre el decaimiento del peso y la actualización basada en el gradiente.
Aprendizaje distributivo permanente mediante una arquitectura alumno-maestro acoplada a un regularizador posterior de modelos cruzados.
Autocodificadores profundos para aprender una buena representación de los datos geométricos de nubes de puntos en 3D; Modelos generativos para nubes de puntos.
Enfoques para aprender modelos generativos de tipo GAN utilizando la arquitectura PointNet y GAN de espacio latente.
Proponemos un método novedoso para suprimir la vulnerabilidad del espacio de características latentes para conseguir redes robustas y compactas.
Este trabajo propone el método de "poda neuronal adversarial" de entrenamiento de una máscara de poda y una nueva pérdida de supresión de vulnerabilidad para mejorar la precisión y la robustez adversarial.
Propusimos dos modificaciones de la VAE que tienen en cuenta los ejemplos de datos negativos, y las utilizamos para la detección de anomalías semisupervisada.
Los artículos proponen dos métodos de aproximación tipo VAE para la detección de novedades semisupervisada, MML-VAE y DP-VAE.
La nueva comprensión de la dinámica del entrenamiento y la métrica de la dureza de la memorización conducen a un aprendizaje curricular eficiente y comprobable.
Este trabajo formula el DIH como un problema de aprendizaje curricular que puede utilizar más eficazmente los datos para entrenar las DNNs, y deriva la teoría sobre el límite de aproximación.
Historia de los desarrollos paralelos en las leyes de actualización y conceptos entre el control adaptativo y la optimización en el aprendizaje de máquinas.
Convolución recurrente para la compresión del modelo y un truco para entrenarlo, que es el aprendizaje de capas BN independientes sobre los pasos.
El autor modifica la red neuronal convolutiva recurrente (RCNN) con normalización de lotes independiente, siendo los resultados experimentales de la RCNN compatibles con la arquitectura de la red neuronal ResNet cuando contiene el mismo número de capas.
Los campos receptivos dinámicos con estructura espacial gaussiana son precisos y eficaces.
Este trabajo propone un operador de convolución estructurado para modelar las deformaciones de las regiones locales de una imagen, lo que reduce significativamente el número de parámetros.
Un truco sobre las muestras adversarias para que las etiquetas mal clasificadas sean imperceptibles en el espacio de etiquetas para los observadores humanos
Un método para construir ataques adversarios que sean menos detectables por los humanos sin coste en el espacio de la imagen cambiando la clase objetivo para que sea similar a la clase original de la imagen.
Este artículo presenta la clasificación por tipo de ruido y posición de varios ruidos de impacto generados en un edificio, lo que constituye un grave problema de conflicto en los complejos de apartamentos
Este trabajo describe el uso de redes neuronales convolucionales en un área de aplicación novedosa de clasificación del tipo de ruido en los edificios y de la posición del ruido. 
Las redes neuronales recurrentes aprenden a aumentar y reducir la dimensionalidad de su representación interna de forma que se adapte a la tarea, en función de la dinámica de la red inicial.
En lugar de las alineaciones de distribución estrictas en los objetivos tradicionales de adaptación de dominio profundo, que fallan cuando la distribución de la etiqueta objetivo cambia, proponemos optimizar un objetivo relajado con nuevos análisis, nuevos algoritmos y validación experimental.
Este artículo sugiere métricas relajadas para la adaptación del dominio que dan nuevos límites teóricos sobre el error objetivo.
exploramos la tarea de generación de resúmenes a artículos y proponemos un esquema de generación jerárquica junto con un marco de aprendizaje de refuerzo conjunto de extremo a extremo para entrenar el modelo jerárquico.
Para abordar el problema de la degeneración en la generación de resúmenes de artículos, este trabajo propone un enfoque de generación jerárquica que primero genera un boceto intermedio del artículo y luego el artículo completo.
Proponemos una regularización contrafactual para protegernos de los cambios de dominio adversos que surgen a través de los cambios en la distribución de las "características de estilo" latentes de las imágenes.
En este artículo se analizan las formas de protegerse contra los cambios de dominio adversos con la regularización contrafactual mediante el aprendizaje de un clasificador que es invariable a los cambios superficiales (o características de "estilo") en las imágenes.
Este trabajo tiene como objetivo la clasificación robusta de imágenes contra los cambios de dominio adversarios y el objetivo se logra evitando el uso de las características de estilo cambiante.
Proponemos un meta-aprendizaje para adaptarse rápidamente en múltiples tareas, incluso un paso en un escenario de pocos disparos.
Este trabajo propone un método para meta-aprender un módulo de corrección de gradiente en el que el precondicionamiento es parametrizado por una red neuronal, y se construye en un proceso de actualización de gradiente de dos etapas durante la adaptación. 
Los modelos de respuesta a preguntas que modelan la distribución conjunta de preguntas y respuestas pueden aprender más que los modelos discriminativos
Este trabajo propone un enfoque generativo para la GC textual y visual, en el que se aprende una distribución conjunta sobre el espacio de preguntas y respuestas dado el contexto, que captura relaciones más complejas.
Este trabajo introduce un modelo generativo para responder a preguntas y propone modelar p(q,a|c), factorizado como p(a|c) * p(q|a,c). 
Los autores proponen un modelo generativo de control de calidad, que optimiza conjuntamente la distribución de preguntas y respuestas dado un documento/contexto. 
Se propone una nueva función de activación denominada Unidad Lineal Rectificadora Desplazada. Se demuestra que mejora el rendimiento del entrenamiento y la inferencia de las redes neuronales convolucionales normalizadas por lotes.
El documento compara y sugiere contra el uso de la normalización por lotes después de utilizar unidades lineales rectificadoras
Este trabajo propone una función de activación, denominada ReLU desplazada, para mejorar el rendimiento de las CNN que utilizan la normalización por lotes.
Construimos redes neuronales convolucionales equivariantes en la forma más general con eficiencia computacional y robustez de deformación demostrada.
Los autores proponen una arquitectura de CNN que es teóricamente equivariante a las escalas y traslaciones isotrópicas añadiendo una dimensión de escala extra a los tensores de activación.
Diagnosticamos redes neuronales profundas para el procesamiento de nubes de puntos en 3D con el fin de explorar la utilidad de diferentes arquitecturas de red. 
El artículo investiga diferentes arquitecturas de redes neuronales para el procesamiento de nubes de puntos en 3D y propone métricas de robustez adversarial, robustez rotacional y consistencia de vecindad.
La utilización de la estructura de las distribuciones mejora la inferencia variacional semiimplícita
Aprendizaje por autoimitación de diversas trayectorias con política condicionada por la trayectoria
Este trabajo aborda tareas de exploración difíciles aplicando la autoimitación a una selección diversa de trayectorias de la experiencia pasada, para impulsar una exploración más eficiente en problemas de recompensa escasa, logrando resultados SOTA.
Un método que entrena redes neuronales de gran capacidad con una precisión significativamente mejorada y un menor coste computacional dinámico
Un método para entrenar una red con gran capacidad, de la que sólo se utilizan partes en el momento de la inferencia en función de la entrada, utilizando la selección condicional de grano fino y un nuevo método de regularización, "batch shaping".
Presentamos una arquitectura diferenciable de extremo a extremo que aprende a asignar píxeles a predicados, y la evaluamos en un conjunto de tareas sencillas de razonamiento relacional
Una arquitectura de red basada en el módulo de autoatención multicabezal para aprender una nueva forma de representaciones relacionales, que mejora la eficiencia de los datos y la capacidad de generalización en el aprendizaje del currículo.
Utilizamos las redes neuronales para proyectar la información superficial para la inferencia en lenguaje natural, definiendo e identificando la información superficial desde la perspectiva de la lógica de primer orden.
Este artículo trata de reducir la información superficial en la inferencia del lenguaje natural para evitar el sobreajuste, e introduce una red neuronal gráfica para modelar la relación entre las premisas y las hipótesis. 
Un enfoque para tratar la inferencia del lenguaje natural usando la lógica de primer orden y para infundir los modelos NLI con información lógica para ser más robustos en la inferencia.
Algoritmo para entrenar un clasificador individualmente justo utilizando la robustez adversarial
Este trabajo propone una nueva definición de equidad algorítmica y un algoritmo para encontrar de forma demostrable un modelo de LD que satisfaga la restricción de equidad.
¿Es todo lo que se necesita para clasificar los dígitos en cualquier idioma?
Este artículo presenta nuevos conjuntos de datos para cinco idiomas y propone un nuevo marco (SAT) para la generación de conjuntos de datos de imágenes de fuentes para la clasificación universal de dígitos.
El éxito de MAML se basa en la reutilización de las características de la meta-inicialización, que también produce una simplificación natural del algoritmo, con la eliminación del bucle interno para el cuerpo de la red, así como otros conocimientos sobre la cabeza y el cuerpo.
El artículo concluye que la reutilización de características es el factor dominante en el éxito de MAML, y propone nuevos algoritmos que gastan mucho menos cálculo que MAML.
Proporcionamos un algoritmo independiente del método para decidir cuándo entrenar de forma incremental en lugar de hacerlo de forma completa, lo que proporciona un aumento significativo de la velocidad respecto al entrenamiento completo y evita el olvido catastrófico.
Este artículo propone un enfoque para decidir cuándo se debe incrementar o reentrenar completamente un modelo en el marco del desarrollo iterativo de modelos en tareas de llenado de huecos.
Desarrollamos un marco teórico para caracterizar qué tareas de razonamiento puede aprender bien una red neuronal.
El artículo propone una medida de clases de alineación algorítmica que mide lo "cerca" que están las redes neuronales de los algoritmos conocidos, demostrando el vínculo entre varias clases de algoritmos conocidos y las arquitecturas de las redes neuronales.
Exploramos las interacciones célula-célula a través de los contextos del entorno tumoral observados en imágenes altamente multiplexadas, mediante la síntesis de imágenes utilizando una novedosa arquitectura GAN de atención.
Un nuevo método para modelar los datos generados por la imagen de haz de iones multiplexada por tiempo de vuelo (MIBI-TOF) mediante el aprendizaje del mapeo de muchos a muchos entre los tipos de células y los niveles de expresión de los marcadores de proteínas.
Un enfoque de dos etapas, consistente en la selección de frases seguida de la selección de tramos, puede ser más robusto frente a los ataques de los adversarios en comparación con un modelo de una sola etapa entrenado en el contexto completo.
Este artículo investiga un modelo existente y descubre que un método de garantía de calidad entrenado en dos etapas no es más robusto frente a los ataques de adversarios en comparación con otros métodos.
Verificación de un modelo de conductor humano basado en una arquitectura cognitiva y síntesis de un ADAS correcto a partir de él.
Un novedoso enfoque de aprendizaje profundo híbrido proporciona la mejor solución a un problema de datos limitados (que es importante para la conservación de la lengua hawaiana)
Estudiamos cuantitativamente la detección de fuera de distribución en un entorno de pocos disparos, establecemos resultados de referencia con ProtoNet, MAML y ABML, y los mejoramos.
El artículo propone dos nuevas puntuaciones de confianza que son más adecuadas para la detección fuera de la distribución de la clasificación de pocos disparos y muestra que un enfoque basado en la métrica de la distancia mejora el rendimiento.
Este trabajo introduce la destilación progresiva de conocimientos para el aprendizaje de modelos generativos orientados a tareas de reconocimiento
Este trabajo demuestra el aprendizaje curricular fácil de entrenar un modelo generativo para mejorar la clasificación de pocos disparos.
Proponemos un nuevo método para mejorar la transferibilidad de los ejemplos adversarios utilizando el gradiente reducido por el ruido.
Este trabajo postula que una perturbación adversarial consiste en un componente específico del modelo y otro específico de los datos, y que la amplificación de este último es lo más adecuado para los ataques adversariales.
Este trabajo se centra en mejorar la transferibilidad de los ejemplos adversos de un modelo a otro.
Presentamos el flujo de descomposición CP iterativo de dos pasos para acelerar eficazmente las redes neuronales convolucionales (CNN) existentes.
El artículo propone un novedoso flujo de trabajo para la aceleración y compresión de CNNs y también propone una forma de determinar el rango objetivo de cada capa dado el objetivo de aceleración global. 
Este trabajo aborda el problema del aprendizaje de una operación de filtro tensorial de bajo rango para capas de filtrado en redes neuronales profundas (DNNs). 
Límites superiores basados en LP para la constante de Lipschitz de las redes neuronales
Los autores estudian el problema de estimar la constante de Lipschitz de una red neuronal profunda con función de activación ELO, formulándolo como un problema de optimización polinómica.
Abordamos la clasificación multidominio de pocos disparos construyendo múltiples modelos para representar esta compleja distribución de tareas de forma colectiva y simplificando la adaptación específica de la tarea como un problema de selección a partir de estos modelos preentrenados.
Este trabajo aborda la clasificación de pocos disparos con muchos dominios diferentes mediante la construcción de un conjunto de modelos de incrustación para capturar características invariantes del dominio y específicas del dominio sin un aumento significativo del número de parámetros.
Eliminación de artefactos de tinta en documentos (subrayados, manchas, etc.) basada en la tecnología neuronal, sin datos de entrenamiento anotados manualmente
Proponemos un ataque de caja negra eficiente en cuanto a consultas que utiliza la optimización bayesiana en combinación con la selección de modelos bayesianos para optimizar la perturbación adversarial y el grado óptimo de reducción de la dimensión del espacio de búsqueda. 
Los autores proponen utilizar la optimización bayesiana con un sustituto de la GP para la generación de imágenes adversas, explotando la estructura aditiva y utilizando la selección del modelo bayesiano para determinar una reducción óptima de la dimensionalidad.
Proponemos un modelo para aprender representaciones multimodales factorizadas que sean discriminativas, generativas e interpretables.
Este artículo presenta un "modelo de factorización multimodal" que factoriza las representaciones en factores discriminativos multimodales compartidos y factores generativos específicos de la modalidad. 
Desarrollamos un algoritmo jerárquico, actor-crítico, para la transferencia compositiva compartiendo componentes de políticas y demostramos la especialización de componentes y los beneficios directos relacionados en dominios multitarea, así como su adaptación para tareas individuales.
Una combinación de diferentes técnicas de aprendizaje para adquirir estructura y aprender con datos asimétricos, utilizada para entrenar una política HRL.
Los autores introducen una estructura de política jerárquica para su uso en el aprendizaje por refuerzo tanto de una sola tarea como de varias, y evalúan la utilidad de la estructura en tareas robóticas complejas.
Contamos empíricamente el número de regiones lineales de las redes de rectificadores y refinamos los límites superior e inferior.
Este trabajo presenta límites mejorados para contar el número de regiones lineales en redes ReLU.
Analizamos las propiedades de memorización mediante un convnet del conjunto de entrenamiento y proponemos varios casos de uso en los que podemos extraer alguna información sobre el conjunto de entrenamiento. 
Ilumina las propiedades de generalización/memorización de ConvNets grandes y profundas e intenta desarrollar procedimientos relacionados con la identificación de si una entrada a una ConvNet entrenada ha sido realmente utilizada para entrenar la red.
En principio, los GANs pueden aprender distribuciones de manera eficiente, si la clase discriminante es compacta y tiene un fuerte poder de distinción contra la clase generadora particular.
Propone la noción de aproximabilidad restringida, y proporciona un límite de complejidad muestral, polinómico en la dimensión, que es útil para investigar la falta de diversidad en los GANs.
Analiza que la métrica de la probabilidad integral puede ser una buena aproximación de la distancia de Wasserstein bajo algunos supuestos suaves.
En la fase inicial del entrenamiento de las redes neuronales profundas existe un "punto de equilibrio" que determina las propiedades de toda la trayectoria de optimización.
Este trabajo analiza la optimización de las redes neuronales profundas considerando cómo los hiperparámetros de tamaño de lote y tamaño de paso modifican las trayectorias de aprendizaje.
Proponemos HWGCN para mezclar la información relevante del vecindario en diferentes órdenes para aprender mejor las representaciones de los nodos.
Los autores proponen una variante de GCN, HWGCN, para considerar la convolución más allá de los vecinos de un paso, que es comparable a los métodos del estado de la técnica.
Introducimos una nueva medida de planitud en los mínimos locales de la superficie de pérdida de las redes neuronales profundas que es invariante con respecto a las reparametrizaciones de las capas y conectamos la planitud con la robustez y la generalización de las características.
Los autores proponen una noción de robustez de los rasgos que es invariable con respecto al reescalado del peso y discuten la relación de la noción con la generalización.
Este trabajo define una noción de robustez de características y la combina con la representatividad epsilon de una función para describir una conexión entre la planitud de los mínimos y la generalización en las redes neuronales profundas.
Proponemos sparsificar las preactivaciones de las puertas y el flujo de información en LSTM para hacerlas constantes y potenciar el nivel de sparsity de las neuronas
En este trabajo se propone un método de sparsificación para redes neuronales recurrentes mediante la eliminación de las neuronas con preactivaciones cero para obtener redes compactas.
Introducimos un enfoque de red neuronal para asistir a los solucionadores de ecuaciones diferenciales parciales.
Los autores pretenden mejorar la precisión de los solucionadores numéricos entrenando una red neuronal sobre datos de referencia simulados que corrige el solucionador numérico.
un método de aprendizaje confederado que entrena el modelo a partir de datos médicos separados horizontal y verticalmente 
Un método de aprendizaje automático "confederado" que aprende a través de divisiones en datos médicos separados tanto horizontal como verticalmente.
Este trabajo propone la Activación Cuantitativa Estocástica que resuelve los problemas de sobreajuste en el entrenamiento adversarial FGSM y alcanza rápidamente la robustez comparable al entrenamiento multipaso.
El trabajo propone un modelo para mejorar el entrenamiento adversarial introduciendo perturbaciones aleatorias en las activaciones de una de las capas ocultas
Estudiamos la estructura del ruido en el cerebro y descubrimos que puede ayudar a la generalización al desplazar las representaciones a lo largo de las variaciones de los estímulos de la clase.
Presentamos un nuevo diseño, es decir, el autoensamblaje con agrupaciones agnósticas de categorías, para la adaptación de dominios tanto de conjunto cerrado como de conjunto abierto.
Un nuevo enfoque para la adaptación de dominios de conjuntos abiertos, donde las categorías del dominio de origen están contenidas en las categorías del dominio de destino para filtrar las categorías atípicas y permitir la adaptación dentro de las clases compartidas.
Mostramos cómo aprender descomposiciones espectrales de operadores lineales con el aprendizaje profundo, y lo utilizamos para el aprendizaje no supervisado sin un modelo generativo.
Los autores proponen utilizar un marco de aprendizaje profundo para resolver el cálculo de los mayores vectores propios.
Este trabajo presenta un marco para aprender las funciones propias a través de un proceso estocástico y propone abordar el reto de calcular las funciones propias en un contexto a gran escala, aproximándolas mediante un proceso de optimización estocástico de dos fases.
Aplicación del algoritmo Riemannian SGD (RSGD) para el entrenamiento de RNNs Tensor-Train para reducir aún más los parámetros del modelo.
El trabajo propone utilizar el algoritmo de gradiente estocástico riemanniano para el aprendizaje de trenes tensoriales de bajo rango en redes profundas.
Propone un algoritmo para la optimización de redes neuronales parametrizadas por la descomposición del tren tensorial basado en la optimización riemanniana y la adaptación del rango, y diseña una arquitectura TT LSTM bidireccional.
Proponemos un nuevo algoritmo que aprende políticas que satisfacen las restricciones, y proporcionamos un análisis teórico y una demostración empírica en el contexto del aprendizaje por refuerzo con restricciones.
Este trabajo introduce un algoritmo de optimización de políticas restringidas que utiliza un proceso de optimización en dos pasos, en el que las políticas que no satisfacen la restricción pueden proyectarse de nuevo al conjunto de restricciones.
Proponemos una representación basada en el gradiente para caracterizar la información que las redes profundas no han aprendido.
Los autores presentan la creación de representaciones basadas en gradientes con respecto a los pesos para complementar la información que falta en el conjunto de datos de entrenamiento para las redes profundas.
Introducimos un marco de reducción de artefactos en imágenes médicas "Zero-Shot", que aprovecha la potencia del aprendizaje profundo pero sin utilizar redes generales preentrenadas ni ninguna referencia de imagen limpia. 
Aplicamos el concepto de cuello de botella informativo a la atribución.
El artículo propone un nuevo método basado en la perturbación para calcular los mapas de atribución/saliencia para los clasificadores de imágenes basados en redes neuronales profundas, inyectando un ruido artificial en una de las primeras capas de la red.
Demostramos que las RNNs pueden podarse para inducir la dispersión de bloques, lo que mejora la velocidad de las operaciones dispersas en el hardware existente.
Los autores proponen un enfoque de poda de sparsity en bloque para comprimir RNNs, utilizando el grupo LASSO para promover la sparsity y para podar, pero con una programación muy especializada en cuanto a la poda y el peso de la misma.
Proponemos una mejora de las redes de iteración de valores, con aplicaciones a la planificación de trayectorias de vehículos planetarios.
Este trabajo aprende una función de recompensa basada en las trayectorias de los expertos utilizando un módulo de iteración de valores para que el paso de planificación sea diferenciable
Una novedosa capa de atención que combina la autoatención y las subcapas feed-forward de las redes Transformer.
Este trabajo propone una modificación del modelo Transformer incorporando la atención sobre los vectores de memoria "persistentes" en la capa de auto-atención, lo que resulta en un rendimiento a la par con los modelos existentes mientras se utilizan menos parámetros.
Encontramos eficazmente un subconjunto de imágenes que tienen activaciones más altas de lo esperado para algún subconjunto de nodos.  Estas imágenes parecen más anómalas y más fáciles de detectar cuando se ven como un grupo. 
El trabajo propuso un esquema para detectar la presencia de entradas anómalas basado en un enfoque de "exploración de subconjuntos" para detectar activaciones anómalas en la red de aprendizaje profundo.
Los modelos recurrentes estables pueden ser aproximados por redes feed-forward y, empíricamente, funcionan tan bien como los modelos inestables en tareas de referencia.
Estudia la estabilidad de las RNNs y la investigación de la normalización espectral a las predicciones secuenciales.
Se ha estudiado el papel del reparto de pesos en las redes neuronales mediante funciones hash, y se ha descubierto que una función hash equilibrada y determinista ayuda al rendimiento de la red.
Proponer ArbNets para estudiar el reparto de pesos de forma más sistemática definiendo la función de reparto de pesos como una función hash.
 Presentamos un sistema de aprendizaje relacional estadístico que toma prestadas las ideas de la lógica de Markov, pero que aprende una representación implícita de las reglas como una red neuronal.
El artículo proporciona una extensión de las redes lógicas de Markov eliminando su dependencia de las reglas lógicas de primer orden predefinidas para modelar más dominios en las tareas de finalización de bases de conocimiento.
Un método escalable para el aprendizaje de un previo expresivo sobre redes neuronales a través de múltiples tareas.
El artículo presenta un método para entrenar un modelo probabilístico para el Aprendizaje de Transferencia de Tareas Múltiples introduciendo una variable latente por tarea para capturar lo común en las instancias de la tarea.
El trabajo propone un enfoque variacional del metaaprendizaje que emplea variables latentes correspondientes a conjuntos de datos específicos de la tarea.
Tiene como objetivo aprender una prioridad sobre las redes neuronales para múltiples tareas. 
MODELOS DE ESPACIO DE ESTADO DESENTRAÑADOS
El artículo presenta un modelo generativo de espacio de estados que utiliza una variable latente global E para capturar la información específica del entorno.
Aprendizaje por divergencia de Bregman para el aprendizaje de pocos disparos. 
Introducimos un marco de red que puede modificar su estructura durante el entrenamiento y demostramos que puede converger a varios arquetipos de redes ML, como los MLP y los LCN. 
El aumento de datos guiado por el dominio proporciona un método robusto y estable de generalización del dominio
Este trabajo propone un enfoque de generalización de dominios mediante el aumento de datos dependientes del dominio
Los autores presentan el método CrossGrad, que entrena tanto una tarea de clasificación de etiquetas como una tarea de clasificación de dominios.
Investigamos alternativas a los enfoques tradicionales de modelado de imágenes de píxeles, y proponemos un modelo generativo para imágenes vectoriales.
Este artículo presenta una arquitectura de red neuronal para generar dibujos de bocetos inspirada en el autoencoder variacional.
Proporcionamos un estudio que trata de ver cómo la reciente adaptación de la tasa de aprendizaje en línea extiende la conclusión hecha por Wilson et al. 2018 sobre los métodos de gradiente adaptativo, junto con la comparación y el análisis de sensibilidad.
Informa de los resultados de las pruebas de varios métodos relacionados con el ajuste del tamaño de los pasos, incluyendo el SGD de vainilla, el SGD con impulso de Neserov y el ADAM, y compara esos métodos con hipergradiente y sin él. 
Investigamos los comportamientos en muestras grandes de las estimaciones del valor Q y propusimos una estrategia de exploración eficiente que se basa en la estimación de las discrepancias relativas entre las estimaciones de Q. 
Este trabajo presenta un algoritmo de exploración pura para el aprendizaje por refuerzo basado en un análisis asintótico de los valores Q y su convergencia a la distribución central de límites, superando a los algoritmos de exploración de referencia.
Entrenamos una red de traducción de imagen a imagen que toma como entrada la imagen de origen y una muestra de una distribución a priori para generar una muestra de la distribución de destino
Este trabajo formaliza el problema de la traducción no supervisada y propone un marco GAN aumentado que utiliza la información mutua para evitar el caso degenerado
Este artículo formula el problema de la traducción no supervisada de imágenes de uno a varios y aborda el problema minimizando la información mutua. 
Aprendiendo a extraer puntos clave distinguibles de una tarea proxy, rechazo de valores atípicos.
Este trabajo está dedicado al aprendizaje auto-supervisado de características locales utilizando RANSAC guiado por neuronas como un proveedor de pérdidas auxiliar adicional para mejorar la interpolación de los descriptores.
Proponemos una formulación de la motivación intrínseca que es adecuada como sesgo de exploración en las tareas sinérgicas de recompensa dispersa de múltiples agentes, al animar a los agentes a afectar al mundo de formas que no se lograrían si actuaran individualmente.
El artículo se centra en el uso de la motivación intrínseca para mejorar el proceso de exploración de los agentes de aprendizaje por refuerzo en tareas que requieren la participación de varios agentes.
Un algoritmo de inferencia probabilística dirigido por redes neuronales para modelos con estructura de grafos
Este trabajo introduce el paso de mensajes por políticas, una red neuronal gráfica con un mecanismo de inferencia que asigna mensajes a las aristas de forma recurrente, lo que indica un rendimiento competitivo en tareas de razonamiento visual.
Mostramos cómo se puede aumentar el rendimiento en una red multitarea ajustando una función de pérdida multitarea adaptativa que se aprende equilibrando directamente los gradientes de la red.
Este trabajo propone un esquema de actualización de pesos dinámico que actualiza los pesos para las diferentes pérdidas de las tareas durante el tiempo de entrenamiento haciendo uso de los ratios de pérdida de las diferentes tareas.
Las DNN para la segmentación de imágenes pueden implementar soluciones para el problema de la insidia, pero sólo algunas redes recurrentes podrían aprenderlas con un tipo específico de supervisión.
Este trabajo introduce la insidia para estudiar la segmentación semántica en la era del aprendizaje profundo, y los resultados pueden ayudar a que los modelos se generalicen mejor.
Dado un modelo preentrenado, exploramos los gradientes por muestra de los parámetros del modelo en relación con una pérdida específica de la tarea, y construimos un modelo lineal que combina los gradientes de los parámetros del modelo y la activación del mismo.
Este trabajo propone utilizar los gradientes de capas específicas de redes convolucionales como características en un modelo linealizado para el aprendizaje por transferencia y la adaptación rápida.
Entrenamos nuestro modelo de reconstrucción facial con pérdida adversarial de forma semi-supervisada en lotes híbridos de imágenes faciales no etiquetadas y etiquetadas para explotar el valor de grandes cantidades de imágenes faciales no etiquetadas de colecciones fotográficas sin restricciones.
Este trabajo propone un proceso de entrenamiento semi-supervisado y adversarial para obtener representaciones no lineales desentrañadas de una imagen facial con funciones de pérdida, logrando un rendimiento de vanguardia en la reconstrucción de rostros.
Este artículo presenta ConceptFlow, que modela explícitamente el flujo de la conversación en el grafo de conocimiento de sentido común para una mejor generación de conversaciones.
El artículo propone un sistema para generar una respuesta de un solo turno a un enunciado publicado en un entorno de diálogo de dominio abierto utilizando la difusión en los vecinos de los conceptos fundamentados.
Examinamos la hipótesis de que la entropía de los espacios de solución para las restricciones de los pesos sinápticos (la "flexibilidad" de la restricción) podría servir como función de coste para el desarrollo de los circuitos neuronales.
Los conjuntos infinitos de redes neuronales de amplitud infinita son una familia de modelos interesante desde la perspectiva de la teoría de la información.
Llevamos a cabo un estudio comparativo de los métodos de alineación multilingüe frente a los de entrenamiento conjunto y unificamos estos dos paradigmas, hasta ahora excluyentes, en un nuevo marco. 
Este artículo compara los enfoques de la inducción del léxico bilingüe y muestra qué método rinde mejor en las tareas de léxico, inducción, y NER y MT.
Combinación de técnicas de compresión de modelos ortogonales para obtener una reducción significativa del tamaño del modelo y del número de flops necesarios durante la inferencia.
Este trabajo propone combinar la descomposición de Tucker con la poda de filtros.
Presenta JAUNE: una metodología para sustituir la puntuación BLEU y ROUGE por evaluadores multidimensionales basados en modelos para evaluar los resúmenes
Este artículo propone una nueva métrica JAUNE para la evaluación de los sistemas de traducción automática y de resumen de textos, demostrando que su modelo se corresponde mejor con las etiquetas de similitud de la verdad terrestre que BLEU.
nuevo formalismo GNN + amplios experimentos; muestra que las diferencias entre GGNN/GCN/GAT son menores de lo que se pensaba
El artículo propone una nueva arquitectura de red neuronal gráfica que utiliza la modulación lineal por características para condicionar el paso de mensajes del nodo origen al nodo destino en función de la representación del nodo destino.
Este trabajo propone un nuevo marco de descomposición de matrices para la incrustación y la agrupación simultánea de datos de redes atribuidas.
Este trabajo propone un algoritmo para realizar conjuntamente la incrustación de la red de atributos y la agrupación.
Proponemos una técnica de renderizado guiado por imágenes que combina las ventajas del renderizado basado en imágenes y la síntesis de imágenes basada en GAN, teniendo en cuenta los efectos dependientes de la vista.
Esta presentación propone un método para manejar los efectos dependientes de la vista en el renderizado neural, que mejora la robustez de los métodos de renderizado neural existentes.
Los GAN se evalúan en conjuntos de datos sintéticos
Proponemos un método eficiente y eficaz de adaptación del tamaño del paso para los métodos de gradiente.
Una nueva adaptación del tamaño del paso en los métodos de gradiente de primer orden que establece un nuevo problema de optimización con la expansión de primer orden de la función de pérdida y la regularización, donde el tamaño del paso se trata como variable.
Disparamos que una amplia clase de colectores puede ser generada por ReLU y redes sigmoides con precisión arbitraria.
Este trabajo proporciona ciertas garantías básicas sobre cuándo las variedades pueden escribirse como la imagen de un mapa aproximado por una red neuronal, y une teoremas de la geometría de las variedades y resultados de aproximación universal estándar.
Este trabajo muestra teóricamente que los modelos generativos basados en redes neuronales pueden aproximar las variedades de datos, y demuestra que bajo suposiciones leves las redes neuronales pueden mapear un espacio latente en un conjunto cercano a la variedad de datos dada dentro de una pequeña distancia de Hausdorff.
Diseñamos algoritmos de aprendizaje por refuerzo basados en modelos con garantías teóricas y logramos resultados de vanguardia en las tareas de referencia Mujuco cuando se permite un millón de muestras o menos.
El artículo propone un marco para diseñar algoritmos de RL basados en modelos y basados en OFU que logran un rendimiento SOTA en las tareas MuJoCo.
Presentamos técnicas adicionales para utilizar la destilación de conocimientos para comprimir la red U en más de 1000 veces.
Los autores introdujeron una estrategia de destilación modificada para comprimir una arquitectura U-net en más de 1000 veces, manteniendo una precisión cercana a la U-net original.
Este trabajo proporciona un enfoque para abordar el olvido catastrófico a través de estimaciones de curvatura sin Hessian
El trabajo propone un método aproximado de Laplace en el entrenamiento de redes neuronales en el entorno del aprendizaje continuo con una baja complejidad espacial.
Método para mejorar el rendimiento de los modelos simples a partir de un modelo complejo (preciso).
El documento propone un medio para mejorar las predicciones de un modelo de baja capacidad que muestra beneficios sobre los enfoques existentes.
Un algoritmo sencillo y práctico para aprender un núcleo invariante de la traslación o esféricamente simétrico que maximice el margen a partir de los datos de entrenamiento, utilizando herramientas del análisis de Fourier y la minimización del arrepentimiento.
El artículo propone aprender un núcleo invariante de la traslación o la rotación en la representación de Fourier para maximizar el margen de la SVM.
Los autores proponen un interesante algoritmo para el aprendizaje conjunto del l1-SVM y del kernel representado por Fourier
Los autores consideran el aprendizaje directo de representaciones de Fourier de núcleos invariantes de desplazamiento/translación para aplicaciones de aprendizaje automático con la alineación del núcleo con los datos como función objetivo a optimizar.
Programación probabilística que admite de forma nativa la inferencia causal y contrafáctica
Inferencia de un modelo de juego de campo medio (MFG) del comportamiento de grandes poblaciones mediante una síntesis de MFG y procesos de decisión de Markov.
Los autores abordan la inferencia en modelos de comportamiento colectivo utilizando el aprendizaje por refuerzo inverso para aprender las funciones de recompensa de los agentes en el modelo.
Combinamos modelos generativos profundos con una supervisión programática débil para generar trayectorias coordinadas de múltiples agentes de una calidad significativamente superior a la de las líneas de base anteriores.
Propone modelos generativos secuenciales multiagente.
El artículo propone entrenar modelos generativos que produzcan trayectorias multiagente utilizando funciones heurísticas que etiqueten variables que de otro modo estarían latentes en los datos de entrenamiento
Aprender a clasificar las curvas de aprendizaje para detener tempranamente los trabajos de entrenamiento poco prometedores. Novedad: uso de la pérdida de clasificación por pares para modelar directamente la probabilidad de mejorar y transferir el aprendizaje entre conjuntos de datos para reducir los datos de entrenamiento necesarios.
El artículo propone un método para clasificar las curvas de aprendizaje de las redes neuronales que puede modelar las curvas de aprendizaje a través de diferentes conjuntos de datos, logrando una mayor velocidad en las tareas de clasificación de imágenes.
Demostramos que, en entornos de aprendizaje continuo, se puede evitar el olvido catastrófico aplicando RL fuera de política a una mezcla de experiencia nueva y de repetición, con una pérdida de clonación del comportamiento.
Propone una variante particular de la repetición de experiencias con clonación de conductas como método de aprendizaje continuo.
Presentamos un método que aprende a integrar la información temporal y la información visual ambigua en el contexto de los agentes que interactúan.
Los autores proponen Graph VRNN que modela la interacción de múltiples agentes desplegando una VRNN para cada agente
Este trabajo presenta una arquitectura basada en redes neuronales gráficas que se entrena para localizar y modelar las interacciones de los agentes en un entorno directamente a partir de los píxeles y muestra las ventajas del modelo para el seguimiento de las tareas y la previsión de las ubicaciones de los agentes.
Consideramos un modelo simplificado de red neuronal convolucional profunda. Demostramos que todas las capas de esta red se pueden aprender de forma aproximada con una aplicación adecuada de la descomposición tensorial.
Proporciona garantías teóricas para el aprendizaje de redes neuronales convolucionales profundas utilizando la descomposición tensorial de rango uno.
Este trabajo propone un método de aprendizaje para un caso restringido de redes convolucionales profundas, en el que las capas se limitan al caso de no solapamiento y sólo tienen un canal de salida por capa
Analiza el problema del aprendizaje de una clase muy especial de CNNs: cada capa consiste en un único filtro, aplicado a parches no solapados de la entrada.
Las redes neuronales de avance que pueden tener pesos podados después del entrenamiento podrían haber tenido los mismos pesos podados antes del entrenamiento
Muestra que existen subredes dispersas que pueden ser entrenadas desde cero con un buen rendimiento de generalización y propone una NN no podada e inicializada aleatoriamente que contiene subredes que pueden ser entrenadas desde cero con una precisión de generalización similar.
El documento examina la hipótesis de que las redes neuronales inicializadas aleatoriamente contienen subredes que convergen igual de rápido o más rápido y pueden alcanzar la misma o mejor precisión de clasificación
En este trabajo destacamos la dificultad de entrenar redes neuronales dispersas haciendo experimentos de interpolación en el paisaje energético 
La simetría del espacio de pesos en los paisajes de las redes neuronales da lugar a numerosos sillines y subespacios planos de alta dimensión.
El artículo presenta un método de baja pérdida para estudiar la función de pérdida con respecto a los parámetros en una red neuronal desde la perspectiva de la simetría peso-espacio.
teoría de la propagación de la señal aplicada a sustitutos continuos de redes binarias; inicialización contraintuitiva; el truco de la reparametrización no es útil
Los autores investigan la dinámica de entrenamiento de las redes neuronales binarias cuando se utilizan sustitutos continuos, estudian qué propiedades deben tener las redes en el momento de la inicialización para que se entrenen de la mejor manera posible y proporcionan consejos concretos sobre los pesos estocásticos en la inicialización.
Una exploración en profundidad de las redes binarias estocásticas, los sustitutos continuos y su dinámica de entrenamiento, con ideas sobre cómo inicializar los pesos para obtener el mejor rendimiento.
Proponemos un enfoque para el aprendizaje semisupervisado de analizadores de dependencias semánticas basado en el marco del autoencoder CRF.
Este artículo se centra en el análisis sintáctico de dependencias semánticas con supervisión, utilizando el autocodificador CRF para entrenar el modelo de forma semisupervisada, lo que indica su eficacia en tareas de datos etiquetados con pocos recursos.
DeFINE utiliza una red profunda, jerárquica y dispersa con nuevas conexiones de salto para aprender mejores incrustaciones de palabras de forma eficiente. 
Este trabajo describe un nuevo método para el aprendizaje de representaciones profundas a nivel de palabra de manera eficiente mediante el uso de una estructura jerárquica con conexiones de salto para el uso de capas de entrada y salida de baja dimensión.
Reproducimos con éxito y hacemos observaciones sobre la comparación con las líneas de base de un enfoque de meta-aprendizaje para la clasificación de pocos disparos que funciona por retropropagación a través de la solución de un solucionador de forma cerrada.
La reasignación dinámica de parámetros permite el entrenamiento directo de redes dispersas compactas, y desempeña un papel indispensable incluso cuando conocemos la red dispersa óptima a priori
3 ejes que sirven de trampolín para el aprendizaje experimental del robot del módulo de visión
Investiga el rendimiento de los clasificadores de imágenes y los detectores de objetos existentes. 
Todas las capas de nuestros modelos acústicos basados en CNNs, excepto las dos primeras, mostraron cierto grado de especificidad lingüística, pero la congelación del entrenamiento permitió una transferencia satisfactoria entre idiomas.
El artículo mide la transferibilidad de las características de cada capa en los modelos acústicos basados en CNN a través de los idiomas, concluyendo que los AMs entrenados con la técnica de "entrenamiento congelado" superaron a otros modelos transferidos.
Vinculación de los gradientes políticos entrópicos de la región de Wasserstein y la ecuación del calor.
El artículo explora las conexiones entre el aprendizaje por refuerzo y la teoría del transporte óptimo cuadrático
Los autores estudiaron el gradiente de políticas con cambio de políticas limitado por una región de confianza de la distancia de Wasserstein en el escenario del bandido multi-armado, mostrando que en el límite de pasos pequeños, la dinámica de las políticas se rige por la ecuación del calor (ecuación de Fokker-Planck).
La capacidad discriminativa de softmax para el aprendizaje de vectores de características de los objetos se mejora eficazmente mediante la virtud de la normalización isotrópica en la distribución global de los puntos de datos.
Adaptamos el aprendizaje Q con bonificación de exploración UCB a MDP de horizonte infinito con recompensas descontadas sin acceder a un modelo generativo, y mejora el mejor resultado conocido anteriormente.
Este trabajo considera un algoritmo de aprendizaje Q con política de exploración UCB para MDP de horizonte infinito.
Las perturbaciones pueden utilizarse para entrenar pesos de retroalimentación para aprender en redes neuronales totalmente conectadas y convolucionales
Este trabajo propone un método que aborda el problema del "transporte de pesos" mediante la estimación de los pesos para el paso hacia atrás utilizando un estimador basado en el ruido 
Identificamos algunos patrones universales (es decir, que se mantienen en todas las arquitecturas) en el comportamiento de diferentes pérdidas sustitutas (CE, MSE, pérdida 0-1) mientras se entrenan las redes neuronales y presentamos pruebas empíricas de apoyo.
