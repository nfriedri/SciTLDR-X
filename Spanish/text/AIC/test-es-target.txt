FearNet es una red neuronal eficiente en cuanto a la memoria, inspirada en la formación de la memoria en el cerebro de los mamíferos, que es capaz de aprender clases de forma incremental sin olvido catastrófico.
Este trabajo presenta una solución novedosa a un problema de clasificación incremental basado en un sistema de memoria dual. 
El aprendizaje multivista mejora el aprendizaje no supervisado de la representación de frases
El enfoque utiliza diferentes codificadores complementarios de la frase de entrada y la maximización del consenso.
El artículo presenta un marco multivista para mejorar la representación de oraciones en tareas de PNL utilizando arquitecturas de objetivos generativos y discriminativos.
Este trabajo demuestra que los marcos multivista son más eficaces que el uso de codificadores individuales para el aprendizaje de representaciones de frases.
Mostramos cómo se pueden aprender objetos discretos de forma no supervisada a partir de los píxeles, y cómo realizar un aprendizaje por refuerzo utilizando esta representación de objetos.
Un método para aprender representaciones de objetos a partir de píxeles para realizar un aprendizaje por refuerzo. 
El artículo propone una arquitectura neuronal para asignar flujos de vídeo a una colección discreta de objetos, sin anotaciones humanas, utilizando una pérdida de reconstrucción de píxeles no supervisada. 
Un conjunto de datos a gran escala para entrenar modelos de atención para el reconocimiento de objetos conduce a un reconocimiento de objetos más preciso, interpretable y similar al humano.
Sostiene que los recientes avances en el reconocimiento visual se deben a la utilización de mecanismos de atención visual en redes convolucionales profundas, que aprenden dónde enfocar mediante una forma débil de supervisión basada en las etiquetas de clase de las imágenes.
Presenta una nueva visión de la atención en la que se recoge un gran conjunto de datos de atención y se utiliza para entrenar una NN de forma supervisada para explotar la atención humana autodeclarada.
Este trabajo propone un nuevo enfoque para utilizar señales más informativas, concretamente, las regiones que los humanos consideran importantes en las imágenes, para mejorar las redes neuronales convolucionales profundas.
Proponemos un método de defensa eficiente en el tiempo contra los ataques adversarios de un solo paso e iterativos.
Proponemos un método novedoso y eficiente desde el punto de vista computacional, denominado e2SAD, que genera conjuntos de dos muestras adversarias de entrenamiento para cada muestra de entrenamiento limpia.
El artículo introduce un método de defensa adversarial en dos pasos, para generar dos ejemplos adversariales por cada muestra limpia e incluirlos en el bucle de entrenamiento real para lograr robustez y afirmar que puede superar a métodos iterativos más costosos.
El artículo presenta un enfoque de 2 pasos para generar ejemplos adversarios fuertes con un coste mucho menor en comparación con los recientes ataques adversarios iterativos de varios pasos.
Una comparación de cinco arquitecturas de redes neuronales profundas para la detección de nombres de dominio maliciosos muestra sorprendentemente pocas diferencias.
Los autores proponen utilizar cinco arquitecturas profundas para la tarea de ciberseguridad de detección de algoritmos de generación de dominios.
Aplica varias arquitecturas NN para clasificar las url's entre benignas y relacionadas con el malware.
Este trabajo propone reconocer automáticamente los nombres de dominio como maliciosos o benignos mediante redes profundas entrenadas para clasificar directamente la secuencia de caracteres como tal.
Los métodos de aprendizaje adversarial animan a los modelos NLI a ignorar los sesgos específicos del conjunto de datos y ayudan a los modelos a transferirse entre conjuntos de datos.
El artículo propone una configuración adversarial para mitigar los artefactos de anotación en los datos de inferencia del lenguaje natural
Este artículo presenta un método para eliminar el sesgo de un modelo de vinculación textual mediante un objetivo de entrenamiento adversarial. 
Un nuevo enfoque del estado del arte para la incrustación de grafos de conocimiento.
Presenta una función de puntuación de predicción de enlaces neuronales que puede inferir patrones de simetría, antisimetría, inversión y composición de relaciones en una base de conocimientos.
Este trabajo propone un enfoque para la incrustación de grafos de conocimiento modelando las relaciones como rotaciones en el espacio vectorial complejo.
Propone un método de incrustación de gráficos para la predicción de enlaces
Modificamos la CNN utilizando HyperNetworks y observamos una mayor robustez frente a los ejemplos adversos.
Mejora de la robustez y fiabilidad de las redes neuronales de convolución profunda mediante el uso de núcleos de convolución dependientes de los datos
Proponemos un enfoque de meta-aprendizaje para la clasificación de pocos disparos que logra un fuerte rendimiento a alta velocidad por retropropagación a través de la solución de solucionadores rápidos, como la regresión de cresta o la regresión logística.
El artículo propone un algoritmo de meta-aprendizaje que consiste en fijar las características (es decir, todas las capas ocultas de una NN profunda), y tratar cada tarea como si tuviera su propia capa final, que podría ser una regresión de cresta o una regresión logística.
Este trabajo propone un enfoque de meta-aprendizaje para el problema de la clasificación de pocos disparos, utilizan un método basado en la parametrización del aprendiz para cada tarea por un solucionador de forma cerrada.
Aportamos una nueva perspectiva sobre el entrenamiento de un modelo de aprendizaje automático desde cero en el entorno de las etiquetas jerárquicas, es decir, pensándolo como una comunicación bidireccional entre humanos y algoritmos, y estudiamos cómo podemos medir y mejorar la eficiencia. 
Introduce una nueva configuración de Aprendizaje Activo en la que el oráculo ofrece una etiqueta parcial o débil en lugar de consultar la etiqueta de un ejemplo concreto, lo que permite una recuperación más sencilla de la información.
Este trabajo propone un método de aprendizaje activo con retroalimentación parcial que supera las líneas de base existentes con un presupuesto limitado.
El artículo considera un problema de clasificación multiclase en el que las etiquetas se agrupan en un número determinado M de subconjuntos, que contienen todas las etiquetas individuales como singletons.
Demostramos que los espacios de Wasserstein son buenos objetivos para incrustar datos con una estructura semántica compleja.
Aprende incrustaciones en un espacio discreto de distribuciones de probabilidad, utilizando una versión minimizada y regularizada de las distancias de Wasserstein.
El artículo describe un nuevo método de incrustación que incrusta los datos en el espacio de medidas de probabilidad dotado de la distancia de Wasserstein. 
El artículo propone incrustar los datos en espacios de Wasserstein de baja dimensión, que pueden capturar la estructura subyacente de los datos con mayor precisión.
Un algoritmo de agrupación que realiza una reducción de la dimensionalidad no lineal y una agrupación conjunta optimizando un objetivo global continuo.
Presenta un algoritmo de clustering resolviendo conjuntamente el autoencoder profundo y el clustering como objetivo global continuo, mostrando mejores resultados que los esquemas de clustering del estado del arte.
El clustering continuo profundo es un método de clustering que integra el objetivo del autoencoder con el objetivo del clustering y luego se entrena utilizando el SGD.
Para acelerar el cálculo de las redes neuronales convolucionales, proponemos una nueva técnica de poda en dos pasos que consigue una mayor dispersión de pesos en el dominio de Winograd sin cambiar la estructura de la red.
Propone un marco de poda espacial-Winograd que permite que el peso podado del dominio espacial se mantenga en el dominio Winograd y mejora la escasez del dominio Winograd.
Propone dos técnicas de poda de capas convolucionales que utilizan el algoritmo de Winograd
Proponemos un modelo bayesiano no paramétrico para el aprendizaje federado con redes neuronales.
Utiliza el proceso beta para realizar la correspondencia neuronal federada.
El documento considera el aprendizaje federado de redes neuronales, donde los datos se distribuyen en múltiples máquinas y la asignación de puntos de datos es potencialmente no homogénea y desequilibrada.
Método general para entrenar núcleos MCMC expresivos parametrizados con redes neuronales profundas. Dada una distribución objetivo p, nuestro método proporciona un muestreador de mezcla rápida, capaz de explorar eficientemente el espacio de estados.
Propone un HMC generalizado modificando el integrador de salto mediante redes neuronales para que el muestreador converja y se mezcle rápidamente. 
Demostramos que los fallos raros pero catastróficos pueden pasar desapercibidos por completo en las pruebas aleatorias, lo que plantea problemas para un despliegue seguro. El enfoque que proponemos para las pruebas adversariales soluciona este problema.
Propone un método que aprende un predictor de probabilidad de fallo para un agente aprendido, lo que lleva a predecir qué estados iniciales hacen que un sistema falle.
Este trabajo propone un enfoque de muestreo de importancia para el muestreo de casos de fallo para los algoritmos de RL basado en una función aprendida a través de una red neuronal sobre los fallos que se producen durante el entrenamiento del agente
Este trabajo propone un enfoque adversarial para identificar casos de fallos catastróficos en el aprendizaje por refuerzo.
Para abordar el colapso posterior en las VAE, proponemos un procedimiento de entrenamiento novedoso pero sencillo que optimiza de forma agresiva la red de inferencia con más actualizaciones. Este nuevo procedimiento de entrenamiento mitiga el colapso posterior y conduce a un mejor modelo VAE. 
Examina el fenómeno del colapso posterior, mostrando que un mayor entrenamiento de la red de inferencia puede reducir el problema y conducir a mejores óptimos.
Los autores proponen cambiar el procedimiento de entrenamiento de las VAE sólo como solución al colapso posterior, dejando el modelo y el objetivo intactos.
Descubrir de forma generosa nuevos pares de entidades significativas con una determinada relación médica mediante el mero aprendizaje a partir de los pares de entidades significativas existentes, sin necesidad de un corpus de texto adicional para la extracción discriminativa.
Presenta un autoencoder variacional para generar pares de entidades dada una relación en un entorno médico.
En el contexto médico, este artículo describe el problema clásico de "completar la base de conocimientos" a partir de datos estructurados únicamente.
Analizamos detenidamente la función objetivo de la VAE y sacamos conclusiones novedosas que conducen a mejoras sencillas.
Propone un método de VAE en dos etapas para generar muestras de alta calidad y evitar la borrosidad.
Este documento analiza las VAE gaussianas.
El artículo proporciona una serie de resultados teóricos sobre los autocodificadores variacionales gaussianos "vainilla", que luego se utilizan para construir un nuevo algoritmo llamado "VAE de 2 etapas".
Un algoritmo de ajuste de hiperparámetros mediante el análisis discreto de Fourier y la detección comprimida
Investiga el problema de la optimización de hiperparámetros bajo el supuesto de que la función desconocida puede ser aproximada, mostrando que la minimización aproximada puede realizarse sobre el hipercubo booleano.
El artículo explora la optimización de los hiperparámetros asumiendo una estructura en la función desconocida que asigna los hiperparámetros a la precisión de la clasificación
Un nuevo método para la inferencia de permutaciones por ascenso de gradiente, con aplicaciones a la inferencia de coincidencias latentes y al aprendizaje supervisado de permutaciones con redes neuronales
El artículo utiliza la aproximación finita del operador Sinkhorn para describir cómo se puede construir una red neuronal para el aprendizaje a partir de datos de entrenamiento con valor de permutación. 
El artículo propone un nuevo método que aproxima la ponderación máxima discreta para el aprendizaje de permutaciones latentes
Un nuevo marco de búsqueda de arquitectura neuronal diferenciable para la cuantificación mixta de ConvNets.
Los autores introducen un nuevo método de búsqueda de arquitectura neuronal que selecciona la cuantificación de precisión de los pesos en cada capa de la red neuronal, y lo utilizan en el contexto de la compresión de la red.
El artículo presenta un nuevo enfoque en la cuantificación de la red mediante la cuantificación de diferentes capas con diferentes anchos de bits e introduce un nuevo marco de búsqueda de arquitectura neuronal diferenciable.
Función de pérdida suave para la minimización de errores Top-k
Propone utilizar la pérdida top-k con modelos profundos para abordar el problema de la confusión de clases con clases similares tanto presentes como ausentes del conjunto de datos de entrenamiento.
Suaviza las pérdidas top-k.
Este trabajo introduce una función de pérdida sustituta suave para la SVM top-k, con el fin de conectar la SVM a las redes neuronales profundas.
Presentamos Mol-CycleGAN - un nuevo modelo generativo para la optimización de moléculas para aumentar el diseño de fármacos.
El artículo presenta un enfoque para la optimización de las propiedades moleculares basado en la aplicación de CycleGANs a autocodificadores variacionales para moléculas y emplea un VAE de dominio específico llamado Junction Tree VAE (JT-VAE).
Este trabajo utiliza un autocodificador variacional para aprender una función de traslación, desde el conjunto de moléculas sin la propiedad interesada al conjunto de moléculas con la propiedad. 
Tomamos el reconocimiento de rostros como punto de ruptura y proponemos la destilación de modelos con transferencia de conocimientos desde la clasificación de rostros hasta la alineación y la verificación
Este trabajo propone transferir el clasificador del modelo de clasificación de caras a la tarea de alineación y verificación.
El manuscrito presenta experimentos sobre la destilación de conocimientos de un modelo de clasificación de caras a modelos de estudiantes para la alineación y verificación de caras.
Mejora de las recomendaciones basadas en sesiones con RNNs (GRU4Rec) en un 35% utilizando funciones de pérdida y muestreo de nuevo diseño.
Este artículo analiza las funciones de pérdida existentes para las recomendaciones basadas en la sesión y propone dos nuevas funciones de pérdida que añaden una ponderación a las funciones de pérdida basadas en la clasificación existentes
Presenta modificaciones sobre trabajos anteriores para la recomendación basada en sesiones utilizando RNN ponderando los ejemplos negativos por su "relevancia"
En este artículo se discuten los problemas para optimizar las funciones de pérdida en GRU4Rec, se proponen trucos para la optimización y se sugiere una versión mejorada.
Desarrollamos un enfoque de aprendizaje permanente para el aprendizaje por transferencia basado en la teoría PAC-Bayes, según la cual las preconcepciones se ajustan a medida que se encuentran nuevas tareas, facilitando así el aprendizaje de tareas nuevas.
Un nuevo límite de riesgo PAC-Bayesiano que sirve como función objetivo para el aprendizaje automático multitarea, y un algoritmo para minimizar una versión simplificada de esa función objetivo.
Amplía los límites de PAC-Bayes existentes al aprendizaje multitarea, para permitir la adaptación de la priorización en diferentes tareas.
Una versión adaptada de Adam para el entrenamiento de DNNs, que salva la brecha de generalización entre Adam y SGD.
Propone una variante del algoritmo de optimización ADAM que normaliza los pesos de cada unidad oculta utilizando la normalización por lotes
Ampliación del algoritmo de optimización de Adam para preservar la dirección de actualización adaptando la tasa de aprendizaje de los pesos entrantes a una unidad oculta de forma conjunta utilizando la norma L2 del vector gradiente
Mostramos cómo podemos utilizar la representación sucesora para descubrir eigenopciones en dominios estocásticos, a partir de píxeles brutos. Las eigenopciones son opciones aprendidas para navegar por las dimensiones latentes de una representación aprendida.
Extiende la idea de las eigenopciones a los dominios con transiciones estocásticas y donde se aprenden las características del estado.
Muestra la equivalencia entre las funciones de valor de los prototipos y las representaciones de los sucesores y deriva la idea de las opciones propias como mecanismo de descubrimiento de opciones
El documento es una continuación del trabajo anterior de Machado et al. (2017) que muestra cómo las funciones de protovalor pueden utilizarse para definir opciones denominadas "eigenopciones".
Proporcionamos límites superiores mejorados para el número de regiones lineales utilizadas en la expresividad de la red, y un algoritmo altamente eficiente (con respecto al recuento exacto) para obtener límites inferiores probabilísticos sobre el número real de regiones lineales.
Contribuye al estudio del número de regiones lineales en las redes neuronales RELU utilizando un algoritmo de recuento probabilístico aproximado y un análisis
Se basa en trabajos anteriores que estudian el recuento de regiones lineales en redes neuronales profundas, y mejora el límite superior propuesto previamente cambiando la restricción de dimensionalidad
El artículo trata de la expresividad de una red neuronal lineal a trozos, caracterizada por el número de regiones lineales de la función modelada, y aprovecha los algoritmos probabilísticos para calcular los límites más rápidamente, y demuestra límites más ajustados.
Una memoria de trabajo de inspiración biológica que puede integrarse en modelos de atención visual recurrente para obtener un rendimiento de vanguardia
Introduce una nueva arquitectura de red inspirada en la memoria de trabajo visual atenta y la aplica a tareas de clasificación y la utiliza como modelo generativo
El artículo aumenta el modelo de atención recurrente con un nuevo modelo de memoria de trabajo de Hebb-Rosenblatt y consigue resultados competitivos en MNIST
El artículo utiliza la Autocodificación Variacional y el condicionamiento de la red para la Transferencia de Timbres Musicales, desarrollamos y generalizamos nuestra arquitectura para las transferencias de muchos a muchos instrumentos junto con visualizaciones y evaluaciones.
Propone un autocodificador variacional modulado para realizar la transferencia tímbrica musical sustituyendo el habitual criterio de traducción adversarial por un Maxiimum Mean Discrepancy
Describe un modelo de transferencia de timbres musicales de muchos a muchos que se basa en los recientes avances en la transferencia de dominios y estilos
Propone un modelo híbrido basado en la VAE para realizar la transferencia tímbrica en grabaciones de instrumentos musicales.
Estudiamos el comportamiento de los autocodificadores de vainilla multicapa ligados a pesos bajo el supuesto de pesos aleatorios. A través de una caracterización exacta en el límite de grandes dimensiones, nuestro análisis revela interesantes fenómenos de transición de fase.
Un análisis teórico de los autocodificadores con pesos ligados entre el codificador y el decodificador (weight-tied) mediante el análisis del campo medio
Analiza las prestaciones de los autocodificadores ligados ponderados basándose en los recientes avances en el análisis de problemas estadísticos de alta dimensión y, en concreto, en el algoritmo de paso de mensajes
Este trabajo estudia los autocodificadores bajo varios supuestos, y señala que este modelo de autocodificador aleatorio puede ser analizado de forma elegante y rigurosa con ecuaciones unidimensionales.
Inspirado en trabajos anteriores sobre autocodificadores Sliced-Wasserstein (SWAE) y suavización de kernel, construimos un nuevo modelo generativo: el autocodificador Cramer-Wold (CWAE).
Este trabajo propone una variante de WAE basada en una nueva distancia estadística entre la distribución de datos codificada y la distribución latente a priori
Introduce una variación de los Audoencoders de Wasserstein que es una novedosa arquitectura de autoencodificación regularizada que propone una elección específica de la penalización por divergencia
Este trabajo propone el autocodificador de Cramer-Wold, que utiliza la distancia de Cramer-Wold entre dos distribuciones basadas en el Teorema de Cramer-Wold.
Utilizamos un discriminador GAN para realizar un esquema de muestreo de rechazo aproximado en la salida del generador GAN.
 Propone un algoritmo de muestreo de rechazo para el muestreo del generador GAN.
Este trabajo propone un esquema de muestreo de rechazo post-procesamiento para GANs, llamado Discriminator Rejection Sampling, para ayudar a filtrar las muestras â€˜buenasâ€™ del generador de GANs.
Un método sencillo y rápido para extraer características visuales de las redes neuronales convolucionales
Propone una forma rápida de aprender características convolucionales que posteriormente pueden ser utilizadas con cualquier clasificador mediante el uso de un número reducido de epocs de entrenamiento y retrasos de programación específicos de la tasa de aprendizaje
Utilizar un esquema de decaimiento de la tasa de aprendizaje que se fija en relación con el número de épocas utilizadas en el entrenamiento y extraer la salida de la penúltima capa como características para entrenar un clasificador convencional.
Proporcionamos nuevas ideas e interpretaciones de las RNN desde la perspectiva de los operadores de spline max-affine.
Reescribe las ecuaciones de la RNN de Elman en términos de los llamados operadores spline max-affine
Proporcionar un enfoque novedoso hacia la comprensión de las RNNs que utilizan operadores spline de máxima afinidad (MASO) reescribiéndolas con MASOs de activaciones afines y convexas a trozos
Los autores se basan en la interpetación del operador spline max-affine de una clase sustancial de redes profundas, centrándose en las redes neuronales recurrentes que utilizan el ruido en el estado oculto inicial como regularización
Escalamos los teoremas neuronales a grandes conjuntos de datos, mejoramos el proceso de aprendizaje de reglas y lo ampliamos para razonar conjuntamente sobre texto y bases de conocimiento.
Propone una extensión del sistema Neural Theorem Provers que aborda los principales problemas de este modelo reduciendo la complejidad temporal y espacial del mismo
Escala los PNT utilizando la búsqueda aproximada del vecino más cercano sobre los hechos y las reglas durante la unificación y sugiere parametrizar los predicados utilizando la atención sobre los predicados conocidos
mejora el enfoque del Prover de Teoremas Neuronales propuesto anteriormente utilizando la búsqueda del vecino más cercano.
Generalización de las relaciones aprendidas entre pares de imágenes utilizando un pequeño dato de entrenamiento a tipos de imágenes no vistas previamente utilizando un modelo de sistemas dinámicos explicables, Reservoir Computing, y una técnica de aprendizaje biológicamente plausible basada en analogías.
Reclama los resultados de la "combinación de transformaciones" en el contexto de la CR utilizando una red de eco-estado con activaciones tanh estándar con la diferencia de que los pesos recurrentes no están entrenados
Nuevo método de clasificación de diferentes distorsiones de datos MNIST
El artículo utiliza una red de estados de eco para aprender a clasificar las transformaciones de imágenes entre pares de imágenes en una de las cinco clases.
Presentamos Generative Adversarial Privacy and Fairness (GAPF), un marco de trabajo basado en datos para el aprendizaje de representaciones privadas y justas con garantías certificadas de privacidad/justicia
Este artículo utiliza un modelo GAN para proporcionar una visión general de los trabajos relacionados con el Aprendizaje de Representación Privada/Justa (PRL).
Este trabajo presenta un enfoque basado en el adversario para representaciones privadas y justas mediante la distorsión aprendida de los datos que minimiza la dependencia de las variables sensibles mientras el grado de distorsión está restringido.
Los autores describen un marco de trabajo sobre cómo aprender una representación de paridad demográfica que puede utilizarse para entrenar ciertos clasificadores.
Presentamos métricas y un ataque óptimo para evaluar los modelos que se defienden de los ejemplos adversos utilizando el umbral de confianza
Este artículo presenta una familia de ataques a los algoritmos de umbralización de confianza, centrándose principalmente en las metodologías de evaluación.
Propone un método de evaluación para los modelos de defensa de umbral de confianza y un enfoque para generar ejemplos adversos eligiendo la clase errónea con más confianza cuando se utilizan ataques dirigidos
El artículo presenta una metodología para evaluar los ataques a los métodos de umbralización de confianza y propone un nuevo tipo de ataque.
un nuevo método de aprendizaje con recompensa dispersa mediante el reetiquetado de recompensa adversarial
Propone utilizar un entorno competitivo multiagente para fomentar la exploración y demuestra que CER + HER > HER ~ CER
Proponer un nuevo método de aprendizaje a partir de recompensas dispersas en entornos de aprendizaje por refuerzo sin modelo y densificar la recompensa
Para hacer frente a los problemas de recompensa escasa y fomentar la exploración en los algoritmos de RL, los autores proponen una estrategia de reetiquetado denominada Respuesta de Experiencia Competitiva (CER).
La construcción de un modelo TTS con VAE de mezcla gaussiana permite un control detallado del estilo de habla, las condiciones de ruido, etc.
Describe el modelo GAN condicionado para generar espectros de Mel condicionados por el hablante aumentando el espacio z correspondiente a la identificación
Este trabajo propone un modelo de variable latente de dos capas para obtener una representación latente desenredada, facilitando así un control de grano fino sobre varios atributos
Este artículo propone un modelo que puede controlar los atributos no anotados, como el estilo de habla, el acento, el ruido de fondo, etc.
Habilitación de modelos de respuesta a preguntas visuales para contar manejando propuestas de objetos superpuestos.
Este trabajo propone una arquitectura de red diseñada a mano sobre un gráfico de propuestas de objetos para realizar una supresión suave no máxima para obtener el recuento de objetos.
Se centra en un problema de recuento en la respuesta a preguntas visuales utilizando el mecanismo de atención y propone un componente de recuento diferenciable que cuenta explícitamente el número de objetos
Este trabajo aborda el problema del recuento de objetos en la respuesta a preguntas visuales, propone muchas heurísticas para encontrar el recuento correcto.
Un enfoque sencillo y sin entrenamiento para la incrustación de frases con un rendimiento competitivo en comparación con los modelos sofisticados que requieren una gran cantidad de datos de entrenamiento o un tiempo de entrenamiento prolongado.
Presentado un nuevo modo de generar incrustaciones de frases sin necesidad de entrenamiento con un análisis sistemático
Propone un nuevo método basado en la geometría para la incrustación de frases a partir de vectores de incrustación de palabras mediante la cuantificación de la novedad, la importancia y la singularidad del corpus de cada palabra
Este trabajo explora la incrustación de oraciones basada en la descomposición ortogonal del espacio abarcado por incrustaciones de palabras
Proponemos nuevas extensiones de las redes prototípicas que se ven aumentadas con la capacidad de utilizar ejemplos no etiquetados al producir prototipos.
Este trabajo es una extensión de una red prototípica que considera el empleo de los ejemplos no etiquetados disponibles para ayudar a entrenar cada episodio
Estudia el problema de la clasificación semisupervisada de pocos disparos extendiendo las redes prototípicas al entorno del aprendizaje semisupervisado con ejemplos de clases distractoras
Extiende la Red de Prototipos a la configuración semi-supervisada mediante la actualización de los prototipos utilizando las pseudo-etiquetas asignadas, tratando con los distractores, y ponderando las muestras utilizando la distancia a los prototipos originales.
Demostramos teóricamente que las interpolaciones lineales son inadecuadas para el análisis de modelos generativos implícitos entrenados. 
Estudia el problema de cuando la interpolante lineal entre dos variables aleatorias sigue la misma distribución, relacionada con la distribución a priori de un modelo generativo implícito
Este trabajo se plantea cómo interpolar en el espacio latente dado un modelo de variable latente.
Detección de nódulos pulmonares a partir de datos de proyección en lugar de imágenes.
Las DNN se utilizan para la detección de nódulos pulmonares basada en parches en datos de proyección de TC.
Modelización conjunta de la reconstrucción por tomografía computarizada y de la detección de lesiones en el pulmón mediante el entrenamiento del mapeo desde el sinograma bruto hasta los resultados de la detección de manera integral
Presenta un entrenamiento de extremo a extremo de una arquitectura CNN que combina el procesamiento de señales de imágenes de TC y el análisis de imágenes.
Evaluamos cuantitativa y cualitativamente los métodos de navegación basados en el aprendizaje por refuerzo profundo bajo una variedad de condiciones para responder a la pregunta de cuán cerca están de reemplazar a los planificadores de rutas y algoritmos de mapeo clásicos.
Evaluar un modelo basado en Deep RL en laberintos de entrenamiento midiendo la latencia repetida hasta la meta y la comparación con la ruta más corta
Evaluamos el aprendizaje de modelos de ruido heteroscedástico con diferentes filtros de Bayes diferenciables
Propone aprender modelos de ruido heteroscedástico a partir de los datos optimizando la probabilidad de predicción de extremo a extremo mediante filtros bayesianos diferenciables y dos versiones diferentes del filtro de Kalman no centrado
Revisa los filtros de Bayes y evalúa las ventajas de entrenar los modelos de ruido de observación y de proceso manteniendo fijos los demás modelos
Este trabajo presenta un método para aprender y utilizar el ruido dependiente del estado y la observación en los algoritmos tradicionales de filtrado bayesiano. El enfoque consiste en construir un modelo de red neuronal que toma como entrada los datos de observación en bruto y produce una representación compacta y una covarianza diagonal asociada.
Repensamos la forma en que se puede explotar la información de manera más eficiente en el grafo de conocimiento para mejorar el rendimiento en la tarea de aprendizaje de tiro cero y proponemos un módulo de propagación de grafos densos (DGP) para este fin.
Estos autores proponen una solución al problema del sobrealisado en las redes Graph conv permitiendo la propagación densa entre todos los nodos relacionados, ponderada por la distancia mutua.
Propone una novedosa red neuronal convolucional de grafos para abordar el problema de la clasificación de cero disparos utilizando estructuras relacionales entre clases como entrada de las redes convolucionales de grafos para aprender clasificadores de clases no vistas
Una segmentación semántica basada en la cápsula, en la que las probabilidades de las etiquetas de clase se rastrean a través de la tubería de la cápsula. 
Los autores presentan un mecanismo de rastreo para asociar el nivel más bajo de las Cápsulas con sus respectivas clases
Propone una capa de rastreo para que las redes de cápsulas realicen una segmentación semántica y hace un uso explícito de la relación parte-entero en las capas de cápsulas
Propone un método de rastreo basado en el concepto CapsNet de Sabour para realizar una segmentación semántica en paralelo a la clasificación.
Consideramos el SGD como una trayectoria en el espacio de medidas de probabilidad, mostramos su conexión con los procesos de Markov, proponemos un modelo de Markov simple de aprendizaje del SGD y lo comparamos experimentalmente con el SGD utilizando cantidades de la teoría de la información. 
Construye una cadena de Markov que sigue un camino corto en la métrica de TV en P y muestra que las trayectorias de SGD y \alpha-SMLC tienen una entropía condicional similar
Estudia la trayectoria de H(\hat{y}) frente a H(\hat{y}|y) en el plano de información para los métodos de descenso de gradiente estocástico para el entrenamiento de redes neuronales
Describe el SGD desde el punto de vista de la distribución p(y',y) donde y es (una etiqueta de clase verdadera posiblemente corrupta) e y' una predicción del modelo.
Este trabajo propone un método para automatizar el diseño de la propuesta MCMC de gradiente estocástico utilizando un enfoque de meta aprendizaje. 
Presenta un enfoque de meta-aprendizaje para diseñar automáticamente el muestreador MCMC basado en la dinámica hamiltoniana para mezclar más rápido en problemas similares a los de entrenamiento
Parametriza las matrices de difusión y rizo mediante redes neuronales y meta-aprende y optimiza un algoritmo sg-mcmc. 
Las técnicas de evaluación de la calidad de la imagen mejoran el entrenamiento y la evaluación de las redes generativas adversariales basadas en la energía
Propone una formulación basada en la energía para el modal BEGAN y lo modifica para incluir un término basado en la evaluación de la calidad de la imagen
Propone algunas nuevas funciones de energía en el marco de BEGAN (equilibrio de límites GAN), incluyendo la puntuación l_1, la puntuación de similitud de magnitudes de gradiente y la puntuación de crominancia.
Introducimos una variante simple de la optimización del momento que es capaz de superar al momento clásico, a Nesterov y a Adam en tareas de aprendizaje profundo con un ajuste mínimo de los hiperparámetros.
Introduce una variante del momento que agrega varias velocidades con diferentes coeficientes de amortiguación que disminuye significativamente la oscilación
Se propone un método de momento agregado para la optimización basada en el gradiente utilizando múltiples vectores de velocidad con diferentes factores de amortiguación en lugar de un único vector de velocidad para mejorar la estabilidad
Los autores combinan varios pasos de actualización para conseguir el impulso agregado demostrando también que es más estable que los otros métodos de impulso
Introducimos la Unidad de Descuento Recurrente que aplica la atención a cualquier secuencia de longitud en tiempo lineal
Este trabajo propone la Atención Descontada Recurrente (ADR), una extensión de la Media Ponderada Recurrente (PGR) añadiendo un factor de descuento.
Amplía el promedio de pesos recurrentes para superar la limitación del método original manteniendo su ventaja y propone el método de utilizar redes Elman como RNN base
Es posible aprender una distribución gaussiana centrada en cero sobre los pesos de una red neuronal aprendiendo sólo las varianzas, y funciona sorprendentemente bien.
Este trabajo investiga los efectos de la media de la posterioridad variacional y propone la capa de varianza, que sólo utiliza la varianza para almacenar información
Estudia las redes neuronales de varianza que aproximan la posterior de las redes neuronales bayesianas con distribuciones gaussianas de media cero
Hacemos una red de redes de convolución de grafos, alimentando cada una de ellas con una potencia diferente de la matriz de adyacencia, combinando toda su representación en una subred de clasificación, logrando el estado del arte en la clasificación de nodos semi-supervisada.
Propone una nueva red de GCNs con dos enfoques: una capa totalmente conectada sobre características apiladas y un mecanismo de atención que utiliza un peso escalar por GCN.
Presenta una red de redes convolucionales de grafos que utiliza la estadística del paseo aleatorio para extraer información de los vecinos cercanos y lejanos en el grafo
Un algoritmo de poda rápida para capas de DNN totalmente conectadas con análisis teórico de la degradación del error de generalización.
Presenta un algoritmo de poda barato para capas densas de DNNs.
Propone una solución al problema de la poda de DNNs planteando la función objetivo Net-trim como una función de diferencia de convexidad (DC).
Proponemos una nueva red temporal híbrida que alcanza el rendimiento más avanzado en la segmentación de acciones de vídeo en tres conjuntos de datos públicos.
Discute el problema de la segmentación de acciones en vídeos largos, de hasta 10 minutos de duración, utilizando una arquitectura de codificador-decodificador convolucional temporal
Propone una combinación de redes convolucionales temporales y recurrentes para la segmentación de acciones en vídeo
Este trabajo propone transferir el conocimiento del modelo profundo al superficial imitando las características etapa por etapa.
Explica un método de transferencia de conocimientos por etapas utilizando diferentes estructuras de redes
Este trabajo propone dividir una red en múltiples partes y destilar cada parte secuencialmente para mejorar el rendimiento de la destilación en las redes de maestros profundos
Se obtienen mejoras en la robustez del adversario, así como garantías de robustez demostrables, aumentando el entrenamiento del adversario con una regularización Lipschitz manejable
Explora el aumento de la pérdida de entrenamiento con un término adicional de regularización del gradiente para mejorar la solidez de los modelos frente a los ejemplos adversos
Utiliza un truco para simplificar la pérdida adversaria por una en la que la perturbación adversaria aparece en forma cerrada.
Un modelo que combina la eliminación y la selección para responder a preguntas de opción múltiple
Da una elaboración sobre el Lector de Atención Cerrada añadiendo puertas basadas en la eliminación de respuestas en la comprensión lectora de opción múltiple
Este trabajo propone el uso de una puerta de eliminación en arquitecturas de modelos para tareas de comprensión lectora, pero no consigue resultados de última generación
Este artículo propone un nuevo modelo de comprensión lectora de opciones múltiples basado en la idea de que algunas opciones deben ser eliminadas para inferir mejores representaciones de pasajes/preguntas.
Proponemos un novedoso marco de razonamiento recursivo probabilístico (PR2) para tareas de aprendizaje de refuerzo profundo multiagente.
Propone un nuevo enfoque para el entrenamiento totalmente descentralizado en el aprendizaje por refuerzo de múltiples agentes
Aborda el problema de dotar a los agentes de RL de capacidades de razonamiento recursivo en un entorno multiagente basándose en la hipótesis de que el razonamiento recursivo es beneficioso para que converjan a equilibrios no trivalentes
El artículo introduce un método de entrenamiento descentralizado para el aprendizaje por refuerzo de múltiples agentes, en el que los agentes infieren las políticas de otros agentes y utilizan los modelos inferidos para la toma de decisiones. 
Un nuevo algoritmo para reducir la sobrecarga de comunicación del aprendizaje profundo distribuido distinguiendo los gradientes "no ambiguos".
Propone un método de compresión de gradiente basado en la varianza para reducir la sobrecarga de comunicación del aprendizaje profundo distribuido
Propone una forma novedosa de comprimir las actualizaciones de gradiente para el SGD distribuido con el fin de acelerar la ejecución general
Introduce el método de compresión de gradiente basado en la varianza para el entrenamiento distribuido eficiente de redes neuronales y la medición de la ambigüedad.
Una nueva técnica no supervisada de adaptación de dominios profundos que unifica eficazmente la alineación de la correlación y la minimización de la entropía
Mejora el enfoque de alineación de correlación para la adaptación de dominios, sustituyendo la distancia euclidiana por la distancia geodésica logarítmica entre dos matrices de covarianza, y seleccionando automáticamente el coste de equilibrio por la entropía en el dominio objetivo.
Propuesta de alineación de correlación de entropía mínima, un algoritmo de adaptación de dominio no supervisado que une los métodos de minimización de entropía y de alineación de correlación.
Proponemos una variante del algoritmo de retropropagación, en la que los gradientes son protegidos por los conceptores contra la degradación de las tareas previamente aprendidas.
Este trabajo aplica la noción de conceptores, una forma de regularizador, para evitar el olvido en el aprendizaje continuo en el entrenamiento de redes neuronales en tareas secuenciales.
Introduce un método para el aprendizaje de nuevas tareas, sin interferir en las anteriores, utilizando conceptores.
Analizar la razón por la que los modelos generativos de respuesta neural prefieren las respuestas universales; Proponer un método para evitarlo.
Investiga el problema de las réplicas universales que afectan a los modelos de generación neuronal de Seq2Seq
El artículo estudia la mejora de la tarea de generación de respuestas neuronales al restar importancia a las respuestas comunes mediante la modificación de la función de pérdida y la presentación de las respuestas comunes/universales durante la fase de entrenamiento.
Aprovechamos la estructura sintáctica del código fuente para generar secuencias de lenguaje natural.
Presenta un método para generar secuencias a partir de código mediante el análisis sintáctico y la producción de un árbol de sintaxis
Este artículo presenta una codificación basada en AST para el código de programación y muestra su eficacia en las tareas de resumen de código extremo y subtitulación de código.
Este artículo presenta un nuevo modelo de código a secuencia que aprovecha la estructura sintáctica de los lenguajes de programación para codificar fragmentos de código fuente y luego decodificarlos a lenguaje natural
Mejoramos las CNN con un novedoso mecanismo de atención para el reconocimiento de grano fino. Se obtiene un rendimiento superior en 5 conjuntos de datos.
Describe un nuevo mecanismo atencional aplicado al reconocimiento de grano fino que mejora sistemáticamente la precisión de reconocimiento de la línea de base
Este trabajo propone un mecanismo de atención de avance para la clasificación de imágenes de grano fino
Este trabajo presenta un interesante mecanismo de atención para la clasificación de imágenes de grano fino.
Sustituimos las convoluciones normales por convoluciones adaptativas para mejorar el generador de GANs.
Propone sustituir las convoluciones en el generador por un bloque de convolución adaptativo que aprende a generar los pesos de convolución y los sesgos de las operaciones de remuestreo de forma adaptativa por ubicación de píxel
Utiliza la Convolución Adaptativa en el contexto de los GANs con un bloque llamado AdaConvBlock que reemplaza a la Convolución regular, esto da más contexto local por peso del kernel para poder generar objetos localmente flexibles.
Llevamos a cabo experimentos a gran escala para demostrar que una simple variante online de la destilación puede ayudarnos a escalar el entrenamiento de redes neuronales distribuidas a más máquinas.
Propone un método para escalar el entrenamiento distribuido más allá de los límites actuales del descenso de gradiente estocástico en mini lotes
Propuesta de un método de destilación en línea llamado codistilación, aplicado a escala, en el que dos modelos diferentes se entrenan para igualar las predicciones del otro modelo además de minimizar su propia pérdida.
Se introduce la técnica de destilación en línea para acelerar los algoritmos tradicionales de entrenamiento de redes neuronales distribuidas a gran escala
Presentamos un algoritmo para acelerar el entrenamiento de SVM en conjuntos de datos masivos mediante la construcción de representaciones compactas que proporcionan una inferencia eficiente y probadamente aproximada.
Estudia el enfoque de coreset para SVM y tiene como objetivo el muestreo de un pequeño conjunto de puntos ponderados de tal manera que la función de pérdida sobre los puntos se aproxima de manera demostrable a la de todo el conjunto de datos
El artículo sugiere una construcción de Coreset basada en el muestreo de importancia para representar grandes datos de entrenamiento para SVMs
Demostramos una tasa de convergencia no convexa para el método del gradiente estocástico de signos. El algoritmo tiene vínculos con algoritmos como Adam y Rprop, así como con esquemas de cuantificación de gradientes utilizados en el aprendizaje automático distribuido.
Proporcionó un análisis de convergencia del algoritmo Sign SGD para casos no covexos
El artículo explora un algoritmo que utiliza el signo de los gradientes en lugar de los gradientes reales para entrenar modelos profundos
Introducimos un middleware transparente para la aceleración de redes neuronales, con un motor de compilación propio, que consigue aumentar la velocidad hasta 11,8 veces en las CPU y 2,3 veces en las GPU.
Este trabajo propone una capa de middleware transparente para la aceleración de redes neuronales y obtiene algunos resultados de aceleración en arquitecturas básicas de CPU y GPU
Proponemos la convolución cónica y la 2D-DFT para codificar la equidistancia de la rotación en una red neuronal.
En el contexto de la clasificación de imágenes, el artículo propone una arquitectura de red neuronal convolucional con mapas de características de rotación-equivariante que eventualmente se hacen invariantes de la rotación mediante el uso de la magnitud de la transformada discreta de Fourier (DFT) en 2D.
Los autores proporcionan una red neuronal invariante de la rotación mediante la combinación de la convolución cónica y la 2D-DFT
Introducimos un novedoso marco de alimentación para generar metamateriales visuales
Propone un modelo NeuroFovea para la generación de metamateriales de punto de fijación mediante un enfoque de transferencia de estilo a través de una arquitectura de estilo Codificador-Decodificador
Un análisis del metamerismo y un modelo capaz de producir rápidamente metameras de valor para la psicofísica experimental y otros dominios.
El artículo propone un método rápido para generar metamers visuales -imágenes físicamente diferentes que no pueden distinguirse de un original- a través de una transferencia de estilo foveada, rápida y arbitraria
Demostramos que el entrenamiento de las redes relu feedforward con un regularizador débil da como resultado un margen máximo y analizamos las implicaciones de este resultado.
Estudia la teoría de los márgenes para conjuntos neuronales y demuestra que el margen máximo es monotónicamente creciente en el tamaño de la red
Este trabajo estudia el sesgo implícito de los minimizadores de una pérdida de entropía cruzada regularizada de una red de dos capas con activaciones ReLU, obteniendo una cota superior de generalización que no aumenta con el tamaño de la red.
Una arquitectura distribuida para el aprendizaje profundo por refuerzo a escala, que utiliza la generación de datos en paralelo para mejorar el estado del arte en el benchmark Arcade Learning Environment en una fracción del tiempo de entrenamiento de los enfoques anteriores.
Examina un sistema de RL profundo distribuido en el que las experiencias, en lugar de los gradientes, se comparten entre los trabajos paralelos y el aprendiz centralizado
Un enfoque paralelo para el entrenamiento de DQN, basado en la idea de tener múltiples actores recogiendo datos en paralelo mientras un único aprendiz entrena el modelo a partir de experiencias muestreadas de la memoria de repetición central.
Este trabajo propone una arquitectura distribuida para el aprendizaje profundo por refuerzo a escala, centrándose en la adición de la paralelización en el algoritmo del actor en el marco de la repetición de la experiencia priorizada
Arquitecturas neuronales que proporcionan representaciones de señales observadas de forma irregular que permiten de forma demostrable la reconstrucción de la señal.
Demuestra que las redes neuronales convolucionales con función de activación Leaky ReLU son marcos no lineales, con resultados similares para series temporales no uniformemente muestreadas
Este artículo considera las redes neuronales sobre series temporales y muestra que los primeros filtros convolucionales pueden ser elegidos para representar una transformada wavelet discreta.
Mecanismos de atención basados en frases para asignar la atención en frases, logrando alineaciones de atención de token a frase, de frase a token, de frase a frase, además de las atenciones existentes de token a token.
El artículo presenta un mecanismo de atención que calcula una suma ponderada no sólo de tokens individuales sino de ngramas (frases).
Las redes profundas son más propensas a equivocarse con confianza cuando se prueban con datos inesperados. Proponemos una metodología experimental para estudiar el problema y dos métodos para reducir los errores de confianza en distribuciones de entrada desconocidas.
Propone dos ideas para reducir el exceso de confianza en las predicciones erróneas: La "destilación G" de un conjunto con datos adicionales no supervisados y la reducción de la confianza en las novedades mediante el detector de novedades
Los autores proponen dos métodos para estimar la confianza de la clasificación en nuevas distribuciones de datos no vistas. La primera idea consiste en utilizar métodos de conjunto como enfoque base para ayudar a identificar los casos inciertos y, a continuación, utilizar métodos de destilación para reducir el conjunto en un único modelo que imite el comportamiento del conjunto. La segunda idea es utilizar un clasificador detector de novedades y ponderar la salida de la red por la puntuación de la novedad.
Describimos un práctico algoritmo de optimización para redes neuronales profundas que funciona más rápido y genera mejores modelos en comparación con los algoritmos más utilizados.
Propone un nuevo algoritmo en el que afirman utilizar el hessiano de forma implícita y utilizan una motivación de las series de potencia
Presenta un nuevo algoritmo de segundo orden que utiliza implícitamente la información de curvatura y muestra la intuición detrás de los esquemas de aproximación en los algoritmos y valida la heurística en varios experimentos.
Introducimos la aproximación del ancho de bits fraccionario y demostramos que tiene ventajas significativas.
Propone un método para variar el grado de cuantificación en una red neuronal durante la fase de propagación hacia delante
Mantener la precisión de la red de 2bits utilizando menos de 2bits de pesos
El reemplazo de la media es un método eficiente para mejorar la pérdida después de la poda y las funciones de puntuación basadas en la aproximación de Taylor funcionan mejor con los valores absolutos. 
Propone una sencilla mejora de los métodos de poda de unidades mediante el "reemplazo medio"
Este trabajo presenta una estrategia de poda de sustitución de la media y utiliza la expansión de Taylor de valor absoluto como función de puntuación para la poda
 Evitar el colapso posterior limitando la tasa.
Presenta un enfoque para evitar el colapso posterior en las VAE limitando la familia de la aproximación variacional a la posterior
Este trabajo introduce una restricción en la familia de posteriors variacionales de manera que el término KL puede ser controlado para combatir el colapso posterior en modelos generativos profundos como los VAE
Desarrollamos un impulso adaptativo por lotes que puede lograr una pérdida menor en comparación con los métodos por mini lotes después de escanear las mismas épocas de datos, y es más robusto frente a un tamaño de paso grande.
Este trabajo aborda el problema de ajustar automáticamente el tamaño del lote durante el entrenamiento del aprendizaje profundo, y pretende extender el SGD adaptativo por lotes al impulso adaptativo y adoptar los algoritmos a problemas de redes neuronales complejas.
El documento propone generalizar un algoritmo que realiza SGD con tamaños de lote adaptables añadiendo el impulso a la función de utilidad
Utilizamos meta-gradientes para atacar el procedimiento de entrenamiento de las redes neuronales profundas para grafos.
Estudia el problema de aprender un mejor parámetro de grafos envenenados que pueda maximizar la pérdida de una red neuronal de grafos. 
Un algoritmo para alterar la estructura del grafo añadiendo/eliminando aristas para degradar el rendimiento global de la clasificación de nodos, y la idea de utilizar el meta-aprendizaje para resolver el problema de optimización de dos niveles.
Demostramos que los modelos modulares estructurados son los mejores en términos de generalización sistemática y que sus versiones de extremo a extremo no generalizan tan bien.
Este artículo evalúa la generalización sistémica entre las redes neuronales modulares y otros modelos genéricos mediante la introducción de un nuevo conjunto de datos de razonamiento espacial
Una evaluación empírica orientada a la generalización en modelos de razonamiento visual, centrada en el problema del reconocimiento de triples (objeto, relación, objeto) en escenas sintéticas con letras y números.
Los modelos relacionales de avance para el aprendizaje de agentes múltiples hacen predicciones precisas del comportamiento futuro de los agentes, producen representaciones intepretables y pueden utilizarse dentro de los agentes.
Una forma de reducir la varianza en el aprendizaje sin modelos al disponer de un modelo explícito, que utiliza una arquitectura tipo red de grafos, de las acciones que realizarán otros agentes. 
Predicción del comportamiento de varios agentes mediante un modelo relacional de avance con un componente recurrente, superando dos líneas de base y dos ablaciones
La solución normalizada del descenso de gradiente en la regresión logística (o una pérdida similar que decaiga) converge lentamente a la solución de margen máximo L2 en datos separables.
El artículo ofrece una prueba formal de que el descenso del gradiente sobre la pérdida logística converge muy lentamente a la solución SVM dura en el caso de que los datos sean linealmente separables. 
Este trabajo se centra en la caracterización del comportamiento de la minimización de la pérdida logarítmica en datos linealmente separables, y muestra que la pérdida logarítmica, minimizada con el descenso de gradiente, conduce a la convergencia a la solución de margen máximo.
Basándonos en trabajos anteriores sobre la generalización de dominios, esperamos producir un clasificador que se generalice a dominios no vistos previamente, incluso cuando los identificadores del dominio no estén disponibles durante el entrenamiento.
Un enfoque de generalización de dominio para revelar información semántica basado en un esquema de proyección lineal de las capas de salida de CNN y NGLCM.
El artículo propone un enfoque no supervisado para identificar las características de la imagen que no son significativas para las tareas de clasificación de imágenes
Proponemos un método para aprender el camuflaje físico de los vehículos para atacar de forma adversa a los detectores de objetos en la naturaleza. Comprobamos que nuestro camuflaje es eficaz y transferible.
Los autores investigan el problema de aprender un patrón de camuflaje que, aplicado a un vehículo simulado, impida que un detector de objetos lo detecte.
Este trabajo se centra en el aprendizaje adversarial para la detección de coches interferentes mediante el aprendizaje de patrones de camuflaje
Combinamos árboles de decisión diferenciables con autocodificadores variacionales supervisados para mejorar la interpretabilidad de la clasificación. 
Este trabajo propone un modelo híbrido de un autoencoder variacional compuesto con un árbol de decisión diferenciable, y un esquema de entrenamiento que lo acompaña, con experimentos que demuestran el rendimiento de la clasificación del árbol, el rendimiento de la probabilidad logarítmica negativa, y la interpretabilidad del espacio latente.
El artículo trata de construir un clasificador interpretable y preciso mediante el apilamiento de un VAE supervisado y un árbol de decisión diferenciable
Un enfoque práctico y con garantías probables para el entrenamiento de clasificadores eficientes en presencia de cambios de etiquetas entre los conjuntos de datos de origen y destino
Los autores proponen un nuevo algoritmo para mejorar la estabilidad del procedimiento de estimación de la importancia de las clases con un procedimiento de dos pasos.
Los autores consideran el problema del aprendizaje bajo cambios de etiquetas, donde las proporciones de las etiquetas difieren mientras las condicionales son iguales, y proponen un estimador mejorado con regularización.
Enfoque para mejorar la precisión de la clasificación en las clases de la cola.
El objetivo principal de este trabajo es aprender un clasificador ConvNet que funcione mejor para las clases en la cola de la distribución de ocurrencia de clases.
Propuesta de un marco bayesiano con un modelo de mezcla gaussiana para abordar un problema en las aplicaciones de clasificación, que el número de datos de entrenamiento de diferentes clases está desequilibrado.
Presentamos un método para sintetizar estados de interés para agentes de aprendizaje por refuerzo con el fin de analizar su comportamiento. 
Este trabajo propone un modelo generativo de observaciones visuales en RL que es capaz de generar observaciones de interés.
Un enfoque para visualizar los estados de interés que implica un autoencoder variacional que aprende a reconstruir el espacio de estados y un paso de optimización que encuentra los parámetros de acondicionamiento para generar imágenes sintéticas.
Una red neuronal profunda de abstención entrenada con una función de pérdida novedosa que aprende representaciones para saber cuándo abstenerse permitiendo un aprendizaje robusto en presencia de diferentes tipos de ruido.
Una nueva función de pérdida para el entrenamiento de una red neuronal profunda que puede abstenerse, con un rendimiento visto desde los ángulos en existencia de ruido estructurado, en existencia de ruido no estructurado, y la detección del mundo abierto.
Este manuscrito introduce clasificadores profundos de abstención que modifican la pérdida de entropía cruzada multiclase con una pérdida de abstención, que luego se aplica a tareas de clasificación de imágenes perturbadas
Una técnica de regularización para el aprendizaje de TD que evita la sobregeneralización temporal, especialmente en redes profundas
Una variación del aprendizaje por diferencia temporal para el caso de aproximación de funciones que intenta resolver el problema de la sobregeneralización a través de estados temporalmente sucesivos.
Este artículo presenta el algoritmo HR-TD, una variación del algoritmo TD(0), cuyo objetivo es mejorar el problema de la sobregeneralización en el TD convencional
Presentamos un nuevo kernel de CNN para cuadrículas no estructuradas para señales esféricas, y mostramos una importante ganancia de precisión y eficiencia de parámetros en tareas como la clasificación 3D y la segmentación de imágenes omnidireccionales.
Un método eficiente que permite el aprendizaje profundo sobre datos esféricos y que alcanza cifras competitivas/de última generación con muchos menos parámetros que los enfoques populares.
El artículo propone un novedoso kernel convolucional para CNN en las mallas no estructuradas y formula la convolución mediante una combinación lineal de operadores diferenciales.
En las tareas de predicción visual, dejar que el modelo de predicción elija qué momentos predecir hace dos cosas: (i) mejora la calidad de la predicción, y (ii) da lugar a predicciones semánticamente coherentes del "estado del cuello de botella", que son útiles para la planificación.
Un método sobre la predicción de fotogramas en un vídeo, el enfoque incluye que la predicción del objetivo es flotante, resuelto por un mínimo en el error de predicción.
Reformula la tarea de predicción/interpolación de vídeo para que un predictor no se vea obligado a generar fotogramas en intervalos de tiempo fijos, sino que se entrene para generar fotogramas que sucedan en cualquier punto del futuro.
Utilizamos un LSTM para detectar cuándo un smartphone entra en un edificio. A continuación, predecimos el nivel del suelo del dispositivo utilizando los datos de los sensores a bordo del smartphone.
El artículo presenta un sistema para estimar el nivel de un piso a través de los datos de los sensores de su dispositivo móvil utilizando un LSTM y los cambios en la presión barométrica
Propuesta de un método de dos pasos para determinar en qué piso se encuentra un teléfono móvil dentro de un edificio alto.
Combinar la representación de los objetivos lingüísticos con las repeticiones de la experiencia retrospectiva.
Este artículo considera la suposición implícita en la repetición de la experiencia retrospectiva, de que hay acceso a un mapeo de los estados a las metas, y propone una representación de las metas en lenguaje natural.
Esta presentación utiliza el marco Hindsight Experience Replay con objetivos de lenguaje natural para mejorar la eficiencia de la muestra de los modelos de seguimiento de instrucciones.
Proponemos un esquema conjunto de libro de códigos y factorización para mejorar el pooling de segundo orden.
Este trabajo presenta una forma de combinar las representaciones factorizadas de segundo orden existentes con una asignación dura al estilo del libro de códigos.
Propuesta de una nueva representación bilineal basada en un modelo de libro de códigos, y una formulación eficiente en la que las proyecciones basadas en el libro de códigos se factorizan mediante una proyección compartida para reducir aún más el tamaño de los parámetros.
Proponemos y aplicamos una metodología de meta-aprendizaje basada en la Supervisión Débil, para combinar el Aprendizaje Semisupervisado y el Ensemble en la tarea de Extracción de Relaciones Biomédicas.
Un método semi-supervisado para la clasificación de relaciones, que entrena a múltiples aprendices de base utilizando un pequeño conjunto de datos etiquetados y aplica algunos de ellos para anotar ejemplos no etiquetados para el aprendizaje semi-supervisado.
Este trabajo aborda el problema de la generación de datos de entrenamiento para la extracción de relaciones biológicas, y utiliza predicciones de datos etiquetados por clasificadores débiles como datos de entrenamiento adicionales para un algoritmo de metaaprendizaje.
Este trabajo propone una combinación de aprendizaje semisupervisado y aprendizaje de conjunto para la extracción de información, con experimentos realizados en una tarea de extracción de relaciones biomédicas
Una clase de redes que generan modelos sencillos sobre la marcha (llamados explicaciones) que actúan como un regularizador y permiten un diagnóstico del modelo consistente y la interpretabilidad.
Los autores afirman que el arte anterior integra directamente las redes neuronales en los modelos gráficos como componentes, lo que hace que los modelos sean ininterpretables.
Propuesta de combinación de redes neuronales y modelos gráficos mediante el uso de una red neuronal profunda para predecir los parámetros de un modelo gráfico.
Proponemos un algoritmo de aprendizaje por imitación sin modelo que es capaz de reducir el número de interacciones con el entorno en comparación con el algoritmo de aprendizaje por imitación más avanzado, el GAIL.
Propone extender el algoritmo de gradiente de política determinista para aprender de las demostraciones, mientras se combina con un tipo de estimación de la densidad del experto.
Este trabajo considera el problema del aprendizaje por imitación sin modelo y propone una extensión del algoritmo de aprendizaje por imitación generativo adversarial sustituyendo la política estocástica del aprendiz por una determinista.
El artículo combina el IRL, el entrenamiento adversario y las ideas de los gradientes de política determinista con el objetivo de disminuir la complejidad de la muestra
CNN gráfica de baja complejidad computacional (sin aproximación) con mayor precisión de clasificación
Propone un nuevo enfoque de la CNN para la clasificación de grafos utilizando un filtro basado en paseos de salida de longitud creciente para incorporar información de vértices más distantes en un solo paso de propagación.
Propuesta de una nueva arquitectura de red neuronal para la clasificación de grafos semi-supervisada, basándose en filtros polinómicos de grafos y utilizándolos en capas sucesivas de la red neuronal con funciones de activación ReLU.
El artículo introduce la GCN adaptativa a la topología para generalizar las redes convolucionales a los datos estructurados en grafos
Demostramos que el olvido catastrófico se produce dentro de lo que se considera una única tarea y descubrimos que los ejemplos que no son propensos al olvido pueden eliminarse del conjunto de entrenamiento sin pérdida de generalización.
Estudia el comportamiento de olvido de los ejemplos de entrenamiento durante el SGD, y muestra que existen "ejemplos de apoyo" en el entrenamiento de redes neuronales a través de diferentes arquitecturas de red.
Este trabajo analiza hasta qué punto las redes aprenden a clasificar correctamente ejemplos específicos y luego olvidan estos ejemplos a lo largo del entrenamiento.
El trabajo estudia si algunos ejemplos en el entrenamiento de redes neuronales son más difíciles de aprender que otros. Dichos ejemplos se olvidan y se vuelven a aprender varias veces a través del aprendizaje.
Un enfoque no supervisado para el aprendizaje de representaciones desenmarañadas de objetos completamente a partir de vídeos monoculares no etiquetados.
Diseña una representación de características a partir de secuencias de vídeo capturadas de una escena desde diferentes puntos de vista.
Propuesta de un método de aprendizaje de representación no supervisado para entradas visuales que incorpora un enfoque de aprendizaje métrico que acerca los pares de parches de imágenes más cercanos en el espacio de incrustación mientras aleja otros pares.
Este trabajo explora el aprendizaje auto-supervisado de las representaciones de objetos, con la idea principal de animar a los objetos con características similares a ser más "atraídos" el uno al otro.
Entrenamos con recompensas vectoriales alineadas con el estado a un agente que predice los cambios de estado a partir de las distribuciones de acciones, utilizando una nueva técnica de aprendizaje por refuerzo inspirada en la regresión cuantílica.
Presenta un algoritmo que pretende acelerar el aprendizaje por refuerzo en situaciones en las que la recompensa está alineada con el espacio de estado. 
Este trabajo aborda la RL en el espacio de acción continua, utilizando una política re-parametrizada y un novedoso objetivo de entrenamiento basado en vectores.
Este trabajo propone mezclar la RL distributiva con una red encargada de modelar la evolución del mundo en términos de cuantiles, alegando mejoras en la eficiencia de la muestra.
Proponemos Episodic Backward Update, un novedoso algoritmo de aprendizaje profundo por refuerzo que muestrea las transiciones episodio a episodio y actualiza los valores de forma recursiva hacia atrás para conseguir un aprendizaje rápido y estable.
Propone un nuevo DQN en el que los objetivos se calculan sobre un episodio completo mediante una actualización hacia atrás (del final al principio) para una propagación más rápida de las recompensas al final del episodio.
Los autores proponen modificar el algoritmo DQN aplicando el operador max Bellman recursivamente sobre una trayectoria con cierto decaimiento para evitar la acumulación de errores con el max anidado.
En las redes deep-Q, se actualizan los valores de Q a partir del final del episodio para facilitar la rápida propagación de las recompensas a lo largo del mismo.
En este trabajo introducimos una novedosa arquitectura de red neuronal profunda siamesa que es capaz de aprender eficazmente de los datos en presencia de múltiples eventos adversos.
Este trabajo introduce las redes neuronales siamesas en el marco de los riesgos competitivos mediante la optimización del índice c directamente
Los autores abordan los problemas de estimación del riesgo en un entorno de análisis de supervivencia con riesgos concurrentes y proponen optimizar directamente el índice de discriminación dependiente del tiempo utilizando una red de supervivencia siamesa
Presentamos un modelo de red de punteros basado en el tipo junto con un método de pérdida basado en el valor para entrenar eficazmente un modelo neuronal para traducir el lenguaje natural a SQL.
El documento pretende desarrollar un método novedoso para mapear consultas en lenguaje natural a SQL utilizando una gramática para guiar la decodificación y utilizando una nueva función de pérdida para el mecanismo de puntero / copia
Un estimador de gradiente insesgado y de baja varianza para modelos de variables latentes discretas
Propone una nueva técnica de reducción de la varianza para utilizar cuando se calcula un gradiente de pérdida esperada donde la expectativa es con respecto a variables aleatorias binarias independientes.
Un algoritmo que combina la Rao-Blackwellización y los números aleatorios comunes para reducir la varianza del estimador del gradiente de la función de puntuación en el caso especial de las redes binarias estocásticas
Un estimador insesgado y de baja varianza augment-REINFORCE-merge (ARM) para calcular y retropropagar gradientes en redes neuronales binarias
Demostramos que la SGD local paralela consigue un aumento de velocidad lineal con una comunicación mucho menor que la SGD paralela en miniatura.
Proporciona una prueba de convergencia para el SGD local, y demuestra que el SGD local puede proporcionar las mismas ganancias de velocidad que el minibatch, pero puede ser capaz de comunicar significativamente menos.
Este trabajo presenta un análisis de SGD local y límites sobre la frecuencia de los estimadores obtenidos mediante la ejecución de SGD que deben ser promediados para producir aumentos de velocidad de paralelización lineal.
Los autores analizan el algoritmo SGD local, en el que se ejecutan $K$ cadenas paralelas de SGD, y los iterados se sincronizan ocasionalmente entre máquinas promediando
Percepción compacta del proceso dinámico
Estudia el problema de representar de forma compacta el modelo de un sistema dinámico complejo preservando la información mediante un método de cuello de botella de información.
Este trabajo estudió la dinámica lineal gaussiana y propuso un algoritmo para calcular la jerarquía de cuellos de botella de información (IBH).
RNN densa que tiene conexiones completas desde cada estado oculto a múltiples estados ocultos precedentes de todas las capas directamente.
Propone una nueva arquitectura RNN que modela mejor las dependencias a largo plazo, puede aprender la representación multiescalar de los datos secuenciales, y elude el problema de los gradientes mediante el uso de unidades de compuerta parametrizadas.
Este artículo propone una arquitectura RNN densa totalmente conectada con conexiones cerradas a cada capa y conexiones de las capas precedentes, y sus resultados en la tarea de modelado a nivel de caracteres de la PTB.
Los SD-GAN desentrañan los códigos latentes en función de los puntos comunes conocidos en un conjunto de datos (por ejemplo, fotografías que representan a la misma persona).
Este artículo investiga el problema de la generación de imágenes controladas y propone un algoritmo que produce un par de imágenes con la misma identidad.
Este trabajo propone, SD-GAN, un método de entrenamiento de GANs para desentrañar la información de identidad y no identidad en el vector latente de entrada Z.
Proponemos un marco de aprendizaje para las traducciones entre dominios que es exactamente consistente con el ciclo y puede aprenderse a través del entrenamiento adversarial, la estimación de máxima probabilidad o un híbrido.
Propone AlignFlow, una forma eficiente de implementar el principio de consistencia de los ciclos utilizando flujos invertibles.
Modelos de flujo para la traducción de imágenes no apareadas
En un contexto de síntesis de programas donde la entrada es un conjunto de ejemplos, reducimos el coste calculando un subconjunto de ejemplos representativos
Propone un método para identificar ejemplos representativos para la síntesis de programas con el fin de aumentar la escalabilidad de las soluciones de programación de restricciones existentes.
Un método para elegir un subconjunto de ejemplos sobre los que ejecutar un solucionador de restricciones para resolver problemas de síntesis de programas.
Este trabajo propone un método para acelerar los sintetizadores de programas de propósito general.
Introducimos las redes relacionales recurrentes, un módulo de red neuronal potente y general para el razonamiento relacional, y lo utilizamos para resolver el 96,6% de los Sudokus más difíciles y las 19/20 tareas BaBi.
Introdujo la red relacional recurrente (RRN) que puede añadirse a cualquier red neuronal para añadir capacidad de razonamiento relacional.
Introducción de una red neuronal profunda para la predicción estructurada que logra un rendimiento de vanguardia en los rompecabezas Soduku y la tarea BaBi.
Este artículo describe un método denominado red relacional para añadir capacidad de razonamiento relacional a las redes neuronales profundas.
Para entrenar modelos profundos a partir de sólo datos de U, basta con tres priores de clase, mientras que dos no deberían ser suficientes.
Propone un estimador insesgado que permite entrenar los modelos con una supervisión débil en dos conjuntos de datos no etiquetados con prejuicios de clase conocidos y discute las propiedades teóricas de los estimadores.
Una metodología para el entrenamiento de cualquier clasificador binario a partir de sólo datos no etiquetados, y un método de minimización del riesgo empírico para dos conjuntos de datos no etiquetados en los que se dan las prebendas de clase.
La agregación de la evidencia de clase de muchos parches de imagen pequeños es suficiente para resolver ImageNet, produce modelos más interpretables y puede explicar aspectos de la toma de decisiones de las DNNs populares.
Este artículo propone una arquitectura de red neuronal novedosa y compacta que utiliza la información de las características de la bolsa de palabras. El algoritmo propuesto sólo utiliza la información del parche de forma independiente y realiza la votación por mayoría utilizando parches clasificados de forma independiente.
 Los métodos actuales de mutaciones somáticas no funcionan con biopsias líquidas (es decir, secuenciación de baja cobertura), aplicamos una arquitectura CNN a una representación única de una lectura y su ailgamiento, mostramos una mejora significativa sobre los métodos anteriores en el entorno de baja frecuencia.
Propone una solución basada en CNN llamada Kittyhawk para la llamada de mutaciones somáticas en frecuencias alélicas ultra bajas.
Un nuevo algoritmo para detectar mutaciones cancerígenas a partir de la secuenciación del ADN libre de células que identificará el contexto de la secuencia que caracteriza los errores de secuenciación de las verdaderas mutaciones.
Este trabajo propone un marco de aprendizaje profundo para predecir las mutaciones somáticas a frecuencias extremadamente bajas que se producen en la detección de tumores a partir de ADN libre de células
El artículo presenta un nuevo corpus gold-standard de literatura científica biomédica anotado manualmente con menciones de conceptos UMLS.
Detalla la construcción de un conjunto de datos anotados manualmente que cubren conceptos biomédicos que son más grandes y están cubiertos por una ontología más grande que los conjuntos de datos anteriores.
Este trabajo utiliza MedMentions, un modelo TaggerOne semi-Markov para el reconocimiento de conceptos de extremo a extremo y la vinculación en un conjunto de resúmenes de Pubmed para etiquetar artículos con conceptos/entidades biomédicas
Proponemos un método de clustering profundo en el que en lugar de un centroide cada cluster está representado por un autoencoder
Presenta el clustering profundo basado en una mezcla de autocodificadores, donde los puntos de datos se asignan a un cluster basándose en el error de representación si se utilizara la red de autocodificadores para representarlo.
Un enfoque de clustering profundo que utiliza un marco de autoencoder para aprender una incrustación de baja dimensión de los datos simultáneamente mientras se agrupan los datos utilizando una red neuronal profunda.
Un método de clustering profundo que representa cada cluster con diferentes autocodificadores, funciona de manera integral, y también puede ser utilizado para agrupar nuevos datos entrantes sin rehacer todo el procedimiento de clustering.
Definimos una nueva métrica de probabilidad integral (IPM de Sobolev) y mostramos cómo puede utilizarse para entrenar GANs para la generación de textos y el aprendizaje semi-supervisado.
Sugiere un novedoso esquema de regularización para GANs basado en una norma de Sobolev, que mide las desviaciones entre las normas L2 de las derivadas.
Los autores proporcionan otro tipo de GAN utilizando la configuración típica de un GAN pero con una clase de función diferente, y producen una receta para entrenar GANs con ese tipo de clase de función.
El artículo propone una penalización de gradiente diferente para los críticos de GAN que obliga a que la norma cuadrada esperada del gradiente sea igual a 1
Proponemos un nuevo enfoque para entrenar GANs con una mezcla de generadores para superar el problema de colapso de modo.
Abordar el problema del colapso de los modos en los GANs utilizando una distribución de mezcla restringida para el generador y un clasificador auxiliar que predice el componente de la mezcla de origen.
El artículo propone una mezcla de generadores para entrenar GANs sin coste computacional adicional
Los autores presentan que el uso de MGAN, cuyo objetivo es superar el problema de colapso del modelo mediante generadores de mezcla, logra resultados de última generación
Presentamos la plataforma BabyAI para estudiar la eficiencia de los datos en el aprendizaje de idiomas con un humano en el bucle
Presenta una plataforma de investigación con un bot en el bucle para aprender a ejecutar instrucciones de lenguaje en el que el lenguaje tiene estructuras de composición
Presenta una plataforma para el aprendizaje de idiomas en tierra que sustituye a cualquier humano en el bucle por un profesor heurístico y utiliza un lenguaje sintético mapeado en un mundo cuadriculado en 2D
Un previo k-means combinado con la regularización L1 produce resultados de compresión de última generación.
Este trabajo explora la vinculación y compresión de parámetros suaves de las DNNs/CNNs
El método SVRG falla en los problemas modernos de aprendizaje profundo
Este artículo presenta un análisis de los métodos de estilo SVRG, mostrando que el abandono, la norma de lotes y el aumento de datos (cultivo/rotación/traducción aleatorios) tienden a aumentar el sesgo y/o la varianza de las actualizaciones.
Este trabajo investiga la aplicabilidad del SVGD a las redes neuronales modernas y muestra que la aplicación ingenua del SVGD suele fallar.
Un método para aplicar el aprendizaje profundo a las superficies 3D utilizando sus descriptores esféricos y la convolución anisotrópica alt-az sobre 2 esferas.
Presenta un esquema de convolución anisotrópica polar en una esfera unitaria sustituyendo la traslación del filtro por la rotación del mismo.
Este trabajo explora el aprendizaje profundo de formas 3D utilizando la convolución anisotrópica de 2 esferas alt-az
Entrenamiento de redes binarias/ternas mediante la reparametrización local con la aproximación CLT
Entrena redes de distribución de pesos binarias y ternarias mediante retropropagación para muestrear las preactivaciones de las neuronas con el truco de la reparametrización
Este trabajo sugiere el uso de parámetros estocásticos en combinación con el truco de reparametrización local para entrenar redes neuronales con pesos binarios o ternarios, lo que conduce a resultados de última generación.
La destilación de terminación óptima (OCD) es un procedimiento de entrenamiento para optimizar los modelos de secuencia a secuencia basados en la distancia de edición que logra el estado del arte en tareas de reconocimiento del habla de extremo a extremo.
Enfoque alternativo para entrenar modelos seq2seq utilizando un programa dinámico para calcular las continuaciones óptimas de los prefijos predichos
Un algoritmo de entrenamiento para modelos autorregresivos que no requiere ningún preentrenamiento MLE y puede optimizar directamente a partir del muestreo.
El artículo considera una deficiencia de los modelos de secuencia a secuencia entrenados utilizando la estimación de máxima verosimilitud y propone un enfoque basado en las distancias de edición y el uso implícito de secuencias de etiquetas dadas durante el entrenamiento
un método conjunto de sparsificación de modelos y gradientes para el aprendizaje federado
Aplica el dropout variacional para reducir el coste de comunicación del entrenamiento distribuido de las redes neuronales, y realiza experimentos en los conjuntos de datos mnist, cifar10 y svhn. 
Los autores proponen un algoritmo que reduce los costes de comunicación en el aprendizaje federado mediante el envío de gradientes dispersos desde el dispositivo al servidor y viceversa.
Combina el algoritmo de optimización distribuida con el abandono variacional para dispersar los gradientes enviados al servidor principal desde los aprendices locales.
Demostramos una teoría de refuerzo multiclase para las arquitecturas ResNet que crea simultáneamente una nueva técnica para el refuerzo multiclase y proporciona un nuevo algoritmo para las arquitecturas tipo ResNet.
Presenta un algoritmo de estilo boosting para el entrenamiento de redes residuales profundas, un análisis de convergencia para el error de entrenamiento y un análisis de la capacidad de generalización.
Un método de aprendizaje para ResNet utilizando el marco de boosting que descompone el aprendizaje de redes complejas y utiliza menos costes computacionales.
Los autores proponen la ResNet profunda como algoritmo de refuerzo, y afirman que es más eficiente que la retropropagación estándar de extremo a extremo.
El artículo analiza el panorama de optimización de las redes neuronales de una capa oculta y diseña un nuevo objetivo que, de forma demostrable, no tiene mínimos locales espurios. 
Este trabajo estudia el problema del aprendizaje de redes neuronales de una capa oculta, establece una conexión entre la pérdida de población por mínimos cuadrados y los polinomios de Hermite, y propone una nueva función de pérdida.
Un método de factorización tensorial para la inclinación de la red neuronal de una capa oculta
Un corpus abierto de extracción de información y su análisis en profundidad
Construye un nuevo corpus para la extracción de información que es más grande que los corpus públicos anteriores y contiene información que no existe en los corpus actuales.
Presenta un conjunto de datos de triples de EI abierta que fueron recogidos de Wikipedia con la ayuda de un sistema de extracción reciente. 
El artículo describe la creación de un corpus Open IE sobre la Wikipedia en inglés de forma automática
Definimos un DSL flexible para la generación de arquitecturas de RNN que permite RNN de distinto tamaño y complejidad y proponemos una función de clasificación que representa las RNN como redes neuronales recursivas, simulando su rendimiento para decidir las arquitecturas más prometedoras.
Introduce un nuevo método para generar arquitecturas RNNs utilizando un lenguaje específico del dominio para dos tipos de generadores (aleatorios y basados en RL) junto con una función de clasificación y un evaluador.
Este trabajo plantea la búsqueda de buenas arquitecturas de células RNN como un problema de optimización de caja negra en el que los ejemplos se representan como un árbol de operadores y se puntúan en función de las funciones aprendidas o generadas por un agente RL.
Este trabajo investiga la estrategia de meta-aprendizaje para la búsqueda automatizada de arquitecturas en el contexto de las RNN utilizando un DSL que especifica las operaciones recurrentes de las RNN.
Aplicamos el entrenamiento y la inferencia sólo con enteros de bajo ancho de bits en las DNNs
Un método llamado WAGE que cuantifica todos los operandos y operadores de una red neuronal para reducir el número de bits de representación en una red.
Los autores proponen pesos, activaciones, gradientes y errores discretizados tanto en el tiempo de entrenamiento como en el de prueba de las redes neuronales
En este artículo, desarrollamos métodos de sparsificación rápidos y sin reentrenamiento que pueden utilizarse para la sparsificación sobre la marcha de las CNN en muchos contextos industriales.
Este trabajo propone enfoques para la poda de CNNs sin reentrenamiento, introduciendo tres esquemas para determinar los umbrales de los pesos de poda.
Este artículo describe un método para la sparsificación de las CNN sin reentrenamiento.
Proponemos que el entrenamiento con conjuntos crecientes etapa por etapa proporciona una optimización para las redes neuronales.
Los autores comparan el aprendizaje del plan de estudios con el aprendizaje en un orden aleatorio con etapas que añaden una nueva muestra de ejemplos al conjunto construido previamente de forma aleatoria
Este trabajo estudia la influencia del ordenamiento en el plan de estudios y el aprendizaje a ritmo propio, y muestra que hasta cierto punto el ordenamiento de las instancias de formación no es importante.
Método de traducción de imagen a imagen que añade a una imagen el contenido de otra, creando así una nueva imagen.
Este documento aborda la tarea de la transferencia de contenidos, con la novalía de la pérdida.
Un conjunto de datos para poner a prueba el razonamiento matemático (y la generalización algebraica), y resultados sobre los modelos actuales de secuencia a secuencia.
Presenta un nuevo conjunto de datos sintéticos para evaluar la capacidad de razonamiento matemático de los modelos secuencia a secuencia, y lo utiliza para evaluar varios modelos.
Modelo para resolver problemas matemáticos básicos.
Este trabajo introduce parametrizaciones eficientes y económicas de redes neuronales convolucionales motivadas por ecuaciones diferenciales parciales 
Introduce cuatro alternativas de "bajo coste" a la operación de convolución estándar que pueden utilizarse en lugar de la operación de convolución estándar para reducir su complejidad computacional.
Este artículo presenta métodos para reducir el coste computacional de las implementaciones de CNN, e introduce nuevas parametrizaciones de las arquitecturas tipo CNN que limitan el acoplamiento de parámetros.
El artículo propone una perspectiva basada en las PDE para entender y parametrizar las CNN
Utilizar la teoría de la distorsión de la tasa para delimitar cuánto se puede mejorar un modelo de variable latente
Aborda los problemas de optimización del prior en el modelo de variable latente y la selección de la función de verosimilitud proponiendo criterios basados en un límite inferior de la log-verosimilitud negativa.
Presenta un teorema que da un límite inferior a la probabilidad logarítmica negativa de la distorsión de la tasa para el modelado de variables latentes
Los autores argumentan que la teoría de la distorsión de la tasa para la compresión con pérdidas proporciona un conjunto de herramientas naturales para estudiar los modelos de variables latentes propone un límite inferior.
Ignoramos las no linealidades y no calculamos los gradientes en el paso hacia atrás para ahorrar cálculos y garantizar que los gradientes siempre fluyan. 
El autor propuso algoritmos backprop lineales para asegurar el flujo de gradientes para todas las partes durante la retropropagación.
Comprender el autocodificador discreto VQ-VAE de forma sistemática utilizando EM y utilizarlo para diseñar un modelo de traducción no autorregresivo que coincida con una línea de base autorregresiva fuerte.
Este trabajo introduce una nueva forma de interpretar el VQ-VAE y propone un nuevo algoritmo de entrenamiento basado en el clustering EM suave.
El artículo presenta una visión alternativa del procedimiento de entrenamiento para el VQ-VAE utilizando el algoritmo EM suave
Este trabajo presenta una red neuronal profunda que incrusta una función de pérdida con respecto a la distribución de márgenes óptima, que alivia el problema de sobreajuste teórica y empíricamente.
Presenta un límite PAC-Bayesiano para una pérdida de margen
Buscamos entender las representaciones aprendidas en redes comprimidas mediante un régimen experimental que llamamos triaje de redes profundas
Compara varios métodos de inicialización y entrenamiento para transferir el conocimiento de la red VGG a una red estudiantil más pequeña, sustituyendo los bloques de capas por capas individuales.
Este documento presenta cinco métodos para realizar el triaje o la compresión de la capa de bloques para las redes profundas.
El artículo propone un método para comprimir un bloque de capas en una NN que evalúa varios subenfoques diferentes
La prueba empírica de un nuevo fenómeno requiere nuevos conocimientos teóricos y es relevante para las discusiones activas en la literatura sobre SGD y la comprensión de la generalización.
El artículo analiza un fenómeno en el que el entrenamiento de redes neuronales en entornos muy específicos puede beneficiarse mucho de un programa que incluya grandes tasas de aprendizaje
Los autores analizan el entrenamiento de redes residuales utilizando grandes tasas de aprendizaje cíclico, y demuestran una rápida convergencia con las tasas de aprendizaje cíclico y la evidencia de que las grandes tasas de aprendizaje actúan como regularización.
Proponemos un método para la construcción de redes arbitrariamente profundas de ancho infinito, basado en el cual derivamos un novedoso esquema de inicialización de pesos para redes de ancho finito y demostramos su rendimiento competitivo.
Propone un enfoque de inicialización de pesos para permitir redes infinitamente profundas y de ancho infinito con resultados experimentales en pequeños conjuntos de datos.
Propone redes neuronales profundas de anchura infinita.
Derivamos reglas de aprendizaje de plasticidad sináptica biológicamente plausibles para que una red neuronal recurrente almacene representaciones de estímulos. 
Un modelo de red neuronal compuesto por neuronas conectadas de forma recurrente y uno o varios redouts cuyo objetivo es retener alguna salida a lo largo del tiempo.
Este trabajo presenta un mecanismo de memoria autoorganizada en un modelo neuronal, e introduce una función objetivo que minimiza los cambios en la señal a memorizar.
Para entender el entrenamiento de GAN, definimos una dinámica simple de GAN, y mostramos las diferencias cuantitativas entre las actualizaciones óptimas y de primer orden en este modelo.
Los autores estudian el impacto de los GANs en configuraciones donde en cada iteración, el discriminador se entrena hasta la convergencia y el generador se actualiza con pasos de gradiente, o donde se realizan unos pocos pasos de gradiente para el discriminador y el generador.
Este trabajo estudia la dinámica del entrenamiento adversarial de GANs sobre un modelo de mezcla gaussiano
Proponemos un método basado en el gradiente para transferir conocimientos de múltiples fuentes a través de diferentes dominios y tareas.
Este trabajo propone combinar los gradientes de los dominios de origen para ayudar al aprendizaje en el dominio de destino. 
La primera formulación Bayes variacional de la inferencia filogenética, un problema de inferencia desafiante sobre estructuras con componentes discretos y continuos entrelazados
Explora una solución de inferencia aproximada al problema de la inferencia bayesiana de árboles filogenéticos aprovechando las redes bayesianas subsplit recientemente propuestas y los modernos estimadores de gradiente para VI.
Propone un enfoque variacional para la inferencia posterior bayesiana en árboles filogenéticos.
Se trata de una arquitectura neuronal híbrida para acelerar el modelo autorregresivo. 
Concluye que para ampliar el tamaño del modelo sin aumentar el tiempo de inferencia para la predicción secuencial, hay que utilizar un modelo que prediga varios pasos de tiempo a la vez.
Este artículo presenta HybridNet, un sistema neural de síntesis de voz y otros tipos de audio que combina el modelo WaveNet con un LSTM con el objetivo de ofrecer un modelo con una generación de audio más rápida en tiempo de inferencia.
Interpretación mediante la identificación de las características aprendidas del modelo que sirven de indicadores para la tarea de interés. Explicar las decisiones del modelo destacando la respuesta de estas características en los datos de prueba. Evaluar las explicaciones de forma objetiva con un conjunto de datos controlado.
Este artículo propone un método para producir explicaciones visuales de las salidas de las redes neuronales profundas y publica un nuevo conjunto de datos sintéticos.
Un método para redes neuronales profundas que identifica automáticamente las características relevantes del conjunto de las clases, apoyando la interpretación y la explicación sin depender de anotaciones adicionales.
Un marco para el aprendizaje de representaciones de frases de alta calidad de manera eficiente.
Propone un algoritmo más rápido para el aprendizaje de representaciones de oraciones tipo SkipThought a partir de corpus de oraciones ordenadas que cambia el decodificador a nivel de palabra por una pérdida de clasificación contrastiva.
Este trabajo propone un marco para el aprendizaje no supervisado de representaciones de oraciones mediante la maximización de un modelo de probabilidad de oraciones de contexto verdadero en relación con oraciones candidatas aleatorias
Derivamos una penalización de la norma en la salida de la red neuronal desde la perspectiva del cuello de botella de la información
Plantea la Penalización de la Norma de Activación, una regularización de tipo L_2 sobre las activaciones, derivándola del principio de Cuello de Botella de Información
Este trabajo crea un mapa entre las penalizaciones de la norma de activación y el marco de cuello de botella de información utilizando el marco de abandono variacional.
Un método totalmente no supervisado, para integrar de forma natural la reducción de la dimensionalidad y la agrupación temporal en un único marco de aprendizaje de extremo a extremo.
Propone un algoritmo que integra el autoencoder con la agrupación de datos de series temporales utilizando una estructura de red que se adapta a los datos de series temporales.
Un algoritmo para realizar conjuntamente la reducción de la dimensionalidad y la agrupación temporal en un contexto de aprendizaje profundo, utilizando un autoencoder y un objetivo de agrupación.
Los autores propusieron un método de agrupación de series temporales no supervisado construido con redes neuronales profundas y equipado con un codificador-decodificador y un modo de agrupación para acortar las series temporales, extraer características temporales locales y obtener las representaciones codificadas.
Una red neuronal aumentada por la memoria que aborda el problema de las pocas clases aprovechando la jerarquía de clases tanto en el aprendizaje supervisado como en el metaaprendizaje.
Este artículo presenta métodos para añadir un sesgo inductivo a un clasificador a través de la predicción de grueso a fino a lo largo de una jerarquía de clases y el aprendizaje de un clasificador KNN basado en la memoria que realiza un seguimiento de las instancias mal etiquetadas durante el aprendizaje.
Este artículo formula el problema de clasificación de muchas clases y pocos disparos desde una perspectiva de aprendizaje supervisado y de metaaprendizaje.
Un novedoso componente de pérdida que obliga a la red a aprender una representación que se adapte bien a la agrupación durante el entrenamiento para una tarea de clasificación.
Este trabajo propone dos términos de regularización basados en una pérdida de bisagra compuesta sobre la divergencia KL entre dos argumentos de entrada normalizados por softmax para fomentar el aprendizaje de representaciones desenmarañadas
Propuesta de dos regularizadores destinados a hacer que las representaciones aprendidas en la penúltima capa de un clasificador se ajusten más a la estructura inherente de los datos.
Mostramos cómo obtener buenas representaciones desde el punto de vista de la Búsqueda de Simiaridad.
Estudia el impacto del cambio de la parte de clasificación de imágenes sobre la DNN en la capacidad de indexar los descriptores con un algoritmo LSH o kd-tree.
Propone utilizar la pérdida de entropía cruzada softmax para aprender una red que intente reducir los ángulos entre las entradas y los vectores de clase correspondientes en un marco supervisado utilizando.
Introducimos una técnica que permite el entrenamiento basado en el gradiente de las redes neuronales cuantizadas.
Propone una forma unificada y general de entrenar redes neuronales con pesos y activaciones sinápticas cuantificadas de precisión reducida.
Un nuevo enfoque para cuantificar las activaciones que es el estado del arte o competitivo en varios problemas de imágenes reales.
Un método de aprendizaje de redes neuronales con pesos y activaciones cuantificados mediante la cuantificación estocástica de valores y la sustitución de la distribución categotica resultante por una relajación continua
Mostramos que el problema del colapso del modo en los GANs puede explicarse por la falta de intercambio de información entre las observaciones de un lote de entrenamiento, y proponemos un marco basado en la distribución para compartir globalmente la información entre los gradientes que conduce a un entrenamiento adversarial más estable y eficaz.
Propone sustituir los discriminadores de una sola muestra en el entrenamiento adversarial por discriminadores que operan explícitamente sobre distribuciones de ejemplos.
Teoría sobre las pruebas de dos muestras y el MMD y cómo puede incorporarse de forma beneficiosa al marco GAN.
Hemos diseñado un marco de extremo a extremo utilizando el modelo de secuencia a secuencia para realizar la normalización de los nombres químicos.
Normaliza los nombres no sistemáticos en la extracción de información química creando un corpus paralelo de nombres no sistemáticos y sistemáticos y construyendo un modelo seq2seq.
Este trabajo presenta un método para traducir nombres no sistemáticos de compuestos químicos a sus equivalentes sistemáticos utilizando una combinación de mecanismos
El SGD se dirige al principio del entrenamiento hacia una región en la que su paso es demasiado grande en comparación con la curvatura, lo que repercute en el resto del entrenamiento. 
Analiza la relación entre la convergencia/generalización y la actualización en los mayores vectores propios del hessiano de las pérdidas empíricas de las DNNs.
Este trabajo estudia la relación entre el tamaño del paso de la SGD y la curvatura de la superficie de pérdida
Introducimos un novedoso algoritmo de aprendizaje por refuerzo, que predice múltiples acciones y muestras de las mismas.
Este trabajo introduce una mezcla uniforme de políticas deterministas, y encuentra que esta parametrización de políticas estocásticas supera a DDPG en varios benchmarks de OpenAI gym.
Los autores investigan un método para mejorar el rendimiento de las redes entrenadas con DDPG, y muestran un rendimiento mejorado en un gran número de entornos de control continuo estándar.
Al darse cuenta de los inconvenientes al aplicar el dropout original en DenseNet, elaboramos el diseño del método de dropout a partir de tres aspectos, cuya idea podría aplicarse también a otros modelos de CNN.
Aplicación de diferentes estructuras de abandono binario y de horarios con el objetivo específico de regularizar la arquitectura DenseNet.
Propone una técnica de pre-salida para densenet que implementa la salida antes de la función de activación no lineal.
Guiar los modelos profundos conscientes de las relaciones hacia un mejor aprendizaje con conocimiento humano.
Este trabajo propone una variante de la red de columnas basada en la inyección de la guía humana modificando los cálculos en la red.
Un método para incorporar consejos humanos al aprendizaje profundo mediante la extensión de Column Network, una red neuronal de grafos para la clasificación colectiva.
Los recientes éxitos de las redes neuronales binarias pueden entenderse a partir de la geometría de los vectores binarios de alta dimensión
Investiga numérica y teóricamente las razones del éxito empírico de las redes neuronales binarizadas.
Este artículo analiza la eficacia de las redes neuronales binarias y por qué la binarización es capaz de preservar el rendimiento del modelo.
Después de demostrar que una neurona actúa como un solucionador de problemas inversos para la superresolución y que una red de neuronas está garantizada para proporcionar una solución, propusimos una arquitectura de red doble que rinde más rápido que el estado de la técnica.
Discute el uso de redes neuronales para la superresolución
Una nueva arquitectura para resolver tareas de superresolución de imágenes, y un análisis que pretende establecer una conexión entre las CNN para resolver la superresolución y la resolución de problemas inversos regularizados dispersos.
Modelo dinámico que aprende estrategias de divide y vencerás mediante una supervisión débil.
Propone añadir un nuevo sesgo inductivo a la arquitectura de la red neuronal mediante una estrategia de "divide y vencerás".
Este trabajo estudia los problemas que pueden resolverse mediante un enfoque de programación dinámica, y propone una arquitectura de red neuronal para resolver dichos problemas que supera las líneas de base de la secuencia.
El artículo propone una arquitectura de red única que puede aprender estrategias de "divide y vencerás" para resolver tareas algorítmicas.
un gradiente tipo Rep para distribuciones continuas/discretas no parametrizables; generalizado además a modelos probabilísticos profundos, dando lugar a la retropropagación estadística
Presenta un estimador de gradiente para objetivos basados en expectativas que es insesgado, tiene baja varianza y se aplica a variables aleatorias continuas y discretas.
Un método mejorado para calcular las derivadas de la expectativa, y un nuevo estimador de gradiente de baja varianza que permite el entrenamiento de modelos generativos en los que las observaciones o variables latentes son discretas.
Diseña un gradiente de baja varianza para distribuciones asociadas a variables aleatorias continuas o discretas.
Demostramos que los paisajes de costes de los parámetros de la NN y de los hiperparámetros pueden generarse como estados cuánticos utilizando un único circuito cuántico y que éstos pueden utilizarse para el entrenamiento y el meta-entrenamiento.
Describe un método en el que se puede cuantificar un marco de aprendizaje profundo considerando la forma de dos estados de una esfera/qubit de Bloch y creando una red neuronal binaria cuántica.
Este trabajo propone la amplificación cuántica, un nuevo algoritmo para el entrenamiento y la selección de modelos en redes neuronales binarias.
Propone una idea novedosa de dar salida a un estado cuántico que representa un paisaje de costes completo de todos los parámetros para una red neuronal binaria dada, mediante la construcción de una red neuronal binaria cuántica (QBNN).
Un método general para el entrenamiento de clasificadores robustos certificados y sensibles a los costes contra perturbaciones adversas
Calcula e introduce los costes de los ataques adversarios en el objetivo de optimización para obtener un modelo que sea sensible a los costes frente a los ataques adversarios. 
Se basa en el trabajo seminal de Dalvi et al. y amplía el enfoque a la robustez certificable con una matriz de costes que especifica para cada par de clases fuente-destino si el modelo debe ser robusto a los ejemplos adversos.
Uso de trillizos para aprender una métrica de comparación de respuestas neuronales y mejorar el rendimiento de una prótesis.
Los autores desarrollan nuevas métricas de distancia de tren de espigas, incluyendo redes neuronales y métricas cuadráticas. Se demuestra que estas métricas superan a la métrica ingenua de la distancia de Hamming, y captura implícitamente cierta estructura en el código neural.
Pensando en la aplicación de la mejora de las prótesis neuronales, los autores proponen aprender una métrica entre las respuestas neuronales mediante la optimización de una forma cuadrática o una red neuronal profunda .
Este artículo presenta un método para generar preguntas (pistas) y consultas (sugerencias) para ayudar a los usuarios a realizar mapas mentales.
Presenta una herramienta de ayuda a la elaboración de mapas mentales mediante sugerencias de contexto relacionadas con los nodos existentes y mediante preguntas que amplían las ramas menos desarrolladas.
Este artículo presenta un enfoque para ayudar a las personas a realizar tareas de mindmapping, diseñando una interfaz y características algorítmicas para apoyar el mindmapping, y contribuye con un estudio evaluativo.
Detección de muestras fuera de la distribución mediante el uso de estadísticas de características de bajo orden sin requerir ningún cambio en la DNN subyacente.
Presenta un algoritmo para detectar muestras fuera de la distribución mediante el uso de la estimación de la media y la varianza dentro de las capas BatchNorm para construir representaciones de características que luego se alimentan en un clasificador lineal.
Un enfoque para detectar muestras fuera de distribución en el que los autores proponen utilizar la regresión logística sobre las estadísticas simples de cada capa de normalización de lotes de la CNN.
El documento sugiere el uso de puntuaciones Z para comparar las muestras de ID y OOD para evaluar lo que las redes profundas están tratando de hacer.
Proponemos un nuevo método llamado Auto-Encoder Secuencial de Divergencia Máxima que aprovecha la representación del Auto-Encoder Variacional para la detección de vulnerabilidades en códigos binarios.
Este trabajo propone una arquitectura basada en un autoencoder variacional para la incrustación de código para la detección de vulnerabilidades de software binario, con incrustaciones aprendidas más eficaces para distinguir entre código binario vulnerable y no vulnerable que las líneas de base.
Este trabajo propone un modelo para extraer automáticamente características para la detección de vulnerabilidades utilizando la técnica de aprendizaje profundo. 
El cálculo de la atención basado en la distribución posterior conduce a una atención más significativa y a un mejor rendimiento
Este trabajo propone un modelo secuencia a secuencia donde la atención es tratada como una variable latente, y deriva novedosos procedimientos de inferencia para este modelo, obteniendo mejoras en tareas de traducción automática y generación de inflexiones morfológicas.
Este trabajo presenta un nuevo modelo de atención posterior para problemas seq2seq
Comprimir los modelos DNN entrenados minimizando su complejidad y limitando su pérdida.
Este trabajo propone un método para la compresión de redes neuronales profundas bajo restricciones de precisión.
Este trabajo presenta un método de codificación de k-means restringido por el valor de las pérdidas para la compresión de redes y desarrolla un algoritmo iterativo para la optimización del modelo.
Desarrollamos una técnica para visualizar los mecanismos de atención en redes neuronales arbitrarias. 
Propone aprender una Red de Atención Latente que puede ayudar a visualizar la estructura interna de una red neuronal profunda.
Los autores de este trabajo proponen un esquema de visualización de caja negra basado en datos. 
Investigamos una variedad de algoritmos de RL para la generación de moléculas y definimos nuevos puntos de referencia (que se publicarán como un Gimnasio OpenAI), encontrando que PPO y un algoritmo MLE de escalada de colinas son los que mejor funcionan.
Considera la evaluación de modelos para la generación de moléculas proponiendo 19 puntos de referencia, ampliando pequeños conjuntos de datos a un gran conjunto de datos estandarizados, y explorando cómo aplicar las técnicas de RL para el diseño molecular.
Este trabajo muestra que los métodos de RL más sofisticados son menos eficaces que la técnica simple de hill-climbing, con la PPO como excepción, a la hora de modelar y sintetizar moléculas.
La capacidad más robusta de razonamiento analógico se induce cuando las redes aprenden analogías contrastando estructuras relacionales abstractas en sus dominios de entrada.
El artículo investiga la capacidad de una red neuronal para aprender la analogía, mostrando que una red neuronal simple es capaz de resolver ciertos problemas de analogía
Este trabajo describe un enfoque para entrenar redes neuronales para tareas de razonamiento analógico, considerando específicamente la analogía visual y las analogías simbólicas.
Un modelo de conversación neuronal orientado a los objetivos por medio de la autoconversación
Un modelo de juego propio para la generación de diálogos orientados a objetivos, con el objetivo de reforzar el acoplamiento entre la recompensa de la tarea y el modelo de lenguaje.
Este artículo describe un método para mejorar un sistema de diálogo orientado a objetivos utilizando la auto-reproducción. 
Completar consultas de búsqueda en tiempo real mediante modelos lingüísticos LSTM a nivel de caracteres
Este artículo presenta métodos para la finalización de consultas que incluyen la corrección de prefijos, y algunos detalles de ingeniería para satisfacer requisitos particulares de latencia en una CPU.
Los autores proponen un algoritmo para resolver el problema de finalización de consultas con corrección de errores, y adoptan un modelado basado en RNN a nivel de caracteres y optimizan la parte de inferencia para lograr objetivos en tiempo real.
En este trabajo demostramos la convergencia a la criticidad del RMSProp (estocástico y determinista) y del ADAM determinista para objetivos suaves no convexos y demostramos una interesante sensibilidad beta_1 para el ADAM en autocodificadores. 
Este trabajo presenta un análisis de convergencia de RMSProp y ADAM en el caso de funciones suaves no convexas
El diseño de mecanismos de defensa no supervisados contra los ataques adversarios es crucial para garantizar la generalización de la defensa. 
Este trabajo presenta un método para detectar ejemplos adversarios en un entorno de clasificación de aprendizaje profundo
Este artículo presenta un método no supervisado para detectar ejemplos adversos de redes neuronales.
Búsqueda de arquitecturas neuronales sin proxy para el aprendizaje directo de arquitecturas en tareas objetivo a gran escala (ImageNet) reduciendo el coste al mismo nivel que el entrenamiento normal.
Este trabajo aborda el problema de la búsqueda de arquitecturas, y específicamente busca hacerlo sin tener que entrenar en tareas "proxy" donde el problema se simplifica a través de una optimización más limitada, la complejidad de la arquitectura, o el tamaño del conjunto de datos.
Un nuevo marco de trabajo basado en la inferencia variacional para la detección de la falta de distribución
Describe un enfoque probabilístico para cuantificar la incertidumbre en las tareas de clasificación de las DNNs que supera a otros métodos SOTA en la tarea de detección de fuera de distribución.
Un nuevo marco para la detección de fuera de la distribución, basado en la inferencia variaitonal y una distribución Dirichlet a priori, que informa de los resultados del estado del arte en varios conjuntos de datos.
Detección de una distribución fuera de lo normal mediante un nuevo método para aproximar la distribución de confianza de la probabilidad de clasificación utilizando la inferencia variacional de la distribución Dirichlet.
Aprendemos una representación del espacio de acción de un agente a partir de puras observaciones visuales. Utilizamos un enfoque de variables latentes recurrentes con una nueva pérdida de composibilidad.
Propone un modelo composicional de variables latentes para aprender modelos que predigan lo que sucederá a continuación en escenarios donde las etiquetas de acción no están disponibles en abundancia.
Un enfoque basado en el IB variacional para aprender representaciones de acciones directamente a partir de los vídeos de las acciones que se están realizando, logrando una mayor eficiencia de los métodos de aprendizaje posteriores y requiriendo una menor cantidad de vídeos de etiquetas de acciones.
Este trabajo propone un enfoque de predicción de vídeo que encuentra de forma autónoma un espacio de acción que codifica las diferencias entre los fotogramas posteriores
El aprendizaje por refuerzo puede utilizarse para entrenar a los agentes a negociar la formación de equipos en muchos protocolos de negociación
Este artículo estudia la RL profunda de múltiples agentes en entornos en los que todos los agentes deben cooperar para realizar una tarea (por ejemplo, búsqueda y rescate, videojuegos multijugador), y utiliza juegos simples de votación ponderada cooperativa para estudiar la eficacia de la RL profunda y para comparar las soluciones encontradas por la RL profunda con una solución justa.
Un enfoque de aprendizaje por refuerzo para negociar coaliciones en entornos de teoría de juegos cooperativos que puede utilizarse en casos en los que se dispone de simulaciones de entrenamiento ilimitadas.
Métodos no supervisados para encontrar, analizar y controlar las neuronas importantes en la NMT
Este artículo presenta enfoques no supervisados para descubrir neuronas importantes en los sistemas neuronales de traducción automática y analiza las propiedades lingüísticas controladas por esas neuronas.
Métodos no supervisados para clasificar las neuronas en la traducción automática, en los que se identifican las neuronas importantes y se utilizan para controlar el resultado de la MT.
Proponemos un marco de trabajo DRL que desentraña el conocimiento específico de la tarea y del entorno.
Los autores proponen descomponer el aprendizaje por refuerzo en una función PATH y una función GOAL
Una arquitectura modular con el objetivo de separar el conocimiento específico del entorno y el conocimiento específico de la tarea en diferentes módulos, a la par que el A3C estándar en una amplia gama de tareas.
pix2scene: un enfoque profundo basado en la generación para modelar implícitamente las propiedades geométricas de una escena 3D a partir de imágenes
Explora la explicación de escenas con superficies en un modelo de reconocimiento neural, y demuestra los resultados en la reconstrucción de imágenes, la síntesis y la rotación de formas mentales.
Los autores introducen un método para crear un modelo de escena 3D dada una imagen 2D y una pose de cámara utilizando un modelo auto-superfizado
Identificar las relaciones que conectan las palabras es importante para varias tareas de PNL. Modelamos la representación de las relaciones como un problema de aprendizaje supervisado y aprendemos operadores parametrizados que mapean incrustaciones de palabras preentrenadas a representaciones de relaciones.
Este trabajo presenta un método novedoso para representar las relaciones léxicas como vectores utilizando sólo incrustaciones de palabras preentrenadas y una función de pérdida novedosa que opera sobre pares de palabras.
Una solución novedosa al problema de la composición de relaciones cuando ya se tienen incrustaciones de palabras/entidades preentrenadas y sólo se está interesado en aprender a componer representaciones de relaciones.
Proponemos entrenar dos copias idénticas de una red neuronal recurrente (que comparten parámetros) con diferentes máscaras de abandono mientras se minimiza la diferencia entre sus predicciones (pre-softmax).
Presenta el abandono fraternal como una mejora sobre el abandono lineal de expectativas en términos de convergencia, y demuestra la utilidad del abandono fraternal en una serie de tareas y conjuntos de datos.
Aprendizaje de la ponderación y las deformaciones de los conjuntos de datos espacio-temporales para obtener aproximaciones muy eficaces del comportamiento de los líquidos.
Se utiliza un modelo basado en redes neuronales para interpolar simulaciones de nuevas condiciones de escena a partir de superficies implícitas 4D densamente registradas para una escena estructurada.
Este trabajo presenta un enfoque de aprendizaje profundo acoplado para generar datos de simulación de líquidos realistas que pueden ser útiles para aplicaciones de apoyo a la toma de decisiones en tiempo real.
Este trabajo presenta un enfoque de aprendizaje profundo para la simulación física que combina dos redes para sintetizar datos 4D que representan simulaciones físicas 3D
Construimos y evaluamos redes neuronales invariantes del color en un nuevo conjunto de datos realistas
Propone un método para que las redes neuronales de reconocimiento de imágenes sean invariantes al color y lo evalúa en el conjunto de datos cifar 10.
Los autores investigan una capa de entrada modificada que da lugar a redes invariantes de color, y muestran que ciertas capas de entrada invariantes de color pueden mejorar la precisión para imágenes de prueba de una distribución de color diferente a la de las imágenes de entrenamiento.
Los autores prueban una CNN en imágenes con canales de color modificados para que sean invariables a las permutaciones, con un rendimiento que no se degrada demasiado. 
Analizamos cómo el grado de solapamiento entre los campos receptivos de una red convolucional afecta a su capacidad expresiva.
El trabajo estudia la potencia expresiva que proporciona el "solapamiento" en las capas de convolución de las DNNs considerando activaciones lineales con agrupación de productos.
Este trabajo analiza la expresividad de los circuitos aritméticos convolucionales y muestra que se requiere un número exponencialmente grande de ConvACs no solapados para aproximar el tensor de malla de un ConvACs solapado.
Un algoritmo teórico para probar la optimalidad local y extraer las direcciones de descenso en los puntos no diferenciables de los riesgos empíricos de las redes ReLU de una capa oculta.
Propone un algoritmo para comprobar si un punto dado es un punto estacionario generalizado de segundo orden.
Un algoritmo teórico, que implica la resolución de programas cuadráticos convexos y no convexos, para comprobar la optimalidad local y escapar de las sillas de montar al entrenar redes ReLU de dos capas.
El autor propone un método para comprobar si un punto es un punto estacionario o no y luego clasificar los puntos estacionarios como mínimo local o estacionario de segundo orden
Una nueva pérdida basada en negativos relativamente duros que logra el rendimiento más avanzado en la recuperación de imágenes.
Aprendizaje de la incrustación conjunta de frases e imágenes utilizando la pérdida de tripletes que se aplica a los negativos más duros en lugar de promediar sobre todos los tripletes
Utilizamos el principio de minimización alternante para proporcionar una técnica novedosa y eficaz para entrenar autocodificadores profundos.
Marco de minimización alternante para el entrenamiento de redes de autoencodificadores y codificadores-decodificadores
Los autores exploran un enfoque de optimización alternante para el entrenamiento de los autocodificadores, tratando cada capa como un modelo lineal generalizado, y sugieren utilizar el GD normalizado estocástico como algoritmo de minimización en cada fase.
Aprendizaje de transferencia para la estimación de efectos causales mediante redes neuronales.
Desarrolla algoritmos para estimar el efecto medio condicional del tratamiento mediante un conjunto de datos auxiliares en diferentes entornos, tanto con como sin aprendiz de base.
Los autores proponen métodos para abordar una tarea novedosa de aprendizaje de transferencia para estimar la función CATE, y los evalúan utilizando un entorno sintético y un conjunto de datos experimentales del mundo real.
Utilización de la regresión de redes neuronales y comparación de marcos de aprendizaje de transferencia para estimar un efecto de tratamiento medio condicional bajo supuestos de ignorabilidad de la cadena
Presentamos LeMoNADe, un método de detección de motivos aprendidos de extremo a extremo que opera directamente sobre vídeos de imágenes de calcio.
Este trabajo propone un modelo de estilo VAE para identificar motivos a partir de vídeos de imágenes de calcio, que se basa en variables de Bernouli y requiere el truco Gumbel-softmax para la inferencia.
Proponemos un marco para aprender una buena política a través del aprendizaje por imitación de un conjunto de demostraciones ruidosas mediante el meta-entrenamiento de un evaluador de idoneidad de las demostraciones.
Aporta un algoritmo basado en MAML para el aprendizaje por imitación que determina automáticamente si las demostraciones proporcionadas son "adecuadas".
Un método para realizar el aprendizaje por imitación a partir de un conjunto de demostraciones que incluye comportamientos inútiles, que selecciona las demostraciones útiles por sus ganancias de rendimiento proporcionadas en el momento del meta-entrenamiento.
Introducimos modelos generativos implícitos causales, que pueden muestrear a partir de distribuciones condicionales e intervenidas y también proponemos dos nuevos GAN condicionales que utilizamos para entrenarlos.
Un método para combinar un grafo casual, que describe la estructura de dependencia de las etiquetas con dos arquitecturas GAN condicionales que generan imágenes condicionadas a la etiqueta binaria
Los autores abordan la cuestión del aprendizaje de un modelo causal entre las variables de la imagen y la propia imagen a partir de datos observacionales, cuando se da una estructura causal entre las etiquetas de la imagen.
Demostramos que el NCE es autonormalizado y lo demostramos en conjuntos de datos
Presenta una prueba de la auto-normalización de NCE como resultado de ser una aproximación matricial de bajo rango de la matriz de probabilidades condicionales normalizadas.
Este artículo considera el problema de los modelos autonormalizadores y explica el mecanismo de autonormalización interpretando la NCE en términos de factorización de matrices.
El enriquecimiento de las incrustaciones de palabras con información sobre el afecto mejora su rendimiento en las tareas de predicción del sentimiento.
Propone utilizar el léxico afectivo para mejorar las incrustaciones de palabras para superar el estándar Word2vec y Glove.
Este artículo propone integrar la información de un recurso semántico que cuantifica el afecto de las palabras en un algoritmo de incrustación de palabras basado en el texto para que los modelos lingüísticos reflejen mejor los fenómenos semánticos y pragmáticos.
Este artículo introduce modificaciones en las funciones de pérdida word2vec y GloVe para incorporar léxicos de afecto y facilitar el aprendizaje de incrustaciones de palabras sensibles al afecto.
Para la incrustación no supervisada e inductiva de la red, proponemos un enfoque novedoso para explorar los vecinos más relevantes y preservar el conocimiento previamente aprendido de los nodos utilizando la arquitectura de bi-atención e introduciendo un sesgo global, respectivamente
Se propone una extensión de GraphSAGE utilizando una matriz de sesgo de incrustación global en las funciones de agregación local y un método para muestrear los nodos interesantes.
Argumentamos que la generalización de la incrustación lineal de grafos no se debe a la restricción de la dimensionalidad, sino a la pequeña norma de los vectores de incrustación.
Los autores demuestran que el error de generalización de los métodos de incrustación de grafos lineales está limitado por la norma de los vectores de incrustación y no por las restricciones de dimensionalidad
Los autores proponen un límite teórico sobre el rendimiento de generalización del aprendizaje de incrustaciones de grafos y argumentan que la norma de las coordenadas determina el éxito de la representación aprendida.
Mezcle SGD y momentum (o haga algo similar con Adam) para obtener grandes beneficios.
El artículo propone modificaciones sencillas de SGD y Adam, llamadas variantes de QH, que pueden recuperar el método principal y una serie de otros trucos de optimización.
Una variante del impulso clásico que toma una media ponderada de la actualización del impulso y del gradiente, y una evaluación de sus relaciones entre otros esquemas de optimización basados en el impulso.
Una forma novedosa de generalizar los retornos lambda permitiendo que el agente RL decida cuánto quiere ponderar cada uno de los retornos de n pasos.
Extiende el algoritmo A3C con retornos lambda, y propone un enfoque para el aprendizaje de los pesos de los retornos.
Los autores presentan los retornos autodidácticos basados en la confianza, un método de RL de aprendizaje profundo para ajustar los pesos de un vector de elegibilidad en la estimación de valores tipo TD(lambda) para favorecer estimaciones más estables del estado.
hemos propuesto un nuevo modelo de autoconducción que se compone de un módulo de percepción para ver y pensar y un módulo de conducción para comportarse con el fin de adquirir una mejor capacidad de generalización y de explicación de los accidentes.
Presenta una arquitectura de aprendizaje multitarea para la estimación del mapa de profundidad y segmentación y la predicción de la conducción utilizando un módulo de percepción y un módulo de decisión de conducción.
Un método para una arquitectura modificada de extremo a extremo que tiene una mejor capacidad de generalización y explicación, es más robusto a un entorno de prueba diferente, y tiene una salida de decodificación que puede ayudar con la depuración del modelo.
Los autores presentan una red neuronal convolucional multitarea para la conducción de extremo a extremo y proporcionan evaluaciones con el simulador de código abierto CARLA que muestran un mejor rendimiento de generalización en nuevas condiciones de conducción que las líneas de base
Un marco genérico para escalar las técnicas de incrustación de grafos existentes a grandes grafos.
Este trabajo propone un marco de incrustación multinivel que se aplica sobre los métodos de incrustación de redes existentes para poder escalar a redes de gran escala con mayor velocidad.
Los autores proponen un marco de tres etapas para la incrustación de gráficos a gran escala con una calidad de incrustación mejorada.
Un método novedoso para aumentar la resistencia de los OCSVMs contra ataques dirigidos a la integridad mediante transformaciones no lineales selectivas de los datos a dimensiones inferiores.
Los autores proponen una defensa contra los ataques a la seguridad de los detectores de anomalías basados en SVM de una clase
Este trabajo explora cómo las proyecciones aleatorias pueden ser utilizadas para hacer que OCSVM sea robusto a los datos de entrenamiento perturbados adversamente.
Aprender una mejor representación de las redes neuronales con el principio del cuello de botella de la información
Propone un método de aprendizaje basado en el marco del cuello de botella de la información, en el que las capas ocultas de las redes profundas comprimen la entrada X manteniendo suficiente información para predecir la salida Y.
Este trabajo presenta una nueva forma de entrenar una red neuronal estocástica siguiendo un marco de relevancia/compresión de la información similar al Cuello de Botella de Información.
Proponemos un estimador de la máxima discrepancia de medias, apropiado cuando una distribución objetivo sólo es accesible a través de un procedimiento de selección de muestras sesgado, y mostramos que puede utilizarse en una red generativa para corregir este sesgo.
Propone un estimador ponderado por importancia de la MMD para estimar la MMD entre distribuciones basadas en muestras sesgadas según un esquema desconocido conocido o estimado.
Los autores abordan el problema del sesgo de selección de la muestra en las MMD-GAN y proponen una estimación de la MMD entre dos distribuciones utilizando la máxima discrepancia media ponderada.
Este trabajo presenta una modificación del objetivo utilizado para entrenar redes generativas con un adversario MMD 
Utilizar la regresión bayesiana para estimar la posterior sobre las funciones Q y desplegar el muestreo de Thompson como estrategia de exploración dirigida con un equilibrio eficiente entre la exploración y la explotación
Los autores proponen un nuevo algoritmo de exploración en Deep RL en el que aplican la regresión lineal bayesiana con características de la última capa de una red DQN para estimar la función Q de cada acción.
Los autores describen cómo utilizar las redes neuronales bayesianas con el muestreo de Thompson para una exploración eficiente en el aprendizaje q y proponen un enfoque que supera los enfoques de exploración epsilon-greedy.
PolyCNN sólo necesita aprender un filtro convolucional semilla en cada capa. Se trata de una variante eficiente de la CNN tradicional, con un rendimiento similar.
Los intentos de reducir el número de parámetros del modelo CNN utilizando la transformación polinómica de los filtros para crear un aumento de las respuestas de los filtros.
Los autores proponen una arquitectura de reparto de pesos para reducir el número de parámetros de las redes neuronales convolucionales con filtros semilla
En este trabajo, proponemos KL-CPD, un novedoso marco de aprendizaje kernel para CPD de series temporales que optimiza un límite inferior de potencia de prueba a través de un modelo generativo auxiliar como sustituto de la distribución anormal. 
Describe un enfoque novedoso para optimizar la elección del núcleo con el fin de aumentar la potencia de las pruebas y demuestra que ofrece mejoras respecto a las alternativas.
Agrupar antes de clasificar; utilizar etiquetas débiles para mejorar la clasificación 
Propone el uso de una función de pérdida basada en la agrupación en múltiples niveles de una red profunda, así como el uso de la estructura jerárquica del espacio de etiquetas para entrenar mejores representaciones.
Este trabajo utiliza la información jerárquica de las etiquetas para imponer pérdidas adicionales en las representaciones intermedias en el entrenamiento de las redes neuronales.
La minimización del arrepentimiento basada en la ventaja es un nuevo algoritmo de aprendizaje profundo por refuerzo que es particularmente eficaz en tareas parcialmente observables, como la navegación en primera persona en Doom y Minecraft.
Este artículo introduce los conceptos de minimización del arrepentimiento contrafactual en el campo de la RL profunda y un algoritmo llamado ARM que puede tratar mejor la observabilidad parcial.
El documento proporciona una variante inspirada en la teoría de juegos del algoritmo de gradiente de políticas basado en la idea de la minimización del arrepentimiento contrafactual y afirma que el enfoque puede tratar con el dominio observable parcial mejor que los métodos estándar.
un modelo de aprendizaje profundo multitarea que adapta la representación del anillo tensorial
Una variante de la formulación del anillo tensorial para el aprendizaje multitarea compartiendo algunos de los núcleos TT para el aprendizaje de la "tarea común" mientras se aprenden núcleos TT individuales para cada tarea por separado
Un modelo de regresión que aprende las distribuciones condicionales de un proceso estocástico, incorporando la atención a los procesos neuronales.
Propone resolver el problema del infraajuste en el método del proceso neural añadiendo un mecanismo de atención a la trayectoria determinista.
Una extensión del marco de los Procesos Neuronales que añade un mecanismo de condicionamiento basado en la atención, permitiendo que el modelo capture mejor las dependencias en el conjunto de condicionamiento.
Los autores amplían los procesos neuronales incorporando la autoatención para enriquecer las características de los puntos de contexto y la atención cruzada para producir una representación específica de la consulta. Resuelven el problema de infraajuste de las PNE y muestran que las PNA convergen mejor y más rápido que las PNE.
Resolver el problema del tablero de ajedrez en la capa deconvolutiva construyendo dependencias entre los píxeles
Este trabajo propone capas deconvolucionales de píxeles para redes neuronales convolucionales como forma de aliviar el efecto damero.
Una nueva técnica para generalizar las operaciones de deconvolución utilizadas en las arquitecturas estándar de las CNN, que propone hacer una predicción secuencial de las características de los píxeles adyacentes, lo que da lugar a salidas más suaves espacialmente para las capas de deconvolución.
Cuantificamos y podamos los pesos de las redes neuronales utilizando la inferencia bayesiana variacional con un previo multimodal que induce la dispersión.
Propone utilizar una mezcla de propto 1/abs de picos continuos como prior para una red neuronal bayesiana y demuestra el buen rendimiento con convnets relativamente dispersos para minist y cifar-10.
Este trabajo presenta un enfoque bayesiano variacional para cuantificar los pesos de las redes neuronales a valores ternarios post-entrenamiento de una manera principista.
Implementamos un enfoque de poda de pesos de la DNN que logra las tasas de poda más altas.
Este trabajo se centra en la poda de pesos para la compresión de redes neuronales, logrando una tasa de compresión de 30 veces para AlexNet y VGG para ImageNet.
Una técnica de poda progresiva que impone una restricción de escasez estructural en el parámetro de peso y reescribe la optimización como un marco ADMM, logrando una mayor precisión que el descenso de gradiente proyectado.
Este trabajo presenta una nueva arquitectura de aprendizaje profundo para abordar el problema del aprendizaje supervisado con series temporales multivariantes dispersas e irregularmente muestreadas.
Propone un marco para realizar predicciones sobre datos de series temporales dispersos e irregulares utilizando un módulo de interpolación que modela los valores perdidos en el uso de la interpolación suave, la interpolación no suave y la intensidad. 
Resuelve el problema del aprendizaje supervisado con series temporales multivariantes dispersas e irregularmente muestreadas utilizando una red de interpolación semiparamétrica seguida de una red de predicción.
Función de pérdida invariable por permutación para la predicción de conjuntos de puntos.
Propone una nueva pérdida para el registro de puntos (alineación de dos conjuntos de puntos) con una propiedad invariante de permutación preferible. 
Este trabajo introduce una nueva función de distancia entre conjuntos de puntos, aplica otras dos distancias de permutación en una tarea de detección de objetos de extremo a extremo, y muestra que en dos dimensiones todos los mínimos locales de la pérdida holográfica son mínimos globales.
Propone funciones de pérdida invariantes de la permutación que dependen de la distancia de los conjuntos.
Introducimos un modelo jerárquico para la colocación eficiente, de extremo a extremo, de gráficos computacionales en dispositivos de hardware.
Propone aprender conjuntamente grupos de operadores para colocarlos y colocar los grupos aprendidos en los dispositivos para distribuir las operaciones de aprendizaje profundo a través del aprendizaje por refuerzo.
Los autores proponen una red de conexión completa para reemplazar el paso de co-ubicación en un método de auto-ubicación propuesto para acelerar el tiempo de ejecución de un modelo TensorFlow.
Propone un algoritmo de colocación de dispositivos para colocar las operaciones de tensorflow en los dispositivos.
Proponemos un método de uso de las propiedades de grupo para aprender una representación del movimiento sin etiquetas y demostramos el uso de este método para representar el movimiento 2D y 3D.
Propone aprender el grupo de movimiento rígido a partir de una representación latente de secuencias de imágenes sin necesidad de etiquetas explícitas y demuestra experimentalmente el método en secuencias de dígitos MINST y en el conjunto de datos KITTI.
Este trabajo propone un enfoque para el aprendizaje de características de movimiento de vídeo de una manera no supervisada, utilizando restricciones para optimizar la red neuronal para producir características que se pueden utilizar para la regresión de la odometría.
Este trabajo propone una novedosa capa convolucional que opera en un Espacio de Hilbert de Núcleo Reproductor continuo.
Proyectar ejemplos en un espacio RK Hilbert y realizar la convolución y el filtrado en ese espacio.
Este trabajo formula una variante de las redes neuronales convolucionales que modela tanto las activaciones como los filtros como funciones continuas compuestas a partir de bases kernel
Las CNN entrenadas en ImageNet se inclinan por la textura del objeto (en lugar de la forma, como los humanos). La superación de esta importante diferencia entre la visión humana y la de las máquinas permite mejorar el rendimiento de la detección y una robustez inédita frente a las distorsiones de la imagen.
Utilización de la estilización de imágenes para aumentar los datos de entrenamiento de las CNN entrenadas en ImageNet, con el fin de que las redes resultantes parezcan más acordes con los juicios humanos
Este artículo estudia CNNs como AlexNet, VGG, GoogleNet y ResNet50, muestra que estos modelos están sesgados hacia la textura cuando se entrenan en ImageNet, y propone un nuevo conjunto de datos de ImageNet.
Evaluamos la eficacia de realizar tareas discriminatorias auxiliares sobre las estadísticas de la distribución posterior aprendida por los autocodificadores variacionales para reforzar la dependencia del hablante.
Proponer un modelo de autoencoder para aprender una representación para la verificación de hablantes utilizando ventanas de análisis de corta duración.
Una versión modificada del modelo de autoencoder variacional que aborda el problema del reconocimiento de hablantes en el contexto de segmentos de corta duración
La inferencia variacional está sesgada, vamos a debiarla.
Introduce la inferencia variacional jackknife, un método para debiar los objetivos de Monte Carlo como el autoencoder ponderado por importancia.
Los autores analizan el sesgo y la varianza del límite IWAE y derivan un enfoque jacknife para estimar los momentos como una forma de debias IWAE para muestras ponderadas de importancia finita.
Un marco que proporciona una política para el cambio de carril autónomo aprendiendo a tomar decisiones tácticas de alto nivel con el aprendizaje de refuerzo profundo, y manteniendo una estrecha integración con un controlador de bajo nivel para tomar acciones de bajo nivel.
Considera el problema del cambio de carril autónomo para los coches de autoconducción en un entorno de coches de slot con múltiples carriles, propone una nueva estrategia de aprendizaje Q-masking - acoplando un controlador definido de bajo nivel con una política de toma de decisiones tácticas de alto nivel.
Este trabajo propone un enfoque de aprendizaje Q profundo para el problema del cambio de carril utilizando "Q-masking", que reduce el espacio de acción según las restricciones o el conocimiento previo.
Los autores proponen un método que utiliza una política de alto nivel basada en el aprendizaje Q que se combina con una máscara contextual derivada de las restricciones de seguridad y los controladores de bajo nivel, que impiden que ciertas acciones sean seleccionables en determinados estados. 
Búsqueda automática de diseños robóticos con redes neuronales gráficas
Propone un enfoque para el diseño automático de robots basado en la evolución de grafos neuronales. Los experimentos demuestran que optimizar tanto el controlador como el hardware es mejor que optimizar solo el controlador.
Los autores proponen un esquema basado en una representación gráfica de la estructura del robot, y una red grafo-neural como controladores para optimizar las estructuras del robot, combinadas con sus controladores.  
Demostramos un autoencoder para grafos.
Aprender a generar grafos utilizando métodos de aprendizaje profundo en "una sola toma", emitiendo directamente probabilidades de existencia de nodos y aristas, y vectores de atributos de nodos.
Un autocodificador variacional para generar gráficos
Proponemos un nuevo algoritmo para el entrenamiento de LSTM mediante el aprendizaje hacia puertas de valor binario que demostramos tiene muchas propiedades agradables.
Proponer una nueva función de "compuerta" para LSTM para habilitar los valores de las compuertas hacia 0 o 1. 
El artículo pretende impulsar las puertas LSTM para que sean binarias empleando el reciente truco Gumbel-Softmax para obtener una distribución categórica entrenable de extremo a extremo.
Mejora de las recomendaciones mediante modelos temporales con redes neuronales en múltiples categorías de productos en un sitio web de venta al por menor
El artículo propone un nuevo método de recomendación basado en redes neuronales.
Los autores describen un procedimiento de construcción de su sistema de recomendación de producción a partir de cero e integran el decaimiento temporal de las compras en el marco de aprendizaje.
Acoplar el marco de restauración de imágenes basado en GAN con otra red específica de la tarea para generar una imagen realista conservando las características específicas de la tarea.
Un novedoso método de acoplamiento de tareas-GAN de imágenes que acopla GAN y una red específica de tareas, que alivia para evitar la alucinación o el colapso de modos.
Los autores proponen aumentar la restauración de imágenes basada en GAN con otra rama de tareas específicas, como las tareas de clasificación, para mejorar aún más.
Una red neuronal profunda entrenada de principio a fin que aprovecha el modelado de mezclas gaussianas para realizar la estimación de la densidad y la detección de anomalías sin supervisión en un espacio de baja dimensión aprendido por autoencoder profundo.
El documento presenta un marco de aprendizaje profundo conjunto para la reducción de la dimensión-agrupamiento, conduce a la detección de anomalías competitiva.
Una nueva técnica para la detección de anomalías en la que se optimizan conjuntamente los pasos de reducción de dimensiones y estimación de la densidad.
Proponemos redes de subespacio proyectivo para el aprendizaje de pocos disparos y semi-supervisado de pocos disparos
Este trabajo propone un nuevo enfoque basado en la incrustación para el problema del aprendizaje de pocos disparos y una extensión de este modelo al entorno del aprendizaje de pocos disparos semi-supervisado.
Nuevo método para la clasificación completa y semi-supervisada de pocos disparos basado en el aprendizaje de una incrustación general y luego el aprendizaje de un subespacio de la misma para cada clase
Investigamos la conciencia de la contingencia y los aspectos controlables en la exploración y logramos el rendimiento más avanzado en la Venganza de Moctezuma sin demostraciones de expertos.
Este trabajo investiga el problema de la extracción de una representación de estado significativa para ayudar a la exploración cuando se enfrenta a una tarea de recompensa escasa mediante la identificación de características controlables (aprendidas) del estado
Este trabajo propone la novedosa idea de utilizar la conciencia de contingencia para ayudar a la exploración en tareas de aprendizaje por refuerzo con recompensas dispersas, obteniendo resultados del estado del arte.
Propusimos un algoritmo supervisado, DNA-GAN, para desentrañar múltiples atributos de las imágenes.
Este trabajo investiga el problema de la generación de imágenes condicionadas por atributos utilizando redes generativas adversariales, y propone generar imágenes a partir de atributos y códigos latentes como representación de alto nivel.
Este trabajo propone un nuevo método para desentrañar diferentes atributos de las imágenes utilizando una nueva estructura de ADN GAN
Este trabajo presenta una novedosa técnica de modelización generativa de variables latentes que permite representar la información global en una variable latente y la información local en otra variable latente.
El artículo presenta un VAE que utiliza etiquetas para separar la representación aprendida en una parte invariante y otra covariante.
Abordamos el problema del aprendizaje activo como un problema de selección de conjuntos de núcleos y mostramos que este enfoque es especialmente útil en el entorno del aprendizaje activo por lotes, que es crucial cuando se entrenan las CNN.
Los autores proporcionan un algoritmo de aprendizaje activo agnóstico para la clasificación multiclase
El artículo propone un algoritmo de aprendizaje activo en modo batch para CNN como un problema de conjunto de núcleos que supera el muestreo aleatorio y el muestreo de incertidumbre.
Estudia el aprendizaje activo para redes neuronales convolucionales y formula el problema de aprendizaje activo como selección de conjuntos de núcleos y presenta una estrategia novedosa
Un algoritmo sencillo para mejorar la optimización y el manejo de las dependencias a largo plazo en LSTM
El artículo introduce un algoritmo estocástico simple llamado h-detach que es específico para la optimización de LSTM y está dirigido a abordar este problema.
Propone una sencilla modificación del proceso de entrenamiento de la LSTM para facilitar la propagación del gradiente a lo largo de los estados de la célula, o el "camino temporal lineal"
El entrenamiento adecuado de las CNN con la clase dustbin aumenta su robustez frente a los ataques de adversarios y su capacidad para hacer frente a las muestras fuera de distribución.
Este trabajo propone añadir una etiqueta adicional para detectar muestras OOD y ejemplos adversos en modelos CNN.
El artículo propone una clase adicional que incorpora imágenes naturales de distribución externa e imágenes interpoladas para las muestras adversas y de distribución externa en las CNN
En una red neuronal convolucional profunda entrenada con un nivel suficiente de aumento de datos, optimizada por SGD, los regularizadores explícitos (decaimiento de pesos y abandono) podrían no proporcionar ninguna mejora adicional de la generalización.
Este trabajo propone el aumento de datos como alternativa a las técnicas de regularización comúnmente utilizadas, y muestra que para unos pocos modelos/tareas de referencia se puede conseguir el mismo rendimiento de generalización utilizando únicamente el aumento de datos.
Este trabajo presenta un estudio sistemático del aumento de datos en la clasificación de imágenes con redes neuronales profundas, sugiriendo que el aumento de datos puede replicar algunos regularizadores comunes como el decaimiento de peso y el abandono.
En este trabajo, presentamos Gedit, un sistema de gestos en el teclado para una cómoda edición de texto en el móvil.
Informa sobre el diseño y la evaluación de las técnicas de interacción de Gedit.
Presenta un nuevo conjunto de gestos táctiles para realizar una transición fluida entre la entrada y la edición de texto en los dispositivos móviles
Demostramos que la DNN es una solución recursivamente aproximada al principio de máxima entropía.
Presenta una derivación que vincula una DNN con la aplicación recursiva del ajuste de modelos de máxima entropía.
El artículo pretende ofrecer una visión del aprendizaje profundo desde la perspectiva del principio de máxima entropía.
Presentamos un novedoso enfoque de aprendizaje por refuerzo que aprovecha una función de recompensa intrínseca independiente de la tarea y entrenada con mediciones de pulso periférico que se correlacionan con las respuestas del sistema nervioso autónomo humano. 
Propone un marco de aprendizaje por refuerzo basado en la reacción emocional humana en el contexto de la conducción autónoma.
Los autores proponen utilizar señales, como las respuestas autonómicas viscerales básicas que influyen en la toma de decisiones, dentro del marco de la RL, aumentando las funciones de recompensa de la RL con un modelo aprendido directamente de las respuestas del sistema nervioso humano.
Propone utilizar señales fisiológicas para mejorar el rendimiento de los algoritmos de aprendizaje por refuerzo y construir una función de recompensa intrínseca que sea menos dispersa mediante la medición de la amplitud del pulso cardíaco
¿Son las CNN robustas o frágiles al ruido de las etiquetas? Prácticamente, robustos.
Los autores ponen a prueba la robustez de las CNN frente al ruido de etiquetas utilizando el árbol ImageNet 1k de WordNet.
Un análisis del rendimiento de los modelos de redes neuronales convolucionales cuando se introduce ruido dependiente e independiente de la clase
Demuestra que las CNN son más robustas al ruido de las etiquetas relevantes para la clase y argumenta que el ruido del mundo real debería ser relevante para la clase
Síntesis de audio de alta calidad con GANs
Propone un enfoque que utiliza el marco GAN para generar audio mediante el modelado de magnitudes de registro y frecuencias instantáneas con suficiente resolución de frecuencia en el dominio espectral. 
Una estrategia para generar muestras de audio a partir de ruido con GANs, con cambios en la arquitectura y representación necesarios para generar un audio convincente que contenga un código latente interpretable.
Presenta una idea sencilla para representar mejor los datos de audio de modo que puedan aplicarse modelos convolucionales como las redes generativas adversariales
Optimización de gráficos con filtrado de señales en el dominio de los vértices.
El artículo investiga el aprendizaje de la matriz de adyacencia de un grafo no dirigido escasamente conectado con pesos de arista no negativos, utilizando un algoritmo de descenso de sub-gradiente proyectado.
Desarrolla un novedoso esquema de retropropagación en la matriz de adyacencia de un grafo de red neuronal
Este artículo describe una herramienta de autoría 3D para proporcionar RA en las líneas de montaje de la industria 4.0
El documento aborda cómo las herramientas de autoría de RA apoyan la formación de los sistemas de la línea de montaje y propone un enfoque
Un sistema de guiado de RA para líneas de montaje industriales que permite la creación de contenidos de RA in situ.
Presenta un sistema que permite formar a los trabajadores de una fábrica de forma más eficiente mediante un sistema de realidad aumentada. 
Demostramos que, con una elección adecuada del tamaño de los pasos, el algoritmo iterativo de primer orden ampliamente utilizado en el entrenamiento de GANs convergería de hecho a una solución estacionaria con una tasa sublineal.
Este trabajo utiliza GANs y el aprendizaje multitarea para proporcionar una garantía de convergencia para los algoritmos primal-dual en ciertos problemas min-max.
Analiza la dinámica de aprendizaje de las GANs formulando el problema como un problema de optimización primal-dual asumiendo una clase limitada de modelos
Mostramos cómo utilizar la RL profunda para construir agentes que puedan resolver dilemas sociales más allá de los juegos matriciales.
Aprender a jugar a juegos de suma general de dos jugadores con estado con información imperfecta
Especifica una estrategia de activación (CCC) y el algoritmo correspondiente, demostrando la convergencia a resultados eficientes en dilemas sociales sin necesidad de que los agentes observen las acciones de los demás.
El artículo propone y analiza dos esquemas de cuantificación para la comunicación de Gradientes Estocásticos en el aprendizaje distribuido que reducirían los costes de comunicación en comparación con el estado del arte, manteniendo la misma precisión.  
Los autores proponen aplicar una cuantificación interpuesta a los gradientes estocásticos calculados mediante el proceso de entrenamiento, lo que mejora el error de cuantificación y consigue resultados superiores a los de las líneas de base, y proponen un esquema anidado para reducir el coste de comunicación.
Los autores establecen una conexión entre la reducción de la comunicación en la optimización distribuida y la cuantificación vacilante, y desarrollan dos nuevos algoritmos de entrenamiento distribuido en los que la sobrecarga de comunicación se reduce significativamente.
Entrenar conjuntamente una red generadora de ruido adversario con una red de clasificación para proporcionar una mayor robustez frente a los ataques adversarios.
Una solución GAN para modelos profundos de clasificación, frente a ataques de caja blanca y negra, que produce modelos robustos. 
El artículo propone un mecanismo defensivo contra los ataques adversarios utilizando GANs con perturbaciones generadas utilizadas como ejemplos adversarios y un discriminador utilizado para distinguir entre ellos
Utilización de métodos de conjunto como defensa frente a las perturbaciones adversarias contra las redes neuronales profundas.
Este trabajo propone utilizar el ensamblaje como mecanismo de defensa adversarial.
Se ha investigado empíricamente la robustez de diferentes conjuntos de redes neuronales profundas frente a los dos tipos de ataques, FGSM y BIM, en dos conjuntos de datos populares, MNIST y CIFAR10
Propuesta del método de generación de frases basado en la fusión entre la información textual y la información visual asociada a la información textual
Este trabajo describe un modelo de aprendizaje profundo para sistemas de diálogo que aprovecha la información visual.
Este artículo propone un nuevo conjunto de datos para el diálogo fundamentado y hace una observación computacional que podría ayudar a razonar sobre la visión incluso cuando se realiza un diálogo basado en texto.
Propone aumentar los enfoques tradicionales de generación de frases/diálogos basados en el texto incorporando información visual mediante la recopilación de un conjunto de datos consistentes tanto en texto como en imágenes o vídeos asociados
proponemos una nueva red convolucional recurrente contextual con una propiedad robusta de aprendizaje visual 
Este artículo introduce la conexión de retroalimentación para mejorar el aprendizaje de características mediante la incorporación de información de contexto.
El trabajo propone añadir conexiones "recurrentes" dentro de una red de convolución con mecanismo de gating.
Mostramos que el entrenamiento de una red profunda utilizando la normalización por lotes es equivalente a la inferencia aproximada en los modelos bayesianos, y demostramos cómo este hallazgo nos permite hacer estimaciones útiles de la incertidumbre del modelo en las redes convencionales.
Este artículo propone utilizar la normalización por lotes en el momento de la prueba para obtener la incertidumbre de la predicción, y muestra que la predicción de Monte Carlo en el momento de la prueba utilizando la norma por lotes es mejor que el abandono.
Propone que el procedimiento de regularización llamado normalización por lotes puede entenderse como la realización de una inferencia bayesiana aproximada, que se comporta de forma similar al abandono de MC en términos de las estimaciones de incertidumbre que produce.
Mejoramos el gradient dropping (una técnica que sólo intercambia gradientes grandes en el entrenamiento distribuido) incorporando gradientes locales mientras se hace una actualización de los parámetros para reducir la pérdida de calidad y mejorar aún más el tiempo de entrenamiento.
Este trabajo propone 3 modos de combinar los gradientes locales y globales para utilizar mejor más nodos de computación
Examina el problema de la reducción de los requisitos de comunicación para la aplicación de las técnicas de optimización distribuida, en particular el SGD
Exploración mediante RL distributiva y varianza truncada.
Presenta un método de RL para gestionar las compensaciones de exploración-explotación mediante técnicas de UCB.
Un método para utilizar la distribución aprendida por la Regresión Cuantil DQN para la exploración, en lugar de la estrategia habitual de epsilon-greedy.
Propone nuevos algoritmos (QUCB y QUCB+) para manejar el compromiso de exploración en los Bandidos Multiarmados y más generalmente en el Aprendizaje por Refuerzo
Motivados por las teorías del lenguaje y la comunicación, presentamos los autocodificadores basados en la comunidad, en los que múltiples codificadores y decodificadores aprenden colectivamente representaciones estructuradas y reutilizables.
Los autores abordan el problema del aprendizaje de la representación, pretenden construir una representación reutilizable y estructurada, argumentan que la coadaptación entre el codificador y el decodificador en la EA tradicional produce una representación pobre, e introducen autocodificadores basados en la comunidad.
Este artículo presenta un marco de autocodificación basado en la comunidad para abordar la coadaptación de codificadores y decodificadores y pretende construir mejores representaciones.
Presentamos MetaMimic, un algoritmo que toma como entrada un conjunto de datos de demostración y produce (i) una política de imitación de alta fidelidad de una sola vez y (ii) una política de tarea incondicional.
El artículo examina el problema de la imitación de una sola vez con alta precisión de imitación, extendiendo DDPGfD para utilizar sólo las trayectorias de estado.
Este trabajo propone un enfoque para la imitación de una sola vez con alta precisión, y aborda el problema común de la exploración en el aprendizaje de la imitación.
Presenta un método de RL para el aprendizaje a partir de una demostración de vídeo sin acceso a las acciones de los expertos
Presentamos un nuevo método de normalización para redes neuronales profundas que es robusto a las multimodalidades en las distribuciones de características intermedias.
Método de normalización que aprende la distribución multimodal en el espacio de características
Propone una generalización de la Normalización por Lotes bajo el supuesto de que la estadística de las activaciones unitarias sobre los lotes y sobre las dimensiones espaciales no es unimodal
Proponemos un método basado en la destilación de conocimientos para aumentar la precisión de la traducción automática neural multilingüe.
Un modelo de traducción automática neural multilingüe que primero entrena modelos separados para cada par de idiomas y luego realiza la destilación.
El objetivo de este trabajo es entrenar un modelo de traducción automática aumentando la pérdida de entropía cruzada estándar con un componente de destilación basado en modelos de profesores individuales (de un solo par de idiomas).
Investigamos los distintos tipos de conocimientos previos que ayudan al aprendizaje humano y descubrimos que los conocimientos previos generales sobre los objetos desempeñan el papel más crítico a la hora de guiar el juego humano.
Los autores estudian, mediante un experimento, qué aspectos de los prejuicios humanos son los más importantes para el aprendizaje por refuerzo en los videojuegos.
Los autores presentan un estudio sobre los prejuicios empleados por los humanos al jugar a los videojuegos y demuestran la existencia de una taxonomía de características que afectan a la capacidad de completar las tareas del juego en distintos grados.
Impulsados por la necesidad de métodos de optimización de hiperparámetros paralelizables y de bucle abierto, proponemos el uso de procesos puntuales k-determinantes en la optimización de hiperparámetros mediante búsqueda aleatoria.
Propone utilizar el k-DPP para seleccionar los puntos candidatos en las búsquedas de hiperparámetros.
Los autores proponen k-DPP como método de bucle abierto para la optimización de hiperparámetros y proporcionan su estudio empírico y su comparación con otros métodos.
Considera la búsqueda no secuencial y no informada de hiperparámetros utilizando procesos puntuales determinantes, que son distribuciones de probabilidad sobre subconjuntos de un conjunto básico con la propiedad de que los subconjuntos con elementos más "diversos" tienen mayor probabilidad
En el aprendizaje inductivo por transferencia, el ajuste fino de las redes convolucionales preentrenadas supera sustancialmente el entrenamiento desde cero.
Aborda el problema del aprendizaje de transferencia en redes profundas y propone tener un término de regularización que penalice la divergencia de la inicialización.
Propone un análisis sobre diferentes técnicas de regularización adaptativa para el aprendizaje profundo de transferencia, centrándose específicamente en el uso de una condición L@-SP
Nos fijamos en las redes neuronales con capas de producto interno diagonal en bloque para mejorar la eficiencia.
Este artículo propone que las capas internas de una red neuronal sean diagonales en bloque, y discute que las matrices diagonales en bloque son más eficientes que la poda y que las capas diagonales en bloque conducen a redes más eficientes.
Sustitución de capas totalmente conectadas por capas totalmente conectadas diagonales en bloque
Proponemos una novedosa técnica de normalización de pesos llamada normalización espectral para estabilizar el entrenamiento del discriminador de los GANs.
Este trabajo utiliza la regularización espectral para normalizar los objetivos del GAN, y el GAN resultante, denominado SN-GAN, garantiza esencialmente la propiedad Lipschitz del discriminador.
Este trabajo propone la "normalización espectral", dando un buen paso adelante en la mejora del entrenamiento de los GANs.
Las políticas de transición permiten a los agentes componer habilidades complejas mediante la conexión fluida de habilidades primitivas previamente adquiridas.
Propone un esquema de transición a estados de estrato favorables para la ejecución de opciones dadas en dominios continuos. Utiliza dos procesos de aprendizaje realizados simultáneamente.
Presenta un método para el aprendizaje de políticas de transición de una tarea a otra con el objetivo de completar tareas complejas utilizando el estimador de proximidad de estado para recompensar la política de transición.
Propone un nuevo esquema de entrenamiento con una función de recompensa auxiliar aprendida para optimizar las políticas de transición que conectan el estado final de una macro acción/opción anterior con buenos estados de iniciación de la siguiente macro acción/opción
Clasificamos las características dinámicas que pueden y no pueden captar las células GRU en tiempo continuo, y verificamos nuestros hallazgos experimentalmente con la predicción de series temporales en k pasos. 
Los autores analizan GRUs con tamaños ocultos de uno y dos como sistemas dinámicos de tiempo continuo, afirmando que el poder expresivo de la representación de estados ocultos puede proporcionar un conocimiento previo sobre el rendimiento de una GRU en un conjunto de datos determinado
Este trabajo analiza las GRUs desde una perspectiva de sistemas dinámicos, y muestra que las GRUs 2d pueden ser entrenadas para adoptar una variedad de puntos fijos y pueden aproximarse a los atractores de línea, pero no pueden imitar un atractor de anillo.
Convierte las ecuaciones de GRU en tiempo continuo y utiliza la teoría y los experimentos para estudiar las redes GRU de 1 y 2 dimensiones y mostrar todas las variedades de topología dinámica disponibles en estos sistemas
Las entradas diferenciadas provocan una diferenciación funcional de la red, y la interacción de las funciones de pérdida entre redes puede afectar al proceso de optimización.
Una modificación de la red original de reloj de arena para la estimación de una sola pose que produce mejoras sobre la línea de base original.
Los autores amplían una red de reloj de arena apilada con módulos inception-resnet-A y proponen un enfoque multiescala para la estimación de la pose humana en imágenes RGB fijas.
Para entrenar una incrustación de frases utilizando documentos técnicos, nuestro enfoque tiene en cuenta la estructura del documento para encontrar un contexto más amplio y manejar las palabras fuera del vocabulario.
Presenta ideas para mejorar la inserción de la frase recurriendo a más contexto.
Aprendizaje de representaciones de oraciones con información de dependencias de oraciones
Amplía la idea de formar una representación no supervisada de las frases utilizada en el enfoque SkipThough utilizando un conjunto más amplio de pruebas para formar la representación de una frase
Exploramos la estructura de las funciones de pérdida neuronales, y el efecto de los paisajes de pérdida en la generalización, utilizando una serie de métodos de visualización.
Este artículo propone un método para visualizar la función de pérdida de una NN y proporciona información sobre la capacidad de entrenamiento y la generalización de las NN.
Investiga la no convexidad de la superficie de pérdidas y las vías de optimización.
Demostramos que aprovechando una codificación de salida multidireccional, en lugar de la ampliamente utilizada codificación de un solo disparo, podemos hacer que los modelos profundos sean más robustos a los ataques adversarios.
Este trabajo propone reemplazar la capa final de entropía cruzada entrenada con etiquetas de un solo punto en los clasificadores, codificando cada etiqueta como un vector de alta dimensión y entrenando el clasificador para minimizar la distancia L2 de la codificación de la clase correcta.
Los autores proponen un nuevo método contra los ataques de adversarios que muestra una cantidad significativa de ganancias en comparación con las líneas de base
Introducimos el primer modelo NMT con decodificación totalmente paralela, reduciendo la latencia de la inferencia en 10 veces.
Este trabajo propone un decodificador no autorregresivo para el marco codificador-decodificador en el que la decisión de generar una palabra no depende de la decisión previa de las palabras generadas
Este artículo describe un enfoque de decodificación no autoregresiva para la traducción automática neural con la posibilidad de una decodificación más paralela que puede dar lugar a un aumento significativo de la velocidad.
Propone la introducción de un conjunto de variables latentes para representar la fertilidad de cada palabra de origen para que la generación de la frase objetivo no sea autorregresiva
Demostramos un método certificable, entrenable y escalable para defenderse de ejemplos adversos.
Propone una nueva defensa contra los ataques de seguridad a las redes neuronales con el modelo de ataque que emite un certificado de seguridad sobre el algoritmo.
Deduce un límite superior de la perturbación adversarial para redes neuronales con una capa oculta
proponemos un regularizador que mejora el rendimiento de clasificación de las redes neuronales
los autores proponen entrenar un modelo desde el punto de maximizar la información mutua entre las predicciones y las salidas verdaderas, con un término de regularización que minimiza la información irrelevante mientras se aprende.
Propone descomponer los parámetros en un mapa de características invertible F y una transformación lineal w en la última capa para maximizar la información mutua I(Y, \hat{T}) mientras se restringe la información irrelevante
Este trabajo introduce un novedoso marco de modelización generativa que evita el colapso de las variables latentes y aclara el uso de ciertos factores ad-hoc en el entrenamiento de los Autoencoders Variacionales.
El artículo propone resolver el problema de un autocodificador variacional que ignora las variables latentes.
Este trabajo propone añadir un autoencoder estocástico al modelo VAE original para abordar el problema de que el decodificador LSTM de un modelo lingüístico podría ser demasiado fuerte para ignorar la información de la variable latente.
Este trabajo presenta AutoGen, que combina un autoencoder generativo variacional con un modelo de reconstrucción de alta fidelidad basado en el autoencoder para utilizar mejor la representación latente
 Este trabajo estudia el problema de la división de dominios mediante la segmentación de instancias extraídas de diferentes distribuciones probabilísticas.  
Este trabajo aborda el problema del reconocimiento de la novedad en el aprendizaje de conjuntos abiertos y el aprendizaje generalizado de disparos cero y propone una posible solución
Un enfoque para la separación de dominios basado en el bootstrapping para identificar los umbrales de corte de similitud para las clases conocidas, seguido de una prueba de Kolmogorov-Smirnoff para refinar las zonas de in-distribución bootstrapped.
Propone introducir un nuevo dominio, el dominio incierto, para manejar mejor la división entre dominios vistos/no vistos en el aprendizaje de conjunto abierto y de tiro cero generalizado
El SGD realiza implícitamente una inferencia variacional; el ruido del gradiente es altamente no isotrópico, por lo que el SGD ni siquiera converge a los puntos críticos de la pérdida original
Este documento proporciona un análisis variacional de la SGD como proceso de no equilibrio.
Este artículo analiza la función objetivo regularizada minimizada por el SGD estándar en el contexto de las redes neuronales, y proporciona una perspectiva de inferencia variacional utilizando la ecuación de Fokker-Planck.
Desarrolla una teoría para estudiar el impacto del ruido de gradiente estocástico para SGD, especialmente para modelos de redes neuronales profundas
Los agentes pueden aprender a imitar únicamente demostraciones visuales (sin acciones) en el momento de la prueba después de aprender de su propia experiencia sin ningún tipo de supervisión en el momento de la formación.
Este artículo propone un enfoque para el aprendizaje visual de cero disparos mediante el aprendizaje de funciones de habilidad paramétricas.
Un artículo sobre la imitación de una tarea presentada justo durante la inferencia, donde el aprendizaje se realiza de forma autosupervisada y durante el entrenamiento el agente explora tareas relacionadas pero diferentes.
Propone un método para eludir el problema de la costosa demostración de expertos utilizando la exploración aleatoria de un agente para aprender habilidades generalizables que puedan aplicarse sin un entrenamiento previo específico
El artículo proporciona una descripción de un procedimiento para mejorar el modelo de espacio vectorial de palabras con una evaluación de los modelos Paragram y GloVe para las pruebas de similitud.
Este trabajo propone un nuevo algoritmo que ajusta los vectores de palabras de GloVe y luego utiliza una función de similitud no euclidiana entre ellos.
Los autores presentan observaciones sobre los puntos débiles de los modelos de espacio vectorial existentes y enumeran un enfoque de 6 pasos para perfeccionar los vectores de palabras existentes
Proponemos un nuevo método de cuantificación y lo aplicamos para cuantificar las RNN tanto para la compresión como para la aceleración
Este trabajo propone un método de cuantificación de múltiples bits para redes neuronales recurrentes.
Una técnica para cuantificar las matrices de pesos de las redes neuronales, y un procedimiento de optimización alternativo para estimar el conjunto de k vectores binarios y coeficientes que mejor representan el vector original.
Sustituimos las capas totalmente conectadas de una red neuronal por el ansatz de renormalización del enredo a escala múltiple, un tipo de operación cuántica que describe las correlaciones de largo alcance. 
En el artículo los autores sugieren utilizar la técnica de tensorización MERA para comprimir las redes neuronales.
Una nueva parametrización de los mapas lineales para su uso en redes neuronales, utilizando una factorización jerárquica del mapa lineal que reduce el número de parámetros, al tiempo que permite modelar interacciones relativamente complejas.
Estudios de compresión de las capas de avance utilizando descomposiciones tensoriales de bajo rango y explorando una descomposición tipo árbol
Presentamos un análisis de una red neuronal entrenada para eliminar la redundancia e identificar la estructura óptima de la red
Este trabajo propone un conjunto de heurísticas para identificar una buena arquitectura de red neuronal, basada en el PCA de las activaciones de las unidades sobre el conjunto de datos
Este artículo presenta un marco para optimizar las arquitecturas de las redes neuronales mediante la identificación de filtros redundantes en las capas
Llevamos a cabo el primer análisis de seguridad en profundidad de los ataques de huellas dactilares de la DNN que explotan los canales laterales de la caché, lo que representa un paso hacia la comprensión de la vulnerabilidad de la DNN a los ataques de canales laterales.
Este artículo considera el problema de la huella digital de las arquitecturas de redes neuronales utilizando los canales laterales de la caché, y discute las defensas de seguridad a través de la obscuridad.
Este trabajo realiza ataques de canal lateral de caché para extraer atributos de un modelo víctima e inferir su arquitectura, además de mostrar que pueden lograr una precisión de clasificación casi perfecta.
Proponemos el Entrenamiento de Objetivos Complementarios (COT), un nuevo paradigma de entrenamiento que optimiza tanto los objetivos primarios como los complementarios para aprender eficazmente los parámetros de las redes neuronales.
Considera la posibilidad de aumentar el objetivo de entropía cruzada con la maximización del objetivo de "complemento", que pretende neutralizar las probabilidades predichas de las clases distintas de las etiquetas de la verdad básica.
Los autores proponen un objetivo secundario para la minimización de softmax basado en la evaluación de la información obtenida de las clases incorrectas, lo que conduce a un nuevo enfoque de entrenamiento.
Trata del entrenamiento de redes neuronales para tareas de clasificación o generación de secuencias utilizando la pérdida de entropía cruzada
Estimación de la incertidumbre en una sola pasada hacia delante sin parámetros adicionales aprendibles.
Un nuevo método para calcular las estimaciones de incertidumbre de salida en las DNNs para problemas de clasificación que se ajusta a los métodos más avanzados para la estimación de la incertidumbre y los supera en las tareas de detección fuera de la distribución.
Los autores presentan el softmax inhibido, una modificación del softmax mediante la adición de una activación constante que proporciona una medida de incertidumbre. 
Hicimos un sistema rico en características para el aprendizaje profundo con entradas cifradas, produciendo salidas cifradas, preservando la privacidad.
Un marco para la inferencia de modelos privados de aprendizaje profundo utilizando esquemas FHE que admiten un bootstrapping rápido y, por tanto, pueden reducir el tiempo de cálculo.
El artículo presenta un medio para evaluar una red neuronal de forma segura utilizando el cifrado homomórfico.
Presentamos un sistema llamado GamePad para explorar la aplicación de métodos de aprendizaje automático a la demostración de teoremas en el asistente de pruebas Coq.
Este artículo describe un sistema para aplicar el aprendizaje automático a la demostración interactiva de teoremas, se centra en las tareas de predicción de tácticas y evaluación de posiciones, y muestra que un modelo neural supera a un SVM en ambas tareas.
Propone que se utilicen técnicas de aprendizaje automático para ayudar a construir pruebas en el prover de teoremas Coq.
En este trabajo, estudiamos el entrenamiento eficiente de redes cuantificadas por peso con pérdidas y gradiente cuantificado en un entorno distribuido, tanto teórica como empíricamente.
Este trabajo estudia las propiedades de convergencia de la cuantificación de pesos con pérdidas con diferentes precisiones de gradiente en el entorno distribuido, y proporciona un análisis de convergencia para la cuantificación de pesos con gradientes de precisión total, cuantificados y cuantificados recortados.
Los autores proponen un análisis del efecto de cuantificar simultáneamente los pesos y los gradientes en el entrenamiento de un modelo parametrizado en un entorno distribuido totalmente sincronizado.
Una estrategia de regularización para mejorar el rendimiento del aprendizaje secuencial
Un enfoque novedoso, basado en la regularización, para el problema del aprendizaje secuencial utilizando un modelo de tamaño fijo que añade términos adicionales a la pérdida, fomentando la dispersión de la representación y combatiendo el olvido catastrófico.
Este trabajo aborda el problema del olvido catastrófico en el aprendizaje permanente proponiendo estrategias de aprendizaje regularizadas
Una red neuronal sináptica con gráfico de sinapsis y aprendizaje que tiene la característica de conjugación topológica y distribución de Bose-Einstein en el espacio de sorpresa.  
Los autores proponen una red neuronal híbrida compuesta por un grafo de sinapsis que puede integrarse en una red neuronal estándar
Presenta un modelo de red neuronal de inspiración biológica basado en los canales iónicos excitatorios e inhibitorios de las membranas de las células reales
Modelos de incrustación de gráficos generalizados
Un enfoque de incrustación de grafos de conocimiento generalizado que aprende las incrustaciones basándose en tres objetivos simultáneos diferentes, y rinde a la par o incluso supera los enfoques existentes del estado del arte.
Aborda la tarea de aprender incrustaciones de grafos multirrelacionales mediante una red neuronal
Propone un nuevo método, GEN, para calcular incrustaciones de grafos multirelacionales, en particular que las llamadas E-Cells y R-Cells pueden responder a consultas de la forma (h,r,?),(?r,t), y (h,?,t)
El Aprendizaje Curricular Minimax es un método de enseñanza de máquinas que implica el aumento de la dureza deseable y la reducción programada de la diversidad.
 Un enfoque de aprendizaje curricular que utiliza una función de conjunto submodular que captura la diversidad de los ejemplos elegidos durante el entrenamiento. 
El artículo presenta el aprendizaje curricular MiniMax como un enfoque para el entrenamiento adaptativo de modelos proporcionándole diferentes subconjuntos de datos. 
Modelos implícitos aplicados a la causalidad y la genética
Los autores proponen utilizar el modelo implícito para abordar el problema de la asociación de todo el genoma.
Este trabajo propone soluciones para los problemas de los estudios de asociación de todo el genoma de confusión debido a la estructura de la población y la posible presencia de interacciones no lineales entre diferentes partes del genoma, y tiende un puente entre la genética estadística y el ML.
Presenta un modelo generativo no lineal para GWAS que modela la estructura de la población donde las no linealidades se modelan usando redes neuronales como aproximadores de funciones no lineales y la inferencia se realiza usando inferencia variacional sin verosimilitud
Aprendizaje de pocos disparos explotando la relación a nivel de objeto para aprender la relación a nivel de imagen (similitud)
Este artículo aborda el problema del aprendizaje de pocos disparos proponiendo un enfoque basado en la incrustación que aprende a comparar las características a nivel de objeto entre los ejemplos del conjunto de apoyo y de consulta
Propone un método de aprendizaje de pocas tomas que explota la relación a nivel de objeto entre las diferentes imágenes, basado en la búsqueda de vecinos cercanos, y concatena los mapas de características de dos imágenes de entrada en un solo mapa de características
Los investigadores que exploran las técnicas de procesamiento del lenguaje natural aplicadas al código fuente no utilizan ninguna forma de incrustación preentrenada, nosotros demostramos que deberían hacerlo.
Este artículo se propone comprender si el preentrenamiento de las incrustaciones de palabras para el código del lenguaje de programación mediante el uso de modelos de lenguaje similares a los de la PNL tiene un impacto en la tarea de resumen de código extremo.
Este trabajo muestra cómo el preentrenamiento de vectores de palabras utilizando corpus de código conduce a representaciones que son más adecuadas que las representaciones inicializadas y entrenadas al azar para la predicción de nombres de funciones/métodos
Resolvemos el cubo de Rubik con aprendizaje por refuerzo puro
Solución a la resolución del cubo de Rubik utilizando el aprendizaje por refuerzo (RL) con la búsqueda en árbol de Monte-Carlo (MCTS) a través de la iteración autodidáctica. 
Este trabajo resuelve el cubo de Rubik utilizando un método de iteración de política aproximada llamado iteración autodidáctica, superando el problema de las recompensas dispersas mediante la creación de su propio sistema de recompensas.
Introduce un algoritmo RL profundo para resolver el cubo de Rubik que maneja el enorme espacio de estados y la recompensa muy dispersa del cubo de Rubik
Describimos un modelo diferenciable de extremo a extremo para la GC que aprende a representar tramos de texto en la pregunta como denotaciones en el grafo de conocimiento, mediante el aprendizaje de módulos neuronales para la composición y la estructura sintáctica de la frase.
Este artículo presenta un modelo para responder a preguntas visuales que puede aprender tanto los parámetros como los predictores de estructura para una red neuronal modular, sin estructuras supervisadas ni asistencia de un analizador sintáctico.
Propone entrenar un modelo de respuesta a preguntas a partir de las respuestas únicamente y una KB mediante el aprendizaje de árboles latentes que capturan la sintaxis y aprenden la semántica de las palabras
Presentamos una novedosa infraestructura de compilación que resuelve las deficiencias de los marcos de aprendizaje profundo existentes.
Propuesta para pasar de la generación de código ad-hoc en los motores de aprendizaje profundo a las mejores prácticas de compiladores y lenguajes.
Este trabajo presenta un marco de compilación que permite definir lenguajes específicos de dominio para sistemas de aprendizaje profundo, y define etapas de compilación que pueden aprovechar optimizaciones estándar y optimizaciones especializadas para redes neuronales.
Este trabajo introduce un DLVM para aprovechar los aspectos del compilador de un tensor
Arquitectura basada en la atención para el aprendizaje del lenguaje mediante el aprendizaje por refuerzo en un nuevo entorno cuadriculado 2D personalizable  
El artículo aborda el problema de la navegación dada una instrucción y propone un enfoque para combinar la información textual y visual a través de un mecanismo de atención
Este trabajo considera el problema de seguir instrucciones en lenguaje natural dada una visión en primera persona de un entorno a priori desconocido, y propone un método de arquitectura neuronal.
Estudia el problema de la navegación hacia un objeto objetivo en un entorno de cuadrícula 2D siguiendo una descripción en lenguaje natural dada y recibiendo información visual como píxeles en bruto.
Una arquitectura simple, compuesta por convoluciones y atención, logra resultados a la par de los modelos recurrentes mejor documentados.
Un método de aumento de datos rápido y de alto rendimiento basado en la paráfrasis y un modelo de comprensión de lectura no recurrente que utiliza sólo convoluciones y atención.
Este trabajo propone aplicar CNNs+módulos de autoatención en lugar de LSTMs y mejorar el entrenamiento del modelo de RC con paráfrasis de pasajes generadas por un modelo neural de paráfrasis para mejorar el rendimiento de la RC.
Este trabajo presenta un modelo de comprensión lectora que utiliza convoluciones y atención y propone aumentar los datos de entrenamiento adicionales mediante la paráfrasis basada en la traducción automática neural.
Introducimos las CNN esféricas, una red convolucional para señales esféricas, y la aplicamos al reconocimiento de modelos 3D y a la regresión de la energía molecular.
El artículo propone un marco para construir redes convolucionales esféricas basado en una novedosa síntesis de varios conceptos existentes
Este trabajo se centra en cómo ampliar las redes neuronales convolucionales para que tengan invariancia esférica incorporada, y adapta herramientas del análisis armónico no abeliano para lograr este objetivo.
Los autores desarrollan un novedoso esquema de representación de datos esféricos desde la base
Un método para realizar el diseño automatizado de objetos del mundo real, como disipadores de calor y perfiles de alas, que hace uso de redes neuronales y descenso de gradiente.
Red neuronal (parametrización y predicción) y descenso de gradiente (back propogation) para el diseño automático de tareas de ingeniería. 
Este trabajo introduce el uso de una red profunda para aproximar el comportamiento de un sistema físico complejo, y luego diseñar dispositivos óptimos optimizando esta red con respecto a sus entradas.
 Proponemos una versión dual de la distancia adversarial logística para la alineación de características y mostramos que produce iteraciones de pasos de gradiente más estables que el objetivo min-max.
El documento aborda la fijación de las GAN a nivel computacional
Este trabajo estudia una formulación dual de una pérdida adversarial basada en un límite superior de la pérdida logística, y convierte el problema estándar min max de entrenamiento adversarial en un único problema de minimización.
Propone reformular el objetivo del punto de equilibrio de GAN (para un discriminador de regresión logística) como un problema de minimización mediante la dualización del objetivo de máxima probabilidad para la regresión logística regularizada
implementación de redes neuronales binarias con rendimiento computacional de última generación
El artículo presenta una biblioteca escrita en C/CUDA que cuenta con todas las funcionalidades necesarias para la propagación hacia delante de las BCNN
Este trabajo se basa en Binary-NET y lo amplía a las arquitecturas CNN, proporciona optimizaciones que mejoran la velocidad del paso hacia delante y ofrece un código optimizado para Binary CNN.
Proponemos un algoritmo de selección de subconjuntos que se puede entrenar con métodos basados en el gradiente, pero que logra un rendimiento casi óptimo mediante la optimización submodular.
Propone un modelo basado en redes neuronales que integra la función submodular combinando la técnica de optimización basada en el gradiente con el marco submodular denominado 'Differentiable Greedy Network' (DGN).
Propone una red neuronal que tiene como objetivo seleccionar un subconjunto de elementos (por ejemplo, seleccionar k frases que están mayormente relacionadas con una reclamación de un conjunto de documentos recuperados)
Introducimos el aprendizaje de representación jerárquica agrupada (HCRL), que optimiza simultáneamente el aprendizaje de representación y la agrupación jerárquica en el espacio de incrustación.
El documento propone utilizar el CRP anidado como modelo de agrupación en lugar de un modelo temático
Presenta un nuevo método de agrupación jerárquica sobre un espacio de incrustación en el que se aprenden simultáneamente el espacio de incrustación y la agrupación jerárquica
Desarrollamos un marco de aumento del aprendizaje estadístico-geométrico no supervisado para las redes neuronales profundas con el fin de hacerlas robustas a los ataques adversarios.
Transfiere las redes neuronales profundas tradicionales a calsificadores robustos adversarios utilizando GRNs
Propone una defensa basada en distribuciones de características condicionales de clase para convertir las redes neuronales profundas en clasificadores robustos
Hacer más eficiente el aprendizaje por refuerzo profundo en grandes espacios estado-acción utilizando la exploración estructurada con políticas jerárquicas profundas.
Un método para coordinar el comportamiento de los agentes mediante el uso de políticas que tienen una estructura latente compartida, un método de optimización de políticas variacional para optimizar las políticas coordinadas, y una derivación de la actualización jerárquica y variacional de los autores.
Este trabajo sugiere una innovación algorítmica consistente en variables latentes jerárquicas para la exploración coordinada en entornos multiagente
Aportamos muchas ideas sobre la generalización de las redes neuronales a partir del caso lineal teóricamente manejable.
Los autores estudian un modelo sencillo de redes lineales para comprender el aprendizaje de generalización y transferencia
La normalización por lotes mantiene la varianza del gradiente a lo largo del entrenamiento, estabilizando así la optimización.
Este trabajo analizó el efecto de la normalización de lotes en la retropropagación de gradiente en redes residuales
Los juicios del comportamiento humano se utilizan para obtener representaciones escasas e interpretables de los objetos que se generalizan a otras tareas
Este artículo describe un experimento a gran escala sobre las representaciones humanas de objetos/semáticas y un modelo de dichas representaciones.
Este trabajo desarrolla un nuevo sistema de representación de objetos a partir del entrenamiento en datos recogidos de juicios humanos de imágenes impares.
Un nuevo enfoque para aprender un espacio semántico escaso, positivo e interpretable que maximiza los juicios de similitud humana mediante el entrenamiento para maximizar específicamente la predicción de los juicios de similitud humana.
Proponemos un agente que se sitúa entre el usuario y un sistema de respuesta de preguntas de caja negra y que aprende a reformular las preguntas para obtener las mejores respuestas posibles
Este trabajo propone una respuesta activa a las preguntas mediante un enfoque de aprendizaje por refuerzo que aprende a reformular las preguntas de manera que ofrezcan las mejores respuestas posibles.
Describe claramente cómo los investigadores diseñaron y entrenaron activamente dos modelos para la reformulación de preguntas y la selección de respuestas durante los episodios de respuesta a preguntas
Aprendizaje de prebendas para autocodificadores adversariales
Propone una extensión sencilla de los autocodificadores adversariales para la generación de imágenes condicionales.
Se centra en los autocodificadores adversarios e introduce una red de generadores de código para transformar una prioridad simple en una que, junto con el generador, pueda ajustarse mejor a la distribución de los datos
Generación de texto utilizando incrustaciones de frases a partir de vectores de salto de pensamiento con la ayuda de redes adversariales generativas.
Describe la aplicación de las redes generativas adversariales para el modelado de datos textuales con la ayuda de vectores de pensamiento de esquí y los experimentos con diferentes sabores de GAN para dos conjuntos de datos diferentes.
Introduce una estimación de gradiente en línea, insesgada y fácilmente implementable para modelos recurrentes.
Los autores introducen un enfoque novedoso para el aprendizaje en línea de los parámetros de las redes neuronales recurrentes a partir de secuencias largas que supera la imitación de la retropropagación truncada a través del tiempo
Este trabajo aborda el entrenamiento en línea de las RNNs de una forma basada en principios, y propone una modificación de la RTRL y el uso de un enfoque hacia adelante para el cálculo del gradiente.
Superresolución de etiquetas gruesas en etiquetas a nivel de píxel, aplicada a imágenes aéreas y escaneos médicos.
Un método para superar las etiquetas de segmentación de baja resolución si se conoce la distribución conjunta de las etiquetas de baja y alta resolución.
Proponemos un método para alinear las características latentes aprendidas de diferentes conjuntos de datos utilizando correlaciones armónicas.
Propone utilizar las correspondencias de rasgos para preformar la alineación de colectores entre lotes de datos de las mismas muestras para evitar la recogida de mediciones ruidosas.
La evolución de la forma del cuerpo en agentes controlados por RL mejora su rendimiento (y ayuda al aprendizaje)
Algoritmo PEOM que incorpora el valor Shapley para acelerar la evolución identificando la contribución de cada parte del cuerpo
Dar forma a la recompensa con motivación intrínseca para evitar estados catastróficos y mitigar el olvido catastrófico.
Un algoritmo RL que combina el algoritmo DQN con un modelo de miedo entrenado en paralelo para predecir estados catastróficos.
El artículo estudia el olvido catastrófico en la RL, haciendo hincapié en las tareas en las que un DQN es capaz de aprender a evitar los eventos catastróficos siempre que evite el olvido.
Un nuevo operador de convolución para el aprendizaje automático de la representación dentro de la bola unitaria
Este trabajo está relacionado con los recientes artículos sobre la CNN esférica y la red equivariante SE(n) y extiende las ideas anteriores a los datos volumétricos en la bola unitaria.
Propone el uso de convoluciones volumétricas en redes convolutivas para el aprendizaje de la bola unitaria y discute la metodología y los resultados del proceso.
Entrenamos políticas de aprendizaje por refuerzo utilizando el aumento de la recompensa, el aprendizaje curricular y el metaaprendizaje para navegar con éxito por las páginas web.
Desarrolla un método de aprendizaje curricular para entrenar a un agente RL a navegar por una web, basado en la idea de descomponer una instrucción en múltiples sub-instrucciones.
Clasificación de textos en varias lenguas mediante codificación universal
Este artículo propone un enfoque para la clasificación de textos multilingües mediante el uso de corpus comparables.
Aprender incrustaciones multilingües y entrenar un clasificador utilizando datos etiquetados en la lengua de origen para abordar el aprendizaje de un categorizador de texto multilingüe sin información etiquetada en la lengua de destino
En este trabajo proponemos los autocodificadores recursivos internos-externos profundos (DIORA), un método totalmente no supervisado para descubrir la sintaxis mientras se aprenden simultáneamente representaciones para los constituyentes descubiertos. 
Un modelo neural de árbol latente entrenado con un objetivo de autocodificación que alcanza el estado del arte en el análisis sintáctico no supervisado de constituyentes y captura la estructura sintáctica mejor que otros modelos de árbol latente.
El artículo propone un modelo de análisis sintáctico de dependencias no supervisado (inducción de árboles latentes) que se basa en una combinación del algoritmo inside-outside con el modelado neuronal (autocodificadores recursivos). 
Investigamos el sesgo en el objetivo de meta-optimización de horizonte corto.
Este trabajo propone un modelo y un problema simplificados para demostrar el sesgo de horizonte corto de la meta-optimización de la tasa de aprendizaje.
Este trabajo estudia la cuestión de la retropropagación truncada para la meta-optimización a través de una serie de experimentos sobre un problema de juguete
una forma jerárquica y compositiva de generar subtítulos
Este artículo presenta un método más interpretable para el subtitulado de imágenes.
Desarrollamos una nueva medida de complejidad topológica para las redes neuronales profundas y demostramos que captura sus propiedades más destacadas.
Este trabajo propone la noción de persistencia neuronal, una medida topológica para asignar puntuaciones a las capas totalmente conectadas de una red neuronal.
El artículo propone analizar la complejidad de una red neuronal utilizando su homología persistente cero.
Observar los límites de la decisión en torno a una entrada le da más información que un pequeño barrio fijo
Los autores presentan un novedoso ataque para generar ejemplos adversos en el que atacan a los clasificadores creados clasificando aleatoriamente L2 pequeñas perturbaciones
Un nuevo enfoque para generar ataques adversos a una red neuronal, y un método para defender una red neuronal de esos ataques.
Entrenamos una red neuronal para que produzca pesos aproximadamente óptimos en función de los hiperparámetros.
Hiperredes para la optimización de hiperparámetros en redes neuronales.
Estimación de la matriz de covarianza de los activos financieros con modelos de variables latentes de proceso gaussiano
Ilustra cómo el modelo de variable latente de proceso gaussiano (GP-LVM) puede sustituir a los modelos clásicos de factores lineales para la estimación de las matrices de covarianza en los problemas de optimización de carteras.
Este trabajo utiliza GPLVMs estándar para modelar la estructura de covarianza y una representación de espacio latente de las series temporales financieras del S&P500, para optimizar las carteras y predecir los valores perdidos.
Este trabajo propone utilizar un GPLVM para modelar los rendimientos financieros
Introducimos el aprendizaje meta-adversarial, una nueva técnica para regularizar los GANs, y proponemos un método de entrenamiento controlando explícitamente la distribución de salida del discriminador.
El artículo propone un aprendizaje adversarial de regularización de la varianza para el entrenamiento de GANs con el fin de garantizar que el gradiente del generador no se desvanezca
Se puede utilizar un agente de aprendizaje por refuerzo profundo con ruido paramétrico añadido a sus pesos para ayudar a la exploración eficiente.
Este artículo presenta las NoisyNets, redes neuronales cuyos parámetros están perturbados por una función de ruido paramétrico, que obtienen una mejora sustancial del rendimiento respecto a los algoritmos de aprendizaje de refuerzo profundo de referencia.
Nuevo método de exploración para la RL profunda mediante la inyección de ruido en los pesos de las redes profundas, adoptando el ruido diversas formas
"Active Neural Localizer", una red neuronal totalmente diferenciable que aprende a localizar de forma eficiente utilizando el aprendizaje profundo por refuerzo.
Este trabajo formula el problema de localización en un mapa conocido utilizando una red de creencias como un problema de RL donde el objetivo del agente es minimizar el número de pasos para localizarse a sí mismo.
Este es un artículo claro e interesante que construye una red parametrizada para seleccionar acciones para un robot en un entorno simulado
Desarrollamos un algoritmo de entrenamiento para modelos de traducción automática no autorregresivos, que consigue una precisión comparable a la de los modelos base autorregresivos fuertes, pero un orden de magnitud más rápido en la inferencia.  
Destila el conocimiento de los estados ocultos intermedios y los pesos de atención para mejorar la traducción automática neuronal no autorregresiva.
Propone aprovechar el modelo autorregresivo bien entrenado para informar de los estados ocultos y la alineación de palabras de los modelos de traducción automática neuronal no autorregresiva.
Utilizando la operación mofológica (dilatación y erosión) hemos definido una clase de red que puede aproximar cualquier función continua. 
Este trabajo propone sustituir las unidades estándar RELU/tanh por una combinación de operaciones de dilatación y erosión, observando que el nuevo operador crea más hiperplanos y tiene más poder expresivo.
Los autores presentan Morph-Net, una red neuronal de una sola capa en la que el mapeo se realiza mediante dilatación y erosión morfológica.
Este trabajo avanza la compresión de la DNN más allá de los pesos a las activaciones integrando la poda de activación con la poda de pesos. 
Un método integral de compresión de modelos que maneja tanto la poda de pesos como la de activación, lo que conduce a un cálculo más eficiente de la red y a una reducción efectiva del número de multiplicaciones y acumulaciones.
Este artículo presenta un enfoque novedoso para reducir el coste computacional de las redes neuronales profundas integrando la poda de activación junto con la poda de pesos y muestra que las técnicas comunes de poda exclusiva de pesos aumentan el número de activaciones no nulas después de ReLU.
Proponemos un método sencillo para entrenar Auto Codificadores Variacionales (VAE) con representaciones latentes discretas, utilizando el muestreo de importancia
Introducción de una distribución de muestreo de importancia y uso de muestras de la distribución para calcular la estimación ponderada por importancia del gradiente
Este trabajo propone utilizar el muestreo importante para optimizar la VAE con variables latentes discretas.
Un nuevo algoritmo asíncrono distribuido de SGD que alcanza la precisión más avanzada en las arquitecturas existentes sin ningún ajuste o sobrecarga adicional.
Propone una mejora de los enfoques ASGD existentes a escala media utilizando el impulso con SGD para el entrenamiento asíncrono a través de un grupo de trabajadores distribuidos.
Este artículo aborda el problema del estancamiento del gradiente frente al rendimiento paralelo en el entrenamiento distribuido del aprendizaje profundo, y propone un enfoque para estimar los futuros parámetros del modelo en las esclavas para reducir los efectos de la latencia de la comunicación.
Proponemos un nuevo algoritmo de aprendizaje de redes neuronales profundas, que desbloquea la dependencia por capas de la retropropagación.
Un paradigma de entrenamiento alternativo para los DNIs en el que el módulo auxiliar se entrena para aproximarse directamente a la salida final del modelo original, ofreciendo beneficios secundarios.
Describe un método de entrenamiento de redes neuronales sin bloqueo de actualizaciones.
Proporciona una versión no sesgada de la retropropagación truncada mediante el muestreo de las longitudes de truncamiento y la reponderación correspondiente.
Propone métodos de determinación estocástica de los puntos de truncamiento en la retropropagación en el tiempo.
Una nueva aproximación a la retropropagación en el tiempo para superar las cargas computacionales y de memoria que surgen al tener que aprender de secuencias largas.
ataque no dirigido y dirigido a la GCN mediante la adición de nodos falsos
Los autores proponen una nueva técnica adversarial para añadir nodos "falsos" para engañar a un clasificador basado en GCN
Aprendizaje de transferencia para la secuencia a través del aprendizaje para alinear la información a nivel de célula a través de los dominios.
El artículo propone utilizar RNN/LSTM con alineación de colocación como método de aprendizaje de representación para el aprendizaje de transferencia/adaptación al dominio en PNL.
Formulamos la incertidumbre del modelo en el Aprendizaje por Refuerzo como un Proceso de Decisión de Markov continuo y adaptativo de Bayes y presentamos un método para la optimización práctica y escalable de la política bayesiana.
Utilizando un enfoque bayesiano, hay un mejor equilibrio entre la exploración y la explotación en RL
Argumentamos que los puntos de referencia de GAN deben requerir una gran muestra del modelo para penalizar la memorización e investigamos si las divergencias de la red neuronal tienen esta propiedad.
Los autores proponen un criterio para evaluar la calidad de las muestras producidas por una Red Generativa Adversarial.
generación de diálogos de dominio abierto con actos de diálogo
Los autores utilizan una técnica de supervisión a distancia para añadir etiquetas de actos de diálogo como factor condicionante para generar respuestas en diálogos de dominio abierto
El artículo describe una técnica para incorporar actos de diálogo en agentes conversacionales neuronales
Nuestra hipótesis es que, dados dos dominios, el mapeo de menor complejidad que tiene una discrepancia baja se aproxima al mapeo objetivo.
El artículo aborda el problema del aprendizaje de mapeos entre diferentes dominios sin ninguna supervisión, enunciando tres conjeturas.
Demuestra que en el aprendizaje no supervisado sobre datos no alineados es posible aprender el mapeo entre dominios utilizando sólo GAN sin una pérdida de reconstrucción.
Perfeccionamos los resultados de sobreaproximación de los verificadores incompletos utilizando solucionadores MILP para demostrar más propiedades de robustez que el estado del arte. 
Presenta un verificador que obtiene la mejora de la precisión de los verificadores incompletos y la escalabilidad de los verificadores completos utilizando la sobreparametrización, la programación lineal entera mixta y la relajación de la programación lineal.
Una estrategia mixta para obtener una mayor precisión en las verificaciones de robustez de las redes neuronales feed-forward con funciones de activación lineales a trozos, logrando una mayor precisión que los verificadores incompletos y una mayor escalabilidad que los verificadores completos.
¿Son los HMM un caso especial de las RNN? Investigamos una serie de transformaciones arquitectónicas entre los HMM y las RNN, tanto a través de derivaciones teóricas como de la hibridación empírica, y aportamos nuevas ideas.
Este artículo explora si los HMMs son un caso especial de RNNs utilizando el modelado del lenguaje y el etiquetado POS
Proponemos un novedoso método de regularización que penaliza la covarianza entre las dimensiones de las capas ocultas de una red.
Este trabajo presenta un mecanismo de regularización que penaliza la covarianza entre todas las dimensiones en la representación latente de una red neuronal para desentrañar la representación latente
El esquema propuesto imita el proceso de clasificación mediado por una serie de picking de un componente.
Un método para aumentar la precisión de las redes profundas en tareas de clasificación multiclase aparentemente mediante una reducción de la clasificación multiclase a binaria.
Un novedoso procedimiento de clasificación de discernimiento, respuesta máxima y comprobación múltiple para mejorar la precisión de las redes mediocres y mejorar las redes feedforward.
Empíricamente se muestra que los modelos más grandes se entrenan en menos pasos de entrenamiento, porque todos los factores en el recorrido del espacio de pesos mejoran.
Este trabajo muestra que las RNN más anchas mejoran la velocidad de convergencia cuando se aplican a problemas de PNL, y por extensión el efecto de aumentar las anchuras en las redes neuronales profundas sobre la convergencia de la optimización
Este artículo caracteriza el impacto de la sobreparametrización en el número de iteraciones que tarda un algoritmo en converger, y presenta otras observaciones empíricas sobre los efectos de la sobreparametrización en el entrenamiento de redes neuronales.
Redes de punteros de varias cabezas para aprender conjuntamente a localizar y reparar los fallos de uso de las variables
Propone un modelo basado en LSTM con punteros para descomponer el problema de VarMisuse en múltiples pasos.
Este artículo presenta un modelo basado en LSTM para la detección y reparación del fallo VarMisuse, y demuestra mejoras significativas en comparación con enfoques anteriores en varios conjuntos de datos.
Clustering de tipo humano con CNNs
El artículo valida la idea de que las redes neuronales convolucionales profundas podrían aprender a agrupar los datos de entrada mejor que otros métodos de agrupación al observar su capacidad para interpretar el contexto de cada punto de entrada gracias a un gran campo de visión.
Este trabajo combina el aprendizaje profundo para la representación de características con la tarea de agrupación no supervisada de tipo humano.
Desarrollamos dos algoritmos de complejidad lineal para la interpretación de modelos agnósticos basados en el valor de Shapley, en los escenarios en los que la contribución de las características al objetivo está bien aproximada por una factorización estructurada en grafos.
El artículo propone dos aproximaciones al valor de Shapley utilizado para generar puntuaciones de características para la interpretabilidad.
Este artículo propone dos métodos para la puntuación de la importancia de las características en función de la instancia utilizando los valores de Shapely, y proporciona dos métodos eficientes para calcular los valores de Shapely aproximados cuando existe una estructura conocida que relaciona las características.
Se han encontrado códigos locales en las redes neuronales feed-forward
Un método para determinar hasta qué punto las neuronas individuales de una capa oculta de un MLP codifican un código localista, que se estudia para diferentes representaciones de entrada.
Estudia el desarrollo de representaciones localistas en las capas ocultas de las redes neuronales feed-forward.
Ampliación de la modelización relacional para soportar datos multimodales mediante codificadores neuronales.
Este trabajo propone realizar la predicción de enlaces en las Bases de Conocimiento complementando las entidades originales con información multimodal, y presenta un modelo capaz de codificar todo tipo de información a la hora de puntuar las triplas.
El artículo trata sobre la incorporación de información de diferentes modalidades en los enfoques de predicción de enlaces
Proponemos un método novedoso que integra el muestreo SG-MCMC, el grupo sparse prior y la poda de la red para aprender el Sparse Structured Ensemble (SSE) con un rendimiento mejorado y un coste significativamente menor que los métodos tradicionales. 
Los autores proponen un procedimiento para generar un conjunto de modelos estructurados dispersos
Un nuevo marco para el entrenamiento de redes neuronales de conjunto que utiliza métodos SG-MCMC dentro del aprendizaje profundo, y luego aumenta la eficiencia computacional mediante sparsity+pruning de grupo.
Este trabajo explora el uso de FNN y LSTMs para hacer que el promedio de modelos bayesianos sea más factible computacionalmente y mejorar el rendimiento promedio del modelo.
Nuevo marco para el meta-aprendizaje que unifica y amplía una amplia clase de métodos de aprendizaje de pocos disparos existentes. Consigue un gran rendimiento en los puntos de referencia de aprendizaje de pocos disparos sin requerir la inferencia iterativa en tiempo de prueba.   
Este trabajo aborda el aprendizaje de pocos disparos desde el punto de vista de la inferencia probabilística, logrando el estado del arte a pesar de una configuración más simple que muchos competidores
Definición de una pérdida softmax parcialmente mutuamente excluyente para datos positivos y aplicación de un esquema de muestreo basado en la cooperación
Este trabajo presenta el Muestreo de Importancia Cooperativo para resolver el problema de la suposición mutuamente excluyente de que el softmax tradicional está sesgado cuando las muestras negativas no están explícitamente definidas
Este trabajo propone métodos PMES para relajar la suposición de resultado exclusivo en la pérdida softmax, demostrando el mérito empírico en la mejora de los modelos de incrustación del tipo word2vec.
Marco de trabajo profesor-estudiante para la clasificación eficiente de vídeos utilizando menos fotogramas 
El artículo propone una idea para destilar, a partir de un modelo de clasificación de vídeo completo, un pequeño modelo que sólo recibe un número menor de fotogramas.
Los autores presentan una red profesor-alumno para resolver el problema de clasificación de vídeos, proponiendo algoritmos de entrenamiento en serie y en paralelo con el fin de reducir los costes computacionales.
Una visión estadística unificada de la amplia clase de modelos generativos profundos 
El artículo desarrolla un marco que interpreta los algoritmos GAN como una forma de inferencia variacional sobre un modelo generativo que reconstruye una variable indicadora de si una muestra pertenece a la verdadera de las distribuciones generativas de datos.
un método que combina el aprendizaje de listas de reglas y el aprendizaje de prototipos 
Presenta un nuevo marco de predicción interpretable, que combina el aprendizaje basado en reglas, el aprendizaje de prototipos y las NN, que es particularmente aplicable a los datos longitudinales.
Este trabajo tiene como objetivo abordar la falta de interpretabilidad de los modelos de aprendizaje profundo, y proponer Prototype lEArning via Rule Lists (PEARL), que combina el aprendizaje de reglas y el aprendizaje de prototipos para lograr una clasificación más precisa y hace que la tarea de interpretabilidad sea más sencilla.
Este trabajo propone una nueva Red Adversarial Generativa que es más estable, más eficiente y produce mejores imágenes que las del status-quo 
Este documento combina Fisher-GAN y Deli-GAN
Este trabajo combina Deli-GAN, que tiene una distribución previa de mezcla en el espacio latente, y Fisher GAN, que utiliza Fisher IPM en lugar de JSD como objetivo.
Presentamos una arquitectura de red modular multisensor con un mecanismo de atención que permite la selección dinámica de sensores en datos ruidosos del mundo real de CHiME-3.
Una arquitectura neuronal genérica capaz de aprender la atención que debe prestarse a los distintos canales de entrada en función de la calidad relativa de cada sensor con respecto a los demás.
 Considera el uso de la atención para la selección de sensores o canales con resultados en TIDIGITS y GRID que muestran un beneficio de la atención sobre la concatenación de características.
Para permitir el entrenamiento de las DNN en la nube y proteger simultáneamente la privacidad de los datos, proponemos aprovechar las representaciones de datos intermedias, lo que se consigue dividiendo las DNN y desplegándolas por separado en plataformas locales y en la nube.
Este trabajo propone una técnica para privatizar los datos mediante el aprendizaje de una representación de características que es difícil de utilizar para la reconstrucción de imágenes, pero útil para la clasificación de las mismas.
GANs recurrentes condicionales para la generación de secuencias médicas de valor real, mostrando nuevos enfoques de evaluación y un análisis empírico de la privacidad.
Propone el uso de datos sintéticos generados por GANs como reemplazo de los datos de identificación personal en el entrenamiento de modelos ML para aplicaciones sensibles a la privacidad
Los autores proponen una nueva arquitectura GAN recurrente que genera secuencias de dominio continuo, y la evalúan en varias tareas sintéticas y en una tarea de datos de series temporales de la UCI.
Propone utilizar RGANs y RCGANs para generar secuencias sintéticas de datos reales.
Nuestros estudios y modelos empíricos aportan información nueva y valiosa para los diseñadores que desean comprender y controlar cómo perciben los usuarios los efectos del énfasis
Este artículo examina qué tipo de resaltado visual se percibe más rápidamente en la visualización de datos y cómo se comparan los distintos métodos de resaltado entre sí
Dos estudios sobre la eficacia de los efectos de énfasis, uno que evalúa los niveles de diferencias útiles, y otro más aplicado que utiliza visualizaciones reales diferentes para una investigación más válida desde el punto de vista ecológico.
Una arquitectura de razonamiento sencilla basada en la red de memoria (MemNN) y la red de relaciones (RN), que reduce la complejidad temporal en comparación con la RN y logra un resultado de vanguardia en la GC basada en la historia bAbI y el diálogo bAbI.
Introduce la Red de Memoria Relacionada (RMN), una mejora de las Redes de Relación (RN).
Demostramos que dividir una red neuronal en ramas paralelas mejora el rendimiento y que el acoplamiento adecuado de las ramas mejora aún más el rendimiento.
El trabajo propone una reconfiguración del modelo de CNN existente en el estado del arte utilizando una nueva arquitectura de ramificación, con un mejor rendimiento.
Este documento muestra las ventajas de ahorro de parámetros del ensamblaje acoplado.
Presenta una arquitectura de red profunda que procesa los datos utilizando múltiples ramas paralelas y combina las posteriores de estas ramas para calcular las puntuaciones finales.
Combina la inyección de ruido, la cuantificación gradual y el aprendizaje de sujeción de la activación para lograr una cuantificación de 3,4 y 5 bits de última generación
Propone inyectar ruido durante el entrenamiento y sujetar los valores de los parámetros en una capa, así como la salida de activación en la cuantificación de la red neuronal.
Un método para la cuantificación de redes neuronales profundas para la clasificación y la regresión, utilizando la inyección de ruido, la sujeción con activaciones máximas aprendidas y la cuantificación gradual de bloques para obtener un rendimiento igual o mejor que los métodos del estado de la técnica.
Proponemos Leap, un marco que transfiere el conocimiento a través de los procesos de aprendizaje minimizando la distancia esperada que el proceso de formación recorre en la superficie de pérdida de una tarea.
El artículo propone un nuevo objetivo de meta-aprendizaje para superar los enfoques más avanzados cuando se trata de colecciones de tareas que presentan una diversidad sustancial entre las tareas
Una alternativa al aprendizaje por transferencia que aprende más rápido, requiere muchos menos parámetros (3-13 %), suele conseguir mejores resultados y conserva con precisión el rendimiento en tareas antiguas.
Módulos de control para el aprendizaje incremental en conjuntos de datos de clasificación de imágenes
Presentamos una técnica general para la inferencia de baja precisión de 8 bits de las redes neuronales convolucionales. 
Este trabajo diseña un sistema para cuantificar automáticamente los modelos preentrenados de la CNN
Proponemos incorporar sesgos inductivos y operaciones procedentes de la geometría hiperbólica para mejorar el mecanismo de atención de las redes neuronales.
Este trabajo sustituye la similitud punto-producto utilizada en los mecanismos de atención por la distancia hiperbólica negativa, y la aplica al modelo Transformer existente, a las redes de atención gráfica y a las redes de relación
Los autores proponen un enfoque novedoso para mejorar la atención relacional cambiando las funciones de emparejamiento y agregación para utilizar la geometría hiperbólica. 
Este artículo demuestra cómo la teoría de control H-infinito puede ayudar a diseñar mejor las políticas profundas robustas para los taks de los motores de los robots
Propone incorporar elementos de control robusto a la investigación de la política guiada para concebir un método resistente a las perturbaciones y al desajuste del modelo.
El artículo presenta un método para evaluar la sensibilidad y robustez de las políticas de RL profunda, y propone un enfoque de juego dinámico para el aprendizaje de políticas robustas.
Análisis de la vulnerabilidad de los clasificadores a las perturbaciones universales y relación con la curvatura de la frontera de decisión.
El artículo proporciona un interesante análisis que relaciona la geometría de los límites de decisión del clasificador con pequeñas perturbaciones adversarias universales.
Este artículo analiza las perturbaciones universales, es decir, las perturbaciones que pueden inducir a error a un clasificador entrenado si se añaden a la mayoría de los puntos de datos de entrada.
El artículo desarrolla modelos que intentan explicar la existencia de perturbaciones universales que engañan a las redes neuronales
Proponemos un método de meta-aprendizaje para la corrección interactiva de políticas con lenguaje natural.
Este artículo proporciona un marco de metaaprendizaje que muestra cómo aprender nuevas tareas en una configuración interactiva. Cada tarea se aprende a través de una configuración de aprendizaje por refuerzo, y luego la tarea se actualiza mediante la observación de nuevas instrucciones.
Este trabajo enseña a los agentes a completar tareas mediante instrucciones en lenguaje natural en un proceso iterativo.
Investigamos la modularidad de los modelos generativos profundos.
El artículo proporciona una forma de investigar la estructura modular del modelo generativo profundo, con el concepto clave de distribuir sobre canales de arquitecturas generadoras.
Presentamos Seq2SQL, que traduce las preguntas a consultas SQL utilizando las recompensas de la ejecución de las consultas en línea, y WikiSQL, un conjunto de datos de tablas/preguntas/consultas SQL órdenes de magnitud mayores que los conjuntos de datos existentes.
Un nuevo conjunto de datos de análisis sintáctico que se centra en la generación de SQL a partir del lenguaje natural mediante un modelo basado en el aprendizaje por refuerzo
El modelado de ruido en la entrada durante el entrenamiento discriminativo mejora la robustez adversarial. Proponer una métrica de evaluación de la robustez adversarial basada en el PCA
Este trabajo propone, ExL, un método de entrenamiento adversarial que utiliza ruido multiplicador y que se muestra útil para defenderse de los ataques de caja negra en tres conjuntos de datos.
Este trabajo incluye el ruido multiplicativo N en los datos de entrenamiento para lograr la robustez adversarial, cuando se entrena tanto en los parámetros del modelo theta como en el propio ruido.
Un método para responder a "¿por qué no la clase B?" para explicar las redes profundas
El artículo propone un enfoque para proporcionar explicaciones visuales contrastivas para las redes neuronales profundas.
Analizamos la invertibilidad de las redes neuronales profundas estudiando las preimágenes de las capas ReLU y la estabilidad de la inversa.
Este trabajo estudia el volumen de preimagen de la activación de una red ReLU en una determinada capa, y se basa en la linealidad a trozos de la función de avance de una red ReLU. 
Este trabajo presenta un análisis de la invariabilidad inversa de las redes ReLU y proporciona límites superiores a los valores singulares de una red de trenes.
El entrenamiento adversarial de los conjuntos proporciona una robustez frente a los ejemplos adversarios mayor que la observada en los modelos entrenados adversariamente y en los conjuntos entrenados independientemente.
 Propone entrenar un conjunto de modelos de forma conjunta, donde en cada paso de tiempo se incorpora al aprendizaje un conjunto de ejemplos adversos para el propio conjunto.
redes de enrutamiento: un nuevo tipo de red neuronal que aprende a enrutar adaptativamente su entrada para el aprendizaje multitarea
El documento sugiere utilizar una red modular con un controlador que tome decisiones, en cada paso de tiempo, sobre el siguiente nódulo a aplicar.
El artículo presenta una formulación novedosa para el aprendizaje de la arquitectura óptima de una red neuronal en un marco de aprendizaje multitarea mediante el uso del aprendizaje de refuerzo multiagente para encontrar una política, y muestra la mejora con respecto a las arquitecturas codificadas con capas compartidas.
Mostramos cómo optimizar la norma L_0 esperada de los modelos paramétricos con el descenso de gradiente e introducimos una nueva distribución que facilita el hard gating.
Los autores introducen un enfoque basado en el gradiente para minimizar una función objetivo con una penalización L0 dispersa para ayudar a aprender redes neuronales dispersas
Proponemos una nueva arquitectura de red neuronal gráfica interpretable basada en la atención que supera a las redes neuronales gráficas actuales en conjuntos de datos de referencia estándar
Los autores proponen dos extensiones de las GCN, eliminando las no linealidades intermedias del cálculo de las GCN y añadiendo un mecanismo de atención en la capa de agregación.
El artículo propone un algoritmo de aprendizaje semi-supervisado para la clasificación de nodos de grafos, inspirado en las redes neuronales de grafos.
Un marco para el entrenamiento de modelos generativos basados en autoencoders, con pérdidas no adversariales y arquitecturas de redes neuronales no restringidas.
Este trabajo utiliza autocodificadores para realizar la correspondencia de distribuciones en un espacio de alta dimensión.
Los espacios de incrustación de colectores de productos con curvatura heterogénea ofrecen representaciones mejoradas en comparación con los espacios de incrustación tradicionales para una variedad de estructuras.
Propone un método de reducción de la dimensionalidad que incrusta los datos en una variedad de productos de variedades esféricas, euclidianas e hiperbólicas. El algoritmo se basa en hacer coincidir las distancias geodésicas en el colector producto con las distancias de los gráficos.
Integramos métodos simbólicos (deductivos) y estadísticos (basados en la neurona) para permitir la síntesis de programas en tiempo real con una generalización casi perfecta a partir de un ejemplo de entrada-salida.
El artículo presenta un enfoque de branch-and-bound para aprender buenos programas en el que se utiliza un LSTM para predecir qué ramas del árbol de búsqueda deberían conducir a buenos programas
Propone un sistema que sintetiza programas a partir de un único ejemplo que generaliza mejor que el estado de la técnica anterior
Exploramos la intersección entre las VAE y la codificación dispersa.
Este trabajo propone una extensión de los VAE con priores y posteriors dispersos para aprender representaciones interpretables dispersas.
La eliminación progresiva de las conexiones de salto de una manera principista evita la degradación en las redes profundas feed-forward.
Los autores presentan una nueva estrategia de entrenamiento, VAN, para entrenar redes feed-forward muy profundas sin conexiones de salto
El artículo introduce una arquitectura que interpola linealmente entre ResNets y redes profundas de vainilla sin conexiones de salto.
Compresión de redes neuronales profundas desplegadas en dispositivos embebidos. 
Los autores presentan un algoritmo de entrenamiento basado en el SVRG regularizado l-1 que es capaz de forzar que muchos pesos de la red sean 0.
Este trabajo reduce los requisitos de memoria.
Un algoritmo de aprendizaje basado en la codificación predictiva para construir modelos de redes neuronales profundas del cerebro
El documento considera el aprendizaje de una red neuronal generativa utilizando una configuración de codificación predictiva
El reconocimiento de instancias de objetos con autocodificadores adversariales se realizó con un nuevo objetivo de "imagen mental" que es una representación canónica de la imagen de entrada.
El artículo propone un método para aprender características para el reconocimiento de objetos que es invariante a varias transformaciones del objeto, sobre todo la pose del objeto.
Este trabajo investigó la tarea de reconocimiento de unos pocos disparos a través de una imagen mental generada como representación intermedia dada la imagen de entrada.
Combinar la lógica temporal con el aprendizaje por refuerzo jerárquico para la composición de habilidades
El artículo ofrece una estrategia para construir un MDP de producto a partir de un MDP original y el autómata asociado a una fórmula LTL.
Propone unir la lógica temporal con el aprendizaje de refuerzo jerárquico para simplificar la composición de habilidades.
Proponemos un esquema de cuantificación para los pesos y las activaciones de las redes neuronales profundas. Esto reduce la huella de memoria sustancialmente y acelera la inferencia.
Compresión de modelos CNN y aceleración de la inferencia mediante cuantificación.
Cuando un robot se despliega en un entorno en el que los humanos han actuado, el estado del entorno ya está optimizado para lo que los humanos quieren, y podemos utilizarlo para inferir las preferencias humanas.
Los autores proponen aumentar la función de recompensa explícita de un agente RL con recompensas/costes auxiliares inferidos del estado inicial y un modelo de la dinámica del estado
Este trabajo propone una forma de inferir la información implícita en el estado inicial utilizando IRL y combinar la recompensa inferida con una recompensa especificada.
Categorización sistemática de los métodos de regularización para el aprendizaje profundo, revelando sus similitudes.
Intenta construir una taxonomía para las técnicas de regularización empleadas en el aprendizaje profundo.
Demostramos la eficiencia exponencial de las redes neuronales de tipo recurrente sobre las redes superficiales.
Los autores comparan la complejidad de las redes de tren tensorial con las redes estructuradas por descomposición CP
Un novedoso tratamiento probabilístico para GAN con garantía teórica.
Este trabajo propone un GAN bayesiano que tiene garantías teóricas de convergencia a la distribución real y pone verosimilitudes sobre el generador y el discriminador con logaritmos proporcionales a las funciones objetivo del GAN tradicional.
Defensa contra las perturbaciones adversas de las redes neuronales a partir de la suposición de la matriz 
El manuscrito propone dos funciones objetivo basadas en la suposición del múltiple como mecanismos de defensa contra los ejemplos adversos.
Defensa contra ataques adversarios basados en la suposición de datos naturales
búsqueda de arquitecturas neuronales de una sola vez mediante la optimización directa de la dispersión
Presenta un método de búsqueda de arquitectura en el que se eliminan las conexiones con regularización dispersa.
Este trabajo propone la Optimización Directa Sparse, que es un método para obtener arquitecturas neuronales en problemas específicos, a un coste computacional razonable.
Este trabajo propone un método de búsqueda de arquitecturas neuronales basado en una optimización directa dispersa
Obtiene la precisión más avanzada para redes cuantificadas y poco profundas aprovechando la destilación. 
Propone modelos pequeños y de bajo coste combinando destilación y cuantificación para experimentos de visión y traducción automática neural
Este trabajo presenta un marco de uso del modelo de maestro para ayudar a la compresión para el modelo de aprendizaje profundo en el contexto de la compresión del modelo.
mejorar la NMT con árboles latentes
Este artículo describe un método para inducir estructuras de dependencia del lado de la fuente al servicio de la traducción automática neural.
Aprenda trabajando hacia atrás a partir de una sola demostración, incluso una ineficiente, y haga que el agente haga progresivamente más de la resolución por sí mismo.
Este trabajo presenta un método para aumentar la eficiencia de los métodos RL de recompensa dispersa a través de un currículo hacia atrás en las demostraciones de los expertos. 
El artículo presenta una estrategia para resolver tareas de recompensa dispersas con RL mediante el muestreo de estados iniciales a partir de demostraciones.
Memoria externa para el aprendizaje de refuerzo en línea basado en la estimación de gradientes sobre una novedosa técnica de muestreo de depósitos.
El artículo propone un enfoque modificado de la RL, en el que el agente mantiene una "memoria episódica" adicional y utiliza una "red de consulta" que se basa en el estado actual.
Conseguimos la descomposición sesgo-varianza para las máquinas de Boltzmann utilizando una formulación geométrica de la información.
El objetivo de este trabajo es analizar la eficacia y la generalizabilidad del aprendizaje profundo presentando un análisis teórico de la descomposición sesgo-varianza para modelos jerárquicos, concretamente las máquinas de Boltzmann  
El documento llega a la conclusión principal de que es posible reducir tanto el sesgo como la varianza en un modelo jerárquico.
Combinando la poda de la red y los núcleos persistentes en una implementación de red práctica, rápida y precisa.
Este trabajo introduce las RNN dispersas persistentes, un mecanismo para añadir la poda al trabajo existente de almacenar los pesos de las RNN en un chip.
Presentamos un nuevo método de poda y un formato de matriz dispersa para permitir un alto índice de compresión y un proceso de decodificación de índices en paralelo.
Los autores utilizan la codificación Viterbi para comprimir drásticamente el índice de la matriz dispersa de un grafo podado, reduciendo una de las principales cargas de memoria y acelerando la inferencia en el entorno paralelo.
Una novedosa red política jerárquica que puede reutilizar habilidades previamente aprendidas junto con y como subcomponentes de nuevas habilidades descubriendo las relaciones subyacentes entre habilidades.
El objetivo de este trabajo es aprender políticas jerárquicas utilizando una estructura de política recursiva regulada por una gramática temporal estocástica
Este trabajo propone un enfoque para el aprendizaje de políticas jerárquicas en un contexto de aprendizaje permanente mediante el apilamiento de políticas y el uso de una política explícita de "cambio".
Proponemos utilizar la proyección de fórmulas algebraicas vectoriales explícitas como una forma alternativa de visualizar los espacios de incrustación específicamente adaptados a las tareas de análisis orientadas a objetivos y supera al t-SNE en nuestro estudio de usuarios.
Análisis de los espacios de incrustación de forma no paramétrica (basada en ejemplos_)
Proponemos un enfoque basado en principios que dota a los clasificadores de la capacidad de resistir las grandes variaciones entre los datos de entrenamiento y los de prueba de una manera inteligente y eficiente.
Utilización del aprendizaje introspectivo para gestionar las variaciones de los datos en el momento de la prueba
Este trabajo sugiere el uso de redes de transformación aprendidas, incrustadas dentro de redes introspectivas para mejorar el rendimiento de la clasificación con ejemplos sintetizados.
La discretización de la entrada permite la robustez frente a los ejemplos adversos
Los autores presentan un estudio en profundidad de la discretización/cuantificación de la entrada como defensa contra los ejemplos adversos
demostramos límites independientes de la dimensión para algoritmos de entrenamiento de baja precisión
Este artículo analiza las condiciones en las que la convergencia de los modelos de entrenamiento con pesos de baja precisión no depende de la dimensión del modelo.
Modificaciones en MAML y RL2 que deberían permitir una mejor exploración. 
El artículo propone un truco de ampliación de las funciones objetivo para impulsar la exploración en meta-RL sobre dos algoritmos meta-RL recientes
Un modelo simbólico neuronal probabilístico con un espacio programático latente, para responder a preguntas más interpretables
Este trabajo propone un modelo discreto y estructurado de variables latentes para la respuesta a preguntas visuales que implica la generalización y el razonamiento composicional con una ganancia significativa en el rendimiento y la capacidad.
Utilizamos la verificación formal para evaluar la eficacia de las técnicas para encontrar ejemplos adversos o para defenderse de ellos.
Este trabajo propone un método para calcular ejemplos adversarios con una distancia mínima a las entradas originales.
Los autores proponen emplear ejemplos de distancia mínima demostrable como herramienta para evaluar la robustez de una red entrenada.
El documento describe un método para generar ejemplos adversarios que tienen una distancia mínima con el ejemplo de entrenamiento utilizado para generarlos
El recuperador de párrafos y el lector automático interactúan entre sí mediante el aprendizaje por refuerzo para obtener grandes mejoras en conjuntos de datos de dominio abierto
El artículo introduce un nuevo marco de interacción bidireccional entre el recuperador de documentos y el lector para la respuesta a preguntas de dominio abierto con la idea del "estado del lector" del lector al recuperador.
El artículo propone un modelo de lectura automática extractiva de varios documentos, compuesto por 3 partes distintas y un algoritmo.
Presenta una nueva arquitectura que aprovecha el poder de globalización de la información de las u-nets en una red más profunda y que se desempeña bien en todas las tareas sin ningún tipo de campanas y silbatos.
Una arquitectura de red para la segmentación semántica de imágenes, basada en la composición de una pila de arquitecturas U-Net básicas, que reduce el número de parámetros y mejora los resultados.
Se propone una arquitectura U-Net apilada para la segmentación de imágenes.
Proponemos una red neuronal capaz de generar preguntas sobre temas específicos.
Presenta un enfoque basado en redes neuronales para generar preguntas sobre temas específicos con la motivación de que las preguntas sobre temas son más significativas en las aplicaciones prácticas.
Propone un método de generación basado en temas utilizando un LSTM para extraer temas mediante una técnica de codificación en dos etapas
Implementamos una red de adaptación de dominio adversarial para estabilizar una interfaz cerebro-máquina fija frente a cambios graduales en las señales neuronales registradas.
Describe un nuevo enfoque para la interfaz cerebro-máquina implantada con el fin de abordar el problema de la calibración y el cambio de covarianza. 
Los autores definen un IMC que utiliza un autocodificador y luego abordan el problema de la deriva de los datos en el IMC.
Analizar y comprender cómo los agentes de las redes neuronales aprenden a comprender un lenguaje sencillo en tierra
Los autores conectan los métodos experimentales psicológicos para entender cómo la caja negra de los métodos de aprendizaje profundo resuelve los problemas.
Este trabajo presenta un análisis de los agentes que aprenden el lenguaje de base mediante el aprendizaje por refuerzo en un entorno sencillo que combina la instrucción verbal con la información visual
Aprender las partes del objeto, la estructura jerárquica y la dinámica observando cómo se mueven
Propone un modelo de aprendizaje no supervisado que aprende a desentrañar los objetos en partes, predecir la estructura jerárquica de las partes y, basándose en las partes desentrañadas y la jerarquía, predecir el movimiento.
Construimos una comprensión de las técnicas de eficiencia de recursos en Super-Resolución
El artículo propone una evaluación empírica detallada de las compensaciones logradas por varias redes neuronales convolucionales en el problema de la superresolución.
Este trabajo propone mejorar la eficiencia de los recursos del sistema para las redes de superresolución.
Investigamos las redes ReLU en el dominio de Fourier y demostramos un comportamiento peculiar.
Análisis de Fourier de la red ReLU, encontrando que están sesgados hacia el aprendizaje de baja frecuencia 
Este trabajo tiene aportaciones teóricas y empíricas sobre el tema de los coeficientes de Fourier de las redes neuronales
El artículo propone utilizar distribuciones de probabilidad en lugar de puntos para tareas de incrustación de instancias como el reconocimiento y la verificación.
El artículo propone una alternativa a la incrustación de puntos actual y una técnica para entrenarlos.
El trabajo propone un modelo que utiliza incertidumbres-embeddings para extender el aprendizaje profundo a las aplicaciones bayesianas
Proponemos capas de contracción tensorial y de regresión tensorial de bajo rango para preservar y aprovechar la estructura multilineal en toda la red, lo que supone un enorme ahorro de espacio con un impacto mínimo en el rendimiento.
Este trabajo propone nuevas arquitecturas de capas de redes neuronales que utilizan una representación de bajo rango de los tensores
Este trabajo incorpora la descomposición tensorial y la regresión tensorial en la CNN utilizando una nueva capa de regresión tensorial.
Utilizamos diccionarios bilingües para aumentar los datos de la traducción automática neural
Este artículo investiga el uso de diccionarios bilingües para crear fuentes sintéticas para los datos monolingües del lado de destino con el fin de mejorar los modelos NMT entrenados con pequeñas cantidades de datos paralelos.
Proponemos un novedoso modelo de curiosidad basado en la memoria episódica y en las ideas de alcanzabilidad que nos permite superar los conocidos problemas de "couch-potato" de los trabajos anteriores.
Propone dar bonificaciones de exploración en los algoritmos de RL dando mayores bonificaciones a las observaciones que son padre en los pasos del entorno.
Los autores proponen un bono de exploración que tiene como objetivo ayudar en los problemas de RL de recompensa dispersa y considera muchos experimentos en entornos 3D complejos
Introducimos un nuevo conjunto de datos de vinculaciones lógicas con el fin de medir la capacidad de los modelos para capturar y explotar la estructura de las expresiones lógicas frente a una tarea de predicción de vinculaciones.
El artículo propone un nuevo modelo para utilizar modelos profundos para detectar la vinculación lógica como un producto de funciones continuas sobre mundos posibles.
Propone un nuevo modelo diseñado para el aprendizaje automático con predicción de vinculación lógica.
El artículo trata de una nueva metodología de eficiencia energética para el aprendizaje incremental
Propone el procedimiento de aprendizaje incremental como aprendizaje por transferencia.
El artículo presenta un método para entrenar redes neuronales convolucionales profundas de forma incremental, en el que los datos están disponibles en pequeños lotes durante un período de tiempo.
Presenta una aproximación al aprendizaje incremental de clase utilizando redes profundas proponiendo tres estrategias de aprendizaje diferentes en el enfoque final/mejor.
utilizar la exploración paralela para paralelizar las redes neuronales recurrentes lineales. entrenar el modelo con una longitud de 1 millón de dependencias
Propone acelerar la RNN aplicando el método de Blelloch.
Los autores proponen un algoritmo paralelo para RNNs Lineales Sustitutos, que produce mejoras de velocidad sobre las implementaciones existentes de Quasi-RNN, SRU y LSTM.
GAN de lenguaje natural para rellenar el espacio en blanco
Este trabajo propone generar texto utilizando GANs.
Generación de muestras de texto mediante GAN y un mecanismo para rellenar las palabras que faltan condicionado por el texto circundante
Comparación de representaciones de textura psicofísicas y codificadas por CNN en una aplicación de detección de novedades con una red neuronal de una clase.
Este artículo se centra en la detección de novedades y muestra que las representaciones psicofísicas pueden superar a las características del codificador VGG en alguna parte de esta tarea
Este trabajo considera la detección de anomalías en las texturas y propone una función de pérdida original.
Propone entrenar dos detectores de anomalías a partir de tres modelos diferentes para detectar anomalías perceptivas en texturas visuales.
Proponemos un nuevo tipo de enfoque de regularización que fomenta el no solapamiento en el aprendizaje de la representación, en aras de mejorar la interpretabilidad y reducir el sobreajuste.
El artículo introduce un regularizador matricial para inducir simultáneamente tanto la dispersión como la ortogonalidad aproximada.
El artículo estudia un método de regularización para promover la dispersión y reducir el solapamiento entre los soportes de los vectores de peso en las representaciones aprendidas para mejorar la interpretabilidad y evitar el sobreajuste
El artículo propone un nuevo enfoque de regularización que fomenta simultáneamente que los vectores de peso (W) sean dispersos y ortogonales entre sí.
Demuestra que los mecanismos de compuerta proporcionan invariabilidad a las transformaciones temporales. Introduce y prueba una nueva inicialización para las LSTMs a partir de esta idea.
El artículo relaciona el diseño de las redes recurrentes y su efecto en la forma en que la red reacciona a las transformaciones del tiempo, y lo utiliza para desarrollar un sencillo esquema de inicialización del sesgo.
Proponemos y comprobamos la eficacia de aprender a enseñar, un nuevo marco para guiar automáticamente el proceso de aprendizaje automático.
Este artículo se centra en la "enseñanza automática" y propone aprovechar el aprendizaje por refuerzo definiendo la recompensa como la rapidez con la que aprende el alumno y utilizando el gradiente de la política para actualizar los parámetros del profesor
Los autores definen un modelo de aprendizaje profundo compuesto por cuatro componentes: un modelo de alumno, un modelo de profesor, una función de pérdida y un conjunto de datos. 
Sugiere un marco de "aprender a enseñar", correspondiente a las opciones sobre los datos que se presentan al alumno.
Una pérdida diferenciable para las restricciones lógicas de entrenamiento y consulta de las redes neuronales.
Un marco para convertir las consultas sobre parámetros y pares de entrada y salida de las redes neuronales en funciones de pérdida diferenciables y un lenguaje declarativo asociado para especificar estas consultas
Este trabajo aborda el problema de combinar enfoques lógicos con redes neuronales traduciendo una fórmula lógica en una función de pérdida no negativa para una red neuronal.
Enfoque basado en algoritmos genéticos para optimizar las políticas de las redes neuronales profundas
Los autores presentan un algoritmo para el entrenamiento de conjuntos de redes de políticas que mezcla regularmente diferentes políticas del conjunto.
Este trabajo propone un método de optimización de políticas inspirado en un algoritmo genético, que imita los operadores de mutación y cruce sobre redes de políticas.
Un marco de principios para la cuantificación de modelos utilizando el método del gradiente proximal, con evaluación empírica y análisis de convergencia teórica.
Propone el método ProxQuant para entrenar redes neuronales con pesos cuantificados.
Propone resolver las redes binarias y sus variantes utilizando el descenso de gradiente proximal.
Los mínimos locales "malos" desaparecen en una red neuronal multicapa: una prueba con supuestos más razonables que antes
En las redes con una sola capa oculta, el volumen de mínimos locales subóptimos disminuye exponencialmente en comparación con los mínimos globales.
Este trabajo pretende responder a por qué los algoritmos estándar basados en SGD sobre redes neuronales convergen a soluciones "buenas".
Proponemos un método de ataque invariante de la atención para generar ejemplos adversarios más transferibles para los ataques de caja negra, que pueden engañar a las defensas del estado de la técnica con una alta tasa de éxito.
El documento propone una nueva forma de superar las defensas del estado del arte contra los ataques adversarios a la CNN.
Este trabajo sugiere que el "cambio de atención" es una propiedad clave detrás del fracaso de los ataques adversarios a la transferencia y propone un método de ataque invariante de la atención
Cómo entrenar 100.000 clases en una sola GPU
Propone un método de hashing eficiente MACH para la aproximación de softmax en el contexto de un gran espacio de salida, que ahorra tanto memoria como computación.
Un método para el esquema de clasificación de problemas que implican un gran número de clases en un entorno multiclase demostrado en los conjuntos de datos ODP e Imagenet-21K
El artículo presenta un esquema basado en hashing para reducir la memoria y el tiempo de cálculo para la clasificación de K vías cuando K es grande
Presentamos un método general para la estimación insesgada de gradientes de funciones de caja negra de variables aleatorias. Aplicamos este método a la inferencia variacional discreta y al aprendizaje por refuerzo. 
Sugiere un nuevo enfoque para realizar el descenso de gradiente para la optimización de la caja negra o el entrenamiento de modelos de variables latentes discretas.
Proponemos un estimador del tamaño de soporte de la distribución aprendida de los GANs para demostrar que efectivamente sufren de colapso de modo, y demostramos que los GANs codificadores-decodificadores tampoco evitan el problema.
El artículo trata de estimar experimentalmente el tamaño del soporte de las soluciones producidas por GANs típicas. 
Este trabajo propone una nueva prueba inteligente basada en la paradoja de cumpleaños para medir la diversidad en la muestra generada, con resultados de experimentos que se interpretan como que el colapso de modos es fuerte en una serie de modelos generativos de última generación.
El artículo utiliza la paradoja del cumpleaños para mostrar que algunas arquitecturas GAN generan distribuciones con un soporte bastante bajo.
Proponemos un marco para generar adversarios naturales contra clasificadores de caja negra tanto para dominios visuales como textuales, haciendo la búsqueda de adversarios en el espacio semántico latente.
Propone un método para la creación de ejemplos de adversarios semánticos.
Propone un marco para generar ejemplos adversarios naturales mediante la búsqueda de adversarios en un espacio latente de representación de datos densos y continuos 
Ampliamos el método K-FAC a las RNN desarrollando una nueva familia de aproximaciones de Fisher.
Los autores extienden el método K-FAC a las RNNs y presentan 3 formas de aproximar F, mostrando resultados de optimización en 3 conjuntos de datos, que superan a ADAM tanto en número de actualizaciones como en tiempo de computación.
Propone extender el método de optimización de Curvatura Apropiada del factor Kronecker al entorno de las redes neuronales recurrentes.
Los autores presentan un método de segundo orden diseñado específicamente para las RNN
El modelo generativo para los núcleos de las redes neuronales convolucionales, que actúa como una distribución a priori mientras se entrena en nuevos conjuntos de datos.
Un método para modelar redes neuronales convolucionales utilizando un método de Bayes.
Propone la "prioridad de peso profundo": la idea es obtener una prioridad en un conjunto de datos auxiliar y luego utilizar esa prioridad sobre los filtros CNN para iniciar la inferencia para un conjunto de datos de interés.
Este trabajo explora el aprendizaje de priores informativos para modelos de redes neuronales convolucionales con dominios de problemas similares mediante el uso de autocodificadores para obtener un prior expresivo sobre los pesos filtrados de las redes entrenadas.
Aplicamos técnicas de aprendizaje profundo a la segmentación de imágenes hiperespectrales y al muestreo iterativo de características.
Propone un esquema codicioso para seleccionar un subconjunto de características espectrales altamente correlacionadas en una tarea de clasificación.
El artículo explora el uso de redes neuronales para la clasificación y segmentación de imágenes hipersepctrales (HSI) de células.
Clasificación de células e implementación de la segmentación celular basada en técnicas de aprendizaje profundo con reducción de características de entrada
Creamos un nuevo conjunto de datos para la interpretación de los datos sobre las parcelas y también proponemos una línea de base para la misma.
Los autores proponen un procedimiento para resolver el problema DIP que implica el aprendizaje a partir de conjuntos de datos que contienen tripletas de la forma {parcela, pregunta, respuesta}
Propone un algoritmo que puede interpretar los datos mostrados en los gráficos científicos.
Mejora de las redes neuronales recurrentes de predicción del estado mediante características aleatorias ortogonales
Propone mejorar las prestaciones de las Redes Neuronales Recurrentes de Estado Predicitivo considerando Características Aleatorias Ortogonales.
El trabajo aborda el problema del entrenamiento de redes neuronales recurrentes de estado predictivo y realiza dos aportaciones.
Proponemos el Aprendizaje Ponderado por Fidelidad, un enfoque semi-supervisado de profesor-estudiante para el entrenamiento de redes neuronales utilizando datos débilmente etiquetados.
Este trabajo sugiere un enfoque para el aprendizaje con supervisión débil utilizando un conjunto de datos limpio y otro con ruido y asumiendo una red de profesores y alumnos
El artículo trata de entrenar modelos de redes neuronales profundas con pocas muestras de entrenamiento etiquetadas.
Los autores proponen un enfoque para el entrenamiento de modelos de aprendizaje profundo para situaciones en las que no hay suficientes datos anotados fiables.
Propusimos dos nuevos enfoques, la regresión inversa incremental rebanada y la regresión inversa incremental superpuesta rebanada, para implementar la reducción de dimensión supervisada de una manera de aprendizaje en línea.
Estudia el problema de la reducción de dimensión suficiente y propone un algoritmo de regresión inversa incremental rebanada.
Este trabajo propone un algoritmo de aprendizaje en línea para la reducción supervisada de la dimensión, llamado regresión inversa incremental rebanada
Proponemos un modelo de aprendizaje que permite a la DNN aprender con sólo 2 bits/peso, lo que es especialmente útil para el aprendizaje en el dispositivo
Propone un método para discretizar una NN de forma incremental para mejorar la memoria y el rendimiento.
El aprendizaje de los operadores de transporte en las variedades constituye una valiosa representación para realizar tareas como el aprendizaje por transferencia.
Utiliza un marco de aprendizaje de diccionario para aprender operadores de transporte múltiple en dígitos aumentados de USPS.
El trabajo considera el marco de aprendizaje de operadores de transporte de colectores de Culpepper y Olshausen (2009), y lo interpreta como la obtención de una estimación MAP bajo un modelo generativo probabilístico.
Presentamos un modelo neuronal variacional para el aprendizaje de conceptos visuales compositivos guiados por el lenguaje.
Propone una novedosa arquitectura de red neuronal que aprende los conceptos de los objetos combinando un beta-VAE y un SCAN.
Este trabajo introduce un modelo basado en VAE para traducir entre imágenes y texto, con su representación latente bien adaptada a la aplicación de operaciones simbólicas, lo que les da un lenguaje más expresivo para el muestreo de imágenes a partir del texto. 
Este trabajo propone un nuevo modelo denominado SCAN (Symbol-Concept Association Network) para el aprendizaje jerárquico de conceptos y permite la generalización a nuevos conceptos compuestos a partir de conceptos existentes utilizando operadores lógicos.
Modelo conversacional de temas latentes, un híbrido de seq2seq y modelo neural de temas para generar respuestas más diversas e interesantes.
Este trabajo propone la combinación de un modelo temático y un modelo conversacional seq2seq
Propone un modelo conversacional con información tópica combinando el modelo seq2seq con modelos neurales de temas y muestra que el modelo propuesto supera a algunos de los modelos de referencia seq2seq y a otra variante de modelo de variable latente de seq2seq.
El artículo aborda la cuestión de la perdurabilidad de la actualidad en los modelos de conversación y propone un modelo que es una combinación de un modelo neural de temas y un sistema de diálogo basado en seq2seq. 
El problema del análisis de gráficos se transforma en un problema de análisis de nubes de puntos. 
Propone una red GNN profunda para problemas de clasificación de grafos utilizando su capa de agrupación de grafos adaptativa.
Los autores proponen un método de aprendizaje de representaciones para grafos
Proponemos generar un ejemplo adversarial basado en redes generativas adversariales en un entorno de caja semiblanca y caja negra.
Describe AdvGAN, un GAN condicional más pérdida adversarial, y evalúa AdvGAN en escenarios de caja semiblanca y caja negra, informando de los resultados del estado del arte.
Este trabajo propone una forma de generar ejemplos adversarios que engañen a los sistemas de clasificación y gana el reto mnista de MadryLab.
Este artículo demuestra cómo entrenar autocodificadores profundos de principio a fin para lograr resultados de SoA en un conjunto de datos de Netflix divididos en el tiempo.
Este artículo presenta un modelo de autocodificador profundo para la predicción de valoraciones que supera a otros enfoques del estado del arte en el conjunto de datos de premios de Netflix. 
Propone utilizar un EA profundo para realizar tareas de predicción de calificación en sistemas de recomendación.
Los autores presentan un modelo para recomendaciones más precisas de Netflix que demuestra que un autocodificador profundo puede superar a modelos más complejos basados en RNN que tienen información temporal. 
Introducimos el Transformador Universal, un modelo de secuencia recurrente paralelo en tiempo que supera a los Transformadores y LSTMs en una amplia gama de tareas de secuencia a secuencia, incluyendo la traducción automática.
Propone un nuevo modelo UT, basado en el modelo Transformer, con recurrencia añadida y detención dinámica de la recurrencia.
Este trabajo amplía Transformer aplicando recursivamente un bloque de autoatención de varias cabezas, en lugar de apilar varios bloques en el Transformer vainilla.
El artículo desarrolla un marco interpretable de aprendizaje continuo en el que las explicaciones de las tareas terminadas se utilizan para mejorar la atención del alumno durante las tareas futuras, y en el que también se propone una métrica de explicación. 
Los autores proponen un marco para el aprendizaje continuo basado en las explicaciones de las clasificaciones realizadas de las tareas previamente aprendidas
Este trabajo propone una extensión del marco de aprendizaje continuo utilizando el aprendizaje continuo variacional existente como método base con el peso de la evidencia.
Canalización de entrenamiento de precisión mixta utilizando enteros de 16 bits en HW de propósito general; Precisión SOTA para CNN de clase ImageNet; Mejor precisión reportada para la tarea de clasificación ImageNet-1K con cualquier entrenamiento de precisión reducida;
Este trabajo muestra que una implementación cuidadosa del cálculo dinámico en punto fijo de precisión mixta puede lograr una precisión de vanguardia utilizando un modelo de aprendizaje profundo de precisión reducida con una representación de enteros de 16 bits
Propone un esquema de "punto fijo dinámico" que comparte la parte del exponente para un tensor y desarrolla procedimientos para hacer computación NN con este formato y lo demuestra para el entrenamiento de precisión limitada.
Un modelo acústico ConvNet basado en letras da lugar a un proceso de reconocimiento del habla sencillo y competitivo.
Este trabajo aplica las redes neuronales convolucionales cerradas al reconocimiento del habla, utilizando el criterio de entrenamiento ASG.
Un marco GAN noval que utiliza características invariantes de la transformación para aprender representaciones ricas y generadores fuertes.
Propone un objetivo GAN modificado que consiste en un término GAN clásico y un término de codificación invariante.
Este artículo presenta el IVE-GAN, un modelo que introduce un codificador en el marco de la Red Generativa Adversarial.
Proponemos un método para el aprendizaje de la estructura de dependencia latente en autocodificadores variacionales.
Utiliza una matriz de variables aleatorias binarias para capturar las dependencias entre las variables latentes en un modelo generativo profundo jerárquico.
Este trabajo presenta un enfoque VAE en el que se aprende una estructura de dependencia sobre la variable latente durante el entrenamiento.
Los autores proponen aumentar el espacio latente de un VAE con una estructura autorregresiva, para mejorar la expresividad tanto de la red de inferencia como de la previa latente
Introducimos una arquitectura de red neuronal invariable en escala para la detección de puntos de cambio en series temporales multivariantes.
El artículo aprovecha el concepto de la transformada wavelet dentro de una arquitectura profunda para resolver la detección de puntos de cambio.
Este trabajo propone una red neuronal basada en una pirámide y la aplica a señales 1D con procesos subyacentes que ocurren en diferentes escalas de tiempo donde la tarea es la detección de puntos de cambio
RL encuentra mejores heurísticas para los algoritmos de razonamiento automatizado.
Tiene como objetivo aprender una heurística para un algoritmo de búsqueda de rastreo utilizando el aprendizaje por refuerzo y propone un modelo que hace uso de redes neuronales gráficas para producir incrustaciones de literales y cláusulas, y utilizarlas para predecir la calidad de cada literal para decidir la probabilidad de cada acción.
El trabajo propone un enfoque para el aprendizaje automático de la heurística de selección de variables para QBF utilizando el aprendizaje profundo
Evalúe si su GAN está haciendo realmente algo más que memorizar los datos de entrenamiento.
Tiene como objetivo proporcionar una medida/prueba de calidad para las GANs y propone evaluar la aproximación actual de una distribución aprendida por una GAN utilizando la distancia Wasserstein entre dos distribuciones hechas de una suma de Diracs como rendimiento de referencia. 
En este trabajo se propone un procedimiento para evaluar el rendimiento de las GANs mediante la reconsideración de la clave de observación, utilizando el procedimiento para probar y mejorar las GANs actuales
Utilizamos técnicas de búsqueda para descubrir nuevas funciones de activación, y nuestra mejor función de activación descubierta, f(x) = x * sigmoide(beta * x), supera a ReLU en una serie de tareas difíciles como ImageNet.
Propone un enfoque basado en el aprendizaje por refuerzo para encontrar la no linealidad mediante la búsqueda de combinaciones a partir de un conjunto de operadores unarios y binarios.
Este trabajo utiliza el aprendizaje por refuerzo para buscar la combinación de un conjunto de funciones unarias y binarias que dan como resultado una nueva función de activación
El autor utiliza el aprendizaje por refuerzo para encontrar nuevas funciones de activación potenciales a partir de un rico conjunto de posibles candidatos. 
Un algoritmo ascendente que amplía las CNNs comenzando con una característica por capa hasta arquitecturas con suficiente capacidad de representación.
Propone ajustar dinámicamente la profundidad del mapa de características de una red neuronal totalmente convolucional, formulando una medida de autorresistencia y potenciando el rendimiento.
Introduce una sencilla métrica basada en la correlación para medir si los filtros de las redes neuronales se utilizan de forma eficaz, como indicador de la capacidad efectiva.
Pretende abordar el problema de búsqueda de la arquitectura de aprendizaje profundo mediante la adición y eliminación incremental de canales en las capas intermedias de la red.
Entrenamos una red feedforward sin backprop utilizando un modelo basado en la energía para proporcionar objetivos locales
El objetivo de este trabajo es acelerar el procedimiento de inferencia iterativa en los modelos basados en la energía entrenados con Propagación de Equilibrio (PE), proponiendo entrenar una red feedforward para predecir un punto fijo de la "red de equilibrio". 
Entrenamiento de una red independiente para inicializar las redes recurrentes entrenadas con propagación de equilibrio 
Aprenda las representaciones de las imágenes que factorizan un solo atributo.
Este trabajo se basa en los GANs VAE condicionales para permitir la manipulación de atributos en el proceso de síntesis.
Este trabajo propone un modelo generativo para aprender la representación que puede separar la identidad de un objeto de un atributo, y amplía el autoencoder adversarial añadiendo una red auxiliar.
Presentamos un modelo para la reconstrucción 3D consistente y la predicción de vídeo con saltos, por ejemplo, produciendo fotogramas de imagen en múltiples pasos de tiempo en el futuro sin generar fotogramas intermedios.
Este trabajo propone un método general para el modelado de datos indexados mediante la codificación de la información del índice junto con la observación en una red neuronal, y luego decodificar la condición de la observación en el índice objetivo.
Propone utilizar un VAE que codifique el vídeo de entrada de forma invariable a la permutación para predecir los fotogramas futuros de un vídeo.
Análisis del popular optimizador Adam
El documento trata de mejorar a Adam basándose en la adaptación de la varianza con el impulso proponiendo dos algoritmos
Este trabajo analiza la invariabilidad de la escala y la forma particular de la tasa de aprendizaje utilizada en Adam, argumentando que la actualización de Adam es una combinación de una actualización de señales y una tasa de aprendizaje basada en la varianza.
El artículo divide el algoritmo ADAM en dos componentes: dirección estocástica en signo de gradiente y paso a paso adaptativo con varianza relativa, y se proponen dos algoritmos para probar cada uno de ellos.
Proponemos un marco novedoso para ajustar de forma adaptativa las tasas de abandono de la red neuronal profunda basándonos en un límite de complejidad de Rademacher.
Los autores relacionan los parámetros de abandono con un límite de la complejidad Rademacher de la red
Relaciona la complejidad de la capacidad de aprendizaje de las redes con las tasas de abandono en la retropropagación.
Se proponen arquitecturas de aprendizaje profundo optimizadas para la fusión de sensores.
Los autores mejoran varias limitaciones de la arquitectura básica de negación proponiendo una arquitectura de fusión cerrada de grano más grueso y una arquitectura de fusión cerrada en dos etapas
Propone dos arquitecturas de aprendizaje profundo con compuerta para la fusión de sensores y, al tener las características agrupadas, demuestra un mejor rendimiento, especialmente en presencia de ruido aleatorio de los sensores y de fallos.
La normalización por lotes provoca la explosión de los gradientes en las redes vanilla feedforward.
Desarrolla una teoría de campo medio para la normalización de lotes (BN) en redes totalmente conectadas con pesos inicializados aleatoriamente.
Proporciona una perspectiva dinámica de la red neuronal profunda utilizando la evolución de la matriz de covarianza junto con las capas.
Entrenamos una red de grafos para predecir la satisfabilidad booleana y demostramos que aprende a buscar soluciones, y que las soluciones que encuentra pueden descodificarse a partir de sus activaciones.
El artículo describe una arquitectura de red neuronal general para predecir la satisfacción
Este artículo presenta la arquitectura NeuroSAT que utiliza una red neuronal profunda de paso de mensajes para predecir la satisfabilidad de las instancias CNF
Un modelo de secuencia neural que aprende a pronosticar en un gráfico dirigido.
El artículo propone la arquitectura de la red neuronal convolucional difusa para el problema de previsión del tráfico espacio-temporal
Propone la construcción de un modelo de previsión del tráfico mediante un proceso de difusión para redes neuronales recurrentes convolucionales para abordar la autocorrelación sápido-temporal.
Entrenamos a las redes neuronales para que sean inciertas en las entradas ruidosas para evitar predicciones excesivamente seguras fuera de la distribución de entrenamiento.
Presenta un enfoque para obtener estimaciones de incertidumbre para las predicciones de las redes neuronales que tiene un buen rendimiento cuando se cuantifica la incertidumbre predictiva en puntos que están fuera de la distribución de entrenamiento.
El artículo considera el problema de la estimación de la incertidumbre de las redes neuronales y propone utilizar el enfoque bayesiano con un previo contrastivo de ruido
Este estudio pone de manifiesto una diferencia clave entre la visión humana y las CNN: mientras que el reconocimiento de objetos en los humanos se basa en el análisis de la forma, las CNN no tienen ese sesgo de forma.
Busca establecer, mediante una serie de experimentos bien diseñados, que las CNNs entrenadas para la clasificación de imágenes no codifican sesgos de forma como la visión humana.
Este artículo destaca el hecho de que las CNNs no necesariamente aprenderán a reconocer objetos basándose en su forma y muestra que sobrepasarán las características basadas en el ruido.
Describimos un novedoso modelo generativo multivista que puede generar múltiples vistas del mismo objeto, o múltiples objetos en la misma vista sin necesidad de etiquetar las vistas.
Este artículo presenta un método basado en GAN para la generación de imágenes que intenta separar las variables latentes que describen el contenido de la imagen de las que describen las propiedades de la vista.
Este trabajo propone una arquitectura GAN que pretende descomponer la distribución subyacente de una clase particular en "contenido" y "vista".
Propone un nuevo modelo generativo basado en la Red Generativa Adversarial (GAN) que desentraña el contenido y la vista de los objetos sin supervisión de la vista y extiende la GAN en un modelo generativo condicional que toma una imagen de entrada y genera diferentes vistas del objeto en la imagen de entrada. 
Se propone un algoritmo de cuantificación del peso que tiene en cuenta las pérdidas y que considera directamente su efecto sobre las mismas.
Propone un método de compresión de la red mediante la ternarización de pesos. 
El artículo propone un nuevo método para entrenar DNNs con pesos cuantificados, mediante la inclusión de la cuantificación como una restricción en un algoritmo proximal cuasi-Newton, que aprende simultáneamente un escalado para los valores cuantificados.
El artículo amplía el esquema de binarización por peso con pérdidas a la terarización y a la cuantificación arbitraria de m bits y demuestra su prometedor rendimiento.
Desarrollamos un novedoso método de gradiente de políticas para el aprendizaje automático de políticas con opciones utilizando un paso de inferencia diferenciable.
El documento presenta una nueva técnica de gradiente de política para el aprendizaje de opciones, en la que una sola muestra puede utilizarse para actualizar todas las opciones.
Propone un método fuera de política para el aprendizaje de opciones en problemas continuos complejos.
Selección de características no supervisada mediante la captura de la estructura lineal local de los datos
Propone una selección de características localmente lineal y no supervisada.
El artículo propone el método LLUFS para la selección de características.
Utilizando una sencilla tarea de navegación basada en el lenguaje, estudiamos las capacidades de composición de las modernas redes recurrentes seq2seq.
Este artículo se centra en las capacidades de composición de aprendizaje de disparo cero de las modernas RNN de secuencia a secuencia y expone las deficiencias de las actuales arquitecturas de RNN de secuencia a secuencia.
El artículo analiza las capacidades de composición de las RNN, en concreto, la capacidad de generalización de las RNN en subconjuntos aleatorios de comandos SCAN, en comandos SCAN más largos, y de composición sobre comandos primitivos. 
Los autores presentan un nuevo conjunto de datos que facilita el análisis de un caso de aprendizaje Seq2Seq
Abordamos el problema del aprendizaje de similitudes para objetos estructurados con aplicaciones en particular en la seguridad informática, y proponemos un nuevo modelo de redes de coincidencia de grafos que destaca en esta tarea.
Los autores introducen una red de emparejamiento de grafos para la recuperación y emparejamiento de objetos estructurados en grafos.
Los autores atacan el problema del emparejamiento de grafos proponiendo una extensión de las redes de incrustación de grafos
Los autores presentan dos métodos para el aprendizaje de una puntuación de similitud entre pares de grafos y muestran lo beneficioso de introducir ideas de la correspondencia de grafos en las redes neuronales de grafos.
Proponemos un modelo codificador-decodificador RNN-CNN para el aprendizaje rápido de la representación de frases sin supervisión.
Modificaciones del marco de pensamiento saltado para el aprendizaje de incrustaciones de oraciones.
Este trabajo presenta un nuevo diseño híbrido de codificador RNN y decodificador CNN para su uso en el preentrenamiento, que no requiere un decodificador autorregresivo cuando se preentrenan codificadores.
Los autores amplían Skip-thought decodificando sólo una frase objetivo mediante un decodificador CNN.
Un enfoque estadístico para calcular las probabilidades de las muestras en las Redes Generativas Adversariales
Demostrar que el WGAN con regularización entrópica maximiza un límite inferior en la probabilidad de la distribución de los datos observados.
Los autores afirman que es posible aprovechar el límite superior de un transporte óptimo regularizado por la entropía para obtener una medida de "probabilidad de la muestra".
Presentamos geomstats, un eficiente paquete de Python para la modelización y optimización riemanniana sobre colectores compatible con numpy y tensorflow .
El artículo presenta el paquete de software geomstats, que proporciona un uso sencillo de las variedades y métricas riemannianas dentro de los modelos de aprendizaje automático
Propone un paquete de Python para la optimización y las aplicaciones en variedades riemannianas y destaca las diferencias entre el paquete Geomstats y otros paquetes.
Presenta una caja de herramientas geométricas, Geomstats, para el aprendizaje automático en variedades riemannianas.
Construimos un grafo dinámico disperso mediante una búsqueda de reducción de dimensiones para reducir los costes de computación y memoria tanto en el entrenamiento como en la inferencia de la DNN.
Los autores proponen utilizar el gráfico de cálculo disperso dinámico para reducir el coste de memoria y tiempo de cálculo en la red neuronal profunda (DNN).
Este trabajo propone un método para acelerar el entrenamiento y la inferencia de las redes neuronales profundas utilizando la poda dinámica del grafo de cálculo.
Desarrollamos una extensión práctica del Muestreo Dirigido a la Información para el Aprendizaje por Refuerzo, que tiene en cuenta la incertidumbre paramétrica y la heteroscedasticidad en la distribución de retorno para la exploración.
Los autores proponen una forma de extender el muestreo dirigido por la información al aprendizaje por refuerzo, combinando dos tipos de incertidumbre para obtener una estrategia de exploración sencilla basada en IDS. 
Este trabajo investiga los enfoques de exploración sofisticada para el aprendizaje por refuerzo construidos sobre el muestreo directo de información y sobre el aprendizaje por refuerzo distribucional
Utilización de redes neuronales gráficas para modelar la información estructural de los agentes con el fin de mejorar la política y la transferibilidad 
Método de representación y aprendizaje de políticas estructuradas para tareas de control continuo mediante redes neuronales gráficas
La presentación propone la incorporación de una estructura adicional en los problemas de aprendizaje por refuerzo, en particular la estructura de la morfología del agente
Proponer una aplicación de las Redes Neuronales Gráficas al aprendizaje de políticas de control de robots "ciempiés" de diferentes longitudes.
Este trabajo presenta un marco de aprendizaje por refuerzo jerárquico basado en políticas de opciones deterministas y en la maximización de la información mutua. 
Propone un algoritmo HRL que intenta aprender las opciones que maximizan su información mutua con la densidad estado-acción bajo la política óptima.
Este trabajo propone un sistema HRL en el que la información mutua de la variable latente y de los pares estado-acción es aproximadamente maximizada.
Propone un criterio que pretende maximizar la información mutua entre las opciones y los pares estado-acción y muestra empíricamente que las opciones aprendidas descomponen el espacio estado-acción pero no el espacio estado. 
Introducimos y validamos las interpretaciones locales jerárquicas, la primera técnica para buscar y mostrar automáticamente las interacciones importantes para las predicciones individuales realizadas por LSTMs y CNNs.
Un nuevo enfoque para explicar las predicciones de las redes neuronales mediante el aprendizaje de representaciones jerárquicas de grupos de características de entrada y su contribución a la predicción final
Extiende un método de interpretación de características existente para LSTMs a DNNs más genéricas e introduce un clustering jerárquico de las características de entrada y las contribuciones de cada cluster a la predicción final.
Este trabajo propone una extensión jerárquica de la descomposición contextual.
Proponemos un método fácil de aplicar, pero eficaz, para la compresión de redes neuronales. PFA explota la correlación intrínseca entre las respuestas de los filtros dentro de las capas de la red para recomendar una huella de red más pequeña.
Propone podar las redes convolucionales analizando la correlación observada entre los filtros de una misma capa, expresada por el espectro de valores propios de su matriz de covarianza.
Este trabajo introduce un enfoque para comprimir las redes neuronales observando la correlación de las respuestas de los filtros en cada capa mediante dos estrategias.
Este trabajo propone un método de compresión basado en el análisis espectral
Agentes de reformulación de consultas múltiples y diversas entrenados con aprendizaje de refuerzo para mejorar los motores de búsqueda.
Parelización del método de conjunto en el aprendizaje de refuerzo para la reformulación de consultas, acelerando el entrenamiento y mejorando la diversidad de las freformulaciones aprendidas
Los autores proponen entrenar a múltiples agentes distintos, cada uno sobre un subconjunto diferente del conjunto de entrenamiento.
Los autores proponen un enfoque de conjunto para la reformulación de consultas
Introducimos un método de incrustación de redes que tiene en cuenta la información previa sobre la red, lo que permite obtener un rendimiento empírico superior.
El trabajo propuso utilizar una distribución a priori para restringir la incrustación de la red, para la formulación este trabajo utilizó distribuciones gaussianas muy restringidas.
Propone el aprendizaje de incrustaciones de nodos sin supervisión considerando las propiedades estructurales de las redes.
Analizamos la convergencia de los algoritmos de tipo Adam y proporcionamos condiciones suaves suficientes para garantizar su convergencia, también mostramos que violar las condiciones puede hacer que un algoritmo diverja.
Presenta un análisis de convergencia en el entorno no convexo para una familia de algoritmos de optimización.
Este trabajo investiga la condición de convergencia de los optimizadores tipo Adam en los problemas de optimización no convexos sin restricciones.
El autocodificador de pesos atados con la función abs como función de activación, aprende a hacer la clasificación en la dirección hacia adelante y la regresión en la dirección hacia atrás debido a la función de coste especialmente definida.
El artículo propone utilizar la función de activación del valor absoluto en una arquitectura de autoencoder con un término adicional de aprendizaje supervisado en la función objetivo
Este trabajo introduce una red reversible con valor absoluto utilizado como función de activación
Proponemos un modelo de extracción de relaciones basado en Transformer que utiliza representaciones lingüísticas preentrenadas en lugar de características lingüísticas explícitas.
Presenta un modelo de extracción de relaciones basado en transformadores que aprovecha el preentrenamiento en texto no etiquetado con un objetivo de modelado del lenguaje.
Este artículo describe una novedosa aplicación de las redes de transformación para la extracción de relaciones.
El artículo presenta una arquitectura basada en transformadores para la extracción de relajación, evaluada en dos conjuntos de datos.
Proponemos un método sencillo y eficaz para la búsqueda de arquitecturas de redes neuronales convolucionales.
Propone un método de búsqueda de arquitecturas neuronales que consigue una precisión cercana al estado del arte en CIFAR10 y requiere muchos menos recursos computacionales.
Presenta un método para buscar arquitecturas de redes neuronales al mismo tiempo que se entrena, lo que ahorra drásticamente el tiempo de entrenamiento y de búsqueda de arquitecturas.
Propone una variante de búsqueda de arquitecturas neuronales utilizando morfismos de red para definir un espacio de búsqueda utilizando arquitecturas CNN completando la tarea de clasificación de imágenes CIFAR
Uso de GANs para generar grafos mediante paseos aleatorios.
Los autores propusieron un modelo generativo de paseos aleatorios sobre grafos que permite el aprendizaje agnóstico del modelo, el ajuste controlable, la generación de grafos conjuntos
Propone una formulación WGAN para la generación de grafos basada en paseos aleatorios utilizando incrustaciones de nodos y una arquitectura LSTM para el modelado.
Proponemos resolver un problema de clasificación simultánea y de detección de novedades en el marco del GAN.
Propone un GAN para unificar la clasificación y la detección de novedades.
El artículo presenta un método de detección de novedades basado en un GAN multiclase que se entrena para obtener imágenes generadas a partir de una mezcla de las distribuciones nominal y de novedades.
El artículo propone un GAN para la detección de novedades utilizando un generador de mezclas con pérdida de coincidencia de características
El rendimiento de la verificación del hablante puede mejorarse significativamente adaptando el modelo a los datos del dominio utilizando redes adversariales generativas. Además, la adaptación puede realizarse de forma no supervisada.
Proponer una serie de variantes de GAN en la tarea de reconocimiento de hablantes en la condición de desajuste de dominio.
Minimización de la información mutua sinérgica dentro de las latentes y los datos para la tarea de desentrañamiento utilizando el marco VAE.
Propone una nueva función objetivo para el aprendizaje de representaciones dientangulares en un marco variacional minimizando la sinergia de la información proporcionada.
Los autores pretenden entrenar una VAE que haya desentrañado las representaciones latentes de una manera "sinérgica" máxima. 
Este trabajo propone un nuevo enfoque para reforzar el desentrañamiento en las VAE utilizando un término que penaliza la información mutua sinérgica entre las variables latentes.
Acelerar el SGD disponiendo los ejemplos de forma diferente
El artículo presenta un método para mejorar la tasa de convergencia del Descenso Gradiente Estocástico para el aprendizaje de incrustaciones agrupando muestras de entrenamiento similares.
Propone una estrategia de muestreo no uniforme para construir minibatches en SGD para la tarea de aprender incrustaciones para asociaciones de objetos.
Combinar la información entre la incrustación de palabras preestablecida y la representación de palabras específica de la tarea para resolver el problema de la falta de vocabulario
Este artículo propone un enfoque para mejorar la predicción de incrustación fuera de vocabulario para la tarea de modelar conversaciones de diálogo con ganancias considerables sobre las líneas de base.
Propone combinar las incrustaciones de palabras externas preentrenadas y las incrustaciones de palabras preentrenadas en los datos de entrenamiento, manteniéndolas como dos vistas.
Propone un método para ampliar la cobertura de las incrustaciones de palabras preentrenadas para hacer frente al problema de OOV que surge al aplicarlas a conjuntos de datos conversacionales y aplica nuevas variantes del modelo basado en LSTM a la tarea de selección de respuestas en el modelado de diálogos.
Analizamos los problemas al entrenar optimizadores aprendidos, abordamos esos problemas mediante la optimización variacional utilizando dos estimadores de gradiente complementarios, y entrenamos optimizadores que son 5 veces más rápidos en tiempo de reloj de pared que los optimizadores de referencia (por ejemplo, Adam).
Este trabajo utiliza la optimización no rodada para aprender redes neuronales de optimización.
Este trabajo aborda el problema del aprendizaje de un optimizador, concretamente los autores se centran en la obtención de gradientes más limpios a partir del procedimiento de entrenamiento desenrollado.
Presenta un método para "aprender un optimizador" utilizando una Optimización Variacional para la pérdida del optimizador "externo" y propone la idea de combinar tanto el gradiente reparametrizado como el estimador de la función de puntuación para el Objetivo Variacional y los pondera utilizando una fórmula de producto de Gauss para la media.
Un método para un entrenamiento distribuido asíncrono eficiente de modelos de aprendizaje profundo junto con límites de arrepentimiento teóricos.
El artículo propone un algoritmo para restringir el estancamiento en el SGD asíncrono y proporciona un análisis teórico
Propone un algoritmo híbrido para eliminar el retraso del gradiente de los métodos asíncronos.
Diseñamos una novedosa metodología de cuantificación para optimizar conjuntamente la eficiencia y la robustez de los modelos de aprendizaje profundo.
Propone un esquema de regularización para proteger las redes neuronales cuantificadas de los ataques de los adversarios utilizando un filtrado constante de Lipschitz de la entrada-salida de las capas internas.
Una modificación para las arquitecturas RNN existentes que les permite omitir las actualizaciones de estado conservando el rendimiento de las arquitecturas originales.
Propone el modelo Skip RNN que permite a una red recurrente omitir selectivamente la actualización de su estado oculto para algunas entradas, lo que permite reducir el cálculo en el momento de la prueba.
Propone un nuevo modelo de RNN en el que tanto la entrada como la actualización de estado de las células recurrentes se saltan de forma adaptativa durante algunos pasos de tiempo.
Un rápido solucionador de segundo orden para el aprendizaje profundo que funciona en problemas a escala de ImageNet sin ajuste de hiperparámetros
Elección de la dirección utilizando un único paso de descenso de gradiente "hacia el paso Newton" a partir de una estimación original, y luego tomando esta dirección en lugar del gradiente original
Un nuevo método aproximado de optimización de segundo orden con bajo coste computacional que sustituye el cálculo de la matriz hessiana por un único paso de gradiente y una estrategia de arranque en caliente.
Modelo basado en la atención y entrenado con REINFORCE con una línea de base de despliegue codicioso para aprender heurísticas con resultados competitivos en TSP y otros problemas de enrutamiento
Presenta un enfoque basado en la atención para el aprendizaje de una política para resolver TSP y otros problemas de optimización combinatoria de tipo de ruta.
Este trabajo trata de aprender heurísticas para resolver problemas de optimización combinatoria
Un algoritmo para optimizar los hiperparámetros de regularización durante el entrenamiento
El artículo propone una forma de reiniciar y en cada actualización de lambda y un procedimiento de recorte de y para mantener la estabilidad del sistema dinámico.
Propone un algoritmo para la optimización de hiperparámetros que puede ser visto como una extensión de Franceschi 2017 donde algunas estimaciones son reiniciadas en caliente para aumentar la estabilidad del método.
Propone una extensión de un método existente para optimizar los hiperparámetros de regularización.
Demostrar que los LSTM son tan buenos o mejores que las innovaciones recientes para el LM y que la evaluación de los modelos es a menudo poco fiable.
Este artículo describe una validación exhaustiva de los modelos lingüísticos de palabras y caracteres basados en LSTM, lo que conduce a un resultado significativo en el modelado del lenguaje y a un hito en el aprendizaje profundo.
Mostramos cómo el uso de las conexiones de salto puede hacer que los modelos de mejora del habla sean más interpretables, ya que hace que utilicen mecanismos similares a los que se han explorado en la literatura de DSP.
Los autores proponen incorporar los bloques Residual, Highway y Masking dentro de un pipeline totalmente convolucional para entender cómo se realiza la inferencia iterativa de la salida y el enmascaramiento en una tarea de mejora del habla
Los autores interpretan las conexiones viales, residuales y de enmascaramiento. 
Los autores generan su propio habla ruidosa añadiendo artificialmente ruido de un conjunto de datos de ruido bien establecido a un conjunto de datos de habla limpia menos conocido.
Un método para eliminar la varianza del gradiente y afinar automáticamente las priores para un entrenamiento eficaz de las redes neuronales bayesianas
Propone un nuevo enfoque para realizar la inferencia variacional determinista para BNN feed-forward con funciones de activación no lineales específicas mediante la aproximación de los momentos de las capas.
El artículo considera un enfoque puramente determinista para el aprendizaje de aproximaciones posteriores variacionales para redes neuronales bayesianas.
Enfoque de un método formal para la composición de habilidades en tareas de aprendizaje por refuerzo
El artículo combina la RL y las restricciones expresadas mediante fórmulas lógicas estableciendo una automatización a partir de fórmulas scTLTL.
Propone un método que ayuda a construir la política a partir de las subtareas aprendidas en el tema de la combinación de tareas RL con fórmulas de lógica temporal lineal.
Derivación de una formulación general de un VAE multimodal a partir de la log-verosimilitud marginal conjunta.
Propone un VAE multimodal con un límite variacional derivado de la regla de la cadena.
Este trabajo propone un objetivo, M^2VAE, para las VAE multimodales, que se supone que aprende una representación del espacio latente más significativa.
Nos basamos en la autocodificación secuencial de Monte Carlo, obtenemos nuevos conocimientos teóricos y desarrollamos un procedimiento de entrenamiento mejorado basado en esos conocimientos.
El artículo propone una versión del entrenamiento tipo IWAE que utiliza el SMC en lugar del clásico muestreo de importancia.
Este trabajo propone la autocodificación de Monte Carlo secuencial (SMC), ampliando el marco VAE a un nuevo objetivo de Monte Carto basado en SMC. 
Proponemos una arquitectura para el aprendizaje de funciones de valor que permite el uso de cualquier algoritmo de evaluación de políticas lineales en tándem con el aprendizaje de características no lineales.
El trabajo propone un marco de dos escalas de tiempo para el aprendizaje de la función de valor y una representación de estado en conjunto con aproximadores no lineales.
Este trabajo propone redes de dos escalas temporales (TTN) y demuestra la convergencia de este método utilizando métodos de aproximación estocástica de dos escalas temporales. 
Este trabajo presenta una red de dos escalas (TTN) que permite utilizar métodos lineales para aprender valores. 
Proponemos algoritmos de factorización matricial (MF) de bajo rango, sencillos pero eficaces, para acelerar el tiempo de ejecución, ahorrar memoria y mejorar el rendimiento de las LSTM.
Propone acelerar LSTM utilizando MF como estrategia de compresión post-procesamiento y lleva a cabo extensos experimentos para mostrar el rendimiento.
Una métrica forense para determinar si una imagen dada es una copia (con posible manipulación) de otra imagen de un conjunto de datos dado.
Introduce la red siamesa para identificar imágenes duplicadas y copiadas/modificadas, que puede utilizarse para mejorar la vigilancia de la literatura publicada y en revisión por pares.
El artículo presenta una aplicación de redes convolucionales profundas para la tarea de detección de imágenes duplicadas
Este trabajo aborda el problema de encontrar imágenes duplicadas/casi duplicadas de publicaciones biomédicas y propone una CNN estándar y funciones de pérdida y las aplica a este campo.
Entrenamiento estable de GAN en altas dimensiones mediante el uso de un conjunto de discriminadores, cada uno con una visión de baja dimensión de las muestras generadas
El artículo propone estabilizar el entrenamiento de GAN utilizando un conjunto de discriminadores, cada uno de los cuales trabaja sobre una proyección aleatoria de los datos de entrada, para proporcionar la señal de entrenamiento para el modelo generador.
El artículo propone un método de entrenamiento GAN para mejorar la estabilidad del entrenamiento. 
El artículo propone un nuevo enfoque para el entrenamiento de GAN, que proporciona gradientes estables para entrenar el generador.
mostramos un método geométrico para codificar perfectamente la información del árbol de categorías en las incrustaciones de palabras preentrenadas.
El artículo propone la incrustación de N bolas para datos taxonómicos, donde una N bola es un par de un vector centroide y el radio desde el centro.
El artículo presenta un método para modificar las incrustaciones vectoriales existentes de objetos categóricos (como las palabras), para convertirlas en incrustaciones de bolas que siguen jerarquías.
Se centra en ajustar las incrustaciones de palabras preentrenadas para que respeten la relación hipernimia/hiponimia mediante una encapsulación adecuada de n-ball.
Proponemos los CRFs convolucionales como una alternativa rápida, potente y entrenable a los CRFs totalmente conectados.
Los autores sustituyen el gran paso de filtrado de la red permutoédrica por un núcleo convolucional que varía espacialmente y muestran que la inferencia es más eficiente y el entrenamiento más sencillo. 
Propone realizar el paso de mensajes en un CRF de núcleo gaussiano truncado utilizando un núcleo definido y el paso de mensajes paralelizado en la GPU.
Proponemos un enfoque agnóstico del modelo para la validación de la solidez del sistema de preguntas y respuestas y demostramos los resultados en los modelos de preguntas y respuestas más avanzados.
Aborda el problema de la solidez ante la información adversa en la respuesta a preguntas.
Mejora de la robustez de la comprensión/respuesta automática de preguntas.
multigenerador para capturar Pdata, resolver la competencia y el problema de un solo golpe
Propone GANs paralelos para evitar el colapso de modos en GANs a través de una combinación de múltiples generadores débiles. 
Segmentación de imágenes débilmente supervisada mediante la estructura compositiva de las imágenes y los modelos generativos.
Este trabajo crea una representación en capas para aprender mejor la segmentación a partir de imágenes no etiquetadas.
Este trabajo propone un modelo generativo basado en GAN que descompone las imágenes en múltiples capas, donde el objetivo del GAN es distinguir las imágenes reales de las imágenes formadas por la combinación de las capas.
Este trabajo propone una arquitectura de red neuronal en torno a la idea de la composición de la escena en capas
Presentamos un marco geométrico para demostrar las garantías de robustez y destacamos la importancia de la codimensión en los ejemplos adversos. 
Este trabajo ofrece un análisis teórico de los ejemplos adversariales, mostrando que existe un compromiso entre la robustez en diferentes normas, el entrenamiento adversarial es ineficiente en cuanto a la muestra, y el clasificador del vecino más cercano puede ser robusto bajo ciertas condiciones.
CharNMT es frágil
Este trabajo investiga el impacto del ruido a nivel de caracteres en 4 sistemas neuronales de traducción automática diferentes
Este artículo investiga empíricamente el rendimiento de los sistemas NMT a nivel de caracteres frente al ruido a nivel de caracteres, tanto sintetizado como natural.
Este artículo investiga el impacto de la entrada ruidosa en la traducción automática y prueba formas de hacer que los modelos NMT sean más robustos
Aprendemos redes profundas de unidades de umbral duro estableciendo los objetivos de las unidades ocultas mediante optimización combinatoria y los pesos mediante optimización convexa, lo que da como resultado un mejor rendimiento en ImageNet.
El artículo explica y generaliza los enfoques para el aprendizaje de redes neuronales con activación dura.
Este trabajo examina el problema de la optimización de redes profundas de unidades de umbral duro.
El artículo analiza el problema de la optimización de las redes neuronales con umbral duro y propone una solución novedosa al mismo con una colección de heurísticas/aproximaciones.
Utilizando un nuevo reto controlado de relaciones visuales, mostramos que las tareas de igual-diferencia ponen a prueba la capacidad de las CNNs; argumentamos que las relaciones visuales pueden resolverse mejor utilizando estrategias de atención-mnemotécnica.
Demuestra que las redes neuronales convolucionales y relacionales no resuelven los problemas de relaciones visuales entrenando las redes con datos de relaciones visuales generados artificialmente. 
Este artículo explora cómo las CNN y las redes relacionales actuales no reconocen las relaciones visuales en las imágenes.
Proponemos AD-VAT, donde el rastreador y el objeto objetivo, vistos como dos agentes aprendibles, son oponentes y pueden mejorarse mutuamente durante el entrenamiento.
Este trabajo pretende abordar el problema del seguimiento visual activo con un mecanismo de entrenamiento en el que el rastreador y el objetivo sirven como oponentes mutuos
Este artículo presenta una sencilla tarea de RL profunda multiagente en la que un rastreador en movimiento intenta seguir a un objetivo en movimiento.
Propone una novedosa función de recompensa - "suma cero parcial", que sólo fomenta la competencia entre rastreadores y objetivos cuando están cerca y penaliza cuando están demasiado lejos.
Presentamos un método para aprender conjuntamente un Embedding Jerárquico de Palabras (HWE) utilizando un corpus y una taxonomía para identificar las relaciones de hipernimia entre las palabras.
El artículo presenta un método para aprender conjuntamente incrustaciones de palabras utilizando estadísticas de co-ocurrencia, así como incorporando información jerárquica de las redes semánticas.
En este trabajo se propone un método de aprendizaje conjunto de hiperónimos a partir de datos de texto bruto y de taxonomía supervisada. 
Este trabajo propone añadir una medida de diferencia de "inclusión distributiva" al objetivo GloVE con el fin de representar las relaciones de hipernimia.
integración de la autoorganización y el aprendizaje supervisado en una red neuronal jerárquica
El artículo analiza el aprendizaje en una red neuronal de tres capas, en la que la capa intermedia está organizada topográficamente, e investiga la interacción entre el aprendizaje no supervisado y el supervisado jerárquico en el contexto biológico.
Una variante supervisada del mapa autoorganizado (SOM) de Kohonen, pero en la que la capa de salida lineal se sustituye con el error cuadrado por una capa softmax con entropía cruzada.
Propone un modelo que utiliza neuronas ocultas con función de activación autoorganizada, cuyas salidas alimentan un clasificador con función de salida softmax. 
autopista de precisión; un concepto generalizado de flujo de información de alta precisión para la cuantificación de sub 4 bits 
Investiga el problema de la cuantificación de las redes neuronales empleando una autopista de precisión de extremo a extremo para reducir el error de cuantificación acumulado y permitir una precisión ultrabaja en las redes neuronales profundas. 
Este trabajo estudia métodos para mejorar el rendimiento de las redes neuronales cuantificadas
Este trabajo propone mantener un alto flujo de activación/gradiente en dos tipos de estructuras de redes, ResNet y LSTM.
Un algoritmo para entrenar redes neuronales de forma eficiente en datos temporalmente redundantes.
El artículo describe un esquema de codificación neuronal para el aprendizaje basado en picos en redes neuronales profundas
Este artículo presenta un método de aprendizaje basado en picos que tiene como objetivo reducir el cálculo necesario durante el aprendizaje y la prueba al clasificar datos temporales redundantes.
Este trabajo aplica una versión de codificación predictiva del esquema de codificación Sigma-Delta para reducir la carga computacional de una red de aprendizaje profundo, combinando los tres componentes de una manera no vista anteriormente.
El cuello de botella de la información se comporta de forma sorprendente cuando la salida es una función determinista de la entrada.
Sostiene que la mayoría de los problemas reales de clasificación muestran una relación determinista de este tipo entre las etiquetas de clase y las entradas X y explora varias cuestiones que se derivan de tales patologías.
Explora los problemas que surgen cuando se aplican los conceptos de bottlenext de información a los modelos de aprendizaje supervisado determinista
Los autores aclaran varios comportamientos contraintuitivos del método del cuello de botella de información para el aprendizaje supervisado de una regla determinista.
Demostramos que las redes neuronales bayesianas idealizadas pueden no tener ejemplos adversos, y damos pruebas empíricas con BNN del mundo real.
El artículo estudia la robustez adversarial de los clasificadores bayesianos y establece dos condiciones que demuestran que son suficientes para que los "modelos idealizados" en "conjuntos de datos idealizados" no tengan ejemplos adversariales
El artículo plantea una clase de clasificadores bayesianos discriminativos que no tienen ejemplos adversarios.
Introducimos el primer caso de ataques adversarios que reprograman el modelo objetivo para realizar una tarea elegida por el atacante, sin que éste tenga que especificar o calcular la salida deseada para cada entrada en tiempo de prueba.
Los autores presentan un novedoso esquema de ataque adversario en el que una red neuronal es reutilizada para realizar una tarea diferente a la que fue entrenada originalmente
En este trabajo se propone la "reprogramación adversarial" de redes neuronales bien entrenadas y fijas y se demuestra que la reprogramación adversarial es menos eficaz en redes no entrenadas.
El artículo amplía la idea de los "ataques adversarios" en el aprendizaje supervisado de las NNs a un replanteamiento completo de la solución de una red entrenada.
Desarrollamos un nuevo esquema para predecir la brecha de generalización en redes profundas con alta precisión.
Los autores sugieren utilizar un margen geométrico y una distribución de márgenes por capas para predecir la brecha de generalización.
Empíricamente se muestra una interesante conexión entre las estadísticas de margen propuestas y la brecha de generalización, que puede utilizarse para proporcionar algunas ideas prescriptivas hacia la comprensión de la generalización en las redes neuronales profundas. 
Proponemos un algoritmo para recuperar de forma demostrable los parámetros (pesos convolucionales y de salida) de una red convolucional con parches superpuestos.
Este trabajo estudia el aprendizaje teórico de las redes neuronales convolucionales de una capa, lo que da como resultado un algoritmo de aprendizaje y garantías demostrables utilizando el algoritmo.
Este trabajo ofrece un nuevo algoritmo para el aprendizaje de una red neuronal de dos capas que implica un único filtro convolucional y un vector de pesos para diferentes ubicaciones.
Un nuevo término de regularización puede mejorar su entrenamiento de wasserstein gans
El trabajo propone un esquema de regularización para Wasserstein GAN basado en la relajación de las restricciones de la constante Lipschitz de 1.
El artículo trata de la regularización/penalización en el ajuste de GANs, cuando se basa en una métrica L_1 Wasserstein.
Proponemos el método proximal de Wasserstein para el entrenamiento de GANs. 
Propone un nuevo procedimiento GAN que tiene en cuenta los puntos generados en la iteración anterior y actualiza el generador para que se realice l veces.
Considera el aprendizaje de gradiente natural en el aprendizaje GAN, donde se emplea la estructura riemanniana inducida por la distancia Wasserstein-2.
El artículo pretende utilizar el gradiente natural inducido por la distancia Wasserstein-2 para entrenar el generador en GAN y los autores proponen el operador proximal de Wasserstein como regularización.
Este trabajo introduce una función heurística basada en la eliminación para la toma de decisiones secuenciales, adecuada para guiar los algoritmos de búsqueda AND/OR para resolver diagramas de influencia.
generaliza la heurística de inferencia de los minibuckets a los diagramas de influencia.
Aproximación de la media y la varianza de la salida de la NN en caso de entrada ruidosa / abandono / parámetros inciertos. Aproximaciones analíticas para las capas argmax, softmax y max.
Los autores se centran en el problema de la propagación de la incertidumbre DNN
Este trabajo revisa la propagación feed-forward de la media y la varianza en las neuronas, abordando el problema de la propagación de la incertidumbre a través de las capas max-pooling y softmax.
Un discriminador que no se deja engañar fácilmente por el ejemplo adversario hace que el entrenamiento del GAN sea más robusto y conduce a un objetivo más suave.
Este trabajo propone una nueva forma de estabilizar el proceso de entrenamiento de GAN mediante la regularización del Discriminador para que sea robusto a los ejemplos adversos.
El artículo propone una forma sistemática de entrenar GANs con términos de regularización de robustez, lo que permite un entrenamiento más suave de los GANs. 
Presenta la idea de que haciendo un discriminador robusto a las perturbaciones adversarias el objetivo del GAN puede hacerse suave lo que resulta en mejores resultados tanto visualmente como en términos de FID.
Modelamos la función de activación de cada neurona como un Proceso Gaussiano y la aprendemos junto con el peso con Inferencia Variacional.
Proponer la colocación de priores de procesos gaussianos en la forma funcional de cada función de activación en la red neuronal para aprender la forma de las funciones de activación.
Proporcionamos un estudio teórico de las propiedades de las redes ReLU circulantes-diagonales profundas y demostramos que son aproximadores universales de anchura limitada.
El artículo propone utilizar matrices circulantes y diagonales para acelerar el cálculo y reducir los requisitos de memoria en las redes neuronales.
Este trabajo demuestra que las redes ReLU diagonales-circulantes de anchura limitada (DC-ReLU) son aproximadores universales.
StarHopper es una novedosa interfaz de pantalla táctil para la navegación eficiente y flexible de drones con cámara centrada en objetos
Los autores esbozan una nueva interfaz de control de drones StarHopper que han desarrollado, combinando el pilotaje automatizado y el manual en una nueva interfaz de navegaciÃ³n hÃbrida y se deshace de la suposiciÃ³n de que el objeto objetivo ya estÃ¡ en el FOV del dron mediante el uso de una cÃ¡mara superior adicional.
Este artículo presenta StarHopper, un sistema de navegación semiautomática de drones en el contexto de la inspección remota.
Presenta StarHopper, una aplicación que utiliza técnicas de visión por ordenador con entrada táctil para apoyar el pilotaje de drones con un enfoque centrado en el objeto.
Una red de autoatención para la codificación de secuencias sin RNN/CNN con un pequeño consumo de memoria, un cálculo altamente paralelizable y un rendimiento de vanguardia en varias tareas de PNL
Propone aplicar la autoatención en dos niveles para limitar el requisito de memoria en los modelos basados en la atención con un impacto insignificante en la velocidad.
Este artículo presenta el modelo de autoatención de bloque bidireccional como codificador de propósito general para varias tareas de modelado de secuencias en PNL
Un nuevo modelo de vanguardia para la respuesta a preguntas con múltiples evidencias utilizando atención jerárquica de grano grueso y fino.
Propone un método de control de calidad multisalto basado en dos módulos separados (módulos de grano grueso y de grano fino).
Este artículo propone una interesante arquitectura de red de atención de grano grueso y fino para abordar la respuesta a preguntas con múltiples evidencias
Se centra en la garantía de calidad de varias opciones y propone un marco de puntuación de grueso a fino.
Proponemos una nueva metodología para el transporte óptimo desequilibrado utilizando redes generativas adversarias.
Los autores consideran el problema de transporte óptimo no equilibrado entre dos medidas con diferente masa total utilizando un algoritmo estocástico min-max y un escalado local
Los autores proponen un enfoque para estimar el transporte óptimo desequilibrado entre las medidas muestreadas que escala bien en la dimensión y en el número de muestras.
El artículo introduce una formulación estática para el transporte óptimo desequilibrado mediante el aprendizaje simultáneo de un mapa de transporte T y un factor de escala xi.
Proponemos un nuevo método de extracción de mapas de saliencia que permite extraer mapas de mayor calidad.
Propone un método agnóstico de clasificación para la extracción de mapas de saliencia.
Este artículo presenta un nuevo extractor de mapas de saliencia que parece mejorar los resultados del estado del arte.
Los autores argumentan que cuando un mapa de saliencia extraído depende directamente de un modelo, entonces podría no ser útil para un clasificador diferente, y sugiere un esquema para aproximar la solución.
Proponemos un método novedoso para incorporar el conjunto de atributos de instancia para la traducción de imagen a imagen.
Este artículo propone un método -InstaGAN- que se basa en CycleGAN teniendo en cuenta la información de las instancias en forma de máscaras de segmentación por instancia, con resultados que se comparan favorablemente con CycleGAN y otras líneas de base.
 Propone añadir máscaras de segmentación conscientes de la instancia para el problema de la traducción de imagen a imagen no emparejada.
El mapa parámetro-función de las redes profundas está enormemente sesgado; esto puede explicar por qué se generalizan. Utilizamos PAC-Bayes y procesos gaussianos para obtener límites no vacíos.
El artículo estudia la capacidad de generalización de las redes neuronales profundas, con la ayuda de la teoría de aprendizaje PAC-Bayesiana y de intuiciones respaldadas empíricamente.
Este artículo propone una explicación de los comportamientos de generalización de las grandes redes neuronales sobreparametrizadas afirmando que el mapa parámetro-función en las redes neuronales está sesgado hacia funciones "simples" y el comportamiento de generalización será bueno si el concepto objetivo también es "simple".
Presentamos el EM Evolutivo como un novedoso algoritmo para el entrenamiento no supervisado de modelos generativos con variables latentes binarias que conecta íntimamente el EM variacional con la optimización evolutiva
El trabajo presenta una combinación de computación evolutiva y EM variacional para modelos con variables latentes binarias representadas mediante una aproximación basada en partículas
El artículo intenta integrar estrechamente los algoritmos de entrenamiento de maximización de expectativas con los algoritmos evolutivos.
Proponemos un modelo de red recurrente eficiente para la predicción a futuro sobre distribuciones variables en el tiempo.
Este artículo propone un método para crear redes neuronales que mapean distribuciones históricas en distribuciones y aplica el método a varias tareas de predicción de distribución.
Propone una Red de Regresión de Distribución Recurrente que utiliza una arquitectura recurrente sobre un modelo anterior de Red de Regresión de Distribución.
Este artículo trata de la regresión sobre las distribuciones de probabilidad estudiando las distribuciones que varían en el tiempo en un entorno de red neuronal recurrente
Un enfoque novedoso para el procesamiento de datos estructurados en forma de grafos mediante redes neuronales, aprovechando la atención sobre la vecindad de un nodo. Obtiene resultados de vanguardia en tareas de redes de citación transductivas y en una tarea de interacción proteína-proteína inductiva.
Este trabajo propone un nuevo método para clasificar los nodos de un grafo, que puede ser utilizado en escenarios semi-supervisados y en un grafo completamente nuevo. 
El artículo presenta una arquitectura de redes neuronales para operar con datos estructurados en forma de grafos, denominada Graph Attention Networks.
Proporciona una discusión justa y casi completa de los enfoques del estado del arte para aprender representaciones vectoriales para los nodos de un gráfico.
Un novedoso enfoque basado en el aprendizaje por refuerzo para comprimir redes neuronales profundas con destilación de conocimientos
Este trabajo propone utilizar el aprendizaje por refuerzo en lugar de una heurística predefinida para determinar la estructura del modelo comprimido en el proceso de destilación del conocimiento
Introduce una forma de principio de compresión de red a red, que utiliza gradientes de política para optimizar dos políticas que comprimen un maestro fuerte en un modelo de estudiante fuerte pero más pequeño.
Demostramos que el colapso del modo en los GANs condicionales se atribuye en gran medida a un desajuste entre la pérdida de reconstrucción y la pérdida del GAN e introducimos un conjunto de nuevas funciones de pérdida como alternativas para la pérdida de reconstrucción.
El artículo propone una modificación del objetivo tradicional del GAN condicional para promover la generación diversa y multimodal de imágenes. 
Este trabajo propone una alternativa a los errores L1/L2 que se utilizan para aumentar las pérdidas adversariales al entrenar GANs condicionales.
Utilizamos la inferencia causal para caracterizar la arquitectura de los modelos generativos
Este artículo examina la naturaleza de los filtros convolucionales en el codificador y el decodificador de un VAE, y en el generador y el discriminador de un GAN.
Este trabajo explota el principio de causalidad para cuantificar cómo se adaptan los pesos de las capas sucesivas entre sí.
Un enfoque para aprender un espacio de incrustación compartido entre juegos visualmente distintos.
Un nuevo enfoque para el aprendizaje de la estructura subyacente de los juegos visualmente distintos que combina capas convolucionales para el procesamiento de las imágenes de entrada, Asynchronous Advantage Actor Critic para el aprendizaje de refuerzo profundo y el enfoque adversarial para forzar la representación de incrustación para ser independiente de la representación visual de los juegos
Presenta un método para aprender una política sobre juegos visualmente distintos adaptando el aprendizaje por refuerzo profundo.
Este artículo analiza una arquitectura de agente que utiliza una representación compartida para entrenar múltiples tareas con diferentes estadísticas visuales a nivel de sprite
Estudiamos la ecuación de estado de una red neuronal recurrente. Demostramos que el SGD puede aprender eficientemente la dinámica desconocida a partir de pocas observaciones de entrada/salida bajo supuestos adecuados.
El artículo estudia sistemas dinámicos de tiempo discreto con una ecuación de estado no lineal, demostrando que la ejecución del SGD en una trayectoria de longitud fija da una convergencia logarítmica.
Este trabajo considera el problema del aprendizaje de un sistema dinámico no lineal en el que la salida es igual al estado. 
Este trabajo estudia la capacidad del SGD para aprender la dinámica de un sistema lineal y la activación no lineal.
Un nuevo método de destilación de conocimientos para el aprendizaje por transferencia
El trabajo introduce un método de destilación del conocimiento utilizando el concepto de colector de neuronas propuesto. 
Propone un método de destilación del conocimiento en el que se toma el colector neuronal como conocimiento transferido.
Presentamos un método sencillo y general para entrenar una única red neuronal ejecutable con diferentes anchuras (número de canales en una capa), lo que permite realizar compensaciones instantáneas y adaptables de precisión y eficiencia en tiempo de ejecución.
El artículo propone la idea de combinar modelos de diferentes tamaños en una red compartida, lo que mejora en gran medida el rendimiento de la detección
Este trabajo entrena un único ejecutable de red con diferentes anchuras.
Red de similitud para aprender una estimación de similitud visual no métrica entre un par de imágenes
Los autores proponen el aprendizaje de la medida de similitud visual y obtienen con ello una mejora en conjuntos de datos muy conocidos de Oxford y París para la recuperación de imágenes.
El documento argumenta que es más adecuado utilizar distancias no métricas en lugar de distancias métricas.
Un nuevo aprendizaje adversarial cíclico aumentado con un modelo de tarea auxiliar que mejora el rendimiento de la adaptación del dominio en situaciones supervisadas y no supervisadas de bajos recursos 
Propone una extensión de los métodos de adaptación adversativa consistente en ciclos para abordar la adaptación del dominio cuando se dispone de datos objetivo supervisados limitados.
Este trabajo introduce un enfoque de adaptación de dominio basado en la idea de GAN cíclico y propone dos algoritmos diferentes.
Desarrollamos un método de aprendizaje de firmas estructurales en redes basado en la difusión de ondículas de grafos espectrales.
Utilización de patrones de difusión de ondículas de gráficos espectrales de un nodo local para incrustar el nodo en un espacio de baja dimensión
El documento deriva una forma de comparar nodos en el gráfico basada en el análisis wavelet del laplaciano del gráfico. 
Un simulador de conducción de realidad mixta que utiliza cámaras estereoscópicas y RV de paso evaluado en un estudio de usuarios con 24 participantes.
Propone un complicado sistema de simulación de conducción.
Este trabajo presenta un montaje de simulador de conducción de realidad mixta para mejorar la sensación de presencia
Propone un simulador de conducción de realidad mixta que incorpora la generación de tráfico y reclama una mayor "presencia" gracias a un sistema de RM.
Reglas de cuadratura para la aproximación del núcleo.
El artículo propone mejorar la aproximación del núcleo de las características aleatorias mediante el uso de reglas de cuadratura como las reglas estocásticas esféricas-radiales.
Los autores proponen una versión novedosa del enfoque del mapa de características aleatorias para resolver de forma aproximada los problemas del núcleo a gran escala.
Este trabajo muestra que las técnicas debidas a Genz & Monahan (1998) pueden ser utilizadas para lograr un bajo error de aproximación del núcleo bajo el marco de la característica aleatoria de Fourier, una nueva forma de aplicar las reglas de cuadratura para mejorar la aproximación del núcleo.
Cómo construir altavoces/escuchas neuronales que aprendan las características finas de los objetos 3D, a partir del lenguaje referencial.
Los autores presentan un estudio sobre el aprendizaje de la referencia a objetos 3D, recopilando un conjunto de datos de expresiones referenciales y entrenando varios modelos mediante la experimentación de una serie de opciones arquitectónicas
Presentamos un marco de trabajo para el aprendizaje de representaciones centradas en el objeto adecuadas para la planificación en tareas que requieren una comprensión de la física.
El artículo presenta una plataforma para predecir imágenes de objetos que interactúan entre sí bajo el efecto de las fuerzas gravitatorias.
El artículo presenta un método que aprende a reproducir "torres de bloques" a partir de una imagen dada.
Propone un método que aprende a razonar sobre la interacción física de diferentes objetos sin supervisar sus propiedades.
Proporcionamos condiciones necesarias y suficientes eficientemente comprobables para la optimización global en redes neuronales lineales profundas, con algunas extensiones iniciales a entornos no lineales.
El artículo da condiciones para la optimalidad global de la función de pérdida de las redes neuronales lineales profundas
El artículo ofrece resultados teóricos sobre la existencia de mínimos locales en la función objetivo de las redes neuronales profundas.
Estudia algunas propiedades teóricas de las redes lineales profundas.
Utilización de un modelo de autoencoder recurrente para extraer características de series temporales multidimensionales
Este escrito describe una aplicación del autoencoder recurrente para analizar series temporales multidimensionales
El artículo describe un modelo de autocodificador secuencia a secuencia que se utiliza para aprender representaciones de la secuencia, mostrando que para su aplicación se obtiene un mejor rendimiento cuando la red sólo se entrena para reconstruir un subconjunto de las mediciones de datos. 
Propone una estrategia inspirada en el modelo de autocodificador recurrente para poder realizar la agrupación de los datos de las series temporales multidimensionales basándose en los vectores de contexto.
Introducimos un marco de codificador-decodificador de grafos para aprender diversas traducciones de grafos.
Propone un modelo de traducción de gráfico a gráfico para la optimización de moléculas inspirado en el análisis de pares moleculares emparejados.
Extensión de JT-VAE al escenario de traducción de gráfico a gráfico añadiendo la variable latente para capturar la multimodalidad y una regularización adversarial en el espacio latente
Propone un sistema bastante complejo, que implica muchas opciones y componentes diferentes, para obtener nubes químicas con propiedades mejoradas a partir de un corpus dado.
Aprendemos un solucionador neuronal rápido para las EDP que tiene garantías de convergencia.
Desarrolla un método para acelerar el método de las diferencias finitas en la resolución de las EDP y propone un marco revisado para la iteración del punto fijo después de la discretización.
Los autores proponen un método lineal para acelerar los solucionadores de EDP.
Realizamos una inferencia variacional funcional sobre los procesos estocásticos definidos por las redes neuronales bayesianas.
Ajuste de aproximaciones de Redes Neuronales Bayesianas variacionales en forma funcional y considerando la adecuación a un proceso estocástico a priori de forma implícita a través de muestras.
Presenta un novedoso objetivo ELBO para el entrenamiento de BNNs que permite codificar en el modelo unas priores más significativas que las características de peso menos informativas de la literatura.
Presenta un nuevo algoritmo de inferencia variacional para modelos bayesianos de redes neuronales en los que la prioridad se especifica funcionalmente en lugar de a través de una prioridad sobre los pesos. 
Incrustamos las palabras en el espacio hiperbólico y establecemos la conexión con las incrustaciones de palabras gaussianas.
Este trabajo adapta la incrustación de palabras Glove a un espacio hiperbólico dado por el modelo de medio plano de Poincare
Este trabajo propone un enfoque para implementar un modelo de incrustación de palabras hiperbólico basado en GLOVE, que se optimiza a través de los métodos de optimización de Riemann.
Las redes de memoria no aprenden el razonamiento multisalto a menos que las supervisemos.
Afirma que el razonamiento multisaltos no es fácil de aprender directamente y requiere supervisión directa, y que hacerlo bien en WikiHop no significa necesariamente que el modelo esté aprendiendo realmente a saltar.
El artículo propone investigar el conocido problema del aprendizaje de redes de memoria y, más concretamente, la dificultad de la supervisión del aprendizaje de la atención con tales modelos.
En este trabajo se argumenta que la red de memoria no aprende un razonamiento multisalto razonable.
Introducimos redes generativas que no requieren ser aprendidas con un discriminador o un codificador; se obtienen invirtiendo un operador especial de incrustación definido por una transformada wavelet de dispersión.
Introduce las transformadas de dispersión como modelos generativos de imágenes en el contexto de las Redes Generativas Adversariales y sugiere por qué podrían verse como transformadas de gaussianización con pérdida de información e invertibilidad controladas. 
El artículo propone un modelo generativo para imágenes que no requiere aprender un discriminador (como en los GANs) o un embedding aprendido.
Proponemos un nuevo modelo de lectura neuronal rápida que utiliza la estructura de puntuación inherente a un texto para definir el comportamiento de salto y omisión efectivo.
El artículo propone un modelo Structural-Jump-LSTM para acelerar la lectura mecánica con dos agentes en lugar de uno
Propone un novedoso modelo de lectura rápida neuronal en el que el nuevo lector tiene la capacidad de saltarse una palabra o secuencia de palabras.
El artículo propone un método de lectura rápida que utiliza acciones de salto y salto, demostrando que el método propuesto es tan preciso como el LSTM pero utiliza mucho menos cálculo.
Proponemos la arquitectura IRN para aumentar la recompensa de compra dispersa y retrasada para la recomendación basada en la sesión.
El artículo propone mejorar el rendimiento de los sistemas de recomendación a través del aprendizaje por refuerzo mediante el uso de una red de reconstrucción de la imaginación.
El artículo presenta un enfoque de recomendación basado en la sesión, centrándose en las compras del usuario en lugar de los clics. 
Explicación de la generalización de los algoritmos de aprendizaje profundo estocástico, teórica y empíricamente, a través de la robustez del conjunto
Este trabajo presenta una adaptación de la robustez algorítmica de Xu&Mannor'12 y presenta límites de aprendizaje y un experimento que muestra la correlación entre la robustez empírica del conjunto y el error de generalización. 
Propone un estudio de la capacidad de generalización de los algoritmos de aprendizaje profundo utilizando una extensión de la noción de estabilidad llamada robustez de conjunto y da límites al error de generalización de un algoritmo aleatorio en términos del parámetro de estabilidad y proporciona un estudio empírico que intenta conectar la teoría con la práctica.
El trabajo estudió la capacidad de generalización de los algoritmos de aprendizaje desde el punto de vista de la robustez en un contexto de aprendizaje profundo
Aprendizaje de pocos disparos PixelCNN
El artículo propone utilizar la estimación de la densidad cuando la disponibilidad de datos de entrenamiento es baja mediante un modelo de meta-aprendizaje.
Este trabajo considera el problema de la estimación de la densidad de uno/varios disparos, utilizando técnicas de metalearning que han sido aplicadas al aprendizaje supervisado de uno/varios disparos
El artículo se centra en el aprendizaje de pocos disparos con estimación de densidad autorregresiva y mejora PixelCNN con técnicas de atención neuronal y metaaprendizaje.
Demostramos que SGD aprende redes neuronales de dos capas sobreparametrizadas con activaciones Leaky ReLU que generalizan de forma demostrable en datos linealmente separables.
El artículo estudia los modelos sobreparametrizados siendo capaces de aprender soluciones bien generalizadas utilizando una red de 1 capa oculta con capa de salida fija.
Este trabajo muestra que en datos linealmente separados, el SGD en una red sobreparametrizada puede aún inclinar un clasificador que se generaliza de forma demostrable.
Entrenar a los agentes con los cuellos de botella de la información de la política de objetivos promueve la transferencia y produce un potente bono de exploración
Propone regularizar las pérdidas estándar de RL con la información mutua condicional negativa para la búsqueda de políticas en un entorno de RL multiobjetivo.
Este trabajo propone el concepto de estado de decisión y propone una regularización de divergencia KL para aprender la estructura de las tareas y utilizar esta información para animar a la política a visitar los estados de decisión.
El artículo propone un método de regularización de las políticas condicionadas por objetivos con un término de información mutua. 
Proponemos un método de optimización para cuando sólo se dispone de gradientes sesgados: definimos un nuevo estimador de gradiente para este escenario, derivamos el sesgo y la varianza de este estimador y lo aplicamos a problemas de ejemplo.
Los autores proponen un enfoque que combina la búsqueda aleatoria con la información del gradiente sustituto y ofrecen una discusión sobre el equilibrio entre la varianza y el sesgo, así como una discusión sobre la optimización de los hiperparámetros.
 El artículo propone un método para mejorar la búsqueda aleatoria mediante la construcción de un subespacio de los k gradientes sustitutos anteriores.
Este trabajo intenta acelerar la evolución del tipo OpenAI introduciendo una distribución no isotrófica con una matriz de covarianza de la forma I + UU^t e información externa como un gradiente sustituto para determinar U
Un GAN que utiliza operaciones de convolución de grafos con grafos calculados dinámicamente a partir de características ocultas
El trabajo propone una versión de GANs específicamente diseñada para la generación de nubes de puntos, siendo la aportación principal del trabajo la operación de upsampling.
Este trabajo propone GANs grafo-convolucionales para nubes de puntos 3D irregulares que aprenden el dominio y las características al mismo tiempo.
Introducimos un algoritmo rápido y fácil de implementar que es robusto al ruido del conjunto de datos.
El documento pretende eliminar los ejemplos potenciales con ruido de etiqueta descartando los que tienen grandes pérdidas en el procedimiento de entrenamiento.
Los ataques basados en el gradiente a las redes neuronales binarizadas no son eficaces debido a la no diferenciabilidad de dichas redes; nuestro algoritmo IPROP resuelve este problema utilizando la optimización de enteros
Propone un nuevo algoritmo de estilo de propagación de objetivos para generar fuertes ataques adversarios en redes neuronales binarizadas.
Este trabajo propone un nuevo algoritmo de ataque basado en MILP sobre redes neuronales binarias.
Este trabajo presenta un algoritmo para encontrar ataques adversarios a redes neuronales binarias que encuentra iterativamente las representaciones deseadas capa por capa desde la parte superior hasta la entrada y es más eficiente que resolver el solucionador de programación lineal entera mixta (MILP) completo.
La decodificación del último token del contexto utilizando la distribución del siguiente token predicho actúa como un regularizador y mejora el modelado del lenguaje.
Los autores introducen la idea de decodificación pasada con el fin de regularizar para mejorar la perplejidad en Penn Treebank
Propone un término de pérdida adicional para usar cuando se entrena un LM LSTM y muestra que añadiendo este término de pérdida pueden alcanzar la perplejidad SOTA en una serie de puntos de referencia de LM.
Sugiere una nueva técnica de regularización que puede añadirse a las utilizadas en AWD-LSTM de Merity et al. (2017) con poca sobrecarga.
Proponer la observación de órdenes implícitas en conjuntos de datos desde el punto de vista de un modelo generativo.
Los autores abordan el problema de la ordenación implícita en un conjunto de datos y el reto de recuperarla y proponen aprender un modelo sin métrica de distancia que asume una cadena de Markov como mecanismo generador de los datos 
El artículo propone las redes de Markov generativas, un enfoque basado en el aprendizaje profundo para modelar secuencias y descubrir el orden en los conjuntos de datos.
Propone aprender el orden de una muestra de datos desordenados mediante el aprendizaje de una cadena de Markov.
Síntesis de programas a partir de la descripción en lenguaje natural y de ejemplos de entrada/salida mediante la búsqueda en árbol sobre el modelo Seq2Tree
Presenta un modelo seq2Tree para traducir el enunciado de un problema en lenguaje natural al correspondiente programa funcional en DSL, que ha mostrado una mejora sobre el enfoque de base seq2seq.
Este artículo aborda el problema de hacer síntesis de programas cuando se da una descripción del problema y un pequeño número de ejemplos de entrada-salida.
El artículo introduce una técnica de síntesis de programas que implica una gramática restringida de problemas que se busca mediante una red atencional de codificador-decodificador.
Este trabajo estudia las propiedades de discriminación y generalización de las GANs cuando el conjunto discriminador es una clase de función restringida como las redes neuronales.
Equilibra las capacidades de las clases de generadores y discriminadores en los GANs garantizando que las MIPs inducidas son métricas y no pseudométricas
Este trabajo proporciona un análisis matemático del papel del tamaño del conjunto adversario/discriminador en los GANs
Todo lo que se necesita para entrenar redes residuales profundas es una buena inicialización; las capas de normalización no son necesarias.
Se presenta un método para la inicialización y normalización de redes residuales profundas. Se basa en observaciones de la explosión hacia delante y hacia atrás en dichas redes. El rendimiento del método está a la altura de los mejores resultados obtenidos por otras redes con una normalización más explícita.
Los autores proponen una forma novedosa de inicializar las redes residuales, motivada por la necesidad de evitar la explosión/desvanecimiento de los gradientes.
Propone un nuevo método de inicialización utilizado para entrenar RedNets muy profundas sin utilizar batch-norm.
Este trabajo pretende aprender una métrica mejor para el aprendizaje no supervisado, como la generación de textos, y muestra una mejora significativa sobre SeqGAN.
Describe un enfoque para generar secuencias temporales mediante el aprendizaje de valores estado-acción, donde el estado es la secuencia generada hasta el momento, y la acción es la elección del siguiente valor. 
Este trabajo considera el problema de mejorar la generación de secuencias mediante el aprendizaje de mejores métricas, específicamente el problema del sesgo de exposición
Descomponer la tarea de aprendizaje de un modelo generativo en el aprendizaje de factores latentes desentrañados para subconjuntos de los datos y luego el aprendizaje de la unión sobre esos factores latentes.  
Factores localmente desentrañados para el modelo generativo jerárquico de variables latentes, que puede considerarse como una variante jerárquica de la inferencia aprendida adversarialmente
El artículo investiga el potencial de los modelos de variables latentes jerárquicas para generar imágenes y secuencias de imágenes y propone entrenar varios modelos ALI apilados unos sobre otros para crear una representación jerárquica de los datos.
El trabajo tiene como objetivo aprender las jerarquías para el entrenamiento de GAN en un programa de optimización jerárquica directamente en lugar de ser diseñado por un humano
Proponemos un modelo conjunto para incorporar el conocimiento visual en las representaciones de frases
El artículo propone un método para utilizar vídeos emparejados con subtítulos para mejorar la incrustación de frases
Esta presentación propone un modelo para el aprendizaje de representaciones de oraciones que se basan en datos de vídeo asociados.
Propone un método para mejorar las incrustaciones de frases basadas en el texto mediante un marco multimodal conjunto.
