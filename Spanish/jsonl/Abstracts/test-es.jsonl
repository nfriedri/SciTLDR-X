{"source": ["Incremental class learning involves sequentially learning classes in bursts of examples from the same class.", "This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting.", "Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale.", "Here, we propose FearNet for incremental class learning.", "FearNet is a generative model that does not store previous examples, making it memory efficient.", "FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex.", "Memory consolidation is inspired by mechanisms that occur during sleep.", "FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  ", "FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n"], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJ1Xmf-Rb", "target": ["\ufeffFearNet es una red neuronal eficiente en cuanto a la memoria, inspirada en la formaci\u00f3n de la memoria en el cerebro de los mam\u00edferos, que es capaz de aprender clases de forma incremental sin olvido catastr\u00f3fico.", "Este trabajo presenta una soluci\u00f3n novedosa a un problema de clasificaci\u00f3n incremental basado en un sistema de memoria dual. "]}
{"source": ["Multi-view learning can provide self-supervision when different views are available of the same data.", "Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora.", "Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion.", "One framework uses a generative objective and the other a discriminative one.", "In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model.", "We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learnt counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1xzyhR9Y7", "target": ["El aprendizaje multivista mejora el aprendizaje no supervisado de la representaci\u00f3n de frases", "El enfoque utiliza diferentes codificadores complementarios de la frase de entrada y la maximizaci\u00f3n del consenso.", "El art\u00edculo presenta un marco multivista para mejorar la representaci\u00f3n de oraciones en tareas de PNL utilizando arquitecturas de objetivos generativos y discriminativos.", "Este trabajo demuestra que los marcos multivista son m\u00e1s eficaces que el uso de codificadores individuales para el aprendizaje de representaciones de frases."]}
{"source": ["We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\n", "More precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\n", "Applying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion.", "During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\n", "This gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJDUjKeA-", "target": ["Mostramos c\u00f3mo se pueden aprender objetos discretos de forma no supervisada a partir de los p\u00edxeles, y c\u00f3mo realizar un aprendizaje por refuerzo utilizando esta representaci\u00f3n de objetos.", "Un m\u00e9todo para aprender representaciones de objetos a partir de p\u00edxeles para realizar un aprendizaje por refuerzo. ", "El art\u00edculo propone una arquitectura neuronal para asignar flujos de v\u00eddeo a una colecci\u00f3n discreta de objetos, sin anotaciones humanas, utilizando una p\u00e9rdida de reconstrucci\u00f3n de p\u00edxeles no supervisada. "]}
{"source": ["Most recent gains in visual recognition have originated from the inclusion of attention mechanisms in deep convolutional networks (DCNs).", "Because these networks are optimized for object recognition, they learn where to attend using only a weak form of supervision derived from image class labels.", "Here, we demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition.", "We first describe a large-scale online experiment (ClickMe) used to supplement ImageNet with nearly half a million human-derived \"top-down\" attention maps.", "Using human psychophysics, we confirm that the identified top-down features from ClickMe are more diagnostic than \"bottom-up\" saliency features for rapid image categorization.", "As a proof of concept, we extend a state-of-the-art attention network and demonstrate that adding ClickMe supervision significantly improves its accuracy and yields visual features that are more interpretable and more similar to those used by human observers."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJgLg3R9KQ", "target": ["Un conjunto de datos a gran escala para entrenar modelos de atenci\u00f3n para el reconocimiento de objetos conduce a un reconocimiento de objetos m\u00e1s preciso, interpretable y similar al humano.", "Sostiene que los recientes avances en el reconocimiento visual se deben a la utilizaci\u00f3n de mecanismos de atenci\u00f3n visual en redes convolucionales profundas, que aprenden d\u00f3nde enfocar mediante una forma d\u00e9bil de supervisi\u00f3n basada en las etiquetas de clase de las im\u00e1genes.", "Presenta una nueva visi\u00f3n de la atenci\u00f3n en la que se recoge un gran conjunto de datos de atenci\u00f3n y se utiliza para entrenar una NN de forma supervisada para explotar la atenci\u00f3n humana autodeclarada.", "Este trabajo propone un nuevo enfoque para utilizar se\u00f1ales m\u00e1s informativas, concretamente, las regiones que los humanos consideran importantes en las im\u00e1genes, para mejorar las redes neuronales convolucionales profundas."]}
{"source": ["In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks.", "However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes.", "Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  ", "However, this robustness is only effective tothe same attack method used for adversarial training.  ", "Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   ", "However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.", "In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  ", "We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BklpOo09tQ", "target": ["Proponemos un m\u00e9todo de defensa eficiente en el tiempo contra los ataques adversarios de un solo paso e iterativos.", "Proponemos un m\u00e9todo novedoso y eficiente desde el punto de vista computacional, denominado e2SAD, que genera conjuntos de dos muestras adversarias de entrenamiento para cada muestra de entrenamiento limpia.", "El art\u00edculo introduce un m\u00e9todo de defensa adversarial en dos pasos, para generar dos ejemplos adversariales por cada muestra limpia e incluirlos en el bucle de entrenamiento real para lograr robustez y afirmar que puede superar a m\u00e9todos iterativos m\u00e1s costosos.", "El art\u00edculo presenta un enfoque de 2 pasos para generar ejemplos adversarios fuertes con un coste mucho menor en comparaci\u00f3n con los recientes ataques adversarios iterativos de varios pasos."]}
{"source": ["Recently several different deep learning architectures have been proposed that take a string of characters as the raw input signal and automatically derive features for text classification.", "Little studies are available that compare the effectiveness of these approaches for character based text classification with each other.", "In this paper we perform such an empirical comparison for the important cybersecurity problem of DGA detection: classifying domain names as either benign vs. produced by malware (i.e., by a Domain Generation Algorithm).", "Training and evaluating on a dataset with 2M domain names shows that there is surprisingly little difference between various convolutional neural network (CNN) and recurrent neural network (RNN) based architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting."], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "BJLmN8xRW", "target": ["Una comparaci\u00f3n de cinco arquitecturas de redes neuronales profundas para la detecci\u00f3n de nombres de dominio maliciosos muestra sorprendentemente pocas diferencias.", "Los autores proponen utilizar cinco arquitecturas profundas para la tarea de ciberseguridad de detecci\u00f3n de algoritmos de generaci\u00f3n de dominios.", "Aplica varias arquitecturas NN para clasificar las url's entre benignas y relacionadas con el malware.", "Este trabajo propone reconocer autom\u00e1ticamente los nombres de dominio como maliciosos o benignos mediante redes profundas entrenadas para clasificar directamente la secuencia de caracteres como tal."]}
{"source": ["Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks.", "Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts.", "We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations.", "We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases.", "Our experiments demonstrate that our models are more robust to new NLI datasets."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rkMlSnAqYX", "target": ["Los m\u00e9todos de aprendizaje adversarial animan a los modelos NLI a ignorar los sesgos espec\u00edficos del conjunto de datos y ayudan a los modelos a transferirse entre conjuntos de datos.", "El art\u00edculo propone una configuraci\u00f3n adversarial para mitigar los artefactos de anotaci\u00f3n en los datos de inferencia del lenguaje natural", "Este art\u00edculo presenta un m\u00e9todo para eliminar el sesgo de un modelo de vinculaci\u00f3n textual mediante un objetivo de entrenamiento adversarial. "]}
{"source": ["We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links.", "The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations.", "In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition.", "Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space.", "In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model.", "Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkgEQnRqYQ", "target": ["Un nuevo enfoque del estado del arte para la incrustaci\u00f3n de grafos de conocimiento.", "Presenta una funci\u00f3n de puntuaci\u00f3n de predicci\u00f3n de enlaces neuronales que puede inferir patrones de simetr\u00eda, antisimetr\u00eda, inversi\u00f3n y composici\u00f3n de relaciones en una base de conocimientos.", "Este trabajo propone un enfoque para la incrustaci\u00f3n de grafos de conocimiento modelando las relaciones como rotaciones en el espacio vectorial complejo.", "Propone un m\u00e9todo de incrustaci\u00f3n de gr\u00e1ficos para la predicci\u00f3n de enlaces"]}
{"source": ["Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification.", "This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks.", "However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms.", "In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels.", "To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps.", "Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels.", "Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\n", "adversarial attacks without employment of additional target detection and rejection algorithms.\n", "We empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016)."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rkeDJ04Mf", "target": ["Modificamos la CNN utilizando HyperNetworks y observamos una mayor robustez frente a los ejemplos adversos.", "Mejora de la robustez y fiabilidad de las redes neuronales de convoluci\u00f3n profunda mediante el uso de n\u00facleos de convoluci\u00f3n dependientes de los datos"]}
{"source": ["Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\n", "Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\n", "Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\n", "In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\n", "The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\n", "This requires back-propagating errors through the solver steps.\n", "While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\n", "We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\n", "Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HyxnZh0ct7", "target": ["Proponemos un enfoque de meta-aprendizaje para la clasificaci\u00f3n de pocos disparos que logra un fuerte rendimiento a alta velocidad por retropropagaci\u00f3n a trav\u00e9s de la soluci\u00f3n de solucionadores r\u00e1pidos, como la regresi\u00f3n de cresta o la regresi\u00f3n log\u00edstica.", "El art\u00edculo propone un algoritmo de meta-aprendizaje que consiste en fijar las caracter\u00edsticas (es decir, todas las capas ocultas de una NN profunda), y tratar cada tarea como si tuviera su propia capa final, que podr\u00eda ser una regresi\u00f3n de cresta o una regresi\u00f3n log\u00edstica.", "Este trabajo propone un enfoque de meta-aprendizaje para el problema de la clasificaci\u00f3n de pocos disparos, utilizan un m\u00e9todo basado en la parametrizaci\u00f3n del aprendiz para cada tarea por un solucionador de forma cerrada."]}
{"source": ["While many active learning papers assume that the learner can simply ask for a label and receive it, real annotation often presents a mismatch between the form of a label (say, one among many classes), and the form of an annotation (typically yes/no binary feedback).", "To annotate examples corpora for multiclass classification, we might need to ask multiple yes/no questions, exploiting a label hierarchy if one is available.", "To address this more realistic setting, we propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask.", "At each step, the learner selects an example, asking if it belongs to a chosen (possibly composite) class.", "Each answer eliminates some classes, leaving the learner with a partial label.", "The learner may then either ask more questions about the same example (until an exact label is uncovered) or move on immediately, leaving the first example partially labeled.", "Active learning with partial labels requires", "(i) a sampling strategy to choose (example, class) pairs, and", "(ii) learning from partial labels between rounds.", "Experiments on Tiny ImageNet demonstrate that our most effective method improves 26% (relative) in top-1 classification accuracy compared to i.i.d. baselines and standard active learners given 30% of the annotation budget that would be required (naively) to annotate the dataset.", "Moreover, ALPF-learners fully annotate TinyImageNet at 42% lower cost.", "Surprisingly, we observe that accounting for per-example annotation costs can alter the conventional wisdom that active learners should solicit labels for hard examples."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJfSEnRqKQ", "target": ["Aportamos una nueva perspectiva sobre el entrenamiento de un modelo de aprendizaje autom\u00e1tico desde cero en el entorno de las etiquetas jer\u00e1rquicas, es decir, pens\u00e1ndolo como una comunicaci\u00f3n bidireccional entre humanos y algoritmos, y estudiamos c\u00f3mo podemos medir y mejorar la eficiencia. ", "Introduce una nueva configuraci\u00f3n de Aprendizaje Activo en la que el or\u00e1culo ofrece una etiqueta parcial o d\u00e9bil en lugar de consultar la etiqueta de un ejemplo concreto, lo que permite una recuperaci\u00f3n m\u00e1s sencilla de la informaci\u00f3n.", "Este trabajo propone un m\u00e9todo de aprendizaje activo con retroalimentaci\u00f3n parcial que supera las l\u00edneas de base existentes con un presupuesto limitado.", "El art\u00edculo considera un problema de clasificaci\u00f3n multiclase en el que las etiquetas se agrupan en un n\u00famero determinado M de subconjuntos, que contienen todas las etiquetas individuales como singletons."]}
{"source": ["Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions.", "Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric.", "Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures.", "We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions.", "We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding.", "We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space.", "This obviates the need for dimensionality reduction techniques such as t-SNE for visualization."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJg4J3CqFm", "target": ["Demostramos que los espacios de Wasserstein son buenos objetivos para incrustar datos con una estructura sem\u00e1ntica compleja.", "Aprende incrustaciones en un espacio discreto de distribuciones de probabilidad, utilizando una versi\u00f3n minimizada y regularizada de las distancias de Wasserstein.", "El art\u00edculo describe un nuevo m\u00e9todo de incrustaci\u00f3n que incrusta los datos en el espacio de medidas de probabilidad dotado de la distancia de Wasserstein. ", "El art\u00edculo propone incrustar los datos en espacios de Wasserstein de baja dimensi\u00f3n, que pueden capturar la estructura subyacente de los datos con mayor precisi\u00f3n."]}
{"source": ["Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces.", "We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly.", "The data is embedded into a lower-dimensional space by a deep autoencoder.", "The autoencoder is optimized as part of the clustering process.", "The resulting network produces clustered data.", "The presented approach does not rely on prior knowledge of the number of ground-truth clusters.", "Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective.", "We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms.", "Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJzMATlAZ", "target": ["Un algoritmo de agrupaci\u00f3n que realiza una reducci\u00f3n de la dimensionalidad no lineal y una agrupaci\u00f3n conjunta optimizando un objetivo global continuo.", "Presenta un algoritmo de clustering resolviendo conjuntamente el autoencoder profundo y el clustering como objetivo global continuo, mostrando mejores resultados que los esquemas de clustering del estado del arte.", "El clustering continuo profundo es un m\u00e9todo de clustering que integra el objetivo del autoencoder con el objetivo del clustering y luego se entrena utilizando el SGD."]}
{"source": ["Deep convolutional neural networks (CNNs) are deployed in various applications but demand immense computational requirements.", "Pruning techniques and Winograd convolution are two typical methods to reduce the CNN computation.", "However, they cannot be directly combined because Winograd transformation fills in the sparsity resulting from pruning.", "Li et al. (2017) propose sparse Winograd convolution in which weights are directly pruned in the Winograd domain, but this technique is not very practical because Winograd-domain retraining requires low learning rates and hence significantly longer training time.", "Besides, Liu et al. (2018) move the ReLU function into the Winograd domain, which can help increase the weight sparsity but requires changes in the network structure.", "To achieve a high Winograd-domain weight sparsity without changing network structures, we propose a new pruning method, spatial-Winograd pruning.", "As the first step, spatial-domain weights are pruned in a structured way, which efficiently transfers the spatial-domain sparsity into the Winograd domain and avoids Winograd-domain retraining.", "For the next step, we also perform pruning and retraining directly in the Winograd domain but propose to use an importance factor matrix to adjust weight importance and weight gradients.", "This adjustment makes it possible to effectively retrain the pruned Winograd-domain network without changing the network structure.", "For the three models on the datasets of CIFAR-10, CIFAR-100, and ImageNet, our proposed method can achieve the Winograd-domain sparsities of 63%, 50%, and 74%, respectively."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJzYdsAqY7", "target": ["Para acelerar el c\u00e1lculo de las redes neuronales convolucionales, proponemos una nueva t\u00e9cnica de poda en dos pasos que consigue una mayor dispersi\u00f3n de pesos en el dominio de Winograd sin cambiar la estructura de la red.", "Propone un marco de poda espacial-Winograd que permite que el peso podado del dominio espacial se mantenga en el dominio Winograd y mejora la escasez del dominio Winograd.", "Propone dos t\u00e9cnicas de poda de capas convolucionales que utilizan el algoritmo de Winograd"]}
{"source": ["In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited.", "We develop a Bayesian nonparametric framework for federated learning with neural networks.", "Each data server is assumed to train local neural network weights, which are modeled through our framework.", "We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling.", "We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SygHGnRqK7", "target": ["Proponemos un modelo bayesiano no param\u00e9trico para el aprendizaje federado con redes neuronales.", "Utiliza el proceso beta para realizar la correspondencia neuronal federada.", "El documento considera el aprendizaje federado de redes neuronales, donde los datos se distribuyen en m\u00faltiples m\u00e1quinas y la asignaci\u00f3n de puntos de datos es potencialmente no homog\u00e9nea y desequilibrada."]}
{"source": ["We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution.", "Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed.", "We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second.", "Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling.", "Python source code will be open-sourced with the camera-ready paper."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1n8LexRZ", "target": ["M\u00e9todo general para entrenar n\u00facleos MCMC expresivos parametrizados con redes neuronales profundas. Dada una distribuci\u00f3n objetivo p, nuestro m\u00e9todo proporciona un muestreador de mezcla r\u00e1pida, capaz de explorar eficientemente el espacio de estados.", "Propone un HMC generalizado modificando el integrador de salto mediante redes neuronales para que el muestreador converja y se mezcle r\u00e1pidamente. "]}
{"source": ["This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences.", "We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure.", "The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents.", "We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation.", "To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach.", "Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities.", "The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization.", "To solve this we propose a continuation approach that learns failure modes in related but less robust agents.", "Our approach also allows reuse of data already collected for training the agent.", "We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving.", "Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1xhQhRcK7", "target": ["Demostramos que los fallos raros pero catastr\u00f3ficos pueden pasar desapercibidos por completo en las pruebas aleatorias, lo que plantea problemas para un despliegue seguro. El enfoque que proponemos para las pruebas adversariales soluciona este problema.", "Propone un m\u00e9todo que aprende un predictor de probabilidad de fallo para un agente aprendido, lo que lleva a predecir qu\u00e9 estados iniciales hacen que un sistema falle.", "Este trabajo propone un enfoque de muestreo de importancia para el muestreo de casos de fallo para los algoritmos de RL basado en una funci\u00f3n aprendida a trav\u00e9s de una red neuronal sobre los fallos que se producen durante el entrenamiento del agente", "Este trabajo propone un enfoque adversarial para identificar casos de fallos catastr\u00f3ficos en el aprendizaje por refuerzo."]}
{"source": ["The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique.", "By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods.", "In practice, however, VAE training often results in a degenerate local optimum known as \"posterior collapse\" where the model learns to ignore the latent variable and the approximate posterior mimics the prior.", "In this paper, we investigate posterior collapse from the perspective of training dynamics.", "We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target.", "As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs.", "Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update.", "Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work.", "Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rylDfnCqF7", "target": ["Para abordar el colapso posterior en las VAE, proponemos un procedimiento de entrenamiento novedoso pero sencillo que optimiza de forma agresiva la red de inferencia con m\u00e1s actualizaciones. Este nuevo procedimiento de entrenamiento mitiga el colapso posterior y conduce a un mejor modelo VAE. ", "Examina el fen\u00f3meno del colapso posterior, mostrando que un mayor entrenamiento de la red de inferencia puede reducir el problema y conducir a mejores \u00f3ptimos.", "Los autores proponen cambiar el procedimiento de entrenamiento de las VAE s\u00f3lo como soluci\u00f3n al colapso posterior, dejando el modelo y el objetivo intactos."]}
{"source": ["Online healthcare services can provide the general public with ubiquitous access to medical knowledge and reduce the information access cost for both individuals and societies.", "To promote these benefits, it is desired to effectively expand the scale of high-quality yet novel relational medical entity pairs that embody rich medical knowledge in a structured form.", "To fulfill this goal, we introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge.", "Rather than discriminatively identifying the relationship between two given medical entities in a free-text corpus, we directly model and understand medical relationships from diversely expressed medical entity pairs.", "The proposed model introduces the generative modeling capacity of variational autoencoder to entity pairs, and has the ability to discover new relational medical entity pairs solely based on the existing entity pairs.", "Beside entity pairs, relationship-enhanced entity representations are obtained as another appealing benefit of the proposed method.", "Both quantitative and qualitative evaluations on real-world medical datasets demonstrate the effectiveness of the proposed method in generating relational medical entity pairs that are meaningful and novel."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJhxcGZCW", "target": ["Descubrir de forma generosa nuevos pares de entidades significativas con una determinada relaci\u00f3n m\u00e9dica mediante el mero aprendizaje a partir de los pares de entidades significativas existentes, sin necesidad de un corpus de texto adicional para la extracci\u00f3n discriminativa.", "Presenta un autoencoder variacional para generar pares de entidades dada una relaci\u00f3n en un entorno m\u00e9dico.", "En el contexto m\u00e9dico, este art\u00edculo describe el problema cl\u00e1sico de \"completar la base de conocimientos\" a partir de datos estructurados \u00fanicamente."]}
{"source": ["Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.", " In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples", ".  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true", ".  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning", ".  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture", ". The code for our model is available at \\url{https://github.com/daib13/TwoStageVAE}."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1e0X3C9tQ", "target": ["Analizamos detenidamente la funci\u00f3n objetivo de la VAE y sacamos conclusiones novedosas que conducen a mejoras sencillas.", "Propone un m\u00e9todo de VAE en dos etapas para generar muestras de alta calidad y evitar la borrosidad.", "Este documento analiza las VAE gaussianas.", "El art\u00edculo proporciona una serie de resultados te\u00f3ricos sobre los autocodificadores variacionales gaussianos \"vainilla\", que luego se utilizan para construir un nuevo algoritmo llamado \"VAE de 2 etapas\"."]}
{"source": ["We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions.  ", "We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters.", "The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.\n \n", "Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning.  ", "In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization.  ", "We also outperform Random Search $8\\times$.\n   \nOur method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform.  ", "We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively)."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1zriGeCZ", "target": ["Un algoritmo de ajuste de hiperpar\u00e1metros mediante el an\u00e1lisis discreto de Fourier y la detecci\u00f3n comprimida", "Investiga el problema de la optimizaci\u00f3n de hiperpar\u00e1metros bajo el supuesto de que la funci\u00f3n desconocida puede ser aproximada, mostrando que la minimizaci\u00f3n aproximada puede realizarse sobre el hipercubo booleano.", "El art\u00edculo explora la optimizaci\u00f3n de los hiperpar\u00e1metros asumiendo una estructura en la funci\u00f3n desconocida que asigna los hiperpar\u00e1metros a la precisi\u00f3n de la clasificaci\u00f3n"]}
{"source": ["Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data.", "Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable.", "In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator.  ", "Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator.", "With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings.", "We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Byt3oJ-0W", "target": ["Un nuevo m\u00e9todo para la inferencia de permutaciones por ascenso de gradiente, con aplicaciones a la inferencia de coincidencias latentes y al aprendizaje supervisado de permutaciones con redes neuronales", "El art\u00edculo utiliza la aproximaci\u00f3n finita del operador Sinkhorn para describir c\u00f3mo se puede construir una red neuronal para el aprendizaje a partir de datos de entrenamiento con valor de permutaci\u00f3n. ", "El art\u00edculo propone un nuevo m\u00e9todo que aproxima la ponderaci\u00f3n m\u00e1xima discreta para el aprendizaje de permutaciones latentes"]}
{"source": ["Recent work in network quantization has substantially reduced the time and space complexity of neural network inference, enabling their deployment on embedded and mobile devices with limited computational and memory resources.", "However, existing quantization methods often represent all weights and activations with the same precision (bit-width).", "In this paper, we explore a new dimension of the design space: quantizing different layers with different bit-widths.", "We formulate this problem as a neural architecture search problem and propose a novel differentiable neural architecture search (DNAS) framework to efficiently explore its exponential search space with gradient-based optimization.", "Experiments show we surpass the state-of-the-art compression of ResNet on CIFAR-10 and ImageNet.", "Our quantized models with 21.1x smaller model size or 103.9x lower computational cost can still outperform baseline quantized or even full precision models."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJGVX3CqYm", "target": ["Un nuevo marco de b\u00fasqueda de arquitectura neuronal diferenciable para la cuantificaci\u00f3n mixta de ConvNets.", "Los autores introducen un nuevo m\u00e9todo de b\u00fasqueda de arquitectura neuronal que selecciona la cuantificaci\u00f3n de precisi\u00f3n de los pesos en cada capa de la red neuronal, y lo utilizan en el contexto de la compresi\u00f3n de la red.", "El art\u00edculo presenta un nuevo enfoque en la cuantificaci\u00f3n de la red mediante la cuantificaci\u00f3n de diferentes capas con diferentes anchos de bits e introduce un nuevo marco de b\u00fasqueda de arquitectura neuronal diferenciable."]}
{"source": ["The top-$k$ error is a common measure of performance in machine learning and computer vision.", "In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss.", "Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data.", "In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements.\n", "Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks.", "Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning.", "The widely used cross-entropy is a special case of our family.", "Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{n}{k})$ operations, where $n$ is the number of classes.", "Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$.", "Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision.", "We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$.", "Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hk5elxbRW", "target": ["Funci\u00f3n de p\u00e9rdida suave para la minimizaci\u00f3n de errores Top-k", "Propone utilizar la p\u00e9rdida top-k con modelos profundos para abordar el problema de la confusi\u00f3n de clases con clases similares tanto presentes como ausentes del conjunto de datos de entrenamiento.", "Suaviza las p\u00e9rdidas top-k.", "Este trabajo introduce una funci\u00f3n de p\u00e9rdida sustituta suave para la SVM top-k, con el fin de conectar la SVM a las redes neuronales profundas."]}
{"source": ["Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties.", "To augment the compound design process we introduce Mol-CycleGAN -- a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest.", "Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property.", "We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP).", "In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BklKFo09YX", "target": ["Presentamos Mol-CycleGAN - un nuevo modelo generativo para la optimizaci\u00f3n de mol\u00e9culas para aumentar el dise\u00f1o de f\u00e1rmacos.", "El art\u00edculo presenta un enfoque para la optimizaci\u00f3n de las propiedades moleculares basado en la aplicaci\u00f3n de CycleGANs a autocodificadores variacionales para mol\u00e9culas y emplea un VAE de dominio espec\u00edfico llamado Junction Tree VAE (JT-VAE).", "Este trabajo utiliza un autocodificador variacional para aprender una funci\u00f3n de traslaci\u00f3n, desde el conjunto de mol\u00e9culas sin la propiedad interesada al conjunto de mol\u00e9culas con la propiedad. "]}
{"source": ["Knowledge distillation is a potential solution for model compression.", "The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one.", "Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network.", "However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered.", "To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification.", "By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks.", "Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates.", "In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification.", "Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJFOptp6Z", "target": ["Tomamos el reconocimiento de rostros como punto de ruptura y proponemos la destilaci\u00f3n de modelos con transferencia de conocimientos desde la clasificaci\u00f3n de rostros hasta la alineaci\u00f3n y la verificaci\u00f3n", "Este trabajo propone transferir el clasificador del modelo de clasificaci\u00f3n de caras a la tarea de alineaci\u00f3n y verificaci\u00f3n.", "El manuscrito presenta experimentos sobre la destilaci\u00f3n de conocimientos de un modelo de clasificaci\u00f3n de caras a modelos de estudiantes para la alineaci\u00f3n y verificaci\u00f3n de caras."]}
{"source": ["RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior.", "The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations.", "In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings.", "The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches.", "Unlike data augmentation-based improvements, our method does not increase training times significantly."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryCM8zWRb", "target": ["Mejora de las recomendaciones basadas en sesiones con RNNs (GRU4Rec) en un 35% utilizando funciones de p\u00e9rdida y muestreo de nuevo dise\u00f1o.", "Este art\u00edculo analiza las funciones de p\u00e9rdida existentes para las recomendaciones basadas en la sesi\u00f3n y propone dos nuevas funciones de p\u00e9rdida que a\u00f1aden una ponderaci\u00f3n a las funciones de p\u00e9rdida basadas en la clasificaci\u00f3n existentes", "Presenta modificaciones sobre trabajos anteriores para la recomendaci\u00f3n basada en sesiones utilizando RNN ponderando los ejemplos negativos por su \"relevancia\"", "En este art\u00edculo se discuten los problemas para optimizar las funciones de p\u00e9rdida en GRU4Rec, se proponen trucos para la optimizaci\u00f3n y se sugiere una versi\u00f3n mejorada."]}
{"source": ["In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks.", "Under the assumption that future tasks are related to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task.", "We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework.", "Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task.", "Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks.", "We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJUBryZ0W", "target": ["Desarrollamos un enfoque de aprendizaje permanente para el aprendizaje por transferencia basado en la teor\u00eda PAC-Bayes, seg\u00fan la cual las preconcepciones se ajustan a medida que se encuentran nuevas tareas, facilitando as\u00ed el aprendizaje de tareas nuevas.", "Un nuevo l\u00edmite de riesgo PAC-Bayesiano que sirve como funci\u00f3n objetivo para el aprendizaje autom\u00e1tico multitarea, y un algoritmo para minimizar una versi\u00f3n simplificada de esa funci\u00f3n objetivo.", "Ampl\u00eda los l\u00edmites de PAC-Bayes existentes al aprendizaje multitarea, para permitir la adaptaci\u00f3n de la priorizaci\u00f3n en diferentes tareas."]}
{"source": ["Optimization algorithms for training deep models not only affects the convergence rate and stability of the training process, but are also highly related to the generalization performance of trained models.", "While adaptive algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in many scenarios, they often lead to worse generalization performance than SGD, when used for training deep neural networks (DNNs).", "In this work, we identify two problems regarding the direction and step size for updating the weight vectors of hidden units, which may degrade the generalization performance of Adam.", "As a solution, we propose the normalized direction-preserving Adam (ND-Adam) algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD.", "Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits.", "By bridging the gap between SGD and Adam, we also shed some light on why certain optimization algorithms generalize better than others."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HJSA_e1AW", "target": ["Una versi\u00f3n adaptada de Adam para el entrenamiento de DNNs, que salva la brecha de generalizaci\u00f3n entre Adam y SGD.", "Propone una variante del algoritmo de optimizaci\u00f3n ADAM que normaliza los pesos de cada unidad oculta utilizando la normalizaci\u00f3n por lotes", "Ampliaci\u00f3n del algoritmo de optimizaci\u00f3n de Adam para preservar la direcci\u00f3n de actualizaci\u00f3n adaptando la tasa de aprendizaje de los pesos entrantes a una unidad oculta de forma conjunta utilizando la norma L2 del vector gradiente"]}
{"source": ["Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning.", "However, autonomously learning effective sets of options is still a major challenge in the field.", "In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process.", "Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment.", "We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available.  ", "We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels.", "It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation.", "We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Bk8ZcAxR-", "target": ["Mostramos c\u00f3mo podemos utilizar la representaci\u00f3n sucesora para descubrir eigenopciones en dominios estoc\u00e1sticos, a partir de p\u00edxeles brutos. Las eigenopciones son opciones aprendidas para navegar por las dimensiones latentes de una representaci\u00f3n aprendida.", "Extiende la idea de las eigenopciones a los dominios con transiciones estoc\u00e1sticas y donde se aprenden las caracter\u00edsticas del estado.", "Muestra la equivalencia entre las funciones de valor de los prototipos y las representaciones de los sucesores y deriva la idea de las opciones propias como mecanismo de descubrimiento de opciones", "El documento es una continuaci\u00f3n del trabajo anterior de Machado et al. (2017) que muestra c\u00f3mo las funciones de protovalor pueden utilizarse para definir opciones denominadas \"eigenopciones\"."]}
{"source": ["One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled.", "We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure.", "However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks.", "In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets.", "In addition, we present a tighter upper bound that leverages network coefficients.", "We test both on trained networks.", "The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks.", "The refined upper bound is particularly stronger on networks with narrow layers.  "], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1MAJhR5YX", "target": ["Proporcionamos l\u00edmites superiores mejorados para el n\u00famero de regiones lineales utilizadas en la expresividad de la red, y un algoritmo altamente eficiente (con respecto al recuento exacto) para obtener l\u00edmites inferiores probabil\u00edsticos sobre el n\u00famero real de regiones lineales.", "Contribuye al estudio del n\u00famero de regiones lineales en las redes neuronales RELU utilizando un algoritmo de recuento probabil\u00edstico aproximado y un an\u00e1lisis", "Se basa en trabajos anteriores que estudian el recuento de regiones lineales en redes neuronales profundas, y mejora el l\u00edmite superior propuesto previamente cambiando la restricci\u00f3n de dimensionalidad", "El art\u00edculo trata de la expresividad de una red neuronal lineal a trozos, caracterizada por el n\u00famero de regiones lineales de la funci\u00f3n modelada, y aprovecha los algoritmos probabil\u00edsticos para calcular los l\u00edmites m\u00e1s r\u00e1pidamente, y demuestra l\u00edmites m\u00e1s ajustados."]}
{"source": ["The ability to look multiple times through a series of pose-adjusted glimpses is fundamental to human vision.", "This critical faculty allows us to understand highly complex visual scenes.", "Short term memory plays an integral role in aggregating the information obtained from these glimpses and informing our interpretation of the scene.", "Computational models have attempted to address glimpsing and visual attention but have failed to incorporate the notion of memory.", "We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory.", "We subsequently introduce a fully differentiable Short Term Attentive Working Memory model (STAWM) which uses transformational attention to learn a memory over each image it sees.", "The state of our Hebb-Rosenblatt memory is embedded in STAWM as the weights space of a layer.", "By projecting different queries through this layer we can obtain goal-oriented latent representations for tasks including classification and visual reconstruction.", "Our model obtains highly competitive classification performance on MNIST and CIFAR-10.", "As demonstrated through the CelebA dataset, to perform reconstruction the model learns to make a sequence of updates to a canvas which constitute a parts-based representation.", "Classification with the self supervised representation obtained from MNIST is shown to be in line with the state of the art models (none of which use a visual attention mechanism).", "Finally, we show that STAWM can be trained under the dual constraints of classification and reconstruction to provide an interpretable visual sketchpad which helps open the `black-box' of deep learning."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1fbosCcYm", "target": ["Una memoria de trabajo de inspiraci\u00f3n biol\u00f3gica que puede integrarse en modelos de atenci\u00f3n visual recurrente para obtener un rendimiento de vanguardia", "Introduce una nueva arquitectura de red inspirada en la memoria de trabajo visual atenta y la aplica a tareas de clasificaci\u00f3n y la utiliza como modelo generativo", "El art\u00edculo aumenta el modelo de atenci\u00f3n recurrente con un nuevo modelo de memoria de trabajo de Hebb-Rosenblatt y consigue resultados competitivos en MNIST"]}
{"source": ["Generative models have been successfully applied to image style transfer and domain translation.", "However, there is still a wide gap in the quality of results when learning such tasks on musical audio.", "Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models.", "In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.", "First, we define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.", "We show that we can achieve and improve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM).", "Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an auxiliary pair of discriminative networks.", "This allows a faster and more stable training, along with a controllable latent space encoder.", "By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.", "Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis on a reduced set of control parameters.", "We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains.", "We show that this architecture incorporates generative controls in multi-domain transfer, yet remaining rather light, fast to train and effective on small datasets."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HJgOl3AqY7", "target": ["El art\u00edculo utiliza la Autocodificaci\u00f3n Variacional y el condicionamiento de la red para la Transferencia de Timbres Musicales, desarrollamos y generalizamos nuestra arquitectura para las transferencias de muchos a muchos instrumentos junto con visualizaciones y evaluaciones.", "Propone un autocodificador variacional modulado para realizar la transferencia t\u00edmbrica musical sustituyendo el habitual criterio de traducci\u00f3n adversarial por un Maxiimum Mean Discrepancy", "Describe un modelo de transferencia de timbres musicales de muchos a muchos que se basa en los recientes avances en la transferencia de dominios y estilos", "Propone un modelo h\u00edbrido basado en la VAE para realizar la transferencia t\u00edmbrica en grabaciones de instrumentos musicales."]}
{"source": ["We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights.", "Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large.", "This, in particular, provides quantitative answers and insights to three questions that were yet fully understood in the literature.", "Firstly, we provide a precise answer on how the random deep weight-tied autoencoder model performs \u201capproximate inference\u201d as posed by Scellier et al. (2018), and its connection to reversibility considered by several theoretical studies.", "Secondly, we show that deep autoencoders display a higher degree of sensitivity to perturbations in the parameters, distinct from the shallow counterparts.", "Thirdly, we obtain insights on pitfalls in training initialization practice, and demonstrate experimentally that it is possible to train a deep autoencoder, even with the tanh activation and a depth as large as 200 layers, without resorting to techniques such as layer-wise pre-training or batch normalization.", "Our analysis is not specific to any depths or any Lipschitz activations, and our analytical techniques may have broader applicability."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJx54i05tX", "target": ["Estudiamos el comportamiento de los autocodificadores de vainilla multicapa ligados a pesos bajo el supuesto de pesos aleatorios. A trav\u00e9s de una caracterizaci\u00f3n exacta en el l\u00edmite de grandes dimensiones, nuestro an\u00e1lisis revela interesantes fen\u00f3menos de transici\u00f3n de fase.", "Un an\u00e1lisis te\u00f3rico de los autocodificadores con pesos ligados entre el codificador y el decodificador (weight-tied) mediante el an\u00e1lisis del campo medio", "Analiza las prestaciones de los autocodificadores ligados ponderados bas\u00e1ndose en los recientes avances en el an\u00e1lisis de problemas estad\u00edsticos de alta dimensi\u00f3n y, en concreto, en el algoritmo de paso de mensajes", "Este trabajo estudia los autocodificadores bajo varios supuestos, y se\u00f1ala que este modelo de autocodificador aleatorio puede ser analizado de forma elegante y rigurosa con ecuaciones unidimensionales."]}
{"source": ["Assessing distance betweeen the true and the sample distribution is a key component of many state of the art generative models, such as Wasserstein Autoencoder (WAE).", "Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and\n", "kernel smoothing we construct a new generative model \u2013 Cramer-Wold AutoEncoder (CWAE).", "CWAE cost function, based on introduced Cramer-Wold distance between samples, has a simple closed-form in the case of normal prior.", "As a consequence, while simplifying the optimization procedure (no need of sampling necessary to evaluate the distance function in the training loop), CWAE performance matches quantitatively and qualitatively that of WAE-MMD (WAE using maximum mean discrepancy based distance function) and often improves upon SWAE."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkgwuiA9F7", "target": ["Inspirado en trabajos anteriores sobre autocodificadores Sliced-Wasserstein (SWAE) y suavizaci\u00f3n de kernel, construimos un nuevo modelo generativo: el autocodificador Cramer-Wold (CWAE).", "Este trabajo propone una variante de WAE basada en una nueva distancia estad\u00edstica entre la distribuci\u00f3n de datos codificada y la distribuci\u00f3n latente a priori", "Introduce una variaci\u00f3n de los Audoencoders de Wasserstein que es una novedosa arquitectura de autoencodificaci\u00f3n regularizada que propone una elecci\u00f3n espec\u00edfica de la penalizaci\u00f3n por divergencia", "Este trabajo propone el autocodificador de Cramer-Wold, que utiliza la distancia de Cramer-Wold entre dos distribuciones basadas en el Teorema de Cramer-Wold."]}
{"source": ["We propose a rejection sampling scheme using the discriminator of a GAN to\n", "approximately correct errors in the GAN generator distribution.", "We show that\n", "under quite strict assumptions, this will allow us to recover the data distribution\n", "exactly.", "We then examine where those strict assumptions break down and design a\n", "practical algorithm\u2014called Discriminator Rejection Sampling (DRS)\u2014that can be\n", "used on real data-sets.", "Finally, we demonstrate the efficacy of DRS on a mixture of\n", "Gaussians and on the state of the art SAGAN model.", "On ImageNet, we train an\n", "improved baseline that increases the best published Inception Score from 52.52 to\n", "62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79.", "We then use\n", "DRS to further improve on this baseline, improving the Inception Score to 76.08\n", "and the FID to 13.75."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1GkToR5tm", "target": ["Utilizamos un discriminador GAN para realizar un esquema de muestreo de rechazo aproximado en la salida del generador GAN.", " Propone un algoritmo de muestreo de rechazo para el muestreo del generador GAN.", "Este trabajo propone un esquema de muestreo de rechazo post-procesamiento para GANs, llamado Discriminator Rejection Sampling, para ayudar a filtrar las muestras \u00e2\u20ac\u02dcbuenas\u00e2\u20ac\u2122 del generador de GANs."]}
{"source": ["The quality of the features used in visual recognition is of fundamental importance for the overall system.", "For a long time, low-level hand-designed feature algorithms as SIFT and HOG have obtained the best results on image recognition.", "Visual features have recently been extracted from trained convolutional neural networks.", "Despite the high-quality results, one of the main drawbacks of this approach, when compared with hand-designed features, is the training time required during the learning process.", "In this paper, we propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality.", "This methodology is evaluated on different datasets and compared with state-of-the-art approaches."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SyGT_6yCZ", "target": ["Un m\u00e9todo sencillo y r\u00e1pido para extraer caracter\u00edsticas visuales de las redes neuronales convolucionales", "Propone una forma r\u00e1pida de aprender caracter\u00edsticas convolucionales que posteriormente pueden ser utilizadas con cualquier clasificador mediante el uso de un n\u00famero reducido de epocs de entrenamiento y retrasos de programaci\u00f3n espec\u00edficos de la tasa de aprendizaje", "Utilizar un esquema de decaimiento de la tasa de aprendizaje que se fija en relaci\u00f3n con el n\u00famero de \u00e9pocas utilizadas en el entrenamiento y extraer la salida de la pen\u00faltima capa como caracter\u00edsticas para entrenar un clasificador convencional."]}
{"source": ["We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators (MASOs).", "We prove that RNNs using piecewise affine and convex nonlinearities can be written as a simple piecewise affine spline operator.", "The resulting representation provides several new perspectives for analyzing RNNs, three of which we study in this paper.", "First, we show that an RNN internally partitions the input space during training and that it builds up the partition through time.", "Second, we show that the affine slope parameter of an RNN corresponds to an input-specific template, from which we can interpret an RNN as performing a simple template matching (matched filtering) given the input.", "Third, by carefully examining the MASO RNN affine mapping, we prove that using a random initial hidden state corresponds to an explicit L2 regularization of the affine parameters, which can mollify exploding gradients and improve generalization.", "Extensive experiments on several datasets of various modalities demonstrate and validate each of the above conclusions.", "In particular, using a random initial hidden states elevates simple RNNs to near state-of-the-art performers on these datasets."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJej72AqF7", "target": ["Proporcionamos nuevas ideas e interpretaciones de las RNN desde la perspectiva de los operadores de spline max-affine.", "Reescribe las ecuaciones de la RNN de Elman en t\u00e9rminos de los llamados operadores spline max-affine", "Proporcionar un enfoque novedoso hacia la comprensi\u00f3n de las RNNs que utilizan operadores spline de m\u00e1xima afinidad (MASO) reescribi\u00e9ndolas con MASOs de activaciones afines y convexas a trozos", "Los autores se basan en la interpetaci\u00f3n del operador spline max-affine de una clase sustancial de redes profundas, centr\u00e1ndose en las redes neuronales recurrentes que utilizan el ruido en el estado oculto inicial como regularizaci\u00f3n"]}
{"source": ["Reasoning over text and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering.", " Transducing text to logical forms which can be operated on is a brittle and error-prone process", ". Operating directly on text by jointly learning representations and transformations thereof by means of neural architectures that lack the ability to learn and exploit general rules can be very data-inefficient and not generalise correctly", ". These issues are addressed by Neural Theorem Provers (NTPs) (Rockt\u00e4schel & Riedel, 2017), neuro-symbolic systems based on a continuous relaxation of Prolog\u2019s backward chaining algorithm, where symbolic unification between atoms is replaced by a differentiable operator computing the similarity between their embedding representations", ". In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs) consisting of two extensions toNTPs, namely", "a) a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, and", "b) an attention mechanism for improving the rule learning process, deeming them usable on real-world datasets.", "Then, we propose a novel approach for jointly reasoning over KB facts and textual mentions, by jointly embedding them in a shared embedding space.", "The proposed method is able to extract rules and provide explanations\u2014involving both textual patterns and KB relations\u2014from large KBs and text corpora.", "We show that NaNTPs perform on par with NTPs at a fraction of a cost, and can achieve competitive link prediction results on challenging large-scale datasets, including WN18, WN18RR, and FB15k-237 (with and without textual mentions) while being able to provide explanations for each prediction and extract interpretable rules."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJzmzn0ctX", "target": ["Escalamos los teoremas neuronales a grandes conjuntos de datos, mejoramos el proceso de aprendizaje de reglas y lo ampliamos para razonar conjuntamente sobre texto y bases de conocimiento.", "Propone una extensi\u00f3n del sistema Neural Theorem Provers que aborda los principales problemas de este modelo reduciendo la complejidad temporal y espacial del mismo", "Escala los PNT utilizando la b\u00fasqueda aproximada del vecino m\u00e1s cercano sobre los hechos y las reglas durante la unificaci\u00f3n y sugiere parametrizar los predicados utilizando la atenci\u00f3n sobre los predicados conocidos", "mejora el enfoque del Prover de Teoremas Neuronales propuesto anteriormente utilizando la b\u00fasqueda del vecino m\u00e1s cercano."]}
{"source": ["We investigate the methods by which a Reservoir Computing Network (RCN) learns concepts such as 'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data.", "Specifically, we show that an RCN trained to identify relationships between image-pairs drawn from a subset of digits from the MNIST database or the depth maps of subset of visual scenes from a moving camera generalizes the learned transformations to images of digits unseen during training or depth maps of different visual scenes.", "We infer, using Principal Component Analysis, that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship.", "Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples.", "Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space.", "We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier.", "Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep Siamese Neural Networks (SNNs) in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera.", "This work helps bridge the gap between explainable machine learning and biological learning through analogies using small datasets, and points to new directions in the investigation of learning processes."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyFaiGbCW", "target": ["Generalizaci\u00f3n de las relaciones aprendidas entre pares de im\u00e1genes utilizando un peque\u00f1o dato de entrenamiento a tipos de im\u00e1genes no vistas previamente utilizando un modelo de sistemas din\u00e1micos explicables, Reservoir Computing, y una t\u00e9cnica de aprendizaje biol\u00f3gicamente plausible basada en analog\u00edas.", "Reclama los resultados de la \"combinaci\u00f3n de transformaciones\" en el contexto de la CR utilizando una red de eco-estado con activaciones tanh est\u00e1ndar con la diferencia de que los pesos recurrentes no est\u00e1n entrenados", "Nuevo m\u00e9todo de clasificaci\u00f3n de diferentes distorsiones de datos MNIST", "El art\u00edculo utiliza una red de estados de eco para aprender a clasificar las transformaciones de im\u00e1genes entre pares de im\u00e1genes en una de las cinco clases."]}
{"source": ["We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data.", "GAPF leverages recent advances in adversarial learning to allow a data holder to learn \"universal\" representations that decouple a set of sensitive attributes from the rest of the dataset.", "Under GAPF, finding the optimal decorrelation scheme is formulated as a constrained minimax game between a generative decorrelator and an adversary.", "We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity.", "We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1xAH2RqK7", "target": ["Presentamos Generative Adversarial Privacy and Fairness (GAPF), un marco de trabajo basado en datos para el aprendizaje de representaciones privadas y justas con garant\u00edas certificadas de privacidad/justicia", "Este art\u00edculo utiliza un modelo GAN para proporcionar una visi\u00f3n general de los trabajos relacionados con el Aprendizaje de Representaci\u00f3n Privada/Justa (PRL).", "Este trabajo presenta un enfoque basado en el adversario para representaciones privadas y justas mediante la distorsi\u00f3n aprendida de los datos que minimiza la dependencia de las variables sensibles mientras el grado de distorsi\u00f3n est\u00e1 restringido.", "Los autores describen un marco de trabajo sobre c\u00f3mo aprender una representaci\u00f3n de paridad demogr\u00e1fica que puede utilizarse para entrenar ciertos clasificadores."]}
{"source": ["Current machine learning algorithms can be easily fooled by adversarial examples.", "One possible solution path is to make models that use confidence thresholding to avoid making mistakes.", "Such models refuse to make a prediction when they are not confident of their answer.", "We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples.", "Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability.", "We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models.", "Experiments show the attack attains good results in practice.", "We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  ", "We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission)."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1g0piA9tQ", "target": ["Presentamos m\u00e9tricas y un ataque \u00f3ptimo para evaluar los modelos que se defienden de los ejemplos adversos utilizando el umbral de confianza", "Este art\u00edculo presenta una familia de ataques a los algoritmos de umbralizaci\u00f3n de confianza, centr\u00e1ndose principalmente en las metodolog\u00edas de evaluaci\u00f3n.", "Propone un m\u00e9todo de evaluaci\u00f3n para los modelos de defensa de umbral de confianza y un enfoque para generar ejemplos adversos eligiendo la clase err\u00f3nea con m\u00e1s confianza cuando se utilizan ataques dirigidos", "El art\u00edculo presenta una metodolog\u00eda para evaluar los ataques a los m\u00e9todos de umbralizaci\u00f3n de confianza y propone un nuevo tipo de ataque."]}
{"source": ["Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided.", "However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization.", "This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required.", "It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals.", "We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents.", "Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum.", "We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm.", "Each task provides only binary rewards indicating whether or not the goal is achieved.", "Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration.", "Extensive experiments demonstrate that this method leads to faster converge and improved task performance."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Sklsm20ctX", "target": ["un nuevo m\u00e9todo de aprendizaje con recompensa dispersa mediante el reetiquetado de recompensa adversarial", "Propone utilizar un entorno competitivo multiagente para fomentar la exploraci\u00f3n y demuestra que CER + HER > HER ~ CER", "Proponer un nuevo m\u00e9todo de aprendizaje a partir de recompensas dispersas en entornos de aprendizaje por refuerzo sin modelo y densificar la recompensa", "Para hacer frente a los problemas de recompensa escasa y fomentar la exploraci\u00f3n en los algoritmos de RL, los autores proponen una estrategia de reetiquetado denominada Respuesta de Experiencia Competitiva (CER)."]}
{"source": ["This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions.", "The model is formulated as a conditional generative model with two levels of hierarchical latent variables.", "The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability.", "The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes.", "This amounts to using a Gaussian mixture model (GMM) for the latent distribution.", "Extensive evaluation demonstrates its ability to control the aforementioned attributes.", "In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rygkk305YQ", "target": ["La construcci\u00f3n de un modelo TTS con VAE de mezcla gaussiana permite un control detallado del estilo de habla, las condiciones de ruido, etc.", "Describe el modelo GAN condicionado para generar espectros de Mel condicionados por el hablante aumentando el espacio z correspondiente a la identificaci\u00f3n", "Este trabajo propone un modelo de variable latente de dos capas para obtener una representaci\u00f3n latente desenredada, facilitando as\u00ed un control de grano fino sobre varios atributos", "Este art\u00edculo propone un modelo que puede controlar los atributos no anotados, como el estilo de habla, el acento, el ruido de fondo, etc."]}
{"source": ["Visual Question Answering (VQA) models have struggled with counting objects in natural images so far.", "We identify a fundamental problem due to soft attention in these models as a cause.", "To circumvent this problem, we propose a neural network component that allows robust counting from object proposals.", "Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model.", "On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B12Js_yRb", "target": ["Habilitaci\u00f3n de modelos de respuesta a preguntas visuales para contar manejando propuestas de objetos superpuestos.", "Este trabajo propone una arquitectura de red dise\u00f1ada a mano sobre un gr\u00e1fico de propuestas de objetos para realizar una supresi\u00f3n suave no m\u00e1xima para obtener el recuento de objetos.", "Se centra en un problema de recuento en la respuesta a preguntas visuales utilizando el mecanismo de atenci\u00f3n y propone un componente de recuento diferenciable que cuenta expl\u00edcitamente el n\u00famero de objetos", "Este trabajo aborda el problema del recuento de objetos en la respuesta a preguntas visuales, propone muchas heur\u00edsticas para encontrar el recuento correcto."]}
{"source": ["We propose a simple and robust training-free approach for building sentence representations.", "Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence.", "We model the semantic meaning of a word in a sentence based on two aspects.", "One is its relatedness to the word vector subspace already spanned by its contextual words.", "The other is its novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace.  ", "Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representation.", "This approach requires zero training and zero parameters, along with efficient inference performance.", "We evaluate our approach on 11 downstream NLP tasks.", "Experimental results show that our model outperforms all existing zero-training alternatives in all the tasks and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rJedbn0ctQ", "target": ["Un enfoque sencillo y sin entrenamiento para la incrustaci\u00f3n de frases con un rendimiento competitivo en comparaci\u00f3n con los modelos sofisticados que requieren una gran cantidad de datos de entrenamiento o un tiempo de entrenamiento prolongado.", "Presentado un nuevo modo de generar incrustaciones de frases sin necesidad de entrenamiento con un an\u00e1lisis sistem\u00e1tico", "Propone un nuevo m\u00e9todo basado en la geometr\u00eda para la incrustaci\u00f3n de frases a partir de vectores de incrustaci\u00f3n de palabras mediante la cuantificaci\u00f3n de la novedad, la importancia y la singularidad del corpus de cada palabra", "Este trabajo explora la incrustaci\u00f3n de oraciones basada en la descomposici\u00f3n ortogonal del espacio abarcado por incrustaciones de palabras"]}
{"source": ["In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples.", "Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set.", "In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode.", "We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided.", "To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes.", "These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully.", "We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples.", "We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure.", "Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJcSzz-CZ", "target": ["Proponemos nuevas extensiones de las redes protot\u00edpicas que se ven aumentadas con la capacidad de utilizar ejemplos no etiquetados al producir prototipos.", "Este trabajo es una extensi\u00f3n de una red protot\u00edpica que considera el empleo de los ejemplos no etiquetados disponibles para ayudar a entrenar cada episodio", "Estudia el problema de la clasificaci\u00f3n semisupervisada de pocos disparos extendiendo las redes protot\u00edpicas al entorno del aprendizaje semisupervisado con ejemplos de clases distractoras", "Extiende la Red de Prototipos a la configuraci\u00f3n semi-supervisada mediante la actualizaci\u00f3n de los prototipos utilizando las pseudo-etiquetas asignadas, tratando con los distractores, y ponderando las muestras utilizando la distancia a los prototipos originales."]}
{"source": ["We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models.", "Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are oversampled, which restricts the usability of linear interpolations as a tool to analyse the latent space.", "We show that the distribution mismatch can be eliminated completely by a proper choice of the latent probability distribution or using non-linear interpolations.", "We prove that there is a trade off between the interpolation being linear, and the latent distribution having even the most basic properties required for stable training, such as finite mean.", "We use the multidimensional Cauchy distribution as an example of the prior distribution, and also provide a general method of creating non-linear interpolations, that is easily applicable to a large family of commonly used latent distributions."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyMhLo0qKQ", "target": ["Demostramos te\u00f3ricamente que las interpolaciones lineales son inadecuadas para el an\u00e1lisis de modelos generativos impl\u00edcitos entrenados. ", "Estudia el problema de cuando la interpolante lineal entre dos variables aleatorias sigue la misma distribuci\u00f3n, relacionada con la distribuci\u00f3n a priori de un modelo generativo impl\u00edcito", "Este trabajo se plantea c\u00f3mo interpolar en el espacio latente dado un modelo de variable latente."]}
{"source": ["Deep neural networks (DNN) have shown promising performance in computer vision.", "In medical imaging, encouraging results have been achieved with deep learning for applications such as segmentation, lesion detection and classification.", "Nearly all of the deep learning based image analysis methods work on reconstructed images, which are obtained from original acquisitions via solving inverse problems (reconstruction).", "The reconstruction algorithms are designed for human observers, but not necessarily optimized for DNNs which can often observe features that are incomprehensible for human eyes.", "Hence, it is desirable to train the DNNs directly from the original data which lie in a different domain with the images.", "In this paper, we proposed an end-to-end DNN for abnormality detection in medical imaging.", "To align the acquisition with the annotations made by radiologists in the image domain, a DNN was built as the unrolled version of iterative reconstruction algorithms to map the acquisitions to images, and followed by a 3D convolutional neural network (CNN) to detect the abnormality in the reconstructed images.", "The two networks were trained jointly in order to optimize the entire DNN for the detection task from the original acquisitions.", "The DNN was implemented for lung nodule detection in low-dose chest computed tomography (CT), where a numerical simulation was done to generate acquisitions from 1,018 chest CT images with radiologists' annotations.", "The proposed end-to-end DNN demonstrated better sensitivity and accuracy for the task compared to a two-step approach, in which the reconstruction and detection DNNs were trained separately.", "A significant reduction of false positive rate on suspicious lesions were observed, which is crucial for the known over-diagnosis in low-dose lung CT imaging.", "The images reconstructed by the proposed end-to-end network also presented enhanced details in the region of interest."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rk1FQA0pW", "target": ["Detecci\u00f3n de n\u00f3dulos pulmonares a partir de datos de proyecci\u00f3n en lugar de im\u00e1genes.", "Las DNN se utilizan para la detecci\u00f3n de n\u00f3dulos pulmonares basada en parches en datos de proyecci\u00f3n de TC.", "Modelizaci\u00f3n conjunta de la reconstrucci\u00f3n por tomograf\u00eda computarizada y de la detecci\u00f3n de lesiones en el pulm\u00f3n mediante el entrenamiento del mapeo desde el sinograma bruto hasta los resultados de la detecci\u00f3n de manera integral", "Presenta un entrenamiento de extremo a extremo de una arquitectura CNN que combina el procesamiento de se\u00f1ales de im\u00e1genes de TC y el an\u00e1lisis de im\u00e1genes."]}
{"source": ["Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.", "As the title of the paper by Mirowski et al. (2016) suggests, one might assume that DRL-based algorithms are able to \u201clearn to navigate\u201d and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments.", "Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal.", "In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?", "Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.", "Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.", "We extend the experiments in Mirowski et al. (2016) by separating the set of training and testing maps and by a more ablative coverage of the space of experiments.", "Our systematic experiments show that the NavA3C-D1-D2-L algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal.", "However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkiIkBJ0b", "target": ["Evaluamos cuantitativa y cualitativamente los m\u00e9todos de navegaci\u00f3n basados en el aprendizaje por refuerzo profundo bajo una variedad de condiciones para responder a la pregunta de cu\u00e1n cerca est\u00e1n de reemplazar a los planificadores de rutas y algoritmos de mapeo cl\u00e1sicos.", "Evaluar un modelo basado en Deep RL en laberintos de entrenamiento midiendo la latencia repetida hasta la meta y la comparaci\u00f3n con la ruta m\u00e1s corta"]}
{"source": ["In many robotic applications, it is crucial to maintain a belief about the state of \n", "a system, like the location of a robot or the pose of an object.\n", "These state estimates serve as input for planning and decision making and \n", "provide feedback during task execution. \n", "Recursive Bayesian Filtering algorithms address the state estimation problem,\n", "but they require a model of the process dynamics and the sensory observations as well as \n", "noise estimates that quantify the accuracy of these models. \n", "Recently, multiple works have demonstrated that the process and sensor models can be \n", "learned by end-to-end training through differentiable versions of Recursive Filtering methods.\n", "However, even if the predictive models are known, finding suitable noise models \n", "remains challenging.", "Therefore, many practical applications rely on very simplistic noise \n", "models. \n", "Our hypothesis is that end-to-end training through differentiable Bayesian \n", "Filters enables us to learn more complex heteroscedastic noise models for\n", "the system dynamics.", "We evaluate learning such models with different types of \n", "filtering algorithms and on two different robotic tasks.", "Our experiments show that especially \n", "for sampling-based filters like the Particle Filter, learning heteroscedastic noise \n", "models can drastically improve the tracking performance in comparison to using \n", "constant noise models."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BylBns0qtX", "target": ["Evaluamos el aprendizaje de modelos de ruido heterosced\u00e1stico con diferentes filtros de Bayes diferenciables", "Propone aprender modelos de ruido heterosced\u00e1stico a partir de los datos optimizando la probabilidad de predicci\u00f3n de extremo a extremo mediante filtros bayesianos diferenciables y dos versiones diferentes del filtro de Kalman no centrado", "Revisa los filtros de Bayes y eval\u00faa las ventajas de entrenar los modelos de ruido de observaci\u00f3n y de proceso manteniendo fijos los dem\u00e1s modelos", "Este trabajo presenta un m\u00e9todo para aprender y utilizar el ruido dependiente del estado y la observaci\u00f3n en los algoritmos tradicionales de filtrado bayesiano. El enfoque consiste en construir un modelo de red neuronal que toma como entrada los datos de observaci\u00f3n en bruto y produce una representaci\u00f3n compacta y una covarianza diagonal asociada."]}
{"source": ["Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning.", "These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data.", "However, we find that the extensive use of Laplacian smoothing at each layer in current approaches can easily dilute the knowledge from distant nodes and consequently decrease the performance in zero-shot learning.", "In order to still enjoy the benefit brought by the graph structure while preventing the dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes.", "DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections.", "These connections are added based on a node's relationship to its ancestors and descendants.", "A weighting scheme is further used to weigh their contribution depending on the distance to the node.", "Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkgs0oAqFQ", "target": ["Repensamos la forma en que se puede explotar la informaci\u00f3n de manera m\u00e1s eficiente en el grafo de conocimiento para mejorar el rendimiento en la tarea de aprendizaje de tiro cero y proponemos un m\u00f3dulo de propagaci\u00f3n de grafos densos (DGP) para este fin.", "Estos autores proponen una soluci\u00f3n al problema del sobrealisado en las redes Graph conv permitiendo la propagaci\u00f3n densa entre todos los nodos relacionados, ponderada por la distancia mutua.", "Propone una novedosa red neuronal convolucional de grafos para abordar el problema de la clasificaci\u00f3n de cero disparos utilizando estructuras relacionales entre clases como entrada de las redes convolucionales de grafos para aprender clasificadores de clases no vistas"]}
{"source": ["In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem.", "By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure.", "We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network.", "Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions.", "Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.\n"], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1xpe2C5Km", "target": ["Una segmentaci\u00f3n sem\u00e1ntica basada en la c\u00e1psula, en la que las probabilidades de las etiquetas de clase se rastrean a trav\u00e9s de la tuber\u00eda de la c\u00e1psula. ", "Los autores presentan un mecanismo de rastreo para asociar el nivel m\u00e1s bajo de las C\u00e1psulas con sus respectivas clases", "Propone una capa de rastreo para que las redes de c\u00e1psulas realicen una segmentaci\u00f3n sem\u00e1ntica y hace un uso expl\u00edcito de la relaci\u00f3n parte-entero en las capas de c\u00e1psulas", "Propone un m\u00e9todo de rastreo basado en el concepto CapsNet de Sabour para realizar una segmentaci\u00f3n sem\u00e1ntica en paralelo a la clasificaci\u00f3n."]}
{"source": ["Studying the evolution of information theoretic quantities during Stochastic Gradient Descent (SGD) learning of Artificial Neural Networks (ANNs) has gained popularity in recent years. \n", "Nevertheless, these type of experiments require estimating mutual information and entropy which becomes intractable for moderately large problems.", "In this work we propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN.", "Through experimental results and theoretical justifications it is shown that, under some assumptions, the SGD learning trajectories appear to be similar for different ANN architectures.", "First, the SGD learning is modeled as a Hidden Markov Process (HMP) whose entropy tends to increase to the maximum.", "Then, it is shown that the SGD learning trajectory appears to move close to the shortest path between the initial and final joint distributions in the space of probability measures equipped with the total variation metric.", "Furthermore, it is shown that the trajectory of learning in the information plane can provide an alternative for observing the learning process, with potentially richer information about the learning than the trajectories in training and test error."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SkMON20ctX", "target": ["Consideramos el SGD como una trayectoria en el espacio de medidas de probabilidad, mostramos su conexi\u00f3n con los procesos de Markov, proponemos un modelo de Markov simple de aprendizaje del SGD y lo comparamos experimentalmente con el SGD utilizando cantidades de la teor\u00eda de la informaci\u00f3n. ", "Construye una cadena de Markov que sigue un camino corto en la m\u00e9trica de TV en P y muestra que las trayectorias de SGD y \\alpha-SMLC tienen una entrop\u00eda condicional similar", "Estudia la trayectoria de H(\\hat{y}) frente a H(\\hat{y}|y) en el plano de informaci\u00f3n para los m\u00e9todos de descenso de gradiente estoc\u00e1stico para el entrenamiento de redes neuronales", "Describe el SGD desde el punto de vista de la distribuci\u00f3n p(y',y) donde y es (una etiqueta de clase verdadera posiblemente corrupta) e y' una predicci\u00f3n del modelo."]}
{"source": ["Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling.", "However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition.", "This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler.", "The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of energy landscapes.", "Experiments validate the proposed approach on Bayesian fully connected neural network, Bayesian convolutional neural network and Bayesian recurrent neural network tasks, showing that the learned sampler outperforms generic, hand-designed SG-MCMC algorithms, and generalizes to different datasets and larger architectures."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HkeoOo09YX", "target": ["Este trabajo propone un m\u00e9todo para automatizar el dise\u00f1o de la propuesta MCMC de gradiente estoc\u00e1stico utilizando un enfoque de meta aprendizaje. ", "Presenta un enfoque de meta-aprendizaje para dise\u00f1ar autom\u00e1ticamente el muestreador MCMC basado en la din\u00e1mica hamiltoniana para mezclar m\u00e1s r\u00e1pido en problemas similares a los de entrenamiento", "Parametriza las matrices de difusi\u00f3n y rizo mediante redes neuronales y meta-aprende y optimiza un algoritmo sg-mcmc. "]}
{"source": ["We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature.", "Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions.", "We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function.", "We then provide a set of systematic experiments that explore its hyper-parameters.", "We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned.", "We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryzm6BATZ", "target": ["Las t\u00e9cnicas de evaluaci\u00f3n de la calidad de la imagen mejoran el entrenamiento y la evaluaci\u00f3n de las redes generativas adversariales basadas en la energ\u00eda", "Propone una formulaci\u00f3n basada en la energ\u00eda para el modal BEGAN y lo modifica para incluir un t\u00e9rmino basado en la evaluaci\u00f3n de la calidad de la imagen", "Propone algunas nuevas funciones de energ\u00eda en el marco de BEGAN (equilibrio de l\u00edmites GAN), incluyendo la puntuaci\u00f3n l_1, la puntuaci\u00f3n de similitud de magnitudes de gradiente y la puntuaci\u00f3n de crominancia."]}
{"source": ["Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions.", "Its performance depends crucially on a damping coefficient.", "Largecamping  coefficients can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9.", "We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients.", "AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive damping coefficients such as 0.999.", "We reinterpret Nesterov's accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives.", "Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence with little to no tuning."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Syxt5oC5YQ", "target": ["Introducimos una variante simple de la optimizaci\u00f3n del momento que es capaz de superar al momento cl\u00e1sico, a Nesterov y a Adam en tareas de aprendizaje profundo con un ajuste m\u00ednimo de los hiperpar\u00e1metros.", "Introduce una variante del momento que agrega varias velocidades con diferentes coeficientes de amortiguaci\u00f3n que disminuye significativamente la oscilaci\u00f3n", "Se propone un m\u00e9todo de momento agregado para la optimizaci\u00f3n basada en el gradiente utilizando m\u00faltiples vectores de velocidad con diferentes factores de amortiguaci\u00f3n en lugar de un \u00fanico vector de velocidad para mejorar la estabilidad", "Los autores combinan varios pasos de actualizaci\u00f3n para conseguir el impulso agregado demostrando tambi\u00e9n que es m\u00e1s estable que los otros m\u00e9todos de impulso"]}
{"source": ["Recurrent Neural Networks architectures excel at processing sequences by\n", "modelling dependencies over different timescales.", "The recently introduced\n", "Recurrent Weighted Average (RWA) unit captures long term dependencies\n", "far better than an LSTM on several challenging tasks.", "The RWA achieves\n", "this by applying attention to each input and computing a weighted average\n", "over the full history of its computations.", "Unfortunately, the RWA cannot\n", "change the attention it has assigned to previous timesteps, and so struggles\n", "with carrying out consecutive tasks or tasks with changing requirements.\n", "We present the Recurrent Discounted Attention (RDA) unit that builds on\n", "the RWA by additionally allowing the discounting of the past.\n", "We empirically compare our model to RWA, LSTM and GRU units on\n", "several challenging tasks.", "On tasks with a single output the RWA, RDA and\n", "GRU units learn much quicker than the LSTM and with better performance.\n", "On the multiple sequence copy task our RDA unit learns the task three\n", "times as quickly as the LSTM or GRU units while the RWA fails to learn at\n", "all.", "On the Wikipedia character prediction task the LSTM performs best\n", "but it followed closely by our RDA unit.", "Overall our RDA unit performs\n", "well and is sample efficient on a large variety of sequence tasks."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJ78bJZCZ", "target": ["Introducimos la Unidad de Descuento Recurrente que aplica la atenci\u00f3n a cualquier secuencia de longitud en tiempo lineal", "Este trabajo propone la Atenci\u00f3n Descontada Recurrente (ADR), una extensi\u00f3n de la Media Ponderada Recurrente (PGR) a\u00f1adiendo un factor de descuento.", "Ampl\u00eda el promedio de pesos recurrentes para superar la limitaci\u00f3n del m\u00e9todo original manteniendo su ventaja y propone el m\u00e9todo de utilizar redes Elman como RNN base"]}
{"source": ["Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging.", "In this paper, we introduce variance layers, a different kind of stochastic layers.", "Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance.", "It means that each object is represented by a zero-mean distribution in the space of the activations.", "We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks.", "We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors.", "We observe that in these cases such zero-mean parameterization leads to a much better training objective than more flexible conventional parameterizations where the mean is being learned."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1GAUs0cKQ", "target": ["Es posible aprender una distribuci\u00f3n gaussiana centrada en cero sobre los pesos de una red neuronal aprendiendo s\u00f3lo las varianzas, y funciona sorprendentemente bien.", "Este trabajo investiga los efectos de la media de la posterioridad variacional y propone la capa de varianza, que s\u00f3lo utiliza la varianza para almacenar informaci\u00f3n", "Estudia las redes neuronales de varianza que aproximan la posterior de las redes neuronales bayesianas con distribuciones gaussianas de media cero"]}
{"source": ["Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data.", "At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks.", "In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work.", "At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective.", "Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI.", "In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SkaPsfZ0W", "target": ["Hacemos una red de redes de convoluci\u00f3n de grafos, alimentando cada una de ellas con una potencia diferente de la matriz de adyacencia, combinando toda su representaci\u00f3n en una subred de clasificaci\u00f3n, logrando el estado del arte en la clasificaci\u00f3n de nodos semi-supervisada.", "Propone una nueva red de GCNs con dos enfoques: una capa totalmente conectada sobre caracter\u00edsticas apiladas y un mecanismo de atenci\u00f3n que utiliza un peso escalar por GCN.", "Presenta una red de redes convolucionales de grafos que utiliza la estad\u00edstica del paseo aleatorio para extraer informaci\u00f3n de los vecinos cercanos y lejanos en el grafo"]}
{"source": ["Recent DNN pruning algorithms have succeeded in reducing the number of parameters in fully connected layers often with little or no drop in classification accuracy.", "However most of the existing pruning schemes either have to be applied during training or require a costly retraining procedure after pruning to regain classification accuracy.", "In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation.", "We also provide theoretical analysis for the growth in the Generalisation Error (GE) of the new pruned network.", "Our method can be used with any convex regulariser and allows for a controlled degradation in classification accuracy while being orders of magnitude faster than competing approaches.", "Experiments on common feedforward neural networks show that for sparsity levels above 90% our method achieves 10% higher classification accuracy compared to Hard Thresholding."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SJtChcgAW", "target": ["Un algoritmo de poda r\u00e1pida para capas de DNN totalmente conectadas con an\u00e1lisis te\u00f3rico de la degradaci\u00f3n del error de generalizaci\u00f3n.", "Presenta un algoritmo de poda barato para capas densas de DNNs.", "Propone una soluci\u00f3n al problema de la poda de DNNs planteando la funci\u00f3n objetivo Net-trim como una funci\u00f3n de diferencia de convexidad (DC)."]}
{"source": ["Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years.", "It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing.", "In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage.", "Our model is simple but extremely effective in terms of video sequence labeling.", "The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "r1nzLmWAb", "target": ["Proponemos una nueva red temporal h\u00edbrida que alcanza el rendimiento m\u00e1s avanzado en la segmentaci\u00f3n de acciones de v\u00eddeo en tres conjuntos de datos p\u00fablicos.", "Discute el problema de la segmentaci\u00f3n de acciones en v\u00eddeos largos, de hasta 10 minutos de duraci\u00f3n, utilizando una arquitectura de codificador-decodificador convolucional temporal", "Propone una combinaci\u00f3n de redes convolucionales temporales y recurrentes para la segmentaci\u00f3n de acciones en v\u00eddeo"]}
{"source": ["Convolutional Neural Networks (CNNs) become deeper and deeper in recent years, making the study of model acceleration imperative.", "It is a common practice to employ a shallow network, called student, to learn from a deep one, which is termed as teacher.", "Prior work made many attempts to transfer different types of knowledge from teacher to student, however, there are two problems remaining unsolved.", "Firstly, the knowledge used by existing methods is highly dependent on task and dataset, limiting their applications.", "Secondly, there lacks an effective training scheme for the transfer process, leading to degradation of performance.", "In this work, we argue that feature is the most important knowledge from teacher.", "It is sufficient for student to just learn good features regardless of the target task.", "From this discovery, we further present an efficient learning strategy to mimic features stage by stage.", "Extensive experiments demonstrate the importance of features and show that the proposed approach significantly narrows down the gap between student and teacher, outperforming the state-of-the-art methods.\n"], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rJegl2C9K7", "target": ["Este trabajo propone transferir el conocimiento del modelo profundo al superficial imitando las caracter\u00edsticas etapa por etapa.", "Explica un m\u00e9todo de transferencia de conocimientos por etapas utilizando diferentes estructuras de redes", "Este trabajo propone dividir una red en m\u00faltiples partes y destilar cada parte secuencialmente para mejorar el rendimiento de la destilaci\u00f3n en las redes de maestros profundos"]}
{"source": ["We augment adversarial training (AT) with worst case adversarial training\n", "(WCAT) which improves adversarial robustness by 11% over the current state-\n", "of-the-art result in the `2-norm on CIFAR-10.", "We interpret adversarial training as\n", "Total Variation Regularization, which is a fundamental tool in mathematical im-\n", "age processing, and WCAT as Lipschitz regularization, which appears in Image\n", "Inpainting.", "We obtain verifiable worst and average case robustness guarantees,\n", "based on the expected and maximum values of the norm of the gradient of the\n", "loss."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkxAisC9FQ", "target": ["Se obtienen mejoras en la robustez del adversario, as\u00ed como garant\u00edas de robustez demostrables, aumentando el entrenamiento del adversario con una regularizaci\u00f3n Lipschitz manejable", "Explora el aumento de la p\u00e9rdida de entrenamiento con un t\u00e9rmino adicional de regularizaci\u00f3n del gradiente para mejorar la solidez de los modelos frente a los ejemplos adversos", "Utiliza un truco para simplificar la p\u00e9rdida adversaria por una en la que la perturbaci\u00f3n adversaria aparece en forma cerrada."]}
{"source": ["The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \\{\\textit{passage, question}\\} pair and select one of the $n$ given options.", "The current state of the art model for this task first computes a query-aware representation for the passage and then \\textit{selects} the option which has the maximum similarity with this representation.", "However, when humans perform this task they do not just focus on option selection but use a combination of \\textit{elimination} and \\textit{selection}. Specifically, a human would first try to eliminate the most irrelevant option and then read the document again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option).", "This process could be repeated multiple times till the reader is finally ready to select the correct option.", "We propose \\textit{ElimiNet}, a neural network based model which tries to mimic this process.", "Specifically, it has gates which decide whether an option can be eliminated given the \\{\\textit{document, question}\\} pair and if so it tries to make the document representation orthogonal to this eliminatedd option (akin to ignoring portions of the document corresponding to the eliminated option).", "The model makes multiple rounds of partial elimination to refine the document representation and finally uses a selection module to pick the best option.", "We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset.", "Further we show that taking an ensemble of our \\textit{elimination-selection} based method with a \\textit{selection} based method gives us an improvement of 7\\% (relative) over the best reported performance on this dataset.    \n"], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1bgpzZAZ", "target": ["Un modelo que combina la eliminaci\u00f3n y la selecci\u00f3n para responder a preguntas de opci\u00f3n m\u00faltiple", "Da una elaboraci\u00f3n sobre el Lector de Atenci\u00f3n Cerrada a\u00f1adiendo puertas basadas en la eliminaci\u00f3n de respuestas en la comprensi\u00f3n lectora de opci\u00f3n m\u00faltiple", "Este trabajo propone el uso de una puerta de eliminaci\u00f3n en arquitecturas de modelos para tareas de comprensi\u00f3n lectora, pero no consigue resultados de \u00faltima generaci\u00f3n", "Este art\u00edculo propone un nuevo modelo de comprensi\u00f3n lectora de opciones m\u00faltiples basado en la idea de que algunas opciones deben ser eliminadas para inferir mejores representaciones de pasajes/preguntas."]}
{"source": ["Humans are capable of attributing latent mental contents such as beliefs, or intentions to others.", "The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead.", "It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs.  ", "In this paper, we start  from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning.", "Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors.", "Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy.", "We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.", "Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge.", "Our experiments show that it is critical to reason about how the opponents believe about what the agent believes.", "We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.  \n"], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkl6As0cF7", "target": ["Proponemos un novedoso marco de razonamiento recursivo probabil\u00edstico (PR2) para tareas de aprendizaje de refuerzo profundo multiagente.", "Propone un nuevo enfoque para el entrenamiento totalmente descentralizado en el aprendizaje por refuerzo de m\u00faltiples agentes", "Aborda el problema de dotar a los agentes de RL de capacidades de razonamiento recursivo en un entorno multiagente bas\u00e1ndose en la hip\u00f3tesis de que el razonamiento recursivo es beneficioso para que converjan a equilibrios no trivalentes", "El art\u00edculo introduce un m\u00e9todo de entrenamiento descentralizado para el aprendizaje por refuerzo de m\u00faltiples agentes, en el que los agentes infieren las pol\u00edticas de otros agentes y utilizan los modelos inferidos para la toma de decisiones. "]}
{"source": ["Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers.", "However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections.", "A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks.", "To address these issues, we propose a method to reduce the communication overhead of distributed deep learning.", "Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated.", "We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost.", "We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy.", "We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkEfPeZRb", "target": ["Un nuevo algoritmo para reducir la sobrecarga de comunicaci\u00f3n del aprendizaje profundo distribuido distinguiendo los gradientes \"no ambiguos\".", "Propone un m\u00e9todo de compresi\u00f3n de gradiente basado en la varianza para reducir la sobrecarga de comunicaci\u00f3n del aprendizaje profundo distribuido", "Propone una forma novedosa de comprimir las actualizaciones de gradiente para el SGD distribuido con el fin de acelerar la ejecuci\u00f3n general", "Introduce el m\u00e9todo de compresi\u00f3n de gradiente basado en la varianza para el entrenamiento distribuido eficiente de redes neuronales y la medici\u00f3n de la ambig\u00fcedad."]}
{"source": ["In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains.", "We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics.", "Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion.", "We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJWechg0Z", "target": ["Una nueva t\u00e9cnica no supervisada de adaptaci\u00f3n de dominios profundos que unifica eficazmente la alineaci\u00f3n de la correlaci\u00f3n y la minimizaci\u00f3n de la entrop\u00eda", "Mejora el enfoque de alineaci\u00f3n de correlaci\u00f3n para la adaptaci\u00f3n de dominios, sustituyendo la distancia euclidiana por la distancia geod\u00e9sica logar\u00edtmica entre dos matrices de covarianza, y seleccionando autom\u00e1ticamente el coste de equilibrio por la entrop\u00eda en el dominio objetivo.", "Propuesta de alineaci\u00f3n de correlaci\u00f3n de entrop\u00eda m\u00ednima, un algoritmo de adaptaci\u00f3n de dominio no supervisado que une los m\u00e9todos de minimizaci\u00f3n de entrop\u00eda y de alineaci\u00f3n de correlaci\u00f3n."]}
{"source": ["Catastrophic interference has been a major roadblock in the research of continual learning.", "Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks.", "Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting.", "CAB extends these results to deep feedforward networks.", "On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1al7jg0b", "target": ["Proponemos una variante del algoritmo de retropropagaci\u00f3n, en la que los gradientes son protegidos por los conceptores contra la degradaci\u00f3n de las tareas previamente aprendidas.", "Este trabajo aplica la noci\u00f3n de conceptores, una forma de regularizador, para evitar el olvido en el aprendizaje continuo en el entrenamiento de redes neuronales en tareas secuenciales.", "Introduce un m\u00e9todo para el aprendizaje de nuevas tareas, sin interferir en las anteriores, utilizando conceptores."]}
{"source": ["Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task.", "Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks.", "In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora.", "Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering.", "It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering.", "On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses.", "The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1eqviAqYX", "target": ["Analizar la raz\u00f3n por la que los modelos generativos de respuesta neural prefieren las respuestas universales; Proponer un m\u00e9todo para evitarlo.", "Investiga el problema de las r\u00e9plicas universales que afectan a los modelos de generaci\u00f3n neuronal de Seq2Seq", "El art\u00edculo estudia la mejora de la tarea de generaci\u00f3n de respuestas neuronales al restar importancia a las respuestas comunes mediante la modificaci\u00f3n de la funci\u00f3n de p\u00e9rdida y la presentaci\u00f3n de las respuestas comunes/universales durante la fase de entrenamiento."]}
{"source": ["The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval.", "Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens.", "We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code.", "Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding.\n", "We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples.", "Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.", "An interactive online demo of our model is available at http://code2seq.org.", "Our code, data and trained models are available at http://github.com/tech-srl/code2seq."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1gKYo09tX", "target": ["Aprovechamos la estructura sint\u00e1ctica del c\u00f3digo fuente para generar secuencias de lenguaje natural.", "Presenta un m\u00e9todo para generar secuencias a partir de c\u00f3digo mediante el an\u00e1lisis sint\u00e1ctico y la producci\u00f3n de un \u00e1rbol de sintaxis", "Este art\u00edculo presenta una codificaci\u00f3n basada en AST para el c\u00f3digo de programaci\u00f3n y muestra su eficacia en las tareas de resumen de c\u00f3digo extremo y subtitulaci\u00f3n de c\u00f3digo.", "Este art\u00edculo presenta un nuevo modelo de c\u00f3digo a secuencia que aprovecha la estructura sint\u00e1ctica de los lenguajes de programaci\u00f3n para codificar fragmentos de c\u00f3digo fuente y luego decodificarlos a lenguaje natural"]}
{"source": ["We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition.", "The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations.", "Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\n", "Differently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD.", "As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\n", "Experiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJe7FW-Cb", "target": ["Mejoramos las CNN con un novedoso mecanismo de atenci\u00f3n para el reconocimiento de grano fino. Se obtiene un rendimiento superior en 5 conjuntos de datos.", "Describe un nuevo mecanismo atencional aplicado al reconocimiento de grano fino que mejora sistem\u00e1ticamente la precisi\u00f3n de reconocimiento de la l\u00ednea de base", "Este trabajo propone un mecanismo de atenci\u00f3n de avance para la clasificaci\u00f3n de im\u00e1genes de grano fino", "Este trabajo presenta un interesante mecanismo de atenci\u00f3n para la clasificaci\u00f3n de im\u00e1genes de grano fino."]}
{"source": ["Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator.", "We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances.", "We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem.", "We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator.", "Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin.", "Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkQsMCJCb", "target": ["Sustituimos las convoluciones normales por convoluciones adaptativas para mejorar el generador de GANs.", "Propone sustituir las convoluciones en el generador por un bloque de convoluci\u00f3n adaptativo que aprende a generar los pesos de convoluci\u00f3n y los sesgos de las operaciones de remuestreo de forma adaptativa por ubicaci\u00f3n de p\u00edxel", "Utiliza la Convoluci\u00f3n Adaptativa en el contexto de los GANs con un bloque llamado AdaConvBlock que reemplaza a la Convoluci\u00f3n regular, esto da m\u00e1s contexto local por peso del kernel para poder generar objetos localmente flexibles."]}
{"source": ["Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model.", "However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings.", "In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters.", "Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast.", "Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent.", "Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made.", "These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted.", "Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible.", "We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rkr1UDeC-", "target": ["Llevamos a cabo experimentos a gran escala para demostrar que una simple variante online de la destilaci\u00f3n puede ayudarnos a escalar el entrenamiento de redes neuronales distribuidas a m\u00e1s m\u00e1quinas.", "Propone un m\u00e9todo para escalar el entrenamiento distribuido m\u00e1s all\u00e1 de los l\u00edmites actuales del descenso de gradiente estoc\u00e1stico en mini lotes", "Propuesta de un m\u00e9todo de destilaci\u00f3n en l\u00ednea llamado codistilaci\u00f3n, aplicado a escala, en el que dos modelos diferentes se entrenan para igualar las predicciones del otro modelo adem\u00e1s de minimizar su propia p\u00e9rdida.", "Se introduce la t\u00e9cnica de destilaci\u00f3n en l\u00ednea para acelerar los algoritmos tradicionales de entrenamiento de redes neuronales distribuidas a gran escala"]}
{"source": ["Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis.", "Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings.", "In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training.", "A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set.", "We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data.", "Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem.", "We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1saNM-RW", "target": ["Presentamos un algoritmo para acelerar el entrenamiento de SVM en conjuntos de datos masivos mediante la construcci\u00f3n de representaciones compactas que proporcionan una inferencia eficiente y probadamente aproximada.", "Estudia el enfoque de coreset para SVM y tiene como objetivo el muestreo de un peque\u00f1o conjunto de puntos ponderados de tal manera que la funci\u00f3n de p\u00e9rdida sobre los puntos se aproxima de manera demostrable a la de todo el conjunto de datos", "El art\u00edculo sugiere una construcci\u00f3n de Coreset basada en el muestreo de importancia para representar grandes datos de entrenamiento para SVMs"]}
{"source": ["The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates.", "Since signSGD carries out one-bit quantization of the gradients, it is extremely practical for distributed optimization where gradients need to be aggregated from different processors.", "For the first time, we establish convergence rates for signSGD on general non-convex functions under transparent conditions.", "We show that the rate of signSGD to reach first-order critical points matches that of SGD in terms of number of stochastic gradient calls, up to roughly a linear factor in the dimension.", "We carry out simple experiments to explore the behaviour of sign gradient descent (without the stochasticity) close to saddle points and show that it often helps completely avoid them without using either stochasticity or curvature information."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyxjwgbRZ", "target": ["Demostramos una tasa de convergencia no convexa para el m\u00e9todo del gradiente estoc\u00e1stico de signos. El algoritmo tiene v\u00ednculos con algoritmos como Adam y Rprop, as\u00ed como con esquemas de cuantificaci\u00f3n de gradientes utilizados en el aprendizaje autom\u00e1tico distribuido.", "Proporcion\u00f3 un an\u00e1lisis de convergencia del algoritmo Sign SGD para casos no covexos", "El art\u00edculo explora un algoritmo que utiliza el signo de los gradientes en lugar de los gradientes reales para entrenar modelos profundos"]}
{"source": ["Deep learning has found numerous applications thanks to its versatility and accuracy on pattern recognition problems such as visual object detection.", "Learning and inference in deep neural networks, however, are memory and compute intensive and so improving efficiency is one of the major challenges for frameworks such as PyTorch, Tensorflow, and Caffe.", "While the efficiency problem can be partially addressed with specialized hardware and its corresponding proprietary libraries, we believe that neural network acceleration should be transparent to the user and should support all hardware platforms and deep learning libraries. \n\n", "To this end, we introduce a transparent middleware layer for neural network acceleration.", "The system is built around a compiler for deep learning, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices.", "In contrast to other projects, we explicitly target the optimization of both prediction and training of neural networks.", "We present the current development status and some preliminary but encouraging results: on a standard x86 server, using CPUs our system achieves a 11.8x speed-up for inference and a 8.0x for batched-prediction (128); on GPUs we achieve a 1.7x and 2.3x speed-up respectively."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rkf5hnNDj7", "target": ["Introducimos un middleware transparente para la aceleraci\u00f3n de redes neuronales, con un motor de compilaci\u00f3n propio, que consigue aumentar la velocidad hasta 11,8 veces en las CPU y 2,3 veces en las GPU.", "Este trabajo propone una capa de middleware transparente para la aceleraci\u00f3n de redes neuronales y obtiene algunos resultados de aceleraci\u00f3n en arquitecturas b\u00e1sicas de CPU y GPU"]}
{"source": ["Performance of neural networks can be significantly improved by encoding known invariance for particular tasks.", "Many image classification tasks, such as those related to cellular imaging, exhibit invariance to rotation.", "In particular, to aid convolutional neural networks in learning rotation invariance, we consider a simple, efficient conic convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) to encode global rotational invariance.", "We call our new method the Conic Convolution and DFT Network (CFNet).", "We evaluated the efficacy of CFNet as compared to a standard CNN and group-equivariant CNN (G-CNN) for several different image classification tasks and demonstrated improved performance, including classification accuracy, computational efficiency, and its robustness to hyperparameter selection.", "Taken together, we believe CFNet represents a new scheme that has the potential to improve many imaging analysis applications."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJepX2A9tX", "target": ["Proponemos la convoluci\u00f3n c\u00f3nica y la 2D-DFT para codificar la equidistancia de la rotaci\u00f3n en una red neuronal.", "En el contexto de la clasificaci\u00f3n de im\u00e1genes, el art\u00edculo propone una arquitectura de red neuronal convolucional con mapas de caracter\u00edsticas de rotaci\u00f3n-equivariante que eventualmente se hacen invariantes de la rotaci\u00f3n mediante el uso de la magnitud de la transformada discreta de Fourier (DFT) en 2D.", "Los autores proporcionan una red neuronal invariante de la rotaci\u00f3n mediante la combinaci\u00f3n de la convoluci\u00f3n c\u00f3nica y la 2D-DFT"]}
{"source": ["The problem of visual metamerism is defined as finding a family of perceptually\n", "indistinguishable, yet physically different images.", "In this paper, we propose our\n", "NeuroFovea metamer model, a foveated generative model that is based on a mixture\n", "of peripheral representations and style transfer forward-pass algorithms.", "Our\n", "gradient-descent free model is parametrized by a foveated VGG19 encoder-decoder\n", "which allows us to encode images in high dimensional space and interpolate\n", "between the content and texture information with adaptive instance normalization\n", "anywhere in the visual field.", "Our contributions include:", "1) A framework for\ncomputing metamers that resembles a noisy communication system via a foveated\nfeed-forward encoder-decoder network \u2013 We observe that metamerism arises as a\nbyproduct of noisy perturbations that partially lie in the perceptual null space;", "2)\nA perceptual optimization scheme as a solution to the hyperparametric nature of\nour metamer model that requires tuning of the image-texture tradeoff coefficients\neverywhere in the visual field which are a consequence of internal noise;", "3) An\n", "ABX psychophysical evaluation of our metamers where we also find that the rate\n", "of growth of the receptive fields in our model match V1 for reference metamers\n", "and V2 between synthesized samples.", "Our model also renders metamers at roughly\n", "a second, presenting a \u00d71000 speed-up compared to the previous work, which now\n", "allows for tractable data-driven metamer experiments."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJzbG20cFQ", "target": ["Introducimos un novedoso marco de alimentaci\u00f3n para generar metamateriales visuales", "Propone un modelo NeuroFovea para la generaci\u00f3n de metamateriales de punto de fijaci\u00f3n mediante un enfoque de transferencia de estilo a trav\u00e9s de una arquitectura de estilo Codificador-Decodificador", "Un an\u00e1lisis del metamerismo y un modelo capaz de producir r\u00e1pidamente metameras de valor para la psicof\u00edsica experimental y otros dominios.", "El art\u00edculo propone un m\u00e9todo r\u00e1pido para generar metamers visuales -im\u00e1genes f\u00edsicamente diferentes que no pueden distinguirse de un original- a trav\u00e9s de una transferencia de estilo foveada, r\u00e1pida y arbitraria"]}
{"source": ["Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks.", "Towards explaining this phenomenon, we adopt a margin-based perspective.", "We establish:", "1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks,", "2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for deep networks.", "In the case of two-layer networks, an infinite-width neural network enjoys the best generalization guarantees.", "The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees.", "We validate this gap between the two approaches empirically.", "Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization.", "We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJGtFoC5Fm", "target": ["Demostramos que el entrenamiento de las redes relu feedforward con un regularizador d\u00e9bil da como resultado un margen m\u00e1ximo y analizamos las implicaciones de este resultado.", "Estudia la teor\u00eda de los m\u00e1rgenes para conjuntos neuronales y demuestra que el margen m\u00e1ximo es monot\u00f3nicamente creciente en el tama\u00f1o de la red", "Este trabajo estudia el sesgo impl\u00edcito de los minimizadores de una p\u00e9rdida de entrop\u00eda cruzada regularizada de una red de dos capas con activaciones ReLU, obteniendo una cota superior de generalizaci\u00f3n que no aumenta con el tama\u00f1o de la red."]}
{"source": ["We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible.", "The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network.", "The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors.", "Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time."], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "H1Dy---0Z", "target": ["Una arquitectura distribuida para el aprendizaje profundo por refuerzo a escala, que utiliza la generaci\u00f3n de datos en paralelo para mejorar el estado del arte en el benchmark Arcade Learning Environment en una fracci\u00f3n del tiempo de entrenamiento de los enfoques anteriores.", "Examina un sistema de RL profundo distribuido en el que las experiencias, en lugar de los gradientes, se comparten entre los trabajos paralelos y el aprendiz centralizado", "Un enfoque paralelo para el entrenamiento de DQN, basado en la idea de tener m\u00faltiples actores recogiendo datos en paralelo mientras un \u00fanico aprendiz entrena el modelo a partir de experiencias muestreadas de la memoria de repetici\u00f3n central.", "Este trabajo propone una arquitectura distribuida para el aprendizaje profundo por refuerzo a escala, centr\u00e1ndose en la adici\u00f3n de la paralelizaci\u00f3n en el algoritmo del actor en el marco de la repetici\u00f3n de la experiencia priorizada"]}
{"source": ["Designing neural networks for continuous-time stochastic processes is challenging, especially when observations are made irregularly.", "In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R).", "Moreover, we show that, under certain assumptions, these properties hold even when signals are irregularly observed.", "As we converge to the family of (convolutional) neural networks that satisfy these conditions, we show that we can optimize our convolution filters while constraining them so that they effectively compute a Discrete Wavelet Transform.", "Such a neural network can efficiently divide the time-axis of a signal into orthogonal sub-spaces of different temporal scale and localization.", "We evaluate the resulting neural network on an assortment of synthetic and real-world tasks: parsimonious auto-encoding, video classification, and financial forecasting."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1fHmlbCW", "target": ["Arquitecturas neuronales que proporcionan representaciones de se\u00f1ales observadas de forma irregular que permiten de forma demostrable la reconstrucci\u00f3n de la se\u00f1al.", "Demuestra que las redes neuronales convolucionales con funci\u00f3n de activaci\u00f3n Leaky ReLU son marcos no lineales, con resultados similares para series temporales no uniformemente muestreadas", "Este art\u00edculo considera las redes neuronales sobre series temporales y muestra que los primeros filtros convolucionales pueden ser elegidos para representar una transformada wavelet discreta."]}
{"source": ["Most state-of-the-art neural machine translation systems, despite being different\n", "in architectural skeletons (e.g., recurrence, convolutional), share an indispensable\n", "feature: the Attention.", "However, most existing attention methods are token-based\n", "and ignore the importance of phrasal alignments, the key ingredient for the success\n", "of phrase-based statistical machine translation.", "In this paper, we propose\n", "novel phrase-based attention methods to model n-grams of tokens as attention\n", "entities.", "We incorporate our phrase-based attentions into the recently proposed\n", "Transformer network, and demonstrate that our approach yields improvements of\n", "1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation\n", "tasks, and 1.75 and 1.35 BLEU points in English-to-Russian and Russian-to-English translation tasks \n", "on WMT newstest2014 using WMT\u201916 training data.\n"], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1xN5oA5tm", "target": ["Mecanismos de atenci\u00f3n basados en frases para asignar la atenci\u00f3n en frases, logrando alineaciones de atenci\u00f3n de token a frase, de frase a token, de frase a frase, adem\u00e1s de las atenciones existentes de token a token.", "El art\u00edculo presenta un mecanismo de atenci\u00f3n que calcula una suma ponderada no s\u00f3lo de tokens individuales sino de ngramas (frases)."]}
{"source": ["Intuitively, unfamiliarity should lead to lack of confidence.", "In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training.", "Unlike domain adaptation methods, we cannot gather an \"unexpected dataset\" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected.", "We compare a number of methods from related fields such as calibration and epistemic uncertainty modeling, as well as two proposed methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: (1) G-distillation, training an ensemble of classifiers and then distill into a single model using both labeled and unlabeled examples, or (2) NCR, reducing prediction confidence based on its novelty detection score.", "Experimentally, we investigate the overconfidence problem and evaluate our solution by creating \"familiar\" and \"novel\" test splits, where \"familiar\" are identically distributed with training and \"novel\" are not.", "We discover that calibrating using temperature scaling on familiar data is the best single-model method for improving novel confidence, followed by our proposed methods.", "In addition, some methods' NLL performance are roughly equivalent to a regularly trained model with certain degree of smoothing.", "Calibrating can also reduce confident errors, for example, in gender recognition by 95% on demographic groups different from the training data."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1giro05t7", "target": ["Las redes profundas son m\u00e1s propensas a equivocarse con confianza cuando se prueban con datos inesperados. Proponemos una metodolog\u00eda experimental para estudiar el problema y dos m\u00e9todos para reducir los errores de confianza en distribuciones de entrada desconocidas.", "Propone dos ideas para reducir el exceso de confianza en las predicciones err\u00f3neas: La \"destilaci\u00f3n G\" de un conjunto con datos adicionales no supervisados y la reducci\u00f3n de la confianza en las novedades mediante el detector de novedades", "Los autores proponen dos m\u00e9todos para estimar la confianza de la clasificaci\u00f3n en nuevas distribuciones de datos no vistas. La primera idea consiste en utilizar m\u00e9todos de conjunto como enfoque base para ayudar a identificar los casos inciertos y, a continuaci\u00f3n, utilizar m\u00e9todos de destilaci\u00f3n para reducir el conjunto en un \u00fanico modelo que imite el comportamiento del conjunto. La segunda idea es utilizar un clasificador detector de novedades y ponderar la salida de la red por la puntuaci\u00f3n de la novedad."]}
{"source": ["Progress in deep learning is slowed by the days or weeks it takes to train large models.", "The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources.", "In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available.", "Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products.", "We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps.", "At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%.", "Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%.", "Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkLyJl-0-", "target": ["Describimos un pr\u00e1ctico algoritmo de optimizaci\u00f3n para redes neuronales profundas que funciona m\u00e1s r\u00e1pido y genera mejores modelos en comparaci\u00f3n con los algoritmos m\u00e1s utilizados.", "Propone un nuevo algoritmo en el que afirman utilizar el hessiano de forma impl\u00edcita y utilizan una motivaci\u00f3n de las series de potencia", "Presenta un nuevo algoritmo de segundo orden que utiliza impl\u00edcitamente la informaci\u00f3n de curvatura y muestra la intuici\u00f3n detr\u00e1s de los esquemas de aproximaci\u00f3n en los algoritmos y valida la heur\u00edstica en varios experimentos."]}
{"source": ["Recent work has shown that performing inference with fast, very-low-bitwidth\n", "(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\n", "results.", "However, although 2-bit approximated networks have been shown to\n", "be quite accurate, 1 bit approximations, which are twice as fast, have restrictively\n", "low accuracy.", "We propose a method to train models whose weights are a mixture\n", "of bitwidths, that allows us to more finely tune the accuracy/speed trade-off.", "We\n", "present the \u201cmiddle-out\u201d criterion for determining the bitwidth for each value, and\n", "show how to integrate it into training models with a desired mixture of bitwidths.\n", "We evaluate several architectures and binarization techniques on the ImageNet\n", "dataset.", "We show that our heterogeneous bitwidth approximation achieves superlinear\n", "scaling of accuracy with bitwidth.", "Using an average of only 1.4 bits, we are\n", "able to outperform state-of-the-art 2-bit architectures."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJDV5YxCW", "target": ["Introducimos la aproximaci\u00f3n del ancho de bits fraccionario y demostramos que tiene ventajas significativas.", "Propone un m\u00e9todo para variar el grado de cuantificaci\u00f3n en una red neuronal durante la fase de propagaci\u00f3n hacia delante", "Mantener la precisi\u00f3n de la red de 2bits utilizando menos de 2bits de pesos"]}
{"source": ["Pruning units in a deep network can help speed up inference and training as well as reduce the size of the model.", "We show that bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset.  ", "We also show how a simple adaptation to an existing scoring function allows us to select the best units to prune.  ", "Finally,  we show that the units selected by the best performing scoring functions are somewhat consistent over the course of training, implying the dead parts of the network appear during the stages of training."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJxRVnC5Fm", "target": ["El reemplazo de la media es un m\u00e9todo eficiente para mejorar la p\u00e9rdida despu\u00e9s de la poda y las funciones de puntuaci\u00f3n basadas en la aproximaci\u00f3n de Taylor funcionan mejor con los valores absolutos. ", "Propone una sencilla mejora de los m\u00e9todos de poda de unidades mediante el \"reemplazo medio\"", "Este trabajo presenta una estrategia de poda de sustituci\u00f3n de la media y utiliza la expansi\u00f3n de Taylor de valor absoluto como funci\u00f3n de puntuaci\u00f3n para la poda"]}
{"source": ["Due to the phenomenon of \u201cposterior collapse,\u201d current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires altering the training objective.", "We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information.", "Our proposed \u03b4-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior.", "For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis.", "We demonstrate our method\u2019s efficacy at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 \u00d7 32."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJe0Gn0cY7", "target": [" Evitar el colapso posterior limitando la tasa.", "Presenta un enfoque para evitar el colapso posterior en las VAE limitando la familia de la aproximaci\u00f3n variacional a la posterior", "Este trabajo introduce una restricci\u00f3n en la familia de posteriors variacionales de manera que el t\u00e9rmino KL puede ser controlado para combatir el colapso posterior en modelos generativos profundos como los VAE"]}
{"source": ["Mini-batch gradient descent and its variants are commonly used in deep learning.", "The principle of mini-batch gradient descent is to use noisy gradient calculated on a batch to estimate the real gradient, thus balancing the computation cost per iteration and the uncertainty of noisy gradient.", "However, its batch size is a fixed hyper-parameter requiring manual setting before training the neural network.", "Yin et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD) that can dynamically choose a proper batch size as learning proceeds.", "We extend the BA-SGD to momentum algorithm and evaluate both the BA-SGD and the batch adaptive momentum (BA-Momentum) on two deep learning tasks from natural language processing to image classification.", "Experiments confirm that batch adaptive methods can achieve a lower loss compared with mini-batch methods after scanning the same epochs of data.", "Furthermore, our BA-Momentum is more robust against larger step sizes, in that it can dynamically enlarge the batch size to reduce the larger uncertainty brought by larger step sizes.", "We also identified an interesting phenomenon, batch size boom.", "The code implementing batch adaptive framework is now open source, applicable to any gradient-based optimization problems."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SybqeKgA-", "target": ["Desarrollamos un impulso adaptativo por lotes que puede lograr una p\u00e9rdida menor en comparaci\u00f3n con los m\u00e9todos por mini lotes despu\u00e9s de escanear las mismas \u00e9pocas de datos, y es m\u00e1s robusto frente a un tama\u00f1o de paso grande.", "Este trabajo aborda el problema de ajustar autom\u00e1ticamente el tama\u00f1o del lote durante el entrenamiento del aprendizaje profundo, y pretende extender el SGD adaptativo por lotes al impulso adaptativo y adoptar los algoritmos a problemas de redes neuronales complejas.", "El documento propone generalizar un algoritmo que realiza SGD con tama\u00f1os de lote adaptables a\u00f1adiendo el impulso a la funci\u00f3n de utilidad"]}
{"source": ["Deep learning models for graphs have advanced the state of the art on many tasks.", "Despite their recent success, little is known about their robustness.", "We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure.  ", "Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.", "Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings.", "Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information.", "Our attacks do not assume any knowledge about or access to the target classifiers."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Bylnx209YX", "target": ["Utilizamos meta-gradientes para atacar el procedimiento de entrenamiento de las redes neuronales profundas para grafos.", "Estudia el problema de aprender un mejor par\u00e1metro de grafos envenenados que pueda maximizar la p\u00e9rdida de una red neuronal de grafos. ", "Un algoritmo para alterar la estructura del grafo a\u00f1adiendo/eliminando aristas para degradar el rendimiento global de la clasificaci\u00f3n de nodos, y la idea de utilizar el meta-aprendizaje para resolver el problema de optimizaci\u00f3n de dos niveles."]}
{"source": ["Numerous models for grounded language understanding have been recently proposed, including", "(i) generic models that can be easily adapted to any given task and", "(ii) intuitively appealing modular models that require background knowledge to be instantiated.", "We compare both types of models in how much they lend themselves to a particular form of systematic generalization.", "Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them.", "Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected.", "We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization.", "We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization.", "Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.\n"], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HkezXnA9YX", "target": ["Demostramos que los modelos modulares estructurados son los mejores en t\u00e9rminos de generalizaci\u00f3n sistem\u00e1tica y que sus versiones de extremo a extremo no generalizan tan bien.", "Este art\u00edculo eval\u00faa la generalizaci\u00f3n sist\u00e9mica entre las redes neuronales modulares y otros modelos gen\u00e9ricos mediante la introducci\u00f3n de un nuevo conjunto de datos de razonamiento espacial", "Una evaluaci\u00f3n emp\u00edrica orientada a la generalizaci\u00f3n en modelos de razonamiento visual, centrada en el problema del reconocimiento de triples (objeto, relaci\u00f3n, objeto) en escenas sint\u00e9ticas con letras y n\u00fameros."]}
{"source": ["The behavioral dynamics of multi-agent systems have a rich and orderly structure, which can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them.", "Here we introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi-agent environments.", "Because these models operate on the discrete entities and relations present in the environment, they produce interpretable intermediate representations which offer insights into what drives agents' behavior, and what events mediate the intensity and valence of social interactions.", "Furthermore, we show that embedding RFM modules inside agents results in faster learning systems compared to non-augmented baselines. \n", "As more and more of the autonomous systems we develop and interact with become multi-agent in nature, developing richer analysis tools for characterizing how and why agents make decisions is increasingly necessary.", "Moreover, developing artificial agents that quickly and safely learn to coordinate with one another, and with humans in shared environments, is crucial."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJlEojAqFm", "target": ["Los modelos relacionales de avance para el aprendizaje de agentes m\u00faltiples hacen predicciones precisas del comportamiento futuro de los agentes, producen representaciones intepretables y pueden utilizarse dentro de los agentes.", "Una forma de reducir la varianza en el aprendizaje sin modelos al disponer de un modelo expl\u00edcito, que utiliza una arquitectura tipo red de grafos, de las acciones que realizar\u00e1n otros agentes. ", "Predicci\u00f3n del comportamiento de varios agentes mediante un modelo relacional de avance con un componente recurrente, superando dos l\u00edneas de base y dos ablaciones"]}
{"source": ["We show that gradient descent on an unregularized logistic regression\n", "problem, for almost all separable datasets, converges to the same direction as the max-margin solution.", "The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss.", "Furthermore,\n", "we show this convergence is very slow, and only logarithmic in the\n", "convergence of the loss itself.", "This can help explain the benefit\n", "of continuing to optimize the logistic or cross-entropy loss even\n", "after the training error is zero and the training loss is extremely\n", "small, and, as we show, even if the validation loss increases.", "Our\n", "methodology can also aid in understanding implicit regularization\n", "in more complex models and with other optimization methods."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1q7n9gAb", "target": ["La soluci\u00f3n normalizada del descenso de gradiente en la regresi\u00f3n log\u00edstica (o una p\u00e9rdida similar que decaiga) converge lentamente a la soluci\u00f3n de margen m\u00e1ximo L2 en datos separables.", "El art\u00edculo ofrece una prueba formal de que el descenso del gradiente sobre la p\u00e9rdida log\u00edstica converge muy lentamente a la soluci\u00f3n SVM dura en el caso de que los datos sean linealmente separables. ", "Este trabajo se centra en la caracterizaci\u00f3n del comportamiento de la minimizaci\u00f3n de la p\u00e9rdida logar\u00edtmica en datos linealmente separables, y muestra que la p\u00e9rdida logar\u00edtmica, minimizada con el descenso de gradiente, conduce a la convergencia a la soluci\u00f3n de margen m\u00e1ximo."]}
{"source": ["Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.", "For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier.", "Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.", "This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals.", "To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image.", "Then we introduce two techniques for improving our networks' out-of-sample performance.", "The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable.", "The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's.\n", "We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJEjjoR9K7", "target": ["Bas\u00e1ndonos en trabajos anteriores sobre la generalizaci\u00f3n de dominios, esperamos producir un clasificador que se generalice a dominios no vistos previamente, incluso cuando los identificadores del dominio no est\u00e9n disponibles durante el entrenamiento.", "Un enfoque de generalizaci\u00f3n de dominio para revelar informaci\u00f3n sem\u00e1ntica basado en un esquema de proyecci\u00f3n lineal de las capas de salida de CNN y NGLCM.", "El art\u00edculo propone un enfoque no supervisado para identificar las caracter\u00edsticas de la imagen que no son significativas para las tareas de clasificaci\u00f3n de im\u00e1genes"]}
{"source": ["In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild.", "In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors.", "Our approach alternates between two threads.", "In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles.", "In the second, we minimize the approximated detection score by searching for the optimal camouflage.", "Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJgEl3A5tm", "target": ["Proponemos un m\u00e9todo para aprender el camuflaje f\u00edsico de los veh\u00edculos para atacar de forma adversa a los detectores de objetos en la naturaleza. Comprobamos que nuestro camuflaje es eficaz y transferible.", "Los autores investigan el problema de aprender un patr\u00f3n de camuflaje que, aplicado a un veh\u00edculo simulado, impida que un detector de objetos lo detecte.", "Este trabajo se centra en el aprendizaje adversarial para la detecci\u00f3n de coches interferentes mediante el aprendizaje de patrones de camuflaje"]}
{"source": ["As deep learning-based classifiers are increasingly adopted in real-world applications, the importance of understanding how a particular label is chosen grows.", "Single decision trees are an example of a simple, interpretable classifier, but are unsuitable for use with complex, high-dimensional data.", "On the other hand, the variational autoencoder (VAE) is designed to learn a factored, low-dimensional representation of data, but typically encodes high-likelihood data in an intrinsically non-separable way.  ", "We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree.  ", "We also explore the power of labeled data in a  supervised VAE (SVAE) with a Gaussian mixture prior, which leverages label information to produce a high-quality generative model with improved bounds on log-likelihood.  ", "We combine the SVAE with the DDT to get our classifier+VAE (C+VAE), which is competitive in both classification error and log-likelihood, despite optimizing both simultaneously and using a very simple encoder/decoder architecture."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rJhR_pxCZ", "target": ["Combinamos \u00e1rboles de decisi\u00f3n diferenciables con autocodificadores variacionales supervisados para mejorar la interpretabilidad de la clasificaci\u00f3n. ", "Este trabajo propone un modelo h\u00edbrido de un autoencoder variacional compuesto con un \u00e1rbol de decisi\u00f3n diferenciable, y un esquema de entrenamiento que lo acompa\u00f1a, con experimentos que demuestran el rendimiento de la clasificaci\u00f3n del \u00e1rbol, el rendimiento de la probabilidad logar\u00edtmica negativa, y la interpretabilidad del espacio latente.", "El art\u00edculo trata de construir un clasificador interpretable y preciso mediante el apilamiento de un VAE supervisado y un \u00e1rbol de decisi\u00f3n diferenciable"]}
{"source": ["We propose Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain.", "We first estimate importance weights using labeled source data and unlabeled target data, and then train a classifier on the weighted source samples.", "We derive a generalization bound for the classifier on the target domain which is independent of the (ambient) data dimensions, and instead only depends on the complexity of the function class.", "To the best of our knowledge, this is the first generalization bound for the label-shift problem where the labels in the target domain are not available.", "Based on this bound, we propose a regularized estimator for the small-sample regime which accounts for the uncertainty in the estimated weights.", "Experiments on the CIFAR-10 and MNIST datasets show that RLLS improves classification accuracy, especially in the low sample and large-shift regimes, compared to previous methods."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJl0r3R9KX", "target": ["Un enfoque pr\u00e1ctico y con garant\u00edas probables para el entrenamiento de clasificadores eficientes en presencia de cambios de etiquetas entre los conjuntos de datos de origen y destino", "Los autores proponen un nuevo algoritmo para mejorar la estabilidad del procedimiento de estimaci\u00f3n de la importancia de las clases con un procedimiento de dos pasos.", "Los autores consideran el problema del aprendizaje bajo cambios de etiquetas, donde las proporciones de las etiquetas difieren mientras las condicionales son iguales, y proponen un estimador mejorado con regularizaci\u00f3n."]}
{"source": ["The statistics of the real visual world presents a long-tailed distribution: a few classes have significantly more training instances than the remaining classes in a dataset.", "This is because the real visual world has a few classes that are common while others are rare.", "Unfortunately, the performance of a convolutional neural network is typically unsatisfactory when trained using a long-tailed dataset.", "To alleviate this issue, we propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes.", "To this end, the proposed approach uses a Gaussian mixture model to factor out class-likelihoods and class-priors in a long-tailed dataset.", "The proposed method is simple and easy-to-implement in existing deep learning frameworks.", "Experiments on publicly available datasets show that the proposed approach improves the performance on classes with few training instances, while maintaining a comparable performance to the state-of-the-art on classes with abundant training examples."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Bk9nkMa4G", "target": ["Enfoque para mejorar la precisi\u00f3n de la clasificaci\u00f3n en las clases de la cola.", "El objetivo principal de este trabajo es aprender un clasificador ConvNet que funcione mejor para las clases en la cola de la distribuci\u00f3n de ocurrencia de clases.", "Propuesta de un marco bayesiano con un modelo de mezcla gaussiana para abordar un problema en las aplicaciones de clasificaci\u00f3n, que el n\u00famero de datos de entrenamiento de diferentes clases est\u00e1 desequilibrado."]}
{"source": ["As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents.", "Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior.", "However, this topic has been relatively under-explored in the reinforcement learning community.", "In this work we present a method for synthesizing states of interest for a trained agent.", "Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary.", "Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system.", "To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest.", "In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods.", "We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator.", "Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJf9k305Fm", "target": ["Presentamos un m\u00e9todo para sintetizar estados de inter\u00e9s para agentes de aprendizaje por refuerzo con el fin de analizar su comportamiento. ", "Este trabajo propone un modelo generativo de observaciones visuales en RL que es capaz de generar observaciones de inter\u00e9s.", "Un enfoque para visualizar los estados de inter\u00e9s que implica un autoencoder variacional que aprende a reconstruir el espacio de estados y un paso de optimizaci\u00f3n que encuentra los par\u00e1metros de acondicionamiento para generar im\u00e1genes sint\u00e9ticas."]}
{"source": ["We introduce the deep abstaining classifier -- a deep neural network trained with a novel loss function that provides an abstention option during training.", "This allows the  DNN to abstain on confusing or difficult-to-learn examples while improving performance on the non-abstained samples.", "We show that such deep abstaining classifiers can:", "(i) learn representations for structured noise -- where noisy training labels or confusing examples are correlated with underlying features -- and then learn to abstain based on such features;", "(ii) enable robust learning in the presence of arbitrary or unstructured noise by identifying noisy samples; and", "(iii) be used as an effective out-of-category detector that learns to reliably abstain when presented with samples from  unknown classes.", "We provide analytical results on loss function behavior that enable automatic tuning of accuracy and coverage, and demonstrate the utility of the deep abstaining classifier using multiple image benchmarks, Results indicate significant improvement in learning in the presence of label noise."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJxF73R9tX", "target": ["Una red neuronal profunda de abstenci\u00f3n entrenada con una funci\u00f3n de p\u00e9rdida novedosa que aprende representaciones para saber cu\u00e1ndo abstenerse permitiendo un aprendizaje robusto en presencia de diferentes tipos de ruido.", "Una nueva funci\u00f3n de p\u00e9rdida para el entrenamiento de una red neuronal profunda que puede abstenerse, con un rendimiento visto desde los \u00e1ngulos en existencia de ruido estructurado, en existencia de ruido no estructurado, y la detecci\u00f3n del mundo abierto.", "Este manuscrito introduce clasificadores profundos de abstenci\u00f3n que modifican la p\u00e9rdida de entrop\u00eda cruzada multiclase con una p\u00e9rdida de abstenci\u00f3n, que luego se aplica a tareas de clasificaci\u00f3n de im\u00e1genes perturbadas"]}
{"source": ["Temporal Difference learning with function approximation has been widely used recently and has led to several successful results.  ", "However, compared with the original tabular-based methods, one major drawback of temporal difference learning with neural networks and other function approximators is that they tend to over-generalize across temporally successive states, resulting in slow convergence and even instability.", "In this work, we propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence.", "This approach can be easily applied to both linear and nonlinear function approximators. \n", "HR-TD is evaluated on several linear and nonlinear benchmark domains, where we show improvement in learning behavior and performance."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rylbWhC5Ym", "target": ["Una t\u00e9cnica de regularizaci\u00f3n para el aprendizaje de TD que evita la sobregeneralizaci\u00f3n temporal, especialmente en redes profundas", "Una variaci\u00f3n del aprendizaje por diferencia temporal para el caso de aproximaci\u00f3n de funciones que intenta resolver el problema de la sobregeneralizaci\u00f3n a trav\u00e9s de estados temporalmente sucesivos.", "Este art\u00edculo presenta el algoritmo HR-TD, una variaci\u00f3n del algoritmo TD(0), cuyo objetivo es mejorar el problema de la sobregeneralizaci\u00f3n en el TD convencional"]}
{"source": ["We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. \n", "To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters.", "Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation.", "As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters.", "We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation.", "Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Bkl-43C9FQ", "target": ["Presentamos un nuevo kernel de CNN para cuadr\u00edculas no estructuradas para se\u00f1ales esf\u00e9ricas, y mostramos una importante ganancia de precisi\u00f3n y eficiencia de par\u00e1metros en tareas como la clasificaci\u00f3n 3D y la segmentaci\u00f3n de im\u00e1genes omnidireccionales.", "Un m\u00e9todo eficiente que permite el aprendizaje profundo sobre datos esf\u00e9ricos y que alcanza cifras competitivas/de \u00faltima generaci\u00f3n con muchos menos par\u00e1metros que los enfoques populares.", "El art\u00edculo propone un novedoso kernel convolucional para CNN en las mallas no estructuradas y formula la convoluci\u00f3n mediante una combinaci\u00f3n lineal de operadores diferenciales."]}
{"source": ["Prediction is arguably one of the most basic functions of an intelligent system.", "In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult.", "However, most phenomena naturally pass through relatively predictable bottlenecks---while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up.", "To exploit this, we decouple visual prediction from a rigid notion of time.", "While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur.", "We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks.", "Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SyzVb3CcFX", "target": ["En las tareas de predicci\u00f3n visual, dejar que el modelo de predicci\u00f3n elija qu\u00e9 momentos predecir hace dos cosas: (i) mejora la calidad de la predicci\u00f3n, y (ii) da lugar a predicciones sem\u00e1nticamente coherentes del \"estado del cuello de botella\", que son \u00fatiles para la planificaci\u00f3n.", "Un m\u00e9todo sobre la predicci\u00f3n de fotogramas en un v\u00eddeo, el enfoque incluye que la predicci\u00f3n del objetivo es flotante, resuelto por un m\u00ednimo en el error de predicci\u00f3n.", "Reformula la tarea de predicci\u00f3n/interpolaci\u00f3n de v\u00eddeo para que un predictor no se vea obligado a generar fotogramas en intervalos de tiempo fijos, sino que se entrene para generar fotogramas que sucedan en cualquier punto del futuro."]}
{"source": ["In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly.", "We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process.", "First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes.", "Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location.", "Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior.", "We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy.\n"], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryBnUWb0b", "target": ["Utilizamos un LSTM para detectar cu\u00e1ndo un smartphone entra en un edificio. A continuaci\u00f3n, predecimos el nivel del suelo del dispositivo utilizando los datos de los sensores a bordo del smartphone.", "El art\u00edculo presenta un sistema para estimar el nivel de un piso a trav\u00e9s de los datos de los sensores de su dispositivo m\u00f3vil utilizando un LSTM y los cambios en la presi\u00f3n barom\u00e9trica", "Propuesta de un m\u00e9todo de dos pasos para determinar en qu\u00e9 piso se encuentra un tel\u00e9fono m\u00f3vil dentro de un edificio alto."]}
{"source": ["Sparse reward is one of the most challenging problems in reinforcement learning (RL).", "Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals.", "Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation.", "We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation.", "We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn.", "We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons.", "We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HyM8V2A9Km", "target": ["Combinar la representaci\u00f3n de los objetivos ling\u00fc\u00edsticos con las repeticiones de la experiencia retrospectiva.", "Este art\u00edculo considera la suposici\u00f3n impl\u00edcita en la repetici\u00f3n de la experiencia retrospectiva, de que hay acceso a un mapeo de los estados a las metas, y propone una representaci\u00f3n de las metas en lenguaje natural.", "Esta presentaci\u00f3n utiliza el marco Hindsight Experience Replay con objetivos de lenguaje natural para mejorar la eficiencia de la muestra de los modelos de seguimiento de instrucciones."]}
{"source": ["Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval.", "Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected.", "Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features.", "Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances.", "This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances.", "Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost.", "This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "B1e0KsRcYQ", "target": ["Proponemos un esquema conjunto de libro de c\u00f3digos y factorizaci\u00f3n para mejorar el pooling de segundo orden.", "Este trabajo presenta una forma de combinar las representaciones factorizadas de segundo orden existentes con una asignaci\u00f3n dura al estilo del libro de c\u00f3digos.", "Propuesta de una nueva representaci\u00f3n bilineal basada en un modelo de libro de c\u00f3digos, y una formulaci\u00f3n eficiente en la que las proyecciones basadas en el libro de c\u00f3digos se factorizan mediante una proyecci\u00f3n compartida para reducir a\u00fan m\u00e1s el tama\u00f1o de los par\u00e1metros."]}
{"source": ["Natural language understanding research has recently shifted towards complex Machine Learning and Deep Learning algorithms.", "Such models often outperform their simpler counterparts significantly.", "However, their performance relies on the availability of large amounts of labeled data, which are rarely available.", "To tackle this problem, we propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision.", "We apply this methodology on biomedical relation extraction, a task where training datasets are excessively time-consuming and expensive to create, yet has a major impact on downstream applications such as drug discovery.", "We demonstrate in two small-scale controlled experiments that our method consistently enhances the performance of an LSTM network, with performance improvements comparable to hand-labeled training data.", "Finally, we discuss the optimal setting for applying weak supervision using this methodology."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rygDeZqap7", "target": ["Proponemos y aplicamos una metodolog\u00eda de meta-aprendizaje basada en la Supervisi\u00f3n D\u00e9bil, para combinar el Aprendizaje Semisupervisado y el Ensemble en la tarea de Extracci\u00f3n de Relaciones Biom\u00e9dicas.", "Un m\u00e9todo semi-supervisado para la clasificaci\u00f3n de relaciones, que entrena a m\u00faltiples aprendices de base utilizando un peque\u00f1o conjunto de datos etiquetados y aplica algunos de ellos para anotar ejemplos no etiquetados para el aprendizaje semi-supervisado.", "Este trabajo aborda el problema de la generaci\u00f3n de datos de entrenamiento para la extracci\u00f3n de relaciones biol\u00f3gicas, y utiliza predicciones de datos etiquetados por clasificadores d\u00e9biles como datos de entrenamiento adicionales para un algoritmo de metaaprendizaje.", "Este trabajo propone una combinaci\u00f3n de aprendizaje semisupervisado y aprendizaje de conjunto para la extracci\u00f3n de informaci\u00f3n, con experimentos realizados en una tarea de extracci\u00f3n de relaciones biom\u00e9dicas"]}
{"source": ["We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations.", "CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations.", "Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly.", "Our approach offers two major advantages:", "(i) for each prediction, valid instance-specific explanations are generated with no computational overhead and", "(ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings.", "We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations.", "Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJUOHGWRb", "target": ["Una clase de redes que generan modelos sencillos sobre la marcha (llamados explicaciones) que act\u00faan como un regularizador y permiten un diagn\u00f3stico del modelo consistente y la interpretabilidad.", "Los autores afirman que el arte anterior integra directamente las redes neuronales en los modelos gr\u00e1ficos como componentes, lo que hace que los modelos sean ininterpretables.", "Propuesta de combinaci\u00f3n de redes neuronales y modelos gr\u00e1ficos mediante el uso de una red neuronal profunda para predecir los par\u00e1metros de un modelo gr\u00e1fico."]}
{"source": ["The goal of imitation learning (IL) is to enable a learner to imitate an expert\u2019s behavior given the expert\u2019s demonstrations.", "Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks.", "However, GAIL requires a huge number of interactions with environment during training.", "We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced.", "To this end, we propose a model free, off-policy IL algorithm for continuous control.", "The keys of our algorithm are two folds:", "1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG),", "2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert\u2019s demonstrations.", "Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJ3fy0k0Z", "target": ["Proponemos un algoritmo de aprendizaje por imitaci\u00f3n sin modelo que es capaz de reducir el n\u00famero de interacciones con el entorno en comparaci\u00f3n con el algoritmo de aprendizaje por imitaci\u00f3n m\u00e1s avanzado, el GAIL.", "Propone extender el algoritmo de gradiente de pol\u00edtica determinista para aprender de las demostraciones, mientras se combina con un tipo de estimaci\u00f3n de la densidad del experto.", "Este trabajo considera el problema del aprendizaje por imitaci\u00f3n sin modelo y propone una extensi\u00f3n del algoritmo de aprendizaje por imitaci\u00f3n generativo adversarial sustituyendo la pol\u00edtica estoc\u00e1stica del aprendiz por una determinista.", "El art\u00edculo combina el IRL, el entrenamiento adversario y las ideas de los gradientes de pol\u00edtica determinista con el objetivo de disminuir la complejidad de la muestra"]}
{"source": ["Convolution acts as a local feature extractor in convolutional neural networks (CNNs).", "However, the convolution operation is not applicable when the input data is supported on an irregular graph such as with social networks, citation networks, or knowledge graphs.", "This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs.", "The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution, replacing the square filter for the grid-structured data in traditional CNNs.", "The outputs are the weighted sum of these filters\u2019 outputs, extraction of both vertex features and strength of correlation between vertices.", "It\n", "can be used with both directed and undirected graphs.", "The proposed TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing.", "Further, as no approximation to the convolution is needed, TAGCN exhibits better performance than existing graph-convolution-approximation methods on a number\n", "of data sets.", "As only the polynomials of degree two of the adjacency matrix are used, TAGCN is also computationally simpler than other recent methods."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H113pWZRb", "target": ["CNN gr\u00e1fica de baja complejidad computacional (sin aproximaci\u00f3n) con mayor precisi\u00f3n de clasificaci\u00f3n", "Propone un nuevo enfoque de la CNN para la clasificaci\u00f3n de grafos utilizando un filtro basado en paseos de salida de longitud creciente para incorporar informaci\u00f3n de v\u00e9rtices m\u00e1s distantes en un solo paso de propagaci\u00f3n.", "Propuesta de una nueva arquitectura de red neuronal para la clasificaci\u00f3n de grafos semi-supervisada, bas\u00e1ndose en filtros polin\u00f3micos de grafos y utiliz\u00e1ndolos en capas sucesivas de la red neuronal con funciones de activaci\u00f3n ReLU.", "El art\u00edculo introduce la GCN adaptativa a la topolog\u00eda para generalizar las redes convolucionales a los datos estructurados en grafos"]}
{"source": ["Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks.", "Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift.", "We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning.", "Across several benchmark data sets, we find that:", "(i) certain examples are forgotten with high frequency, and some not at all;", "(ii) a data set's (un)forgettable examples generalize across neural architectures; and", "(iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "BJlxm30cKm", "target": ["Demostramos que el olvido catastr\u00f3fico se produce dentro de lo que se considera una \u00fanica tarea y descubrimos que los ejemplos que no son propensos al olvido pueden eliminarse del conjunto de entrenamiento sin p\u00e9rdida de generalizaci\u00f3n.", "Estudia el comportamiento de olvido de los ejemplos de entrenamiento durante el SGD, y muestra que existen \"ejemplos de apoyo\" en el entrenamiento de redes neuronales a trav\u00e9s de diferentes arquitecturas de red.", "Este trabajo analiza hasta qu\u00e9 punto las redes aprenden a clasificar correctamente ejemplos espec\u00edficos y luego olvidan estos ejemplos a lo largo del entrenamiento.", "El trabajo estudia si algunos ejemplos en el entrenamiento de redes neuronales son m\u00e1s dif\u00edciles de aprender que otros. Dichos ejemplos se olvidan y se vuelven a aprender varias veces a trav\u00e9s del aprendizaje."]}
{"source": ["Discovering objects and their attributes is of great importance for autonomous agents to effectively operate in human environments.", "This task is particularly challenging due to the ubiquitousness of objects and all their nuances in perceptual and semantic detail.", "In this paper we present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos.", "These continuous representations are not biased by or limited by a discrete set of labels determined by human labelers.", "The proposed representation is trained with a metric learning loss, where objects with homogeneous features are pushed together, while those with heterogeneous features are pulled apart.", "We show these unsupervised embeddings allow to discover object attributes and can enable robots to self-supervise in previously unseen environments.", "We quantitatively evaluate performance on a large-scale synthetic dataset with 12k object models, as well as on a real dataset collected by a robot and show that our unsupervised object understanding generalizes to previously unseen objects.", "Specifically, we demonstrate the effectiveness of our approach on robotic manipulation tasks, such as pointing at and grasping of objects.", "An interesting and perhaps surprising finding in this approach is that given a limited set of objects, object correspondences will naturally emerge when using metric learning without requiring explicit positive pairs."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1g6XnCcKQ", "target": ["Un enfoque no supervisado para el aprendizaje de representaciones desenmara\u00f1adas de objetos completamente a partir de v\u00eddeos monoculares no etiquetados.", "Dise\u00f1a una representaci\u00f3n de caracter\u00edsticas a partir de secuencias de v\u00eddeo capturadas de una escena desde diferentes puntos de vista.", "Propuesta de un m\u00e9todo de aprendizaje de representaci\u00f3n no supervisado para entradas visuales que incorpora un enfoque de aprendizaje m\u00e9trico que acerca los pares de parches de im\u00e1genes m\u00e1s cercanos en el espacio de incrustaci\u00f3n mientras aleja otros pares.", "Este trabajo explora el aprendizaje auto-supervisado de las representaciones de objetos, con la idea principal de animar a los objetos con caracter\u00edsticas similares a ser m\u00e1s \"atra\u00eddos\" el uno al otro."]}
{"source": ["Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions.  ", "We introduce state aligned vector rewards, which are easily defined in metric state spaces and allow our deep reinforcement learning agent to tackle the curse of dimensionality.  ", "Our agent learns to map from action distributions to state change distributions implicitly defined in a quantile function neural network.   ", "We further introduce a new reinforcement learning technique inspired by quantile regression which does not limit agents to explicitly parameterized action distributions.  ", "Our results in high dimensional state spaces show that training with vector rewards allows our agent to learn multiple times faster than an agent training with scalar rewards."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "r1lFYoRcFm", "target": ["Entrenamos con recompensas vectoriales alineadas con el estado a un agente que predice los cambios de estado a partir de las distribuciones de acciones, utilizando una nueva t\u00e9cnica de aprendizaje por refuerzo inspirada en la regresi\u00f3n cuant\u00edlica.", "Presenta un algoritmo que pretende acelerar el aprendizaje por refuerzo en situaciones en las que la recompensa est\u00e1 alineada con el espacio de estado. ", "Este trabajo aborda la RL en el espacio de acci\u00f3n continua, utilizando una pol\u00edtica re-parametrizada y un novedoso objetivo de entrenamiento basado en vectores.", "Este trabajo propone mezclar la RL distributiva con una red encargada de modelar la evoluci\u00f3n del mundo en t\u00e9rminos de cuantiles, alegando mejoras en la eficiencia de la muestra."]}
{"source": ["We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation.", "In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states.", "Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode.", "We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJvWjcgAZ", "target": ["Proponemos Episodic Backward Update, un novedoso algoritmo de aprendizaje profundo por refuerzo que muestrea las transiciones episodio a episodio y actualiza los valores de forma recursiva hacia atr\u00e1s para conseguir un aprendizaje r\u00e1pido y estable.", "Propone un nuevo DQN en el que los objetivos se calculan sobre un episodio completo mediante una actualizaci\u00f3n hacia atr\u00e1s (del final al principio) para una propagaci\u00f3n m\u00e1s r\u00e1pida de las recompensas al final del episodio.", "Los autores proponen modificar el algoritmo DQN aplicando el operador max Bellman recursivamente sobre una trayectoria con cierto decaimiento para evitar la acumulaci\u00f3n de errores con el max anidado.", "En las redes deep-Q, se actualizan los valores de Q a partir del final del episodio para facilitar la r\u00e1pida propagaci\u00f3n de las recompensas a lo largo del mismo."]}
{"source": ["Survival Analysis (time-to-event analysis) in the presence of multiple possible adverse events, i.e., competing risks, is a challenging, yet very important problem in medicine, finance, manufacturing, etc.", "Extending classical survival analysis to competing risks is not trivial since only one event (e.g. one cause of death) is observed and hence, the incidence of an event of interest is often obscured by other related competing events.", "This leads to the nonidentifiability of the event times\u2019 distribution parameters, which makes the problem significantly more challenging.", "In this work we introduce Siamese Survival Prognosis Network, a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events.", "The Siamese Survival Network is especially crafted to issue pairwise concordant time-dependent risks, in which longer event times are assigned lower risks.", "Furthermore, our architecture is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics of cross-entropy etc., and which are not able to capture the unique requirements of survival analysis with competing risks.", "Our results show consistent performance improvements on a number of publicly available medical datasets over both statistical and deep learning state-of-the-art methods."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkjL6MiTb", "target": ["En este trabajo introducimos una novedosa arquitectura de red neuronal profunda siamesa que es capaz de aprender eficazmente de los datos en presencia de m\u00faltiples eventos adversos.", "Este trabajo introduce las redes neuronales siamesas en el marco de los riesgos competitivos mediante la optimizaci\u00f3n del \u00edndice c directamente", "Los autores abordan los problemas de estimaci\u00f3n del riesgo en un entorno de an\u00e1lisis de supervivencia con riesgos concurrentes y proponen optimizar directamente el \u00edndice de discriminaci\u00f3n dependiente del tiempo utilizando una red de supervivencia siamesa"]}
{"source": ["The digitization of data has resulted in making datasets available to millions of users in the form of relational databases and spreadsheet tables.", "However, a majority of these users come from diverse backgrounds and lack the programming expertise to query and analyze such tables.", "We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query.", "We use a deep sequence to sequence model in wich the decoder uses a simple type system of SQL expressions to structure the output prediction.", "Based on the type, the decoder either copies an output token from the input question using an attention-based copying mechanism or generates it from a fixed vocabulary.", "We also introduce a value-based loss function that transforms a distribution over locations to copy from into a distribution over the set of input tokens to improve training of our model.", "We evaluate our model on the recently released WikiSQL dataset and show that our model trained using only supervised learning significantly outperforms the current state-of-the-art Seq2SQL model that uses reinforcement learning."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkUDW_lCb", "target": ["Presentamos un modelo de red de punteros basado en el tipo junto con un m\u00e9todo de p\u00e9rdida basado en el valor para entrenar eficazmente un modelo neuronal para traducir el lenguaje natural a SQL.", "El documento pretende desarrollar un m\u00e9todo novedoso para mapear consultas en lenguaje natural a SQL utilizando una gram\u00e1tica para guiar la decodificaci\u00f3n y utilizando una nueva funci\u00f3n de p\u00e9rdida para el mecanismo de puntero / copia"]}
{"source": ["To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity.", "Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers.", "The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric \"self-control\" baseline function together with the REINFORCE estimator in that augmented space.", "Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers.", "Python code for reproducible research is publicly available."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "S1lg0jAcYm", "target": ["Un estimador de gradiente insesgado y de baja varianza para modelos de variables latentes discretas", "Propone una nueva t\u00e9cnica de reducci\u00f3n de la varianza para utilizar cuando se calcula un gradiente de p\u00e9rdida esperada donde la expectativa es con respecto a variables aleatorias binarias independientes.", "Un algoritmo que combina la Rao-Blackwellizaci\u00f3n y los n\u00fameros aleatorios comunes para reducir la varianza del estimador del gradiente de la funci\u00f3n de puntuaci\u00f3n en el caso especial de las redes binarias estoc\u00e1sticas", "Un estimador insesgado y de baja varianza augment-REINFORCE-merge (ARM) para calcular y retropropagar gradientes en redes neuronales binarias"]}
{"source": ["Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training.", "The scheme can reach a linear speed-up with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits.", "To overcome this communication bottleneck recent works propose to reduce the communication frequency.", "An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while.", "This scheme shows promising results in practice, but eluded thorough theoretical analysis.\n    \n", "We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size.", "The number of  communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD.", "This also holds for asynchronous implementations.\n\n", "Local SGD can also be used for large scale training of deep learning models.", "The results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1g2JnRcFX", "target": ["Demostramos que la SGD local paralela consigue un aumento de velocidad lineal con una comunicaci\u00f3n mucho menor que la SGD paralela en miniatura.", "Proporciona una prueba de convergencia para el SGD local, y demuestra que el SGD local puede proporcionar las mismas ganancias de velocidad que el minibatch, pero puede ser capaz de comunicar significativamente menos.", "Este trabajo presenta un an\u00e1lisis de SGD local y l\u00edmites sobre la frecuencia de los estimadores obtenidos mediante la ejecuci\u00f3n de SGD que deben ser promediados para producir aumentos de velocidad de paralelizaci\u00f3n lineal.", "Los autores analizan el algoritmo SGD local, en el que se ejecutan $K$ cadenas paralelas de SGD, y los iterados se sincronizan ocasionalmente entre m\u00e1quinas promediando"]}
{"source": ["Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems.", "The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams.", "Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions.", "We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions.", "To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation.", "We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded.", "We provide theoretical guarantees concerning the convergence of the proposed learning algorithm.", "To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters.", "Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems.", "Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rJgTciR9tm", "target": ["Percepci\u00f3n compacta del proceso din\u00e1mico", "Estudia el problema de representar de forma compacta el modelo de un sistema din\u00e1mico complejo preservando la informaci\u00f3n mediante un m\u00e9todo de cuello de botella de informaci\u00f3n.", "Este trabajo estudi\u00f3 la din\u00e1mica lineal gaussiana y propuso un algoritmo para calcular la jerarqu\u00eda de cuellos de botella de informaci\u00f3n (IBH)."]}
{"source": ["We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly.", "As the density of the connection increases, the number of paths through which the gradient flows can be increased.", "It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time.", "Larger gradients, however, can also cause exploding gradient problem.", "To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows.", "We describe the relation between the attention gate and the gradient flows by approximation.", "The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model\u2019s performance."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJVruWZRW", "target": ["RNN densa que tiene conexiones completas desde cada estado oculto a m\u00faltiples estados ocultos precedentes de todas las capas directamente.", "Propone una nueva arquitectura RNN que modela mejor las dependencias a largo plazo, puede aprender la representaci\u00f3n multiescalar de los datos secuenciales, y elude el problema de los gradientes mediante el uso de unidades de compuerta parametrizadas.", "Este art\u00edculo propone una arquitectura RNN densa totalmente conectada con conexiones cerradas a cada capa y conexiones de las capas precedentes, y sus resultados en la tarea de modelado a nivel de caracteres de la PTB."]}
{"source": ["We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs).", "In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose.", "Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code.", "Corresponding samples from the real dataset consist of two distinct photographs of the same subject.", "In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person.", "We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training.", "Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm\u2019s ability to generate convincing, identity-matched photographs."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1nQvfgA-", "target": ["Los SD-GAN desentra\u00f1an los c\u00f3digos latentes en funci\u00f3n de los puntos comunes conocidos en un conjunto de datos (por ejemplo, fotograf\u00edas que representan a la misma persona).", "Este art\u00edculo investiga el problema de la generaci\u00f3n de im\u00e1genes controladas y propone un algoritmo que produce un par de im\u00e1genes con la misma identidad.", "Este trabajo propone, SD-GAN, un m\u00e9todo de entrenamiento de GANs para desentra\u00f1ar la informaci\u00f3n de identidad y no identidad en el vector latente de entrada Z."]}
{"source": ["The goal of unpaired cross-domain translation is to learn useful mappings between two domains, given unpaired sets of datapoints from these domains.", "While this formulation is highly underconstrained, recent work has shown that it is possible to learn mappings useful for downstream tasks by encouraging approximate cycle consistency in the mappings between the two domains [Zhu et al., 2017].", "In this work, we propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings.", "Our framework uses a normalizing flow model to specify a single invertible mapping between the two domains.", "In contrast to prior works in cycle-consistent translations, we can learn AlignFlow via adversarial training, maximum likelihood estimation, or a hybrid of the two methods.", "Theoretically, we derive consistency results for AlignFlow which guarantee recovery of desirable mappings under suitable assumptions.", "Empirically, AlignFlow demonstrates significant improvements over relevant baselines on image-to-image translation and unsupervised domain adaptation tasks on benchmark datasets."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "S1lNELLKuN", "target": ["Proponemos un marco de aprendizaje para las traducciones entre dominios que es exactamente consistente con el ciclo y puede aprenderse a trav\u00e9s del entrenamiento adversarial, la estimaci\u00f3n de m\u00e1xima probabilidad o un h\u00edbrido.", "Propone AlignFlow, una forma eficiente de implementar el principio de consistencia de los ciclos utilizando flujos invertibles.", "Modelos de flujo para la traducci\u00f3n de im\u00e1genes no apareadas"]}
{"source": ["Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, that maps the inputs to their corresponding outputs exactly.", "Due to its precise and combinatorial nature, it is commonly formulated as a constraint satisfaction problem, where input-output examples are expressed constraints, and solved with a constraint solver.", "A key challenge of this formulation is that of scalability: While constraint solvers work well with few well-chosen examples, constraining the entire set of example constitutes a significant overhead in both time and memory.", "In this paper we address this challenge by constructing a representative subset of examples that is both small and is able to constrain the solver sufficiently.", "We build the subset one example at a time, using a trained discriminator to predict the probability of unchosen input-output examples conditioned on the chosen input-output examples, adding the least probable example to the subset.", "Experiment on a diagram drawing domain shows our approach produces subset of examples that are small and representative for the constraint solver."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1CQGfZ0b", "target": ["En un contexto de s\u00edntesis de programas donde la entrada es un conjunto de ejemplos, reducimos el coste calculando un subconjunto de ejemplos representativos", "Propone un m\u00e9todo para identificar ejemplos representativos para la s\u00edntesis de programas con el fin de aumentar la escalabilidad de las soluciones de programaci\u00f3n de restricciones existentes.", "Un m\u00e9todo para elegir un subconjunto de ejemplos sobre los que ejecutar un solucionador de restricciones para resolver problemas de s\u00edntesis de programas.", "Este trabajo propone un m\u00e9todo para acelerar los sintetizadores de programas de prop\u00f3sito general."]}
{"source": ["Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models.", "Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address.", "We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning.", "We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any.", "We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers.", "The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkJKHMW0Z", "target": ["Introducimos las redes relacionales recurrentes, un m\u00f3dulo de red neuronal potente y general para el razonamiento relacional, y lo utilizamos para resolver el 96,6% de los Sudokus m\u00e1s dif\u00edciles y las 19/20 tareas BaBi.", "Introdujo la red relacional recurrente (RRN) que puede a\u00f1adirse a cualquier red neuronal para a\u00f1adir capacidad de razonamiento relacional.", "Introducci\u00f3n de una red neuronal profunda para la predicci\u00f3n estructurada que logra un rendimiento de vanguardia en los rompecabezas Soduku y la tarea BaBi.", "Este art\u00edculo describe un m\u00e9todo denominado red relacional para a\u00f1adir capacidad de razonamiento relacional a las redes neuronales profundas."]}
{"source": ["Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification.", "In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM.", "We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors.", "These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data.", "Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent.", "Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "B1xWcj0qYm", "target": ["Para entrenar modelos profundos a partir de s\u00f3lo datos de U, basta con tres priores de clase, mientras que dos no deber\u00edan ser suficientes.", "Propone un estimador insesgado que permite entrenar los modelos con una supervisi\u00f3n d\u00e9bil en dos conjuntos de datos no etiquetados con prejuicios de clase conocidos y discute las propiedades te\u00f3ricas de los estimadores.", "Una metodolog\u00eda para el entrenamiento de cualquier clasificador binario a partir de s\u00f3lo datos no etiquetados, y un m\u00e9todo de minimizaci\u00f3n del riesgo emp\u00edrico para dos conjuntos de datos no etiquetados en los que se dan las prebendas de clase."]}
{"source": ["Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions.", "We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain.", "Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering.", "This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 32 x 32 px features and Alexnet performance for 16 x16 px features).", "The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification.", "Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts.", "This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SkfMWhAqYQ", "target": ["La agregaci\u00f3n de la evidencia de clase de muchos parches de imagen peque\u00f1os es suficiente para resolver ImageNet, produce modelos m\u00e1s interpretables y puede explicar aspectos de la toma de decisiones de las DNNs populares.", "Este art\u00edculo propone una arquitectura de red neuronal novedosa y compacta que utiliza la informaci\u00f3n de las caracter\u00edsticas de la bolsa de palabras. El algoritmo propuesto s\u00f3lo utiliza la informaci\u00f3n del parche de forma independiente y realiza la votaci\u00f3n por mayor\u00eda utilizando parches clasificados de forma independiente."]}
{"source": ["Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) is an unmet challenge that is intractable with current state-of-the-art mutation calling methods.", "Specifically, the limit of VAF detection is closely related to the depth of coverage, due to the requirement of multiple supporting reads in extant methods, precluding the detection of mutations at VAFs that are orders of magnitude lower than the depth of coverage.", "Nevertheless, the ability to detect cancer-associated mutations in ultra low VAFs is a fundamental requirement for low-tumor burden cancer diagnostics applications such as early detection, monitoring, and therapy nomination using liquid biopsy methods (cell-free DNA).", "Here we defined a spatial representation of sequencing information adapted for convolutional architecture that enables variant detection at VAFs, in a manner independent of the depth of sequencing.", "This method enables the detection of cancer mutations even in VAFs as low as 10x-4^, >2 orders of magnitude below the current state-of-the-art.", "We validated our method on both simulated plasma and on clinical cfDNA plasma samples from cancer patients and non-cancer controls.", "This method introduces a new domain within bioinformatics and personalized medicine \u2013 somatic whole genome mutation calling for liquid biopsy."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1DkN7ZCZ", "target": [" Los m\u00e9todos actuales de mutaciones som\u00e1ticas no funcionan con biopsias l\u00edquidas (es decir, secuenciaci\u00f3n de baja cobertura), aplicamos una arquitectura CNN a una representaci\u00f3n \u00fanica de una lectura y su ailgamiento, mostramos una mejora significativa sobre los m\u00e9todos anteriores en el entorno de baja frecuencia.", "Propone una soluci\u00f3n basada en CNN llamada Kittyhawk para la llamada de mutaciones som\u00e1ticas en frecuencias al\u00e9licas ultra bajas.", "Un nuevo algoritmo para detectar mutaciones cancer\u00edgenas a partir de la secuenciaci\u00f3n del ADN libre de c\u00e9lulas que identificar\u00e1 el contexto de la secuencia que caracteriza los errores de secuenciaci\u00f3n de las verdaderas mutaciones.", "Este trabajo propone un marco de aprendizaje profundo para predecir las mutaciones som\u00e1ticas a frecuencias extremadamente bajas que se producen en la detecci\u00f3n de tumores a partir de ADN libre de c\u00e9lulas"]}
{"source": ["This paper presents the formal release of {\\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts.", "What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines.", "In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval.", "To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "SylxCx5pTQ", "target": ["El art\u00edculo presenta un nuevo corpus gold-standard de literatura cient\u00edfica biom\u00e9dica anotado manualmente con menciones de conceptos UMLS.", "Detalla la construcci\u00f3n de un conjunto de datos anotados manualmente que cubren conceptos biom\u00e9dicos que son m\u00e1s grandes y est\u00e1n cubiertos por una ontolog\u00eda m\u00e1s grande que los conjuntos de datos anteriores.", "Este trabajo utiliza MedMentions, un modelo TaggerOne semi-Markov para el reconocimiento de conceptos de extremo a extremo y la vinculaci\u00f3n en un conjunto de res\u00famenes de Pubmed para etiquetar art\u00edculos con conceptos/entidades biom\u00e9dicas"]}
{"source": ["In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm.", "It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder.", "A clustering network transforms the data into another space and then selects one of the clusters.", "Next, the autoencoder associated with this cluster is used to reconstruct the data-point.", "The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders.", "The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network.", "Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point.", "Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJg_fnRqF7", "target": ["Proponemos un m\u00e9todo de clustering profundo en el que en lugar de un centroide cada cluster est\u00e1 representado por un autoencoder", "Presenta el clustering profundo basado en una mezcla de autocodificadores, donde los puntos de datos se asignan a un cluster bas\u00e1ndose en el error de representaci\u00f3n si se utilizara la red de autocodificadores para representarlo.", "Un enfoque de clustering profundo que utiliza un marco de autoencoder para aprender una incrustaci\u00f3n de baja dimensi\u00f3n de los datos simult\u00e1neamente mientras se agrupan los datos utilizando una red neuronal profunda.", "Un m\u00e9todo de clustering profundo que representa cada cluster con diferentes autocodificadores, funciona de manera integral, y tambi\u00e9n puede ser utilizado para agrupar nuevos datos entrantes sin rehacer todo el procedimiento de clustering."]}
{"source": ["We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM.", "The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu.", "We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis.", "The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared.", "Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions.", "We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs).", "We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation.", "Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJA7xfb0b", "target": ["Definimos una nueva m\u00e9trica de probabilidad integral (IPM de Sobolev) y mostramos c\u00f3mo puede utilizarse para entrenar GANs para la generaci\u00f3n de textos y el aprendizaje semi-supervisado.", "Sugiere un novedoso esquema de regularizaci\u00f3n para GANs basado en una norma de Sobolev, que mide las desviaciones entre las normas L2 de las derivadas.", "Los autores proporcionan otro tipo de GAN utilizando la configuraci\u00f3n t\u00edpica de un GAN pero con una clase de funci\u00f3n diferente, y producen una receta para entrenar GANs con ese tipo de clase de funci\u00f3n.", "El art\u00edculo propone una penalizaci\u00f3n de gradiente diferente para los cr\u00edticos de GAN que obliga a que la norma cuadrada esperada del gradiente sea igual a 1"]}
{"source": ["We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem.", "The main intuition is to employ multiple generators, instead of using a single one as in the original GAN.", "The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results.", "A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN.", "Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from.", "The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model.", "We term our method Mixture Generative Adversarial Nets (MGAN).", "We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators\u2019 distributions and the empirical data distribution is minimal, whilst the JSD among generators\u2019 distributions is maximal, hence effectively avoiding the mode collapsing problem.", "By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets.", "We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkmu5b0a-", "target": ["Proponemos un nuevo enfoque para entrenar GANs con una mezcla de generadores para superar el problema de colapso de modo.", "Abordar el problema del colapso de los modos en los GANs utilizando una distribuci\u00f3n de mezcla restringida para el generador y un clasificador auxiliar que predice el componente de la mezcla de origen.", "El art\u00edculo propone una mezcla de generadores para entrenar GANs sin coste computacional adicional", "Los autores presentan que el uso de MGAN, cuyo objetivo es superar el problema de colapso del modelo mediante generadores de mezcla, logra resultados de \u00faltima generaci\u00f3n"]}
{"source": ["Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons.  ", "Though, given  the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts.", "We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning.", "The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty.", "Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English.", "The platform also provides a hand-crafted bot agent, which simulates a human teacher.  ", "We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels.", "We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJeXCo0cYX", "target": ["Presentamos la plataforma BabyAI para estudiar la eficiencia de los datos en el aprendizaje de idiomas con un humano en el bucle", "Presenta una plataforma de investigaci\u00f3n con un bot en el bucle para aprender a ejecutar instrucciones de lenguaje en el que el lenguaje tiene estructuras de composici\u00f3n", "Presenta una plataforma para el aprendizaje de idiomas en tierra que sustituye a cualquier humano en el bucle por un profesor heur\u00edstico y utiliza un lenguaje sint\u00e9tico mapeado en un mundo cuadriculado en 2D"]}
{"source": ["Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance.", "However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both.", "In this paper, we propose a novel algorithm that jointly learns and compresses a neural network.", "The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty.", "We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints.", "Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HkinqfbAb", "target": ["Un previo k-means combinado con la regularizaci\u00f3n L1 produce resultados de compresi\u00f3n de \u00faltima generaci\u00f3n.", "Este trabajo explora la vinculaci\u00f3n y compresi\u00f3n de par\u00e1metros suaves de las DNNs/CNNs"]}
{"source": ["The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success.", "The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem.", "We show that naive application of the SVRG technique and related approaches fail, and explore why."], "source_labels": [0, 1, 0], "rouge_scores": [], "paper_id": "B1MIBs05F7", "target": ["El m\u00e9todo SVRG falla en los problemas modernos de aprendizaje profundo", "Este art\u00edculo presenta un an\u00e1lisis de los m\u00e9todos de estilo SVRG, mostrando que el abandono, la norma de lotes y el aumento de datos (cultivo/rotaci\u00f3n/traducci\u00f3n aleatorios) tienden a aumentar el sesgo y/o la varianza de las actualizaciones.", "Este trabajo investiga la aplicabilidad del SVGD a las redes neuronales modernas y muestra que la aplicaci\u00f3n ingenua del SVGD suele fallar."]}
{"source": ["The ground-breaking performance obtained by deep convolutional neural networks (CNNs) for image processing tasks is inspiring research efforts attempting to extend it for 3D geometric tasks.", "One of the main challenge in applying CNNs to 3D shape analysis is how to define a natural convolution operator on non-euclidean surfaces.", "In this paper, we present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere.", "A cascade set of geodesic disk filters rotate on the 2-sphere and collect spherical patterns and so to extract geometric features for various 3D shape analysis tasks.", "We demonstrate theoretically and experimentally that our proposed method has the possibility to bridge the gap between 2D images and 3D shapes with the desired rotation equivariance/invariance, and its effectiveness is evaluated in applications of non-rigid/ rigid shape classification and shape retrieval."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkeSiiA5Fm", "target": ["Un m\u00e9todo para aplicar el aprendizaje profundo a las superficies 3D utilizando sus descriptores esf\u00e9ricos y la convoluci\u00f3n anisotr\u00f3pica alt-az sobre 2 esferas.", "Presenta un esquema de convoluci\u00f3n anisotr\u00f3pica polar en una esfera unitaria sustituyendo la traslaci\u00f3n del filtro por la rotaci\u00f3n del mismo.", "Este trabajo explora el aprendizaje profundo de formas 3D utilizando la convoluci\u00f3n anisotr\u00f3pica de 2 esferas alt-az"]}
{"source": ["Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs.", "For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  ", "One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size.", "In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters.", "We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights.", "Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BySRH6CpW", "target": ["Entrenamiento de redes binarias/ternas mediante la reparametrizaci\u00f3n local con la aproximaci\u00f3n CLT", "Entrena redes de distribuci\u00f3n de pesos binarias y ternarias mediante retropropagaci\u00f3n para muestrear las preactivaciones de las neuronas con el truco de la reparametrizaci\u00f3n", "Este trabajo sugiere el uso de par\u00e1metros estoc\u00e1sticos en combinaci\u00f3n con el truco de reparametrizaci\u00f3n local para entrenar redes neuronales con pesos binarios o ternarios, lo que conduce a resultados de \u00faltima generaci\u00f3n."]}
{"source": ["We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.", "OCD is efficient, has no hyper-parameters of its own, and does not require pre-training or joint optimization with conditional log-likelihood.", "Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm.  ", "Then, for each position of the generated sequence, we use a target distribution which puts equal probability on the first token of all the optimal suffixes.", "OCD achieves the state-of-the-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving $9.3\\%$ WER and $4.5\\%$ WER, respectively."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkMW1hRqKX", "target": ["La destilaci\u00f3n de terminaci\u00f3n \u00f3ptima (OCD) es un procedimiento de entrenamiento para optimizar los modelos de secuencia a secuencia basados en la distancia de edici\u00f3n que logra el estado del arte en tareas de reconocimiento del habla de extremo a extremo.", "Enfoque alternativo para entrenar modelos seq2seq utilizando un programa din\u00e1mico para calcular las continuaciones \u00f3ptimas de los prefijos predichos", "Un algoritmo de entrenamiento para modelos autorregresivos que no requiere ning\u00fan preentrenamiento MLE y puede optimizar directamente a partir del muestreo.", "El art\u00edculo considera una deficiencia de los modelos de secuencia a secuencia entrenados utilizando la estimaci\u00f3n de m\u00e1xima verosimilitud y propone un enfoque basado en las distancias de edici\u00f3n y el uso impl\u00edcito de secuencias de etiquetas dadas durante el entrenamiento"]}
{"source": ["As an emerging field, federated learning has recently attracted considerable attention.\n", "Compared to distributed learning in the datacenter setting, federated learning\n", "has more strict constraints on computate efficiency of the learned model and communication\n", "cost during the training process.", "In this work, we propose an efficient\n", "federated learning framework based on variational dropout.", "Our approach is able\n", "to jointly learn a sparse model while reducing the amount of gradients exchanged\n", "during the iterative training process.", "We demonstrate the superior performance\n", "of our approach on achieving significant model compression and communication\n", "reduction ratios with no accuracy loss."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkeAf2CqY7", "target": ["un m\u00e9todo conjunto de sparsificaci\u00f3n de modelos y gradientes para el aprendizaje federado", "Aplica el dropout variacional para reducir el coste de comunicaci\u00f3n del entrenamiento distribuido de las redes neuronales, y realiza experimentos en los conjuntos de datos mnist, cifar10 y svhn. ", "Los autores proponen un algoritmo que reduce los costes de comunicaci\u00f3n en el aprendizaje federado mediante el env\u00edo de gradientes dispersos desde el dispositivo al servidor y viceversa.", "Combina el algoritmo de optimizaci\u00f3n distribuida con el abandono variacional para dispersar los gradientes enviados al servidor principal desde los aprendices locales."]}
{"source": ["We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures.  ", "Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures.  ", "Our method only requires the relatively inexpensive sequential training of T \"shallow ResNets\".", "We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.  ", "In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  ", "A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SksY3deAW", "target": ["Demostramos una teor\u00eda de refuerzo multiclase para las arquitecturas ResNet que crea simult\u00e1neamente una nueva t\u00e9cnica para el refuerzo multiclase y proporciona un nuevo algoritmo para las arquitecturas tipo ResNet.", "Presenta un algoritmo de estilo boosting para el entrenamiento de redes residuales profundas, un an\u00e1lisis de convergencia para el error de entrenamiento y un an\u00e1lisis de la capacidad de generalizaci\u00f3n.", "Un m\u00e9todo de aprendizaje para ResNet utilizando el marco de boosting que descompone el aprendizaje de redes complejas y utiliza menos costes computacionales.", "Los autores proponen la ResNet profunda como algoritmo de refuerzo, y afirman que es m\u00e1s eficiente que la retropropagaci\u00f3n est\u00e1ndar de extremo a extremo."]}
{"source": ["We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector.", "We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\n", "Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1.", "All local minima of $G$ are also global minima.\n", "2.", "All global minima of $G$ correspond to the ground truth parameters.\n", "3.", "The value and gradient of $G$ can be estimated using samples.\n\t\n", "With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters.", "We also prove finite sample complexity results and validate the results by simulations."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkwHObbRZ", "target": ["El art\u00edculo analiza el panorama de optimizaci\u00f3n de las redes neuronales de una capa oculta y dise\u00f1a un nuevo objetivo que, de forma demostrable, no tiene m\u00ednimos locales espurios. ", "Este trabajo estudia el problema del aprendizaje de redes neuronales de una capa oculta, establece una conexi\u00f3n entre la p\u00e9rdida de poblaci\u00f3n por m\u00ednimos cuadrados y los polinomios de Hermite, y propone una nueva funci\u00f3n de p\u00e9rdida.", "Un m\u00e9todo de factorizaci\u00f3n tensorial para la inclinaci\u00f3n de la red neuronal de una capa oculta"]}
{"source": ["Open information extraction (OIE) systems extract relations and their\n  arguments from natural language text in an unsupervised manner.", "The resulting\n  extractions are a valuable resource for downstream tasks such as knowledge\n  base construction, open question answering, or event schema induction.", "In this\n  paper, we release, describe, and analyze an OIE corpus called OPIEC, which was\n  extracted from the text of English Wikipedia.", "OPIEC complements the available\n  OIE resources: It is the largest OIE corpus publicly available to date (over\n  340M triples) and contains valuable metadata such as provenance information,\n  confidence scores, linguistic annotations, and semantic annotations including\n  spatial and temporal information.", "We analyze the OPIEC corpus by comparing its\n  content with knowledge bases such as DBpedia or YAGO, which are also based on\n  Wikipedia.", "We found that most of the facts between entities present in OPIEC\n  cannot be found in DBpedia and/or YAGO, that OIE facts \n  often differ in the level of specificity compared to knowledge base facts, and\n  that OIE open relations are generally highly polysemous.", "We believe that the\n  OPIEC corpus is a valuable resource for future research on automated knowledge\n  base construction."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJxeGb5pTm", "target": ["Un corpus abierto de extracci\u00f3n de informaci\u00f3n y su an\u00e1lisis en profundidad", "Construye un nuevo corpus para la extracci\u00f3n de informaci\u00f3n que es m\u00e1s grande que los corpus p\u00fablicos anteriores y contiene informaci\u00f3n que no existe en los corpus actuales.", "Presenta un conjunto de datos de triples de EI abierta que fueron recogidos de Wikipedia con la ayuda de un sistema de extracci\u00f3n reciente. ", "El art\u00edculo describe la creaci\u00f3n de un corpus Open IE sobre la Wikipedia en ingl\u00e9s de forma autom\u00e1tica"]}
{"source": ["The process of designing neural architectures requires expert knowledge and extensive trial and error.\n", "While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.\n", "We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.\n", "The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.  ", "Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, \nwe explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.\n", "The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkOb1Fl0Z", "target": ["Definimos un DSL flexible para la generaci\u00f3n de arquitecturas de RNN que permite RNN de distinto tama\u00f1o y complejidad y proponemos una funci\u00f3n de clasificaci\u00f3n que representa las RNN como redes neuronales recursivas, simulando su rendimiento para decidir las arquitecturas m\u00e1s prometedoras.", "Introduce un nuevo m\u00e9todo para generar arquitecturas RNNs utilizando un lenguaje espec\u00edfico del dominio para dos tipos de generadores (aleatorios y basados en RL) junto con una funci\u00f3n de clasificaci\u00f3n y un evaluador.", "Este trabajo plantea la b\u00fasqueda de buenas arquitecturas de c\u00e9lulas RNN como un problema de optimizaci\u00f3n de caja negra en el que los ejemplos se representan como un \u00e1rbol de operadores y se punt\u00faan en funci\u00f3n de las funciones aprendidas o generadas por un agente RL.", "Este trabajo investiga la estrategia de meta-aprendizaje para la b\u00fasqueda automatizada de arquitecturas en el contexto de las RNN utilizando un DSL que especifica las operaciones recurrentes de las RNN."]}
{"source": ["Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics.", "Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously.", "In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers.", "To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation.", "Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization.", "Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJGXzmspb", "target": ["Aplicamos el entrenamiento y la inferencia s\u00f3lo con enteros de bajo ancho de bits en las DNNs", "Un m\u00e9todo llamado WAGE que cuantifica todos los operandos y operadores de una red neuronal para reducir el n\u00famero de bits de representaci\u00f3n en una red.", "Los autores proponen pesos, activaciones, gradientes y errores discretizados tanto en el tiempo de entrenamiento como en el de prueba de las redes neuronales"]}
{"source": ["Modern Convolutional Neural Networks (CNNs) are complex, encompassing millions of parameters.", "Their deployment exerts computational, storage and energy demands, particularly on embedded platforms.", "Existing approaches to prune or sparsify CNNs require retraining to maintain inference accuracy.", "Such retraining is not feasible in some contexts.", "In this paper, we explore the sparsification of CNNs by proposing three model-independent methods.", "Our methods are applied on-the-fly and require no retraining.", "We show that the state-of-the-art models' weights can be reduced by up to 73% (compression factor of 3.7x) without incurring more than 5% loss in Top-5 accuracy.", "Additional fine-tuning gains only 8% in sparsity, which indicates that our fast on-the-fly methods are effective."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rkz1YD0vjm", "target": ["En este art\u00edculo, desarrollamos m\u00e9todos de sparsificaci\u00f3n r\u00e1pidos y sin reentrenamiento que pueden utilizarse para la sparsificaci\u00f3n sobre la marcha de las CNN en muchos contextos industriales.", "Este trabajo propone enfoques para la poda de CNNs sin reentrenamiento, introduciendo tres esquemas para determinar los umbrales de los pesos de poda.", "Este art\u00edculo describe un m\u00e9todo para la sparsificaci\u00f3n de las CNN sin reentrenamiento."]}
{"source": ["Curriculum learning and Self paced learning are popular topics in the machine learning that suggest to put the training samples in order by considering their difficulty levels.", "Studies in these topics show that starting with a small training set and adding new samples according to difficulty levels improves the learning performance.", "In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order.", "We compared our method with classical training, Curriculum learning, Self paced learning and their reverse ordered versions.", "Results of the statistical tests show that the proposed method is better than classical method and similar with the others.", "These results point a new training regime that removes the process of difficulty level determination in Curriculum and Self paced learning and as successful as these methods."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJ1fQYlCZ", "target": ["Proponemos que el entrenamiento con conjuntos crecientes etapa por etapa proporciona una optimizaci\u00f3n para las redes neuronales.", "Los autores comparan el aprendizaje del plan de estudios con el aprendizaje en un orden aleatorio con etapas que a\u00f1aden una nueva muestra de ejemplos al conjunto construido previamente de forma aleatoria", "Este trabajo estudia la influencia del ordenamiento en el plan de estudios y el aprendizaje a ritmo propio, y muestra que hasta cierto punto el ordenamiento de las instancias de formaci\u00f3n no es importante."]}
{"source": ["We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\\vb \\in B$ contain all the information that exists in samples $\\va\\in A$ and some additional information.", "For example, ignoring occlusions, $B$ can be people with glasses, $A$ people without, and the glasses, would be the added information.", "When mapping a sample $\\va$ from the first domain to the other domain, the missing information is replicated from an independent reference sample $\\vb\\in B$.", "Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image. \n\n", "Our solution employs a single two-pathway encoder and a single decoder for both domains.", "The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain $A$.", "The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term.", "Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains.", "We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BylE1205Fm", "target": ["M\u00e9todo de traducci\u00f3n de imagen a imagen que a\u00f1ade a una imagen el contenido de otra, creando as\u00ed una nueva imagen.", "Este documento aborda la tarea de la transferencia de contenidos, con la noval\u00eda de la p\u00e9rdida."]}
{"source": ["Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules.", "In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format.", "The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test spits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes.", "Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.\n"], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "H1gR5iR5FX", "target": ["Un conjunto de datos para poner a prueba el razonamiento matem\u00e1tico (y la generalizaci\u00f3n algebraica), y resultados sobre los modelos actuales de secuencia a secuencia.", "Presenta un nuevo conjunto de datos sint\u00e9ticos para evaluar la capacidad de razonamiento matem\u00e1tico de los modelos secuencia a secuencia, y lo utiliza para evaluar varios modelos.", "Modelo para resolver problemas matem\u00e1ticos b\u00e1sicos."]}
{"source": ["Convolutional Neural Networks (CNNs) filter the input data using a series of spatial convolution operators with compactly supported stencils and point-wise nonlinearities.\n", "Commonly, the convolution operators couple features from all channels.\n", "For wide networks, this leads to immense computational cost in the training of and prediction with CNNs.\n", "In this paper, we present novel ways to parameterize the convolution more efficiently, aiming to decrease the number of parameters in CNNs and their computational complexity.\n", "We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN.\n", "Our architectures arise as new types of residual neural network (ResNet) that can be seen as discretizations of a Partial Differential Equations (PDEs) and thus have predictable theoretical properties.", "Our first architecture involves a convolution operator with a special sparsity structure, and is applicable to a large class of CNNs.", "Next, we present an architecture that can be seen as a discretization of a diffusion reaction PDE, and use it with three different convolution operators.", "We outline in our experiments that the proposed architectures,  although considerably reducing the number of trainable weights, yield comparable accuracy to existing CNNs that are fully coupled in the channel dimension.\n"], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1eRIoA5Y7", "target": ["Este trabajo introduce parametrizaciones eficientes y econ\u00f3micas de redes neuronales convolucionales motivadas por ecuaciones diferenciales parciales ", "Introduce cuatro alternativas de \"bajo coste\" a la operaci\u00f3n de convoluci\u00f3n est\u00e1ndar que pueden utilizarse en lugar de la operaci\u00f3n de convoluci\u00f3n est\u00e1ndar para reducir su complejidad computacional.", "Este art\u00edculo presenta m\u00e9todos para reducir el coste computacional de las implementaciones de CNN, e introduce nuevas parametrizaciones de las arquitecturas tipo CNN que limitan el acoplamiento de par\u00e1metros.", "El art\u00edculo propone una perspectiva basada en las PDE para entender y parametrizar las CNN"]}
{"source": ["In this article we use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model?", "One way to address this question is to find an upper bound on the probability (equivalently a lower bound on the negative log likelihood) that the model can assign to some data as one varies the prior and/or the likelihood function in a latent variable model.", "The core of our contribution is to formally show that the problem of optimizing priors in latent variable models is exactly an instance of the variational optimization problem that information theorists solve when computing rate-distortion functions, and then to use this to derive a lower bound on negative log likelihood.", "Moreover, we will show that if changing the prior can improve the log likelihood, then there is a way to change the likelihood function instead and attain the same log likelihood, and thus rate-distortion theory is of relevance to both optimizing priors as well as optimizing likelihood functions.", "We will experimentally argue for the usefulness of quantities derived from rate-distortion theory in latent variable modeling by applying them to a problem in image modeling."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rkemqsC9Fm", "target": ["Utilizar la teor\u00eda de la distorsi\u00f3n de la tasa para delimitar cu\u00e1nto se puede mejorar un modelo de variable latente", "Aborda los problemas de optimizaci\u00f3n del prior en el modelo de variable latente y la selecci\u00f3n de la funci\u00f3n de verosimilitud proponiendo criterios basados en un l\u00edmite inferior de la log-verosimilitud negativa.", "Presenta un teorema que da un l\u00edmite inferior a la probabilidad logar\u00edtmica negativa de la distorsi\u00f3n de la tasa para el modelado de variables latentes", "Los autores argumentan que la teor\u00eda de la distorsi\u00f3n de la tasa para la compresi\u00f3n con p\u00e9rdidas proporciona un conjunto de herramientas naturales para estudiar los modelos de variables latentes propone un l\u00edmite inferior."]}
{"source": ["Backprop is the primary learning algorithm used in many machine learning algorithms.", "In practice, however, Backprop in deep neural networks is a highly sensitive learning algorithm and successful learning depends on numerous conditions and constraints.", "One set of constraints is to avoid weights that lead to saturated units.", "The motivation for avoiding unit saturation is that gradients vanish and as a result learning comes to a halt.", "Careful weight initialization and re-scaling schemes such as batch normalization ensure that input activity to the neuron is within the linear regime where gradients are not vanished and can flow.", "Here we investigate backpropagating error terms only linearly.", "That is, we ignore the saturation that arise by ensuring gradients always flow.", "We refer to this learning rule as Linear Backprop since in the backward pass the network appears to be linear.", "In addition to ensuring persistent gradient flow, Linear Backprop is also favorable when computation is expensive since gradients are never computed.", "Our early results suggest that learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByfPDyrYim", "target": ["Ignoramos las no linealidades y no calculamos los gradientes en el paso hacia atr\u00e1s para ahorrar c\u00e1lculos y garantizar que los gradientes siempre fluyan. ", "El autor propuso algoritmos backprop lineales para asegurar el flujo de gradientes para todas las partes durante la retropropagaci\u00f3n."]}
{"source": ["Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks.", "There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts.", "Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.", "In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.", "Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.\n"], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HkGGfhC5Y7", "target": ["Comprender el autocodificador discreto VQ-VAE de forma sistem\u00e1tica utilizando EM y utilizarlo para dise\u00f1ar un modelo de traducci\u00f3n no autorregresivo que coincida con una l\u00ednea de base autorregresiva fuerte.", "Este trabajo introduce una nueva forma de interpretar el VQ-VAE y propone un nuevo algoritmo de entrenamiento basado en el clustering EM suave.", "El art\u00edculo presenta una visi\u00f3n alternativa del procedimiento de entrenamiento para el VQ-VAE utilizando el algoritmo EM suave"]}
{"source": ["Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution.", "In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies.", "In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution.", "We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks.", "In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations.", "And our ODN\n", "model also outperforms the cross-entropy loss (Xent), hinge loss and soft hinge loss model in generalization task through limited training data."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HygcvsAcFX", "target": ["Este trabajo presenta una red neuronal profunda que incrusta una funci\u00f3n de p\u00e9rdida con respecto a la distribuci\u00f3n de m\u00e1rgenes \u00f3ptima, que alivia el problema de sobreajuste te\u00f3rica y emp\u00edricamente.", "Presenta un l\u00edmite PAC-Bayesiano para una p\u00e9rdida de margen"]}
{"source": ["Deep network compression seeks to reduce the number of parameters in the network while maintaining a certain level of performance.  ", "Deep network distillation seeks to train a smaller network that matches soft-max performance of a larger network.  ", "While both regimes have led to impressive performance for their respective goals, neither provide insight into the importance of a given layer in the original model, which is useful if we are to improve our understanding of these highly parameterized models.  ", "In this paper, we present the concept of deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance, which we call \\emph{criticality}.  We call it triage because we assess this criticality by answering the question: what is the impact to the health of the overall network if we compress a block of layers into a single layer.\n", "We propose a suite of triage methods and compare them on problem spaces of varying complexity.  ", "We ultimately show that, across these problem spaces, deep net triage is able to indicate the of relative importance of different layers.  ", "Surprisingly, our local structural compression technique also leads to an improvement in overall accuracy when the final model is fine-tuned globally."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HJWpQCa7z", "target": ["Buscamos entender las representaciones aprendidas en redes comprimidas mediante un r\u00e9gimen experimental que llamamos triaje de redes profundas", "Compara varios m\u00e9todos de inicializaci\u00f3n y entrenamiento para transferir el conocimiento de la red VGG a una red estudiantil m\u00e1s peque\u00f1a, sustituyendo los bloques de capas por capas individuales.", "Este documento presenta cinco m\u00e9todos para realizar el triaje o la compresi\u00f3n de la capa de bloques para las redes profundas.", "El art\u00edculo propone un m\u00e9todo para comprimir un bloque de capas en una NN que eval\u00faa varios subenfoques diferentes"]}
{"source": ["In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   ", "The existence of super-convergence is relevant to understanding why deep networks generalize well.  ", "One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  ", "Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.", "In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  ", "We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  ", "The architectures to replicate this work will be made available upon publication.\n"], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "H1A5ztj3b", "target": ["La prueba emp\u00edrica de un nuevo fen\u00f3meno requiere nuevos conocimientos te\u00f3ricos y es relevante para las discusiones activas en la literatura sobre SGD y la comprensi\u00f3n de la generalizaci\u00f3n.", "El art\u00edculo analiza un fen\u00f3meno en el que el entrenamiento de redes neuronales en entornos muy espec\u00edficos puede beneficiarse mucho de un programa que incluya grandes tasas de aprendizaje", "Los autores analizan el entrenamiento de redes residuales utilizando grandes tasas de aprendizaje c\u00edclico, y demuestran una r\u00e1pida convergencia con las tasas de aprendizaje c\u00edclico y la evidencia de que las grandes tasas de aprendizaje act\u00faan como regularizaci\u00f3n."]}
{"source": ["Infinite-width neural networks have been extensively used to study the theoretical properties underlying the extraordinary empirical success of standard, finite-width neural networks.", "Nevertheless, until now, infinite-width networks have been limited to at most two hidden layers.", "To address this shortcoming, we study the initialisation requirements of these networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.", "Based on these observations, we propose a principled approach to weight initialisation that correctly accounts for the functional nature of the hidden layer activations and facilitates the construction of arbitrarily many infinite-width layers, thus enabling the construction of arbitrarily deep infinite-width networks.", "The main idea of our approach is to iteratively reparametrise the hidden-layer activations into appropriately defined reproducing kernel Hilbert spaces and use the canonical way of constructing probability distributions over these spaces for specifying the required weight distributions in a principled way.", "Furthermore, we examine the practical implications of this construction for standard, finite-width networks.", "In particular, we derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand.", "We demonstrate the effectiveness of this weight initialisation approach on the MNIST, CIFAR-10 and Year Prediction MSD datasets."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkGT6sRcFX", "target": ["Proponemos un m\u00e9todo para la construcci\u00f3n de redes arbitrariamente profundas de ancho infinito, basado en el cual derivamos un novedoso esquema de inicializaci\u00f3n de pesos para redes de ancho finito y demostramos su rendimiento competitivo.", "Propone un enfoque de inicializaci\u00f3n de pesos para permitir redes infinitamente profundas y de ancho infinito con resultados experimentales en peque\u00f1os conjuntos de datos.", "Propone redes neuronales profundas de anchura infinita."]}
{"source": ["Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away.", "This information is encoded in the activities of neurons, and neural activities change over timescales of tens of milliseconds.", "Information in working memory, however, is retained for tens of seconds, suggesting the question of how time-varying neural activities maintain stable representations.", "Prior work shows that, if the neural dynamics are in the `  null space' of the representation - so that changes to neural activity do not affect the downstream read-out of stimulus information - then information can be retained for periods much longer than the time-scale of individual-neuronal activities.", "The prior work, however, requires precisely constructed synaptic connectivity matrices, without explaining how this would arise in a biological neural network.", "To identify mechanisms through which biological networks can self-organize to learn  memory function, we derived biologically plausible synaptic plasticity rules that dynamically modify the connectivity matrix to enable information storing.", "Networks implementing this plasticity rule can successfully learn to form memory representations even if only 10% of the synapses are plastic, they are robust to synaptic noise, and they can represent information about multiple stimuli."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Syl3_2JCZ", "target": ["Derivamos reglas de aprendizaje de plasticidad sin\u00e1ptica biol\u00f3gicamente plausibles para que una red neuronal recurrente almacene representaciones de est\u00edmulos. ", "Un modelo de red neuronal compuesto por neuronas conectadas de forma recurrente y uno o varios redouts cuyo objetivo es retener alguna salida a lo largo del tiempo.", "Este trabajo presenta un mecanismo de memoria autoorganizada en un modelo neuronal, e introduce una funci\u00f3n objetivo que minimiza los cambios en la se\u00f1al a memorizar."]}
{"source": ["Generative Adversarial Networks (GANs) have been proposed as an approach to learning generative models.", "While GANs have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, neither in theory nor in practice.", "In particular, the work in this domain has been focused so far only on understanding the properties of the stationary solutions that this dynamics might converge to, and of the behavior of that dynamics in this solutions\u2019 immediate neighborhood.\n\n", "To address this issue, in this work we take a first step towards a principled study of the GAN dynamics itself.", "To this end, we propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis.\n\n", "This methodology enables us to exhibit an interesting phenomena: a GAN with an optimal discriminator provably converges, while guiding the GAN training using only a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse.", "This suggests that such usage of the first order approximation of the discriminator, which is a de-facto standard in all the existing GAN dynamics, might be one of the factors that makes GAN training so challenging in practice.", "Additionally, our convergence result constitutes the first rigorous analysis of a dynamics of a concrete parametric GAN."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJYQLb-RW", "target": ["Para entender el entrenamiento de GAN, definimos una din\u00e1mica simple de GAN, y mostramos las diferencias cuantitativas entre las actualizaciones \u00f3ptimas y de primer orden en este modelo.", "Los autores estudian el impacto de los GANs en configuraciones donde en cada iteraci\u00f3n, el discriminador se entrena hasta la convergencia y el generador se actualiza con pasos de gradiente, o donde se realizan unos pocos pasos de gradiente para el discriminador y el generador.", "Este trabajo estudia la din\u00e1mica del entrenamiento adversarial de GANs sobre un modelo de mezcla gaussiano"]}
{"source": ["The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network.", "However, it is expensive and time-consuming to manually label sufficient amount of training data.", "Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task.", "While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting.", "We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule.", "GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training.", "Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset.", "In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain.", "We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1xL_iR9Km", "target": ["Proponemos un m\u00e9todo basado en el gradiente para transferir conocimientos de m\u00faltiples fuentes a trav\u00e9s de diferentes dominios y tareas.", "Este trabajo propone combinar los gradientes de los dominios de origen para ayudar al aprendizaje en el dominio de destino. "]}
{"source": ["Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates.", "In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis.", "We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions.", "We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models.", "We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference.", "Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling.", "Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SJVmjjR9FX", "target": ["La primera formulaci\u00f3n Bayes variacional de la inferencia filogen\u00e9tica, un problema de inferencia desafiante sobre estructuras con componentes discretos y continuos entrelazados", "Explora una soluci\u00f3n de inferencia aproximada al problema de la inferencia bayesiana de \u00e1rboles filogen\u00e9ticos aprovechando las redes bayesianas subsplit recientemente propuestas y los modernos estimadores de gradiente para VI.", "Propone un enfoque variacional para la inferencia posterior bayesiana en \u00e1rboles filogen\u00e9ticos."]}
{"source": ["This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\n", "models for raw audio waveform generation.", "As an example, we propose\n", "a hybrid model that combines an autoregressive network named WaveNet and a\n", "conventional LSTM model to address speech synthesis.", "Instead of generating\n", "one sample per time-step, the proposed HybridNet generates multiple samples per\n", "time-step by exploiting the long-term memory utilization property of LSTMs.", "In\n", "the evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\n", "HybridNet achieves a 3.83 subjective 5-scale mean opinion score on\n", "US English, largely outperforming the same size WaveNet in terms of naturalness\n", "and provide 2x speed up at inference."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJoXrxZAZ", "target": ["Se trata de una arquitectura neuronal h\u00edbrida para acelerar el modelo autorregresivo. ", "Concluye que para ampliar el tama\u00f1o del modelo sin aumentar el tiempo de inferencia para la predicci\u00f3n secuencial, hay que utilizar un modelo que prediga varios pasos de tiempo a la vez.", "Este art\u00edculo presenta HybridNet, un sistema neural de s\u00edntesis de voz y otros tipos de audio que combina el modelo WaveNet con un LSTM con el objetivo de ofrecer un modelo con una generaci\u00f3n de audio m\u00e1s r\u00e1pida en tiempo de inferencia."]}
{"source": ["Visual Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them.", "In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations.", "We interpret the model through average visualizations of this reduced set of features.", "Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features.", "In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations.", "Moreover, we introduce an8Flower , a dataset specifically designed for objective quantitative evaluation of methods for visual explanation.", "Experiments on the MNIST , ILSVRC 12, Fashion 144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1ziPjC5Fm", "target": ["Interpretaci\u00f3n mediante la identificaci\u00f3n de las caracter\u00edsticas aprendidas del modelo que sirven de indicadores para la tarea de inter\u00e9s. Explicar las decisiones del modelo destacando la respuesta de estas caracter\u00edsticas en los datos de prueba. Evaluar las explicaciones de forma objetiva con un conjunto de datos controlado.", "Este art\u00edculo propone un m\u00e9todo para producir explicaciones visuales de las salidas de las redes neuronales profundas y publica un nuevo conjunto de datos sint\u00e9ticos.", "Un m\u00e9todo para redes neuronales profundas que identifica autom\u00e1ticamente las caracter\u00edsticas relevantes del conjunto de las clases, apoyando la interpretaci\u00f3n y la explicaci\u00f3n sin depender de anotaciones adicionales."]}
{"source": ["In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data.", "Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem.", "Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations.", "This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations.", "We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJvJXZb0W", "target": ["Un marco para el aprendizaje de representaciones de frases de alta calidad de manera eficiente.", "Propone un algoritmo m\u00e1s r\u00e1pido para el aprendizaje de representaciones de oraciones tipo SkipThought a partir de corpus de oraciones ordenadas que cambia el decodificador a nivel de palabra por una p\u00e9rdida de clasificaci\u00f3n contrastiva.", "Este trabajo propone un marco para el aprendizaje no supervisado de representaciones de oraciones mediante la maximizaci\u00f3n de un modelo de probabilidad de oraciones de contexto verdadero en relaci\u00f3n con oraciones candidatas aleatorias"]}
{"source": ["Many regularization methods have been proposed to prevent overfitting in neural networks.", "Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian.", "However, this method cannot be generalized to regular neural network architectures.", "We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework.", "Unlike in previous literature, it can be applied to any general neural network.", "We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification.", "We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SySpa-Z0Z", "target": ["Derivamos una penalizaci\u00f3n de la norma en la salida de la red neuronal desde la perspectiva del cuello de botella de la informaci\u00f3n", "Plantea la Penalizaci\u00f3n de la Norma de Activaci\u00f3n, una regularizaci\u00f3n de tipo L_2 sobre las activaciones, deriv\u00e1ndola del principio de Cuello de Botella de Informaci\u00f3n", "Este trabajo crea un mapa entre las penalizaciones de la norma de activaci\u00f3n y el marco de cuello de botella de informaci\u00f3n utilizando el marco de abandono variacional."]}
{"source": ["Unsupervised learning of timeseries data is a challenging problem in machine learning.", "Here, \nwe propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method, to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework.", "The algorithm starts with an initial cluster estimates using an autoencoder for dimensionality reduction and a novel temporal clustering layer for cluster assignment.", "Then it jointly optimizes the clustering objective and the dimensionality reduction objective.", "Based on requirement and application, the temporal clustering layer can be customized with any temporal similarity metric.", "Several similarity metrics are considered and compared.  ", "To gain insight into features that the network has learned for its clustering, we apply a visualization method that generates a heat map of regions of interest in the timeseries.", "The viability of the algorithm is demonstrated using timeseries data from diverse domains, ranging from earthquakes to sensor data from spacecraft.", "In each case, we show that our algorithm outperforms traditional methods.", "This performance is attributed to fully integrated temporal dimensionality reduction and clustering criterion."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJFM0ZWCb", "target": ["Un m\u00e9todo totalmente no supervisado, para integrar de forma natural la reducci\u00f3n de la dimensionalidad y la agrupaci\u00f3n temporal en un \u00fanico marco de aprendizaje de extremo a extremo.", "Propone un algoritmo que integra el autoencoder con la agrupaci\u00f3n de datos de series temporales utilizando una estructura de red que se adapta a los datos de series temporales.", "Un algoritmo para realizar conjuntamente la reducci\u00f3n de la dimensionalidad y la agrupaci\u00f3n temporal en un contexto de aprendizaje profundo, utilizando un autoencoder y un objetivo de agrupaci\u00f3n.", "Los autores propusieron un m\u00e9todo de agrupaci\u00f3n de series temporales no supervisado construido con redes neuronales profundas y equipado con un codificador-decodificador y un modo de agrupaci\u00f3n para acortar las series temporales, extraer caracter\u00edsticas temporales locales y obtener las representaciones codificadas."]}
{"source": ["We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios.", "Compared to the well-studied many-class many-shot and few-class few-shot problems, MCFS problem commonly occurs in practical applications but is rarely studied.", "MCFS brings new challenges because it needs to distinguish between many classes, but only a few samples per class are available for training.", "In this paper, we propose ``memory-augmented hierarchical-classification network (MahiNet)'' for MCFS learning.", "It addresses the ``many-class'' problem by exploring the class hierarchy, e.g., the coarse-class label that covers a subset of fine classes, which helps to narrow down the candidates for the fine class and is cheaper to obtain.", "MahiNet uses a convolutional neural network (CNN) to extract features, and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to produce the probabilities over coarse and fine classes.", "While the MLP extends the linear classifier, the attention module extends a KNN classifier, both together targeting the ''`few-shot'' problem.", "We design different training strategies of MahiNet for supervised learning and meta-learning.", "Moreover, we propose two novel benchmark datasets ''mcfsImageNet'' (as a subset of ImageNet) and ''mcfsOmniglot'' (re-splitted Omniglot) specifically for MCFS problem.", "In experiments, we show that MahiNet outperforms several state-of-the-art models on MCFS classification tasks in both supervised learning and meta-learning scenarios."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJlcV2Actm", "target": ["Una red neuronal aumentada por la memoria que aborda el problema de las pocas clases aprovechando la jerarqu\u00eda de clases tanto en el aprendizaje supervisado como en el metaaprendizaje.", "Este art\u00edculo presenta m\u00e9todos para a\u00f1adir un sesgo inductivo a un clasificador a trav\u00e9s de la predicci\u00f3n de grueso a fino a lo largo de una jerarqu\u00eda de clases y el aprendizaje de un clasificador KNN basado en la memoria que realiza un seguimiento de las instancias mal etiquetadas durante el aprendizaje.", "Este art\u00edculo formula el problema de clasificaci\u00f3n de muchas clases y pocos disparos desde una perspectiva de aprendizaje supervisado y de metaaprendizaje."]}
{"source": ["Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years.", "In this work, we focus on learning a representation that would be useful in a clustering task.", "We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.", "We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "S17mtzbRb", "target": ["Un novedoso componente de p\u00e9rdida que obliga a la red a aprender una representaci\u00f3n que se adapte bien a la agrupaci\u00f3n durante el entrenamiento para una tarea de clasificaci\u00f3n.", "Este trabajo propone dos t\u00e9rminos de regularizaci\u00f3n basados en una p\u00e9rdida de bisagra compuesta sobre la divergencia KL entre dos argumentos de entrada normalizados por softmax para fomentar el aprendizaje de representaciones desenmara\u00f1adas", "Propuesta de dos regularizadores destinados a hacer que las representaciones aprendidas en la pen\u00faltima capa de un clasificador se ajusten m\u00e1s a la estructura inherente de los datos."]}
{"source": ["In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data.\n", "While traditional nearest neighbor datasets consisted mostly of hand-crafted feature vectors, an increasing number of datasets comes from representations learned with neural networks.\n", "We study the interaction between nearest neighbor algorithms and neural networks in more detail.\n", "We find that the network architecture can significantly influence the efficacy of nearest neighbor algorithms even when the classification accuracy is unchanged.\n", "Based on our experiments, we propose a number of training modifications that lead to significantly better datasets for nearest neighbor algorithms.\n", "Our modifications lead to learned representations that can accelerate nearest neighbor queries by 5x."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkrHeXbCW", "target": ["Mostramos c\u00f3mo obtener buenas representaciones desde el punto de vista de la B\u00fasqueda de Simiaridad.", "Estudia el impacto del cambio de la parte de clasificaci\u00f3n de im\u00e1genes sobre la DNN en la capacidad de indexar los descriptores con un algoritmo LSH o kd-tree.", "Propone utilizar la p\u00e9rdida de entrop\u00eda cruzada softmax para aprender una red que intente reducir los \u00e1ngulos entre las entradas y los vectores de clase correspondientes en un marco supervisado utilizando."]}
{"source": ["Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices.", "In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure.", "Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid.", "These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization.", "We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent.", "We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkxjYoCqKX", "target": ["Introducimos una t\u00e9cnica que permite el entrenamiento basado en el gradiente de las redes neuronales cuantizadas.", "Propone una forma unificada y general de entrenar redes neuronales con pesos y activaciones sin\u00e1pticas cuantificadas de precisi\u00f3n reducida.", "Un nuevo enfoque para cuantificar las activaciones que es el estado del arte o competitivo en varios problemas de im\u00e1genes reales.", "Un m\u00e9todo de aprendizaje de redes neuronales con pesos y activaciones cuantificados mediante la cuantificaci\u00f3n estoc\u00e1stica de valores y la sustituci\u00f3n de la distribuci\u00f3n categotica resultante por una relajaci\u00f3n continua"]}
{"source": ["In most current formulations of adversarial training, the discriminators can be expressed as single-input operators, that is, the mapping they define is separable over observations.", "In this work, we argue that this property might help explain the infamous mode collapse phenomenon in adversarially-trained generative models.", "Inspired by discrepancy measures and two-sample tests between probability distributions, we propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations.", "We show how they can be easily implemented on top of existing models.", "Various experimental results show that generators trained in combination with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with observation-wise prediction discriminators.", "In addition, the application of our framework to domain adaptation results in strong improvement over recent state-of-the-art."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SyKoKWbC-", "target": ["Mostramos que el problema del colapso del modo en los GANs puede explicarse por la falta de intercambio de informaci\u00f3n entre las observaciones de un lote de entrenamiento, y proponemos un marco basado en la distribuci\u00f3n para compartir globalmente la informaci\u00f3n entre los gradientes que conduce a un entrenamiento adversarial m\u00e1s estable y eficaz.", "Propone sustituir los discriminadores de una sola muestra en el entrenamiento adversarial por discriminadores que operan expl\u00edcitamente sobre distribuciones de ejemplos.", "Teor\u00eda sobre las pruebas de dos muestras y el MMD y c\u00f3mo puede incorporarse de forma beneficiosa al marco GAN."]}
{"source": ["Chemical information extraction is to convert chemical knowledge in text into true chemical database, which is a text processing task heavily relying on chemical compound name identification and standardization.", "Once a systematic name for a chemical compound is given, it will naturally and much simply convert the name into the eventually required molecular formula.", "However, for many chemical substances, they have been shown in many other names besides their systematic names which poses a great challenge for this task.", "In this paper, we propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model.", "Our framework is trained end to end and is fully data-driven.", "Our standardization accuracy on the test dataset achieves 54.04% which has a great improvement compared to previous state-of-the-art result."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rJg_NjCqtX", "target": ["Hemos dise\u00f1ado un marco de extremo a extremo utilizando el modelo de secuencia a secuencia para realizar la normalizaci\u00f3n de los nombres qu\u00edmicos.", "Normaliza los nombres no sistem\u00e1ticos en la extracci\u00f3n de informaci\u00f3n qu\u00edmica creando un corpus paralelo de nombres no sistem\u00e1ticos y sistem\u00e1ticos y construyendo un modelo seq2seq.", "Este trabajo presenta un m\u00e9todo para traducir nombres no sistem\u00e1ticos de compuestos qu\u00edmicos a sus equivalentes sistem\u00e1ticos utilizando una combinaci\u00f3n de mecanismos"]}
{"source": ["The training of deep neural networks with Stochastic Gradient Descent (SGD) with a large learning rate or a small batch-size typically ends in flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss.", "This was found to correlate with a good final generalization performance.  ", "In this paper we extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint.", "We find that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD.", "At this peak value SGD starts to fail to minimize the loss along directions in the loss surface corresponding to the largest curvature (sharpest directions).", "To further investigate the effect of these dynamics in the training process, we study a variant of SGD using a reduced learning rate along the sharpest directions which we show can improve training speed while finding both sharper and better generalizing solution, compared to vanilla SGD.", "Overall, our results show that the SGD dynamics in the subspace of the sharpest directions influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SkgEaj05t7", "target": ["El SGD se dirige al principio del entrenamiento hacia una regi\u00f3n en la que su paso es demasiado grande en comparaci\u00f3n con la curvatura, lo que repercute en el resto del entrenamiento. ", "Analiza la relaci\u00f3n entre la convergencia/generalizaci\u00f3n y la actualizaci\u00f3n en los mayores vectores propios del hessiano de las p\u00e9rdidas emp\u00edricas de las DNNs.", "Este trabajo estudia la relaci\u00f3n entre el tama\u00f1o del paso de la SGD y la curvatura de la superficie de p\u00e9rdida"]}
{"source": ["We introduce a new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems.", "Policy gradient methods usually predict one continuous action estimate or parameters of a presumed distribution (most commonly Gaussian) for any given state which might not be optimal as it may not capture the complete description of the target distribution.", "Our approach instead predicts M actions with the policy network (actor) and then uniformly sample one action during training as well as testing at each state.", "This allows the agent to learn a simple stochastic policy that has an easy to compute expected return.", "In all experiments, this facilitates better exploration of the state space during training and converges to a better policy."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJgf6Z-0W", "target": ["Introducimos un novedoso algoritmo de aprendizaje por refuerzo, que predice m\u00faltiples acciones y muestras de las mismas.", "Este trabajo introduce una mezcla uniforme de pol\u00edticas deterministas, y encuentra que esta parametrizaci\u00f3n de pol\u00edticas estoc\u00e1sticas supera a DDPG en varios benchmarks de OpenAI gym.", "Los autores investigan un m\u00e9todo para mejorar el rendimiento de las redes entrenadas con DDPG, y muestran un rendimiento mejorado en un gran n\u00famero de entornos de control continuo est\u00e1ndar."]}
{"source": ["Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks.", "DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse.", "However, like other CNN models, DenseNets also face overfitting problem if not severer.", "Existing dropout method can be applied but not as effective due to the introduced nonlinear connections.", "In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps.", "To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability.", "The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections.", "Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1gOe209t7", "target": ["Al darse cuenta de los inconvenientes al aplicar el dropout original en DenseNet, elaboramos el dise\u00f1o del m\u00e9todo de dropout a partir de tres aspectos, cuya idea podr\u00eda aplicarse tambi\u00e9n a otros modelos de CNN.", "Aplicaci\u00f3n de diferentes estructuras de abandono binario y de horarios con el objetivo espec\u00edfico de regularizar la arquitectura DenseNet.", "Propone una t\u00e9cnica de pre-salida para densenet que implementa la salida antes de la funci\u00f3n de activaci\u00f3n no lineal."]}
{"source": ["While extremely successful in several applications, especially with low-level representations; sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models.", "Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples.", "Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples.", "Our experiments demonstrate how our approach leads to either superior overall performance or faster convergence."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "HJeOMhA5K7", "target": ["Guiar los modelos profundos conscientes de las relaciones hacia un mejor aprendizaje con conocimiento humano.", "Este trabajo propone una variante de la red de columnas basada en la inyecci\u00f3n de la gu\u00eda humana modificando los c\u00e1lculos en la red.", "Un m\u00e9todo para incorporar consejos humanos al aprendizaje profundo mediante la extensi\u00f3n de Column Network, una red neuronal de grafos para la clasificaci\u00f3n colectiva."]}
{"source": ["Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent.", "However, there is a dearth of work to explain why one can effectively capture the features in data with binary weights and activations.", "Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors.", "In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved.", "Compared to previous research that demonstrated good classification performance with BNNs, our work explains why these BNNs work in terms of HD geometry.  ", "Furthermore, the results and analysis used on BNNs are shown to generalize to neural networks with ternary weights and activations.", "Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks.", "Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1IDRdeCW", "target": ["Los recientes \u00e9xitos de las redes neuronales binarias pueden entenderse a partir de la geometr\u00eda de los vectores binarios de alta dimensi\u00f3n", "Investiga num\u00e9rica y te\u00f3ricamente las razones del \u00e9xito emp\u00edrico de las redes neuronales binarizadas.", "Este art\u00edculo analiza la eficacia de las redes neuronales binarias y por qu\u00e9 la binarizaci\u00f3n es capaz de preservar el rendimiento del modelo."]}
{"source": ["In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR).", "In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations.", "We show how a single neuron is able to provide the optimum solution for inverse problem, given a low resolution image dictionary as an operator.", "Introducing a new concept called Representation Dictionary Duality, we show that CNN elements (filters) are trained to be representation vectors and then, during reconstruction, used as dictionaries.", "In the light of theoretical work, we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and show that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SyqAPeWAZ", "target": ["Despu\u00e9s de demostrar que una neurona act\u00faa como un solucionador de problemas inversos para la superresoluci\u00f3n y que una red de neuronas est\u00e1 garantizada para proporcionar una soluci\u00f3n, propusimos una arquitectura de red doble que rinde m\u00e1s r\u00e1pido que el estado de la t\u00e9cnica.", "Discute el uso de redes neuronales para la superresoluci\u00f3n", "Una nueva arquitectura para resolver tareas de superresoluci\u00f3n de im\u00e1genes, y un an\u00e1lisis que pretende establecer una conexi\u00f3n entre las CNN para resolver la superresoluci\u00f3n y la resoluci\u00f3n de problemas inversos regularizados dispersos."]}
{"source": ["We consider the learning of algorithmic tasks by mere observation of input-output\n", "pairs.", "Rather than studying this as a black-box discrete regression problem with\n", "no assumption whatsoever on the input-output mapping, we concentrate on tasks\n", "that are amenable to the principle of divide and conquer, and study what are its\n", "implications in terms of learning.\n", "This principle creates a powerful inductive bias that we leverage with neural\n", "architectures that are defined recursively and dynamically, by learning two scale-\n", "invariant atomic operations: how to split a given input into smaller sets, and how\n", "to merge two partially solved tasks into a larger partial solution.", "Our model can be\n", "trained in weakly supervised environments, namely by just observing input-output\n", "pairs, and in even weaker environments, using a non-differentiable reward signal.\n", "Moreover, thanks to the dynamic aspect of our architecture, we can incorporate\n", "the computational complexity as a regularization term that can be optimized by\n", "backpropagation.", "We demonstrate the flexibility and efficiency of the Divide-\n", "and-Conquer Network on several combinatorial and geometric tasks: convex hull,\n", "clustering, knapsack and euclidean TSP.", "Thanks to the dynamic programming\n", "nature of our model, we show significant improvements in terms of generalization\n", "error and computational complexity."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1jscMbAW", "target": ["Modelo din\u00e1mico que aprende estrategias de divide y vencer\u00e1s mediante una supervisi\u00f3n d\u00e9bil.", "Propone a\u00f1adir un nuevo sesgo inductivo a la arquitectura de la red neuronal mediante una estrategia de \"divide y vencer\u00e1s\".", "Este trabajo estudia los problemas que pueden resolverse mediante un enfoque de programaci\u00f3n din\u00e1mica, y propone una arquitectura de red neuronal para resolver dichos problemas que supera las l\u00edneas de base de la secuencia.", "El art\u00edculo propone una arquitectura de red \u00fanica que puede aprender estrategias de \"divide y vencer\u00e1s\" para resolver tareas algor\u00edtmicas."]}
{"source": ["Within many machine learning algorithms, a fundamental problem concerns efficient calculation of an unbiased gradient wrt parameters $\\boldsymbol{\\gamma}$ for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}} (\\boldsymbol{y})} [f (\\boldsymbol{y}) ]$.", "Most existing methods either ($i$) suffer from high variance, seeking help from (often) complicated variance-reduction techniques; or ($ii$) they only apply to reparameterizable continuous random variables and employ a reparameterization trick.", "To address these limitations, we propose a General and One-sample (GO) gradient that ($i$) applies to many distributions associated with non-reparameterizable continuous {\\em or} discrete random variables, and ($ii$) has the same low-variance as the reparameterization trick.", "We find that the GO gradient often works well in practice based on only one Monte Carlo sample (although one can of course use more samples if desired).", "Alongside the GO gradient, we develop a means of propagating the chain rule through distributions, yielding statistical back-propagation, coupling neural networks to common random variables."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "ryf6Fs09YX", "target": ["un gradiente tipo Rep para distribuciones continuas/discretas no parametrizables; generalizado adem\u00e1s a modelos probabil\u00edsticos profundos, dando lugar a la retropropagaci\u00f3n estad\u00edstica", "Presenta un estimador de gradiente para objetivos basados en expectativas que es insesgado, tiene baja varianza y se aplica a variables aleatorias continuas y discretas.", "Un m\u00e9todo mejorado para calcular las derivadas de la expectativa, y un nuevo estimador de gradiente de baja varianza que permite el entrenamiento de modelos generativos en los que las observaciones o variables latentes son discretas.", "Dise\u00f1a un gradiente de baja varianza para distribuciones asociadas a variables aleatorias continuas o discretas."]}
{"source": ["Quantum computers promise significant advantages over classical computers for a number of different applications.", "We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer.", "We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification.", "We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously.", "We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SyxvSiCcFQ", "target": ["Demostramos que los paisajes de costes de los par\u00e1metros de la NN y de los hiperpar\u00e1metros pueden generarse como estados cu\u00e1nticos utilizando un \u00fanico circuito cu\u00e1ntico y que \u00e9stos pueden utilizarse para el entrenamiento y el meta-entrenamiento.", "Describe un m\u00e9todo en el que se puede cuantificar un marco de aprendizaje profundo considerando la forma de dos estados de una esfera/qubit de Bloch y creando una red neuronal binaria cu\u00e1ntica.", "Este trabajo propone la amplificaci\u00f3n cu\u00e1ntica, un nuevo algoritmo para el entrenamiento y la selecci\u00f3n de modelos en redes neuronales binarias.", "Propone una idea novedosa de dar salida a un estado cu\u00e1ntico que representa un paisaje de costes completo de todos los par\u00e1metros para una red neuronal binaria dada, mediante la construcci\u00f3n de una red neuronal binaria cu\u00e1ntica (QBNN)."]}
{"source": ["Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations.", "These methods assume that all the adversarial transformations are equally important, which is seldom the case in real-world applications.", "We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for tasks where some adversarial transformation are more important than others.", "We encode the potential harm of each adversarial transformation in a cost matrix, and propose a general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness.", "Our experiments on simple MNIST and CIFAR10 models with a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BygANhA9tQ", "target": ["Un m\u00e9todo general para el entrenamiento de clasificadores robustos certificados y sensibles a los costes contra perturbaciones adversas", "Calcula e introduce los costes de los ataques adversarios en el objetivo de optimizaci\u00f3n para obtener un modelo que sea sensible a los costes frente a los ataques adversarios. ", "Se basa en el trabajo seminal de Dalvi et al. y ampl\u00eda el enfoque a la robustez certificable con una matriz de costes que especifica para cada par de clases fuente-destino si el modelo debe ser robusto a los ejemplos adversos."]}
{"source": ["Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons,  causing them to send artificial visual signals to the brain.", "However, electrical stimulation generally cannot precisely reproduce  normal patterns of neural activity in the retina.", "Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response.", "This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed.", "Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina.", "The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input.", "Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HJhIM0xAW", "target": ["Uso de trillizos para aprender una m\u00e9trica de comparaci\u00f3n de respuestas neuronales y mejorar el rendimiento de una pr\u00f3tesis.", "Los autores desarrollan nuevas m\u00e9tricas de distancia de tren de espigas, incluyendo redes neuronales y m\u00e9tricas cuadr\u00e1ticas. Se demuestra que estas m\u00e9tricas superan a la m\u00e9trica ingenua de la distancia de Hamming, y captura impl\u00edcitamente cierta estructura en el c\u00f3digo neural.", "Pensando en la aplicaci\u00f3n de la mejora de las pr\u00f3tesis neuronales, los autores proponen aprender una m\u00e9trica entre las respuestas neuronales mediante la optimizaci\u00f3n de una forma cuadr\u00e1tica o una red neuronal profunda ."]}
{"source": ["We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping.", "Mind-mapping is a powerful tool whose intent is to allow one to externalize ideas and their relationships surrounding a central problem.", "The key challenge in mind-mapping is the difficulty in balancing the exploration of different aspects of the problem (breadth) with a detailed exploration of each of those aspects (depth).", "Our idea behind QCue is based on two mechanisms: (1) computer-generated automatic cues to stimulate the user to explore the breadth of topics based on the temporal and topological evolution of a mind-map and (2) user-elicited queries for helping the user explore the depth for a given topic.", "We present a two-phase study wherein the first phase provided insights that led to the development of our work-flow for stimulating the user through cues and queries.", "In the second phase, we present a between-subjects evaluation comparing QCue with a digital mind-mapping work-flow without computer intervention.", "Finally, we present an expert rater evaluation of the mind-maps created by users in conjunction with user feedback."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r5vnRRwrgX", "target": ["Este art\u00edculo presenta un m\u00e9todo para generar preguntas (pistas) y consultas (sugerencias) para ayudar a los usuarios a realizar mapas mentales.", "Presenta una herramienta de ayuda a la elaboraci\u00f3n de mapas mentales mediante sugerencias de contexto relacionadas con los nodos existentes y mediante preguntas que ampl\u00edan las ramas menos desarrolladas.", "Este art\u00edculo presenta un enfoque para ayudar a las personas a realizar tareas de mindmapping, dise\u00f1ando una interfaz y caracter\u00edsticas algor\u00edtmicas para apoyar el mindmapping, y contribuye con un estudio evaluativo."]}
{"source": ["The ability to detect when an input sample was not drawn from the training distribution is an important  desirable property of deep neural networks.", "In this paper, we show that a simple ensembling of first and second order deep feature statistics can be exploited to effectively differentiate in-distribution and out-of-distribution samples.", "Specifically, we observe that  the mean and standard deviation within feature maps  differs greatly between in-distribution and out-of-distribution samples.", "Based on this observation, we propose a simple and  efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model.  ", "The proposed method outperforms the state-of-the-art by a large margin in all standard benchmarking tasks, while being much simpler to implement and execute.", "Notably, our method improves the true negative rate from 39.6% to 95.3% when 95% of in-distribution (CIFAR-100) are correctly detected using a DenseNet and the out-of-distribution dataset is TinyImageNet resize.", "The source code of our method will be made publicly available."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkgpCoRctm", "target": ["Detecci\u00f3n de muestras fuera de la distribuci\u00f3n mediante el uso de estad\u00edsticas de caracter\u00edsticas de bajo orden sin requerir ning\u00fan cambio en la DNN subyacente.", "Presenta un algoritmo para detectar muestras fuera de la distribuci\u00f3n mediante el uso de la estimaci\u00f3n de la media y la varianza dentro de las capas BatchNorm para construir representaciones de caracter\u00edsticas que luego se alimentan en un clasificador lineal.", "Un enfoque para detectar muestras fuera de distribuci\u00f3n en el que los autores proponen utilizar la regresi\u00f3n log\u00edstica sobre las estad\u00edsticas simples de cada capa de normalizaci\u00f3n de lotes de la CNN.", "El documento sugiere el uso de puntuaciones Z para comparar las muestras de ID y OOD para evaluar lo que las redes profundas est\u00e1n tratando de hacer."]}
{"source": ["Due to the sharp increase in the severity of the threat imposed by software vulnerabilities, the detection of vulnerabilities in binary code has become an important concern in the software industry, such as the embedded systems industry, and in the field of computer security.", "However, most of the work in binary code vulnerability detection has relied on handcrafted features which are manually chosen by a select few, knowledgeable domain experts.", "In this paper, we attempt to alleviate this severe binary vulnerability detection bottleneck by leveraging recent advances in deep learning representations and propose the Maximal Divergence Sequential Auto-Encoder.", "In particular, latent codes representing vulnerable and non-vulnerable binaries are encouraged to be maximally divergent, while still being able to maintain crucial information from the original binaries.", "We conducted extensive experiments to compare and contrast our proposed methods with the baselines, and the results show that our proposed methods outperform the baselines in all performance measures of interest."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByloIiCqYQ", "target": ["Proponemos un nuevo m\u00e9todo llamado Auto-Encoder Secuencial de Divergencia M\u00e1xima que aprovecha la representaci\u00f3n del Auto-Encoder Variacional para la detecci\u00f3n de vulnerabilidades en c\u00f3digos binarios.", "Este trabajo propone una arquitectura basada en un autoencoder variacional para la incrustaci\u00f3n de c\u00f3digo para la detecci\u00f3n de vulnerabilidades de software binario, con incrustaciones aprendidas m\u00e1s eficaces para distinguir entre c\u00f3digo binario vulnerable y no vulnerable que las l\u00edneas de base.", "Este trabajo propone un modelo para extraer autom\u00e1ticamente caracter\u00edsticas para la detecci\u00f3n de vulnerabilidades utilizando la t\u00e9cnica de aprendizaje profundo. "]}
{"source": ["Modern neural architectures critically rely on attention for mapping structured inputs to sequences.", "In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence.\n", "We present an alternative architecture called  Posterior Attention Models that after a principled factorization of the full joint distribution of the attention and output variables, proposes two major changes.  ", "First, the position where attention is marginalized is changed from the input to the output.", "Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output.", "Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better BLEU score and alignment accuracy than existing attention models."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "BkltNhC9FX", "target": ["El c\u00e1lculo de la atenci\u00f3n basado en la distribuci\u00f3n posterior conduce a una atenci\u00f3n m\u00e1s significativa y a un mejor rendimiento", "Este trabajo propone un modelo secuencia a secuencia donde la atenci\u00f3n es tratada como una variable latente, y deriva novedosos procedimientos de inferencia para este modelo, obteniendo mejoras en tareas de traducci\u00f3n autom\u00e1tica y generaci\u00f3n de inflexiones morfol\u00f3gicas.", "Este trabajo presenta un nuevo modelo de atenci\u00f3n posterior para problemas seq2seq"]}
{"source": ["The growing interest to implement Deep Neural Networks (DNNs) on resource-bound hardware has motivated innovation of compression algorithms.", "Using these algorithms, DNN model sizes can be substantially reduced, with little to no accuracy degradation.", "This is achieved by either eliminating components from the model, or penalizing complexity during training.", "While both approaches demonstrate considerable compressions, the former often ignores the loss function during compression while the later produces unpredictable compressions.", "In this paper, we propose a technique that directly minimizes both the model complexity and the changes in the loss function.", "In this technique, we formulate compression as a constrained optimization problem, and then present a solution for it.", "We will show that using this technique, we can achieve competitive results."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "By0ANxbRW", "target": ["Comprimir los modelos DNN entrenados minimizando su complejidad y limitando su p\u00e9rdida.", "Este trabajo propone un m\u00e9todo para la compresi\u00f3n de redes neuronales profundas bajo restricciones de precisi\u00f3n.", "Este trabajo presenta un m\u00e9todo de codificaci\u00f3n de k-means restringido por el valor de las p\u00e9rdidas para la compresi\u00f3n de redes y desarrolla un algoritmo iterativo para la optimizaci\u00f3n del modelo."]}
{"source": ["Deep neural networks are able to solve tasks across a variety of domains and modalities of data.", "Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes.", "In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.", "Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs.", "The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output.", "We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing.", "The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJ60SbW0b", "target": ["Desarrollamos una t\u00e9cnica para visualizar los mecanismos de atenci\u00f3n en redes neuronales arbitrarias. ", "Propone aprender una Red de Atenci\u00f3n Latente que puede ayudar a visualizar la estructura interna de una red neuronal profunda.", "Los autores de este trabajo proponen un esquema de visualizaci\u00f3n de caja negra basado en datos. "]}
{"source": ["The design of small molecules with bespoke properties is of central importance to drug discovery.  ", "However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  ", "This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  ", "The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  ", "Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HkcTe-bR-", "target": ["Investigamos una variedad de algoritmos de RL para la generaci\u00f3n de mol\u00e9culas y definimos nuevos puntos de referencia (que se publicar\u00e1n como un Gimnasio OpenAI), encontrando que PPO y un algoritmo MLE de escalada de colinas son los que mejor funcionan.", "Considera la evaluaci\u00f3n de modelos para la generaci\u00f3n de mol\u00e9culas proponiendo 19 puntos de referencia, ampliando peque\u00f1os conjuntos de datos a un gran conjunto de datos estandarizados, y explorando c\u00f3mo aplicar las t\u00e9cnicas de RL para el dise\u00f1o molecular.", "Este trabajo muestra que los m\u00e9todos de RL m\u00e1s sofisticados son menos eficaces que la t\u00e9cnica simple de hill-climbing, con la PPO como excepci\u00f3n, a la hora de modelar y sintetizar mol\u00e9culas."]}
{"source": ["Analogical reasoning has been a principal focus of various waves of AI research.", "Analogy is particularly challenging for machines because it requires relational structures to be represented such that they can be flexibly applied across diverse domains of experience.", "Here, we study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data.", "We find that the critical factor for inducing such a capacity is not an elaborate architecture, but rather, careful attention to the choice of data and the manner in which it is presented to the model.", "The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains, a training method that uses only the input data to force models to learn about important abstract features.", "Using this technique we demonstrate capacities for complex, visual and symbolic analogy making and generalisation in even the simplest neural network architectures."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SylLYsCcFm", "target": ["La capacidad m\u00e1s robusta de razonamiento anal\u00f3gico se induce cuando las redes aprenden analog\u00edas contrastando estructuras relacionales abstractas en sus dominios de entrada.", "El art\u00edculo investiga la capacidad de una red neuronal para aprender la analog\u00eda, mostrando que una red neuronal simple es capaz de resolver ciertos problemas de analog\u00eda", "Este trabajo describe un enfoque para entrenar redes neuronales para tareas de razonamiento anal\u00f3gico, considerando espec\u00edficamente la analog\u00eda visual y las analog\u00edas simb\u00f3licas."]}
{"source": ["Building chatbots that can accomplish goals such as booking a flight ticket is an unsolved problem in natural language understanding.", "Much progress has been made to build conversation models using techniques such as sequence2sequence modeling.", "One challenge in applying such techniques to building goal-oriented conversation models is that maximum likelihood-based models are not optimized toward accomplishing goals.", "Recently, many methods have been proposed to address this issue by optimizing a reward that contains task status or outcome.", "However, adding the reward optimization on the fly usually provides little guidance for language construction and the conversation model soon becomes decoupled from the language model.", "In this paper, we propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents.", "Language construction now becomes an important part in reward optimization since it is the only way information can be exchanged.", "We experimented our models using self-play and results showed that our method not only beat the baseline sequence2sequence model in rewards but can also generate human-readable meaningful conversations of comparable quality."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJXyS7bRb", "target": ["Un modelo de conversaci\u00f3n neuronal orientado a los objetivos por medio de la autoconversaci\u00f3n", "Un modelo de juego propio para la generaci\u00f3n de di\u00e1logos orientados a objetivos, con el objetivo de reforzar el acoplamiento entre la recompensa de la tarea y el modelo de lenguaje.", "Este art\u00edculo describe un m\u00e9todo para mejorar un sistema de di\u00e1logo orientado a objetivos utilizando la auto-reproducci\u00f3n. "]}
{"source": ["Search engine users nowadays heavily depend on query completion and correction to shape their queries.", " Typically, the completion is done by database lookup which does not understand the context and cannot generalize to prefixes not in the database", ". In the paper, we propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix", ".  We show how to address two main challenges that renders this method practical for large-scale deployment", ": 1) we propose a method for integrating error correction into the language model completion via a edit-distance potential and a variant of beam search that can exploit these potential functions; and", "2) we show how to efficiently perform CPU-based computation to complete the queries, with error correction, in real time (generating top 10 completions within 16 ms).", "Experiments show that the method substantially increases hit rate over standard approaches, and is capable of handling tail queries.\n"], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "By3VrbbAb", "target": ["Completar consultas de b\u00fasqueda en tiempo real mediante modelos ling\u00fc\u00edsticos LSTM a nivel de caracteres", "Este art\u00edculo presenta m\u00e9todos para la finalizaci\u00f3n de consultas que incluyen la correcci\u00f3n de prefijos, y algunos detalles de ingenier\u00eda para satisfacer requisitos particulares de latencia en una CPU.", "Los autores proponen un algoritmo para resolver el problema de finalizaci\u00f3n de consultas con correcci\u00f3n de errores, y adoptan un modelado basado en RNN a nivel de caracteres y optimizan la parte de inferencia para lograr objetivos en tiempo real."]}
{"source": ["RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear.", "Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants.", "In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways.", "First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time.\n\n", "Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10.", "Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \\beta_1.", "We show that at very high values of the momentum parameter (\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses.", "On the other hand, NAG can sometimes do better when ADAM's \\beta_1 is set to the most commonly used value: \\beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance.\n\n", "We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkgd0iA9FQ", "target": ["En este trabajo demostramos la convergencia a la criticidad del RMSProp (estoc\u00e1stico y determinista) y del ADAM determinista para objetivos suaves no convexos y demostramos una interesante sensibilidad beta_1 para el ADAM en autocodificadores. ", "Este trabajo presenta un an\u00e1lisis de convergencia de RMSProp y ADAM en el caso de funciones suaves no convexas"]}
{"source": ["Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems.", "We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model.", "The proposed PCL methodology is unsupervised, meaning that no adversarial sample is leveraged to build/train parallel checkpointing learners.", "We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network.", "To solve the aforementioned minimization problem, a set of complementary but disjoint checkpointing modules are trained and leveraged to validate the victim model execution in parallel.", "Each checkpointing learner explicitly characterizes the geometry of the input data and the corresponding high-level data abstractions within a particular DL layer.", "As such, the adversary is required to simultaneously deceive all the defender modules in order to succeed.", "We extensively evaluate the performance of the PCL methodology against the state-of-the-art attack scenarios, including Fast-Gradient-Sign (FGS), Jacobian Saliency Map Attack (JSMA), Deepfool, and Carlini&WagnerL2 algorithm.", "Extensive proof-of-concept evaluations for analyzing various data collections including MNIST, CIFAR10, and ImageNet corroborate the effectiveness of our proposed defense mechanism against adversarial samples."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HyI6s40a-", "target": ["El dise\u00f1o de mecanismos de defensa no supervisados contra los ataques adversarios es crucial para garantizar la generalizaci\u00f3n de la defensa. ", "Este trabajo presenta un m\u00e9todo para detectar ejemplos adversarios en un entorno de clasificaci\u00f3n de aprendizaje profundo", "Este art\u00edculo presenta un m\u00e9todo no supervisado para detectar ejemplos adversos de redes neuronales."]}
{"source": ["Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures.", "However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet).", "Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size).", "As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs.", "These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task.", "In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms.", "We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set.", "Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization.", "On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters.", "On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency.", "We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HylVB3AqYm", "target": ["B\u00fasqueda de arquitecturas neuronales sin proxy para el aprendizaje directo de arquitecturas en tareas objetivo a gran escala (ImageNet) reduciendo el coste al mismo nivel que el entrenamiento normal.", "Este trabajo aborda el problema de la b\u00fasqueda de arquitecturas, y espec\u00edficamente busca hacerlo sin tener que entrenar en tareas \"proxy\" donde el problema se simplifica a trav\u00e9s de una optimizaci\u00f3n m\u00e1s limitada, la complejidad de la arquitectura, o el tama\u00f1o del conjunto de datos."]}
{"source": ["With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications.", "However, deep neural networks are also known to have very little control over its uncertainty for test examples, which potentially causes very harmful and annoying consequences in practical scenarios.", "In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\\cite{hendrycks2016baseline}.", "Our method first assumes there exists a underlying higher-order distribution $\\mathcal{P}(z)$", ", which generated label-wise distribution $\\mathcal{P}(y)$", "over classes on the K-dimension simplex, and then approximate such higher-order distribution via parameterized posterior function $p_{\\theta}(z|x)$ under variational inference framework, finally we use the entropy of learned posterior distribution $p_{\\theta}(z|x)$ as uncertainty measure to detect out-of-distribution examples. However", ", we identify the overwhelming over-concentration issue in such a framework, which greatly hinders the detection performance. Therefore", ", we further design a log-smoothing function to alleviate such issue to greatly increase the robustness of the proposed entropy-based uncertainty measure. Through", "comprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework with entropy-based uncertainty measure is consistently observed to yield significant improvements over many baseline systems."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByxmXnA9FQ", "target": ["Un nuevo marco de trabajo basado en la inferencia variacional para la detecci\u00f3n de la falta de distribuci\u00f3n", "Describe un enfoque probabil\u00edstico para cuantificar la incertidumbre en las tareas de clasificaci\u00f3n de las DNNs que supera a otros m\u00e9todos SOTA en la tarea de detecci\u00f3n de fuera de distribuci\u00f3n.", "Un nuevo marco para la detecci\u00f3n de fuera de la distribuci\u00f3n, basado en la inferencia variaitonal y una distribuci\u00f3n Dirichlet a priori, que informa de los resultados del estado del arte en varios conjuntos de datos.", "Detecci\u00f3n de una distribuci\u00f3n fuera de lo normal mediante un nuevo m\u00e9todo para aproximar la distribuci\u00f3n de confianza de la probabilidad de clasificaci\u00f3n utilizando la inferencia variacional de la distribuci\u00f3n Dirichlet."]}
{"source": ["Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.", "Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.", "In this work, we address the problem of learning an agent\u2019s action space purely from visual observation.", "We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.", "We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.", "We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP).", "We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.", "When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels.", "Project website: https://daniilidis-group.github.io/learned_action_spaces"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SylPMnR9Ym", "target": ["Aprendemos una representaci\u00f3n del espacio de acci\u00f3n de un agente a partir de puras observaciones visuales. Utilizamos un enfoque de variables latentes recurrentes con una nueva p\u00e9rdida de composibilidad.", "Propone un modelo composicional de variables latentes para aprender modelos que predigan lo que suceder\u00e1 a continuaci\u00f3n en escenarios donde las etiquetas de acci\u00f3n no est\u00e1n disponibles en abundancia.", "Un enfoque basado en el IB variacional para aprender representaciones de acciones directamente a partir de los v\u00eddeos de las acciones que se est\u00e1n realizando, logrando una mayor eficiencia de los m\u00e9todos de aprendizaje posteriores y requiriendo una menor cantidad de v\u00eddeos de etiquetas de acciones.", "Este trabajo propone un enfoque de predicci\u00f3n de v\u00eddeo que encuentra de forma aut\u00f3noma un espacio de acci\u00f3n que codifica las diferencias entre los fotogramas posteriores"]}
{"source": ["When autonomous agents interact in the same environment, they must often cooperate to achieve their goals.", "One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it.", "However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement.", "Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols.", "More general methods usually require human input or domain-specific data, and so do not scale.", "To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning.", "Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven.", "We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory.", "Additionally, we investigate how the physical location of agents influences negotiation outcomes."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJG0ojCcFm", "target": ["El aprendizaje por refuerzo puede utilizarse para entrenar a los agentes a negociar la formaci\u00f3n de equipos en muchos protocolos de negociaci\u00f3n", "Este art\u00edculo estudia la RL profunda de m\u00faltiples agentes en entornos en los que todos los agentes deben cooperar para realizar una tarea (por ejemplo, b\u00fasqueda y rescate, videojuegos multijugador), y utiliza juegos simples de votaci\u00f3n ponderada cooperativa para estudiar la eficacia de la RL profunda y para comparar las soluciones encontradas por la RL profunda con una soluci\u00f3n justa.", "Un enfoque de aprendizaje por refuerzo para negociar coaliciones en entornos de teor\u00eda de juegos cooperativos que puede utilizarse en casos en los que se dispone de simulaciones de entrenamiento ilimitadas."]}
{"source": ["Neural machine translation (NMT) models learn representations containing substantial linguistic information.", "However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons.", "We develop unsupervised methods for discovering important neurons in NMT models.", "Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision.", "We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena.", "Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1z-PsR5KX", "target": ["M\u00e9todos no supervisados para encontrar, analizar y controlar las neuronas importantes en la NMT", "Este art\u00edculo presenta enfoques no supervisados para descubrir neuronas importantes en los sistemas neuronales de traducci\u00f3n autom\u00e1tica y analiza las propiedades ling\u00fc\u00edsticas controladas por esas neuronas.", "M\u00e9todos no supervisados para clasificar las neuronas en la traducci\u00f3n autom\u00e1tica, en los que se identifican las neuronas importantes y se utilizan para controlar el resultado de la MT."]}
{"source": ["Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task.", "Hence, both environment and task specific knowledge are entangled into one framework.", "However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes.", "Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units.", "The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task.", "The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1mvVm-C-", "target": ["Proponemos un marco de trabajo DRL que desentra\u00f1a el conocimiento espec\u00edfico de la tarea y del entorno.", "Los autores proponen descomponer el aprendizaje por refuerzo en una funci\u00f3n PATH y una funci\u00f3n GOAL", "Una arquitectura modular con el objetivo de separar el conocimiento espec\u00edfico del entorno y el conocimiento espec\u00edfico de la tarea en diferentes m\u00f3dulos, a la par que el A3C est\u00e1ndar en una amplia gama de tareas."]}
{"source": ["Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics.", "We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images.", "Our method learns the depth and orientation of scene points visible in images.", "Our model can then predict the structure of a scene from various, previously unseen view points.", "It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry.", "We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images.", "We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet.", "Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJeem3C9F7", "target": ["pix2scene: un enfoque profundo basado en la generaci\u00f3n para modelar impl\u00edcitamente las propiedades geom\u00e9tricas de una escena 3D a partir de im\u00e1genes", "Explora la explicaci\u00f3n de escenas con superficies en un modelo de reconocimiento neural, y demuestra los resultados en la reconstrucci\u00f3n de im\u00e1genes, la s\u00edntesis y la rotaci\u00f3n de formas mentales.", "Los autores introducen un m\u00e9todo para crear un modelo de escena 3D dada una imagen 2D y una pose de c\u00e1mara utilizando un modelo auto-superfizado"]}
{"source": ["Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical reasoning.", "Simple unsupervised operators such as vector offset between two-word embeddings have shown to recover some specific relationships between those words, if any.", "Despite this, how to accurately learn generic relation representations from word representations remains unclear.", "We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations.", "We propose a method for learning relation representations using a feed-forward neural network that performs relation prediction.", "Our evaluations on two benchmark datasets reveal that the penultimate layer of the trained neural network-based relational predictor acts as a good representation for the relations between words."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1e3WW5aTX", "target": ["Identificar las relaciones que conectan las palabras es importante para varias tareas de PNL. Modelamos la representaci\u00f3n de las relaciones como un problema de aprendizaje supervisado y aprendemos operadores parametrizados que mapean incrustaciones de palabras preentrenadas a representaciones de relaciones.", "Este trabajo presenta un m\u00e9todo novedoso para representar las relaciones l\u00e9xicas como vectores utilizando s\u00f3lo incrustaciones de palabras preentrenadas y una funci\u00f3n de p\u00e9rdida novedosa que opera sobre pares de palabras.", "Una soluci\u00f3n novedosa al problema de la composici\u00f3n de relaciones cuando ya se tienen incrustaciones de palabras/entidades preentrenadas y s\u00f3lo se est\u00e1 interesado en aprender a componer representaciones de relaciones."]}
{"source": ["Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction.", "However, optimizing RNNs is known to be harder compared to feed-forward neural networks.", "A number of techniques have been proposed in literature to address this problem.", "In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal.", "Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions.", "In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust.", "We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout.", "We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2.", "We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJyVzQ-C-", "target": ["Proponemos entrenar dos copias id\u00e9nticas de una red neuronal recurrente (que comparten par\u00e1metros) con diferentes m\u00e1scaras de abandono mientras se minimiza la diferencia entre sus predicciones (pre-softmax).", "Presenta el abandono fraternal como una mejora sobre el abandono lineal de expectativas en t\u00e9rminos de convergencia, y demuestra la utilidad del abandono fraternal en una serie de tareas y conjuntos de datos."]}
{"source": ["We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields.", "Our method specifically targets the space-time representation of physical surfaces from liquid simulations.", "Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions.", "Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface.", "Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients.", "To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes.", "Our representation makes it possible to rapidly generate the desired implicit surfaces.", "We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyeGBj09Fm", "target": ["Aprendizaje de la ponderaci\u00f3n y las deformaciones de los conjuntos de datos espacio-temporales para obtener aproximaciones muy eficaces del comportamiento de los l\u00edquidos.", "Se utiliza un modelo basado en redes neuronales para interpolar simulaciones de nuevas condiciones de escena a partir de superficies impl\u00edcitas 4D densamente registradas para una escena estructurada.", "Este trabajo presenta un enfoque de aprendizaje profundo acoplado para generar datos de simulaci\u00f3n de l\u00edquidos realistas que pueden ser \u00fatiles para aplicaciones de apoyo a la toma de decisiones en tiempo real.", "Este trabajo presenta un enfoque de aprendizaje profundo para la simulaci\u00f3n f\u00edsica que combina dos redes para sintetizar datos 4D que representan simulaciones f\u00edsicas 3D"]}
{"source": ["This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set.", "The paper studies the simplest possible case of color invariance: invariance under pixel-wise permutation of the color channels.", "Thus the network is aware not of the specific color object, but its colorfulness.", "The data set introduced in the paper consists of images showing crashed cars from which ten classes were extracted.", "An additional annotation was done which labeled whether the car shown was red or non-red.  ", "The networks were evaluated by their performance on the classification task.", "With the color annotation we altered the color ratios  in the training data and analyzed the generalization capabilities of the networks on the unaltered test data.", "We further split the test data in red and non-red cars and did a similar evaluation.", "It is shown in the paper that an pixel-wise ordering of the rgb-values of the images performs better or at least similarly for small deviations from the true color ratios.", "The limits of these networks are also discussed."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkoCeqgR-", "target": ["Construimos y evaluamos redes neuronales invariantes del color en un nuevo conjunto de datos realistas", "Propone un m\u00e9todo para que las redes neuronales de reconocimiento de im\u00e1genes sean invariantes al color y lo eval\u00faa en el conjunto de datos cifar 10.", "Los autores investigan una capa de entrada modificada que da lugar a redes invariantes de color, y muestran que ciertas capas de entrada invariantes de color pueden mejorar la precisi\u00f3n para im\u00e1genes de prueba de una distribuci\u00f3n de color diferente a la de las im\u00e1genes de entrenamiento.", "Los autores prueban una CNN en im\u00e1genes con canales de color modificados para que sean invariables a las permutaciones, con un rendimiento que no se degrada demasiado. "]}
{"source": ["Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.", "For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.", "In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).\n", "To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well.", "Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.", "Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkNGsseC-", "target": ["Analizamos c\u00f3mo el grado de solapamiento entre los campos receptivos de una red convolucional afecta a su capacidad expresiva.", "El trabajo estudia la potencia expresiva que proporciona el \"solapamiento\" en las capas de convoluci\u00f3n de las DNNs considerando activaciones lineales con agrupaci\u00f3n de productos.", "Este trabajo analiza la expresividad de los circuitos aritm\u00e9ticos convolucionales y muestra que se requiere un n\u00famero exponencialmente grande de ConvACs no solapados para aproximar el tensor de malla de un ConvACs solapado."]}
{"source": ["We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks.", "Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction.", "The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult.", "By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP.", "For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity.", "In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast.", "In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints.", "Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HylTXn0qYX", "target": ["Un algoritmo te\u00f3rico para probar la optimalidad local y extraer las direcciones de descenso en los puntos no diferenciables de los riesgos emp\u00edricos de las redes ReLU de una capa oculta.", "Propone un algoritmo para comprobar si un punto dado es un punto estacionario generalizado de segundo orden.", "Un algoritmo te\u00f3rico, que implica la resoluci\u00f3n de programas cuadr\u00e1ticos convexos y no convexos, para comprobar la optimalidad local y escapar de las sillas de montar al entrenar redes ReLU de dos capas.", "El autor propone un m\u00e9todo para comprobar si un punto es un punto estacionario o no y luego clasificar los puntos estacionarios como m\u00ednimo local o estacionario de segundo orden"]}
{"source": ["We present a new technique for learning visual-semantic embeddings for cross-modal retrieval.  ", "Inspired by the use of hard negatives in structured prediction, and ranking loss functions used in retrieval, we introduce a simple change to common loss functions used to learn multi-modal embeddings.  ", "That, combined with fine-tuning and the use of augmented data, yields significant gains in retrieval performance.  ", "We showcase our approach, dubbed VSE++, on the MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods.  ", "On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval, and 11.3% in image retrieval (based on R@1)."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "BkTQ8UckG", "target": ["Una nueva p\u00e9rdida basada en negativos relativamente duros que logra el rendimiento m\u00e1s avanzado en la recuperaci\u00f3n de im\u00e1genes.", "Aprendizaje de la incrustaci\u00f3n conjunta de frases e im\u00e1genes utilizando la p\u00e9rdida de tripletes que se aplica a los negativos m\u00e1s duros en lugar de promediar sobre todos los tripletes"]}
{"source": ["We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle.", "DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks.", "It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem.", "We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively.", "DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations.", "In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1D6ty-A-", "target": ["Utilizamos el principio de minimizaci\u00f3n alternante para proporcionar una t\u00e9cnica novedosa y eficaz para entrenar autocodificadores profundos.", "Marco de minimizaci\u00f3n alternante para el entrenamiento de redes de autoencodificadores y codificadores-decodificadores", "Los autores exploran un enfoque de optimizaci\u00f3n alternante para el entrenamiento de los autocodificadores, tratando cada capa como un modelo lineal generalizado, y sugieren utilizar el GD normalizado estoc\u00e1stico como algoritmo de minimizaci\u00f3n en cada fase."]}
{"source": ["We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature.", "By taking advantage of transfer learning, we are able to efficiently use different data sources that are related to the same underlying causal mechanisms.", "We compare our algorithms with those in the extant literature using extensive simulation studies based on large-scale voter persuasion experiments and the MNIST database.", "Our methods can perform an order of magnitude better than existing benchmarks while using a fraction of the data."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "ByzoVi0cFQ", "target": ["Aprendizaje de transferencia para la estimaci\u00f3n de efectos causales mediante redes neuronales.", "Desarrolla algoritmos para estimar el efecto medio condicional del tratamiento mediante un conjunto de datos auxiliares en diferentes entornos, tanto con como sin aprendiz de base.", "Los autores proponen m\u00e9todos para abordar una tarea novedosa de aprendizaje de transferencia para estimar la funci\u00f3n CATE, y los eval\u00faan utilizando un entorno sint\u00e9tico y un conjunto de datos experimentales del mundo real.", "Utilizaci\u00f3n de la regresi\u00f3n de redes neuronales y comparaci\u00f3n de marcos de aprendizaje de transferencia para estimar un efecto de tratamiento medio condicional bajo supuestos de ignorabilidad de la cadena"]}
{"source": ["Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or \"motifs\", are thought to be building blocks of neural representations and information processing.", "We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology.", "Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction.", "Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used.", "An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance.", "In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkloDjAqYm", "target": ["Presentamos LeMoNADe, un m\u00e9todo de detecci\u00f3n de motivos aprendidos de extremo a extremo que opera directamente sobre v\u00eddeos de im\u00e1genes de calcio.", "Este trabajo propone un modelo de estilo VAE para identificar motivos a partir de v\u00eddeos de im\u00e1genes de calcio, que se basa en variables de Bernouli y requiere el truco Gumbel-softmax para la inferencia."]}
{"source": ["A noisy and diverse demonstration set may hinder the performances of an agent aiming to acquire certain skills via imitation learning.", "However, state-of-the-art imitation learning algorithms often assume the optimality of the given demonstration set.\n", "In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set.", "Suitability of a demonstration is estimated by whether imitating it produce desirable outcomes for achieving the goals of the tasks.", "For more efficient demonstration suitability assessments, the learning agent should be capable of imitating a demonstration as quick as possible, which shares similar spirit with fast adaptation in the meta-learning regime.", "Our framework, thus built on top of Model-Agnostic Meta-Learning, evaluates how desirable the imitated outcomes are, after adaptation to each demonstration in the set.", "The resulting assessments hence enable us to select suitable demonstration subsets for acquiring better imitated skills.", "The videos related to our experiments are available at: https://sites.google.com/view/deepdj"], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkxkHnA5tX", "target": ["Proponemos un marco para aprender una buena pol\u00edtica a trav\u00e9s del aprendizaje por imitaci\u00f3n de un conjunto de demostraciones ruidosas mediante el meta-entrenamiento de un evaluador de idoneidad de las demostraciones.", "Aporta un algoritmo basado en MAML para el aprendizaje por imitaci\u00f3n que determina autom\u00e1ticamente si las demostraciones proporcionadas son \"adecuadas\".", "Un m\u00e9todo para realizar el aprendizaje por imitaci\u00f3n a partir de un conjunto de demostraciones que incluye comportamientos in\u00fatiles, que selecciona las demostraciones \u00fatiles por sus ganancias de rendimiento proporcionadas en el momento del meta-entrenamiento."]}
{"source": ["We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions.", "We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph.", "We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young.", "We preserve the dependency structure between the labels with a given causal graph.", "We devise a two-stage procedure for learning a CiGM over the labels and the image.", "First we train a CiGM over the binary labels using a  Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels.", "Later, we combine this with a conditional GAN to generate images conditioned on the binary labels.", "We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN.", "We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels.", "The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image.", "We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJE-4xW0W", "target": ["Introducimos modelos generativos impl\u00edcitos causales, que pueden muestrear a partir de distribuciones condicionales e intervenidas y tambi\u00e9n proponemos dos nuevos GAN condicionales que utilizamos para entrenarlos.", "Un m\u00e9todo para combinar un grafo casual, que describe la estructura de dependencia de las etiquetas con dos arquitecturas GAN condicionales que generan im\u00e1genes condicionadas a la etiqueta binaria", "Los autores abordan la cuesti\u00f3n del aprendizaje de un modelo causal entre las variables de la imagen y la propia imagen a partir de datos observacionales, cuando se da una estructura causal entre las etiquetas de la imagen."]}
{"source": ["Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function.", "This property is useful to computationally-intensive neural network classifiers, as the cost of computing the partition function grows linearly with the number of classes and may become prohibitive.", "In particular, since neural language models may deal with up to millions of classes, their self-normalization properties received notable attention.", "Several\n", "recent studies empirically found that language models, trained using Noise Contrastive Estimation (NCE), exhibit self-normalization, but could not explain why.", "In this study, we provide a theoretical justification to this property by viewing\n", "NCE as a low-rank matrix approximation.", "Our empirical investigation compares NCE to the alternative explicit approach for self-normalizing language models.", "It also uncovers a surprising negative correlation between self-normalization and\n", "perplexity, as well as some regularity in the observed errors that may potentially be used for improving self-normalization algorithms in the future."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1-IBSgMz", "target": ["Demostramos que el NCE es autonormalizado y lo demostramos en conjuntos de datos", "Presenta una prueba de la auto-normalizaci\u00f3n de NCE como resultado de ser una aproximaci\u00f3n matricial de bajo rango de la matriz de probabilidades condicionales normalizadas.", "Este art\u00edculo considera el problema de los modelos autonormalizadores y explica el mecanismo de autonormalizaci\u00f3n interpretando la NCE en t\u00e9rminos de factorizaci\u00f3n de matrices."]}
{"source": ["Learning word representations from large available corpora relies on the distributional hypothesis that words present in similar contexts tend to have similar meanings.", "Recent work has shown that word representations learnt in this manner lack sentiment information which, fortunately, can be leveraged using external knowledge.", "Our work addresses the question: can affect lexica improve the word representations learnt from a corpus?", "In this work, we propose techniques to incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec SkipGram, Word2Vec CBOW and GloVe methods using a joint learning approach.", "We use affect scores from Warriner's affect lexicon to regularize the vector representations learnt from an unlabelled corpus.", "Our proposed method outperforms previously proposed methods on standard tasks for word similarity detection, outlier detection and sentiment detection.", "We also demonstrate the usefulness of our approach for a new task related to the prediction of formality, frustration and politeness in corporate communication."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "By5SY2gA-", "target": ["El enriquecimiento de las incrustaciones de palabras con informaci\u00f3n sobre el afecto mejora su rendimiento en las tareas de predicci\u00f3n del sentimiento.", "Propone utilizar el l\u00e9xico afectivo para mejorar las incrustaciones de palabras para superar el est\u00e1ndar Word2vec y Glove.", "Este art\u00edculo propone integrar la informaci\u00f3n de un recurso sem\u00e1ntico que cuantifica el afecto de las palabras en un algoritmo de incrustaci\u00f3n de palabras basado en el texto para que los modelos ling\u00fc\u00edsticos reflejen mejor los fen\u00f3menos sem\u00e1nticos y pragm\u00e1ticos.", "Este art\u00edculo introduce modificaciones en las funciones de p\u00e9rdida word2vec y GloVe para incorporar l\u00e9xicos de afecto y facilitar el aprendizaje de incrustaciones de palabras sensibles al afecto."]}
{"source": ["Different kinds of representation learning techniques on graph have shown significant effect in downstream machine learning tasks.", "Recently, in order to inductively learn representations for graph structures that is unobservable during training, a general framework with sampling and aggregating (GraphSAGE) was proposed by Hamilton and Ying and had been proved more efficient than transductive methods on fileds like transfer learning or evolving dataset.", "However, GraphSAGE is uncapable of selective neighbor sampling and lack of memory of known nodes that've been trained.", "To address these problems, we present an unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph.", "Experiments show that our approach outperforms the state-of-the-art inductive and unsupervised methods for representation learning on graphs."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SygxYoC5FX", "target": ["Para la incrustaci\u00f3n no supervisada e inductiva de la red, proponemos un enfoque novedoso para explorar los vecinos m\u00e1s relevantes y preservar el conocimiento previamente aprendido de los nodos utilizando la arquitectura de bi-atenci\u00f3n e introduciendo un sesgo global, respectivamente", "Se propone una extensi\u00f3n de GraphSAGE utilizando una matriz de sesgo de incrustaci\u00f3n global en las funciones de agregaci\u00f3n local y un m\u00e9todo para muestrear los nodos interesantes."]}
{"source": ["Learning distributed representations for nodes in graphs is a crucial primitive in network analysis with a wide spectrum of applications.", "Linear graph embedding methods learn such representations by optimizing the likelihood of both positive and negative edges while constraining the dimension of the embedding vectors.", "We argue that the generalization performance of these methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors.", "Both theoretical and empirical evidence are provided to support this argument:", "(a) we prove that the generalization error of these methods can be bounded by limiting the norm of vectors, regardless of the embedding dimension;", "(b) we show that the generalization performance of linear graph embedding methods is correlated with the norm of embedding vectors, which is small due to the early stopping of SGD and the vanishing gradients.", "We performed extensive experiments to validate our analysis and showcased the importance of proper norm regularization in practice."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1e9csRcFm", "target": ["Argumentamos que la generalizaci\u00f3n de la incrustaci\u00f3n lineal de grafos no se debe a la restricci\u00f3n de la dimensionalidad, sino a la peque\u00f1a norma de los vectores de incrustaci\u00f3n.", "Los autores demuestran que el error de generalizaci\u00f3n de los m\u00e9todos de incrustaci\u00f3n de grafos lineales est\u00e1 limitado por la norma de los vectores de incrustaci\u00f3n y no por las restricciones de dimensionalidad", "Los autores proponen un l\u00edmite te\u00f3rico sobre el rendimiento de generalizaci\u00f3n del aprendizaje de incrustaciones de grafos y argumentan que la norma de las coordenadas determina el \u00e9xito de la representaci\u00f3n aprendida."]}
{"source": ["Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning.", "We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step.", "We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover.", "Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE.", "We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers.", "Code is immediately available."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1fUpoR5FQ", "target": ["Mezcle SGD y momentum (o haga algo similar con Adam) para obtener grandes beneficios.", "El art\u00edculo propone modificaciones sencillas de SGD y Adam, llamadas variantes de QH, que pueden recuperar el m\u00e9todo principal y una serie de otros trucos de optimizaci\u00f3n.", "Una variante del impulso cl\u00e1sico que toma una media ponderada de la actualizaci\u00f3n del impulso y del gradiente, y una evaluaci\u00f3n de sus relaciones entre otros esquemas de optimizaci\u00f3n basados en el impulso."]}
{"source": ["Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks.", "A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using the next state's value function.", "lambda-returns define the target of the RL agent as a weighted combination of rewards estimated by using multiple many-step look-aheads.", "Although mathematically tractable, the use of  exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice.", "Our major contribution  is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner.", "In contrast to lambda-returns wherein the RL agent is restricted to use an exponentially decaying weighting scheme, CAR allows the agent to learn to decide how much it wants to weigh the n-step returns based targets.", "Our experiments, in addition to showing the efficacy of CAR, also empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns.", "We perform our experiments on the  Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HkpRBFxRb", "target": ["Una forma novedosa de generalizar los retornos lambda permitiendo que el agente RL decida cu\u00e1nto quiere ponderar cada uno de los retornos de n pasos.", "Extiende el algoritmo A3C con retornos lambda, y propone un enfoque para el aprendizaje de los pesos de los retornos.", "Los autores presentan los retornos autodid\u00e1cticos basados en la confianza, un m\u00e9todo de RL de aprendizaje profundo para ajustar los pesos de un vector de elegibilidad en la estimaci\u00f3n de valores tipo TD(lambda) para favorecer estimaciones m\u00e1s estables del estado."]}
{"source": ["Current end-to-end deep learning driving models have two problems: (1) Poor\n", "generalization ability of unobserved driving environment when diversity of train-\n", "ing driving dataset is limited (2) Lack of accident explanation ability when driving\n", "models don\u2019t work as expected.", "To tackle these two problems, rooted on the be-\n", "lieve that knowledge of associated easy task is benificial for addressing difficult\n", "task, we proposed a new driving model which is composed of perception module\n", "for see and think and driving module for behave, and trained it with multi-task\n", "perception-related basic knowledge and driving knowledge stepwisely.", " Specifi-\n", "cally segmentation map and depth map (pixel level understanding of images) were\n", "considered as what & where and how far knowledge for tackling easier driving-\n", "related perception problems before generating final control commands for difficult\n", "driving task.", "The results of experiments demonstrated the effectiveness of multi-\n", "task perception knowledge for better generalization and accident explanation abil-\n", "ity.", "With our method the average sucess rate of finishing most difficult navigation\n", "tasks in untrained city of CoRL test surpassed current benchmark method for 15\n", "percent in trained weather and 20 percent in untrained weathers."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B14rPj0qY7", "target": ["hemos propuesto un nuevo modelo de autoconducci\u00f3n que se compone de un m\u00f3dulo de percepci\u00f3n para ver y pensar y un m\u00f3dulo de conducci\u00f3n para comportarse con el fin de adquirir una mejor capacidad de generalizaci\u00f3n y de explicaci\u00f3n de los accidentes.", "Presenta una arquitectura de aprendizaje multitarea para la estimaci\u00f3n del mapa de profundidad y segmentaci\u00f3n y la predicci\u00f3n de la conducci\u00f3n utilizando un m\u00f3dulo de percepci\u00f3n y un m\u00f3dulo de decisi\u00f3n de conducci\u00f3n.", "Un m\u00e9todo para una arquitectura modificada de extremo a extremo que tiene una mejor capacidad de generalizaci\u00f3n y explicaci\u00f3n, es m\u00e1s robusto a un entorno de prueba diferente, y tiene una salida de decodificaci\u00f3n que puede ayudar con la depuraci\u00f3n del modelo.", "Los autores presentan una red neuronal convolucional multitarea para la conducci\u00f3n de extremo a extremo y proporcionan evaluaciones con el simulador de c\u00f3digo abierto CARLA que muestran un mejor rendimiento de generalizaci\u00f3n en nuevas condiciones de conducci\u00f3n que las l\u00edneas de base"]}
{"source": ["Recently there has been a surge of interest in designing graph embedding methods.", "Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements.", "In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework \u2013 a generic methodology allowing contemporary graph embedding methods to scale to large graphs.", "MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph.", "It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns.", "The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them.", "We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs.", "Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification.", "MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJeKCi0qYX", "target": ["Un marco gen\u00e9rico para escalar las t\u00e9cnicas de incrustaci\u00f3n de grafos existentes a grandes grafos.", "Este trabajo propone un marco de incrustaci\u00f3n multinivel que se aplica sobre los m\u00e9todos de incrustaci\u00f3n de redes existentes para poder escalar a redes de gran escala con mayor velocidad.", "Los autores proponen un marco de tres etapas para la incrustaci\u00f3n de gr\u00e1ficos a gran escala con una calidad de incrustaci\u00f3n mejorada."]}
{"source": ["Anomaly detection discovers regular patterns in unlabeled data and identifies the non-conforming data points, which in some cases are the result of malicious attacks by adversaries.", "Learners such as One-Class Support Vector Machines (OCSVMs) have been successfully in anomaly detection, yet their performance may degrade significantly in the presence of sophisticated adversaries, who target the algorithm itself by compromising the integrity of the training data.", "With the rise in the use of machine learning in mission critical day-to-day activities where errors may have significant consequences, it is imperative that machine learning systems are made secure.", "To address this, we propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs.", "The proposed approach introduces a layer of uncertainty on top of the OCSVM learner, making it infeasible for the adversary to guess the specific configuration of the learner.", "We theoretically analyze the effects of adversarial perturbations on the separating margin of OCSVMs and provide empirical evidence on several benchmark datasets, which show that by carefully contracting the data in low dimensional spaces, we can successfully identify adversarial samples that would not have been identifiable in the original dimensional space.", "The numerical results show that the proposed method improves OCSVMs performance significantly (2-7%)"], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJgd7m0xRZ", "target": ["Un m\u00e9todo novedoso para aumentar la resistencia de los OCSVMs contra ataques dirigidos a la integridad mediante transformaciones no lineales selectivas de los datos a dimensiones inferiores.", "Los autores proponen una defensa contra los ataques a la seguridad de los detectores de anomal\u00edas basados en SVM de una clase", "Este trabajo explora c\u00f3mo las proyecciones aleatorias pueden ser utilizadas para hacer que OCSVM sea robusto a los datos de entrenamiento perturbados adversamente."]}
{"source": ["In this paper, we present a layer-wise learning of stochastic neural networks (SNNs) in an information-theoretic perspective.", "In each layer of an SNN, the compression and the relevance are defined to quantify the amount of information that the layer contains about the input space and the target space, respectively.", "We jointly optimize the compression and the relevance of all parameters in an SNN to better exploit the neural network's representation.", "Previously, the Information Bottleneck (IB) framework (\\cite{Tishby99}) extracts relevant information for a target variable.", "Here, we propose Parametric Information Bottleneck (PIB) for a neural network by utilizing (only) its model parameters explicitly to approximate the compression and the relevance.", "We show that, as compared to the maximum likelihood estimate (MLE) principle, PIBs : (i) improve the generalization of neural networks in classification tasks, (ii) push the representation of neural networks closer to the optimal information-theoretical representation in a faster manner.  "], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByED-X-0W", "target": ["Aprender una mejor representaci\u00f3n de las redes neuronales con el principio del cuello de botella de la informaci\u00f3n", "Propone un m\u00e9todo de aprendizaje basado en el marco del cuello de botella de la informaci\u00f3n, en el que las capas ocultas de las redes profundas comprimen la entrada X manteniendo suficiente informaci\u00f3n para predecir la salida Y.", "Este trabajo presenta una nueva forma de entrenar una red neuronal estoc\u00e1stica siguiendo un marco de relevancia/compresi\u00f3n de la informaci\u00f3n similar al Cuello de Botella de Informaci\u00f3n."]}
{"source": ["The maximum mean discrepancy (MMD) between two probability measures P\n", "and Q is a metric that is zero if and only if all moments of the two measures\n", "are equal, making it an appealing statistic for two-sample tests.", "Given i.i.d. samples\n", "from P and Q, Gretton et al. (2012) show that we can construct an unbiased\n", "estimator for the square of the MMD between the two distributions.", "If P is a\n", "distribution of interest and Q is the distribution implied by a generative neural\n", "network with stochastic inputs, we can use this estimator to train our neural network.\n", "However, in practice we do not always have i.i.d. samples from our target\n", "of interest.", "Data sets often exhibit biases\u2014for example, under-representation of\n", "certain demographics\u2014and if we ignore this fact our machine learning algorithms\n", "will propagate these biases.", "Alternatively, it may be useful to assume our data has\n", "been gathered via a biased sample selection mechanism in order to manipulate\n", "properties of the estimating distribution Q.\n", "In this paper, we construct an estimator for the MMD between P and Q when we\n", "only have access to P via some biased sample selection mechanism, and suggest\n", "methods for estimating this sample selection mechanism when it is not already\n", "known.", "We show that this estimator can be used to train generative neural networks\n", "on a biased data sample, to give a simulator that reverses the effect of that\n", "bias."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyuWNMZ0W", "target": ["Proponemos un estimador de la m\u00e1xima discrepancia de medias, apropiado cuando una distribuci\u00f3n objetivo s\u00f3lo es accesible a trav\u00e9s de un procedimiento de selecci\u00f3n de muestras sesgado, y mostramos que puede utilizarse en una red generativa para corregir este sesgo.", "Propone un estimador ponderado por importancia de la MMD para estimar la MMD entre distribuciones basadas en muestras sesgadas seg\u00fan un esquema desconocido conocido o estimado.", "Los autores abordan el problema del sesgo de selecci\u00f3n de la muestra en las MMD-GAN y proponen una estimaci\u00f3n de la MMD entre dos distribuciones utilizando la m\u00e1xima discrepancia media ponderada.", "Este trabajo presenta una modificaci\u00f3n del objetivo utilizado para entrenar redes generativas con un adversario MMD "]}
{"source": ["We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm.", "Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive.", "We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution.", "We apply our method to a wide range of Atari Arcade Learning Environments.", "Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Bk6qQGWRb", "target": ["Utilizar la regresi\u00f3n bayesiana para estimar la posterior sobre las funciones Q y desplegar el muestreo de Thompson como estrategia de exploraci\u00f3n dirigida con un equilibrio eficiente entre la exploraci\u00f3n y la explotaci\u00f3n", "Los autores proponen un nuevo algoritmo de exploraci\u00f3n en Deep RL en el que aplican la regresi\u00f3n lineal bayesiana con caracter\u00edsticas de la \u00faltima capa de una red DQN para estimar la funci\u00f3n Q de cada acci\u00f3n.", "Los autores describen c\u00f3mo utilizar las redes neuronales bayesianas con el muestreo de Thompson para una exploraci\u00f3n eficiente en el aprendizaje q y proponen un enfoque que supera los enfoques de exploraci\u00f3n epsilon-greedy."]}
{"source": ["In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN.", "The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out.", "Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer.", "Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10x to 50x parameters during learning.", "While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers.", "By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency.", "We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1GHb2RqYX", "target": ["PolyCNN s\u00f3lo necesita aprender un filtro convolucional semilla en cada capa. Se trata de una variante eficiente de la CNN tradicional, con un rendimiento similar.", "Los intentos de reducir el n\u00famero de par\u00e1metros del modelo CNN utilizando la transformaci\u00f3n polin\u00f3mica de los filtros para crear un aumento de las respuestas de los filtros.", "Los autores proponen una arquitectura de reparto de pesos para reducir el n\u00famero de par\u00e1metros de las redes neuronales convolucionales con filtros semilla"]}
{"source": ["Detecting the emergence of abrupt property changes in time series is a challenging problem.", "Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches.", "However, selecting kernels is non-trivial in practice.", "Although kernel selection for the two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms.", "In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model.", "With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications.", "The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1GbfhRqF7", "target": ["En este trabajo, proponemos KL-CPD, un novedoso marco de aprendizaje kernel para CPD de series temporales que optimiza un l\u00edmite inferior de potencia de prueba a trav\u00e9s de un modelo generativo auxiliar como sustituto de la distribuci\u00f3n anormal. ", "Describe un enfoque novedoso para optimizar la elecci\u00f3n del n\u00facleo con el fin de aumentar la potencia de las pruebas y demuestra que ofrece mejoras respecto a las alternativas."]}
{"source": ["Theories in cognitive psychology postulate that humans use similarity as a basis\n", "for object categorization.", "However, work in image classification generally as-\n", "sumes disjoint and equally dissimilar classes to achieve super-human levels of\n", "performance on certain datasets.", "In our work, we adapt notions of similarity using\n", "weak labels over multiple hierarchical levels to boost classification performance.\n", "Instead of pitting clustering directly against classification, we use a warm-start\n", "based evaluation to explicitly provide value to a clustering representation by its\n", "ability to aid classification.", "We evaluate on CIFAR10 and a fine-grained classifi-\n", "cation dataset to show improvements in performance with the procedural addition\n", "of intermediate losses and weak labels based on multiple hierarchy levels.", "Further-\n", "more, we show that pretraining AlexNet on hierarchical weak labels in conjunc-\n", "tion with intermediate losses outperforms a classification baseline by over 17% on\n", "a subset of Birdsnap dataset.", "Finally, we show improvement over AlexNet trained\n", "using ImageNet pre-trained weights as initializations which further supports our \n", "claim of the importance of similarity."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1-4BLaQz", "target": ["Agrupar antes de clasificar; utilizar etiquetas d\u00e9biles para mejorar la clasificaci\u00f3n ", "Propone el uso de una funci\u00f3n de p\u00e9rdida basada en la agrupaci\u00f3n en m\u00faltiples niveles de una red profunda, as\u00ed como el uso de la estructura jer\u00e1rquica del espacio de etiquetas para entrenar mejores representaciones.", "Este trabajo utiliza la informaci\u00f3n jer\u00e1rquica de las etiquetas para imponer p\u00e9rdidas adicionales en las representaciones intermedias en el entrenamiento de las redes neuronales."]}
{"source": ["Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels.", "However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks.", "In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state.", "We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malm\u00f6) first-person navigation benchmarks."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "BkCV_W-AZ", "target": ["La minimizaci\u00f3n del arrepentimiento basada en la ventaja es un nuevo algoritmo de aprendizaje profundo por refuerzo que es particularmente eficaz en tareas parcialmente observables, como la navegaci\u00f3n en primera persona en Doom y Minecraft.", "Este art\u00edculo introduce los conceptos de minimizaci\u00f3n del arrepentimiento contrafactual en el campo de la RL profunda y un algoritmo llamado ARM que puede tratar mejor la observabilidad parcial.", "El documento proporciona una variante inspirada en la teor\u00eda de juegos del algoritmo de gradiente de pol\u00edticas basado en la idea de la minimizaci\u00f3n del arrepentimiento contrafactual y afirma que el enfoque puede tratar con el dominio observable parcial mejor que los m\u00e9todos est\u00e1ndar."]}
{"source": ["Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks.", "Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed.", "To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL).", "TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers.", "Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers.", "Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task\u2019s performance, particularly favourable in scenarios where some of the tasks have insufficient data."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJxmXhRcK7", "target": ["un modelo de aprendizaje profundo multitarea que adapta la representaci\u00f3n del anillo tensorial", "Una variante de la formulaci\u00f3n del anillo tensorial para el aprendizaje multitarea compartiendo algunos de los n\u00facleos TT para el aprendizaje de la \"tarea com\u00fan\" mientras se aprenden n\u00facleos TT individuales para cada tarea por separado"]}
{"source": ["Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions.", "Each function models the distribution of the output given an input, conditioned on the context.", "NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size.", "Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on.", "We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction.", "We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkE6PjC9KX", "target": ["Un modelo de regresi\u00f3n que aprende las distribuciones condicionales de un proceso estoc\u00e1stico, incorporando la atenci\u00f3n a los procesos neuronales.", "Propone resolver el problema del infraajuste en el m\u00e9todo del proceso neural a\u00f1adiendo un mecanismo de atenci\u00f3n a la trayectoria determinista.", "Una extensi\u00f3n del marco de los Procesos Neuronales que a\u00f1ade un mecanismo de condicionamiento basado en la atenci\u00f3n, permitiendo que el modelo capture mejor las dependencias en el conjunto de condicionamiento.", "Los autores ampl\u00edan los procesos neuronales incorporando la autoatenci\u00f3n para enriquecer las caracter\u00edsticas de los puntos de contexto y la atenci\u00f3n cruzada para producir una representaci\u00f3n espec\u00edfica de la consulta. Resuelven el problema de infraajuste de las PNE y muestran que las PNA convergen mejor y m\u00e1s r\u00e1pido que las PNE."]}
{"source": ["Deconvolutional layers have been widely used in a variety of deep\n", "models for up-sampling, including encoder-decoder networks for\n", "semantic segmentation and deep generative models for unsupervised\n", "learning.", "One of the key limitations of deconvolutional operations\n", "is that they result in the so-called checkerboard problem.", "This is\n", "caused by the fact that no direct relationship exists among adjacent\n", "pixels on the output feature map.", "To address this problem, we\n", "propose the pixel deconvolutional layer (PixelDCL) to establish\n", "direct relationships among adjacent pixels on the up-sampled feature\n", "map.", "Our method is based on a fresh interpretation of the regular\n", "deconvolution operation.", "The resulting PixelDCL can be used to\n", "replace any deconvolutional layer in a plug-and-play manner without\n", "compromising the fully trainable capabilities of original models.\n", "The proposed PixelDCL may result in slight decrease in efficiency,\n", "but this can be overcome by an implementation trick.", "Experimental\n", "results on semantic segmentation demonstrate that PixelDCL can\n", "consider spatial features such as edges and shapes and yields more\n", "accurate segmentation outputs than deconvolutional layers.", "When used\n", "in image generation tasks, our PixelDCL can largely overcome the\n", "checkerboard problem suffered by regular deconvolution operations."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1spAqUp-", "target": ["Resolver el problema del tablero de ajedrez en la capa deconvolutiva construyendo dependencias entre los p\u00edxeles", "Este trabajo propone capas deconvolucionales de p\u00edxeles para redes neuronales convolucionales como forma de aliviar el efecto damero.", "Una nueva t\u00e9cnica para generalizar las operaciones de deconvoluci\u00f3n utilizadas en las arquitecturas est\u00e1ndar de las CNN, que propone hacer una predicci\u00f3n secuencial de las caracter\u00edsticas de los p\u00edxeles adyacentes, lo que da lugar a salidas m\u00e1s suaves espacialmente para las capas de deconvoluci\u00f3n."]}
{"source": ["In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem.", "To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived.", "After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0).", "The method does not require fine-tuning after quantization.", "Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10)."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ry-TW-WAb", "target": ["Cuantificamos y podamos los pesos de las redes neuronales utilizando la inferencia bayesiana variacional con un previo multimodal que induce la dispersi\u00f3n.", "Propone utilizar una mezcla de propto 1/abs de picos continuos como prior para una red neuronal bayesiana y demuestra el buen rendimiento con convnets relativamente dispersos para minist y cifar-10.", "Este trabajo presenta un enfoque bayesiano variacional para cuantificar los pesos de las redes neuronales a valores ternarios post-entrenamiento de una manera principista."]}
{"source": ["Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.", "Extensive research work have been conducted on DNN model compression or pruning.", "However, most of the previous work took heuristic approaches.", "This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.", "Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.", "Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.", "It achieves up to 34\u00d7 pruning rate for ImageNet dataset and 167\u00d7 pruning rate for MNIST dataset, significantly higher than those reached by the literature work.", "Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.", "The codes and pruned DNN models are released in the anonymous link bit.ly/2zxdlss."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rygo9iR9F7", "target": ["Implementamos un enfoque de poda de pesos de la DNN que logra las tasas de poda m\u00e1s altas.", "Este trabajo se centra en la poda de pesos para la compresi\u00f3n de redes neuronales, logrando una tasa de compresi\u00f3n de 30 veces para AlexNet y VGG para ImageNet.", "Una t\u00e9cnica de poda progresiva que impone una restricci\u00f3n de escasez estructural en el par\u00e1metro de peso y reescribe la optimizaci\u00f3n como un marco ADMM, logrando una mayor precisi\u00f3n que el descenso de gradiente proyectado."]}
{"source": ["In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series.", "The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network.", "The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network.", "This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate.", "We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.\n"], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1efr3C9Ym", "target": ["Este trabajo presenta una nueva arquitectura de aprendizaje profundo para abordar el problema del aprendizaje supervisado con series temporales multivariantes dispersas e irregularmente muestreadas.", "Propone un marco para realizar predicciones sobre datos de series temporales dispersos e irregulares utilizando un m\u00f3dulo de interpolaci\u00f3n que modela los valores perdidos en el uso de la interpolaci\u00f3n suave, la interpolaci\u00f3n no suave y la intensidad. ", "Resuelve el problema del aprendizaje supervisado con series temporales multivariantes dispersas e irregularmente muestreadas utilizando una red de interpolaci\u00f3n semiparam\u00e9trica seguida de una red de predicci\u00f3n."]}
{"source": ["We introduce an analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications.", "We compare our novel construction to other point set distance functions and show proof of concept experiments for training neural networks end-to-end on point set prediction tasks such as object detection."], "source_labels": [0, 1], "rouge_scores": [], "paper_id": "rJlpUiAcYX", "target": ["Funci\u00f3n de p\u00e9rdida invariable por permutaci\u00f3n para la predicci\u00f3n de conjuntos de puntos.", "Propone una nueva p\u00e9rdida para el registro de puntos (alineaci\u00f3n de dos conjuntos de puntos) con una propiedad invariante de permutaci\u00f3n preferible. ", "Este trabajo introduce una nueva funci\u00f3n de distancia entre conjuntos de puntos, aplica otras dos distancias de permutaci\u00f3n en una tarea de detecci\u00f3n de objetos de extremo a extremo, y muestra que en dos dimensiones todos los m\u00ednimos locales de la p\u00e9rdida hologr\u00e1fica son m\u00ednimos globales.", "Propone funciones de p\u00e9rdida invariantes de la permutaci\u00f3n que dependen de la distancia de los conjuntos."]}
{"source": ["We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices.", "Our method learns to assign graph operations to groups and to allocate those groups to available devices.", "The grouping and device allocations are learned jointly.", "The proposed method is trained with policy gradient and requires no human intervention.", "Experiments with widely-used\n", "computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations.", "In addition, our approach outperforms placements by human\n", "experts as well as a previous state-of-the-art placement method based on deep reinforcement learning.", "Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hkc-TeZ0W", "target": ["Introducimos un modelo jer\u00e1rquico para la colocaci\u00f3n eficiente, de extremo a extremo, de gr\u00e1ficos computacionales en dispositivos de hardware.", "Propone aprender conjuntamente grupos de operadores para colocarlos y colocar los grupos aprendidos en los dispositivos para distribuir las operaciones de aprendizaje profundo a trav\u00e9s del aprendizaje por refuerzo.", "Los autores proponen una red de conexi\u00f3n completa para reemplazar el paso de co-ubicaci\u00f3n en un m\u00e9todo de auto-ubicaci\u00f3n propuesto para acelerar el tiempo de ejecuci\u00f3n de un modelo TensorFlow.", "Propone un algoritmo de colocaci\u00f3n de dispositivos para colocar las operaciones de tensorflow en los dispositivos."]}
{"source": ["Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem.", "We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion.", "While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself.", "We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels.", "Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types.", "In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry.", "Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJLlmG-AZ", "target": ["Proponemos un m\u00e9todo de uso de las propiedades de grupo para aprender una representaci\u00f3n del movimiento sin etiquetas y demostramos el uso de este m\u00e9todo para representar el movimiento 2D y 3D.", "Propone aprender el grupo de movimiento r\u00edgido a partir de una representaci\u00f3n latente de secuencias de im\u00e1genes sin necesidad de etiquetas expl\u00edcitas y demuestra experimentalmente el m\u00e9todo en secuencias de d\u00edgitos MINST y en el conjunto de datos KITTI.", "Este trabajo propone un enfoque para el aprendizaje de caracter\u00edsticas de movimiento de v\u00eddeo de una manera no supervisada, utilizando restricciones para optimizar la red neuronal para producir caracter\u00edsticas que se pueden utilizar para la regresi\u00f3n de la odometr\u00eda."]}
{"source": ["This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general.", "Rather than directly using discretized information, input data is first projected into a high-dimensional Reproducing Kernel Hilbert Space (RKHS), where it can be modeled as a continuous function using a series of kernel bases.", "We then proceed to derive a closed-form solution to the continuous convolution operation between two arbitrary functions operating in different RKHS.", "Within this framework, convolutional filters also take the form of continuous functions, and the training procedure involves learning the RKHS to which each of these filters is projected, alongside their weight parameters.", "This results in much more expressive filters, that do not require spatial discretization and benefit from properties such as adaptive support and non-stationarity.", "Experiments on image classification are performed, using classical datasets, with results indicating that the proposed continuous convolutional neural network is able to achieve competitive accuracy rates with far fewer parameters and a faster convergence rate."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJjBnN9a-", "target": ["Este trabajo propone una novedosa capa convolucional que opera en un Espacio de Hilbert de N\u00facleo Reproductor continuo.", "Proyectar ejemplos en un espacio RK Hilbert y realizar la convoluci\u00f3n y el filtrado en ese espacio.", "Este trabajo formula una variante de las redes neuronales convolucionales que modela tanto las activaciones como los filtros como funciones continuas compuestas a partir de bases kernel"]}
{"source": ["Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes.", "Some recent studies suggest a more important role of image textures.", "We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict.", "We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies.", "We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet.", "This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Bygh9j09KX", "target": ["Las CNN entrenadas en ImageNet se inclinan por la textura del objeto (en lugar de la forma, como los humanos). La superaci\u00f3n de esta importante diferencia entre la visi\u00f3n humana y la de las m\u00e1quinas permite mejorar el rendimiento de la detecci\u00f3n y una robustez in\u00e9dita frente a las distorsiones de la imagen.", "Utilizaci\u00f3n de la estilizaci\u00f3n de im\u00e1genes para aumentar los datos de entrenamiento de las CNN entrenadas en ImageNet, con el fin de que las redes resultantes parezcan m\u00e1s acordes con los juicios humanos", "Este art\u00edculo estudia CNNs como AlexNet, VGG, GoogleNet y ResNet50, muestra que estos modelos est\u00e1n sesgados hacia la textura cuando se entrenan en ImageNet, y propone un nuevo conjunto de datos de ImageNet."]}
{"source": ["In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities.", "Namely, convolutional variational autoencoders are employed, and statistics of its learned posterior distribution are used as low dimensional representations of fixed length short-duration utterances.", "In order to enforce speaker dependency in the latent layer, we introduced a variation of the commonly used prior within the variational autoencoders framework, i.e. the model is simultaneously trained for reconstruction of inputs along with a discriminative task performed on top of latent layers outputs.", "The effectiveness of both triplet loss minimization and speaker recognition are evaluated as implicit priors on the challenging cross-language NIST SRE 2016 setting and compared against fully supervised and unsupervised baselines."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "ryeHw1vjiQ", "target": ["Evaluamos la eficacia de realizar tareas discriminatorias auxiliares sobre las estad\u00edsticas de la distribuci\u00f3n posterior aprendida por los autocodificadores variacionales para reforzar la dependencia del hablante.", "Proponer un modelo de autoencoder para aprender una representaci\u00f3n para la verificaci\u00f3n de hablantes utilizando ventanas de an\u00e1lisis de corta duraci\u00f3n.", "Una versi\u00f3n modificada del modelo de autoencoder variacional que aborda el problema del reconocimiento de hablantes en el contexto de segmentos de corta duraci\u00f3n"]}
{"source": ["The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models.", "Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions.", "In this work, we provide yet another perspective on the IWAE bounds.", "We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K).", "In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions.", "Based on this analysis we develop jackknife variational inference (JVI),\n", "a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency.", "Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders.", "We also report first results on applying JVI to learning variational autoencoders.\n\n", "Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference"], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HyZoi-WRb", "target": ["La inferencia variacional est\u00e1 sesgada, vamos a debiarla.", "Introduce la inferencia variacional jackknife, un m\u00e9todo para debiar los objetivos de Monte Carlo como el autoencoder ponderado por importancia.", "Los autores analizan el sesgo y la varianza del l\u00edmite IWAE y derivan un enfoque jacknife para estimar los momentos como una forma de debias IWAE para muestras ponderadas de importancia finita."]}
{"source": ["In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting.", "We present a framework that demonstrates a more structured and data efficient alternative to end-to-end complete policy learning on problems where the high-level policy is hard to formulate using traditional optimization or rule based methods but well designed low-level controllers are available.", "Our framework uses deep reinforcement learning solely to obtain a high-level policy for tactical decision making, while still maintaining a tight integration with the low-level controller, thus getting the best of both worlds.", "We accomplish this with Q-masking, a technique with which we are able to incorporate prior knowledge, constraints, and information from a low-level controller, directly in to the learning process thereby simplifying the reward function and making learning faster and data efficient.", "We provide preliminary results in a simulator and show our approach to be more efficient than a greedy baseline, and more successful and safer than human driving."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1G6uM0WG", "target": ["Un marco que proporciona una pol\u00edtica para el cambio de carril aut\u00f3nomo aprendiendo a tomar decisiones t\u00e1cticas de alto nivel con el aprendizaje de refuerzo profundo, y manteniendo una estrecha integraci\u00f3n con un controlador de bajo nivel para tomar acciones de bajo nivel.", "Considera el problema del cambio de carril aut\u00f3nomo para los coches de autoconducci\u00f3n en un entorno de coches de slot con m\u00faltiples carriles, propone una nueva estrategia de aprendizaje Q-masking - acoplando un controlador definido de bajo nivel con una pol\u00edtica de toma de decisiones t\u00e1cticas de alto nivel.", "Este trabajo propone un enfoque de aprendizaje Q profundo para el problema del cambio de carril utilizando \"Q-masking\", que reduce el espacio de acci\u00f3n seg\u00fan las restricciones o el conocimiento previo.", "Los autores proponen un m\u00e9todo que utiliza una pol\u00edtica de alto nivel basada en el aprendizaje Q que se combina con una m\u00e1scara contextual derivada de las restricciones de seguridad y los controladores de bajo nivel, que impiden que ciertas acciones sean seleccionables en determinados estados. "]}
{"source": ["Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering.", "Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates.", "To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space.", "We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively.", "Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs.", "In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation.", "We show that NGE significantly outperforms previous methods by an order of magnitude.", "As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs.", "Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2\n", "machine.\n"], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkgWHnR5tm", "target": ["B\u00fasqueda autom\u00e1tica de dise\u00f1os rob\u00f3ticos con redes neuronales gr\u00e1ficas", "Propone un enfoque para el dise\u00f1o autom\u00e1tico de robots basado en la evoluci\u00f3n de grafos neuronales. Los experimentos demuestran que optimizar tanto el controlador como el hardware es mejor que optimizar solo el controlador.", "Los autores proponen un esquema basado en una representaci\u00f3n gr\u00e1fica de la estructura del robot, y una red grafo-neural como controladores para optimizar las estructuras del robot, combinadas con sus controladores.  "]}
{"source": ["Deep learning on graphs has become a popular research topic with many applications.", "However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text.", "Is it possible to transfer this progress to the domain of graphs?", "We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once.", "Our method is formulated as a variational autoencoder.", "We evaluate on the challenging task of conditional molecule generation."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SJlhPMWAW", "target": ["Demostramos un autoencoder para grafos.", "Aprender a generar grafos utilizando m\u00e9todos de aprendizaje profundo en \"una sola toma\", emitiendo directamente probabilidades de existencia de nodos y aristas, y vectores de atributos de nodos.", "Un autocodificador variacional para generar gr\u00e1ficos"]}
{"source": ["Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling.", "Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit.", "In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1.", "By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability.", "However, learning towards discrete values of the gates is generally difficult.", "To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation.", "Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks.", "Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJiaRbk0-", "target": ["Proponemos un nuevo algoritmo para el entrenamiento de LSTM mediante el aprendizaje hacia puertas de valor binario que demostramos tiene muchas propiedades agradables.", "Proponer una nueva funci\u00f3n de \"compuerta\" para LSTM para habilitar los valores de las compuertas hacia 0 o 1. ", "El art\u00edculo pretende impulsar las puertas LSTM para que sean binarias empleando el reciente truco Gumbel-Softmax para obtener una distribuci\u00f3n categ\u00f3rica entrenable de extremo a extremo."]}
{"source": ["We present a personalized recommender system using neural network for recommending\n", "products, such as eBooks, audio-books, Mobile Apps, Video and Music.\n", "It produces recommendations based on customer\u2019s implicit feedback history such\n", "as purchases, listens or watches.", "Our key contribution is to formulate recommendation\n", "problem as a model that encodes historical behavior to predict the future\n", "behavior using soft data split, combining predictor and auto-encoder models.", "We\n", "introduce convolutional layer for learning the importance (time decay) of the purchases\n", "depending on their purchase date and demonstrate that the shape of the time\n", "decay function can be well approximated by a parametrical function.", "We present\n", "offline experimental results showing that neural networks with two hidden layers\n", "can capture seasonality changes, and at the same time outperform other modeling\n", "techniques, including our recommender in production.", "Most importantly, we\n", "demonstrate that our model can be scaled to all digital categories, and we observe\n", "significant improvements in an online A/B test.", "We also discuss key enhancements\n", "to the neural network model and describe our production pipeline.", "Finally\n", "we open-sourced our deep learning library which supports multi-gpu model parallel\n", "training.", "This is an important feature in building neural network based recommenders\n", "with large dimensionality of input and output data."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1lMMx1CW", "target": ["Mejora de las recomendaciones mediante modelos temporales con redes neuronales en m\u00faltiples categor\u00edas de productos en un sitio web de venta al por menor", "El art\u00edculo propone un nuevo m\u00e9todo de recomendaci\u00f3n basado en redes neuronales.", "Los autores describen un procedimiento de construcci\u00f3n de su sistema de recomendaci\u00f3n de producci\u00f3n a partir de cero e integran el decaimiento temporal de las compras en el marco de aprendizaje."]}
{"source": ["Deep Learning (DL) algorithms based on Generative Adversarial Network (GAN) have demonstrated great potentials in computer vision tasks such as image restoration.", "Despite the rapid development of image restoration algorithms using DL and GANs, image restoration for specific scenarios, such as medical image enhancement and super-resolved identity recognition, are still facing challenges.", "How to ensure visually realistic restoration while avoiding hallucination or mode- collapse?", "How to make sure the visually plausible results do not contain hallucinated features jeopardizing downstream tasks such as pathology identification and subject identification?\n", "Here we propose to resolve these challenges by coupling the GAN based image restoration framework with another task-specific network.", "With medical imaging restoration as an example, the proposed model conducts additional pathology recognition/classification task to ensure the preservation of detailed structures that are important to this task.", "Validated on multiple medical datasets, we demonstrate the proposed method leads to improved deep learning based image restoration while preserving the detailed structure and diagnostic features.", "Additionally, the trained task network show potentials to achieve super-human level performance in identifying pathology and diagnosis.\n", "Further validation on super-resolved identity recognition tasks also show that the proposed method can be generalized for diverse image restoration tasks."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hylnis0qKX", "target": ["Acoplar el marco de restauraci\u00f3n de im\u00e1genes basado en GAN con otra red espec\u00edfica de la tarea para generar una imagen realista conservando las caracter\u00edsticas espec\u00edficas de la tarea.", "Un novedoso m\u00e9todo de acoplamiento de tareas-GAN de im\u00e1genes que acopla GAN y una red espec\u00edfica de tareas, que alivia para evitar la alucinaci\u00f3n o el colapso de modos.", "Los autores proponen aumentar la restauraci\u00f3n de im\u00e1genes basada en GAN con otra rama de tareas espec\u00edficas, como las tareas de clasificaci\u00f3n, para mejorar a\u00fan m\u00e1s."]}
{"source": ["Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core.", "Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space.", "In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection.", "Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM).", "Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model.", "The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training.", "Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJJLHbb0-", "target": ["Una red neuronal profunda entrenada de principio a fin que aprovecha el modelado de mezclas gaussianas para realizar la estimaci\u00f3n de la densidad y la detecci\u00f3n de anomal\u00edas sin supervisi\u00f3n en un espacio de baja dimensi\u00f3n aprendido por autoencoder profundo.", "El documento presenta un marco de aprendizaje profundo conjunto para la reducci\u00f3n de la dimensi\u00f3n-agrupamiento, conduce a la detecci\u00f3n de anomal\u00edas competitiva.", "Una nueva t\u00e9cnica para la detecci\u00f3n de anomal\u00edas en la que se optimizan conjuntamente los pasos de reducci\u00f3n de dimensiones y estimaci\u00f3n de la densidad."]}
{"source": ["Generalization from limited examples, usually studied under the umbrella of meta-learning, equips learning techniques with the ability to adapt quickly in dynamical environments and proves to be an essential aspect of lifelong learning.", "In this paper, we introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision.", "In contrast to previous studies, the embedding in PSN deems samples of a given class to form an affine subspace.", "We will show that such modeling leads to robust solutions, yielding competitive results on supervised and semi-supervised few-shot classification.", "Moreover, our PSN approach has the ability of end-to-end learning.", "In contrast to previous works, our projective subspace can be thought of as a richer representation capturing higher-order information datapoints for modeling new concepts."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkzfuiA9F7", "target": ["Proponemos redes de subespacio proyectivo para el aprendizaje de pocos disparos y semi-supervisado de pocos disparos", "Este trabajo propone un nuevo enfoque basado en la incrustaci\u00f3n para el problema del aprendizaje de pocos disparos y una extensi\u00f3n de este modelo al entorno del aprendizaje de pocos disparos semi-supervisado.", "Nuevo m\u00e9todo para la clasificaci\u00f3n completa y semi-supervisada de pocos disparos basado en el aprendizaje de una incrustaci\u00f3n general y luego el aprendizaje de un subespacio de la misma para cada clase"]}
{"source": ["This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning.", "To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE).", "In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games.", "The ADM is trained in a self-supervised fashion to predict the actions taken by the agent.", "The learned contingency information is used as a part of the state representation for exploration purposes.", "We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards.", "For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data.", "Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyxGB2AcY7", "target": ["Investigamos la conciencia de la contingencia y los aspectos controlables en la exploraci\u00f3n y logramos el rendimiento m\u00e1s avanzado en la Venganza de Moctezuma sin demostraciones de expertos.", "Este trabajo investiga el problema de la extracci\u00f3n de una representaci\u00f3n de estado significativa para ayudar a la exploraci\u00f3n cuando se enfrenta a una tarea de recompensa escasa mediante la identificaci\u00f3n de caracter\u00edsticas controlables (aprendidas) del estado", "Este trabajo propone la novedosa idea de utilizar la conciencia de contingencia para ayudar a la exploraci\u00f3n en tareas de aprendizaje por refuerzo con recompensas dispersas, obteniendo resultados del estado del arte."]}
{"source": ["Disentangling factors of variation has always been a challenging problem in representation learning.", "Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc.", "In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images.", "The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation.", "By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images.", "In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training.", "Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Syr8Qc1CW", "target": ["Propusimos un algoritmo supervisado, DNA-GAN, para desentra\u00f1ar m\u00faltiples atributos de las im\u00e1genes.", "Este trabajo investiga el problema de la generaci\u00f3n de im\u00e1genes condicionadas por atributos utilizando redes generativas adversariales, y propone generar im\u00e1genes a partir de atributos y c\u00f3digos latentes como representaci\u00f3n de alto nivel.", "Este trabajo propone un nuevo m\u00e9todo para desentra\u00f1ar diferentes atributos de las im\u00e1genes utilizando una nueva estructura de ADN GAN"]}
{"source": ["Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode.", "We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations).", "This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data).", "We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning."], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1e4wo09K7", "target": ["Este trabajo presenta una novedosa t\u00e9cnica de modelizaci\u00f3n generativa de variables latentes que permite representar la informaci\u00f3n global en una variable latente y la informaci\u00f3n local en otra variable latente.", "El art\u00edculo presenta un VAE que utiliza etiquetas para separar la representaci\u00f3n aprendida en una parte invariante y otra covariante."]}
{"source": ["Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe;  training a deep model on a very large dataset of supervised examples.", "However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive.", "One way to ease this problem is coming up with smart ways for choosing images to be labelled from a  very large collection (i.e. active learning).\n\n", "Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting.", "Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points.", "We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints.", "As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization.", "Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.\n"], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1aIuk-RW", "target": ["Abordamos el problema del aprendizaje activo como un problema de selecci\u00f3n de conjuntos de n\u00facleos y mostramos que este enfoque es especialmente \u00fatil en el entorno del aprendizaje activo por lotes, que es crucial cuando se entrenan las CNN.", "Los autores proporcionan un algoritmo de aprendizaje activo agn\u00f3stico para la clasificaci\u00f3n multiclase", "El art\u00edculo propone un algoritmo de aprendizaje activo en modo batch para CNN como un problema de conjunto de n\u00facleos que supera el muestreo aleatorio y el muestreo de incertidumbre.", "Estudia el aprendizaje activo para redes neuronales convolucionales y formula el problema de aprendizaje activo como selecci\u00f3n de conjuntos de n\u00facleos y presenta una estrategia novedosa"]}
{"source": ["Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP).", "This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps.", "We introduce a simple stochastic algorithm (\\textit{h}-detach) that is specific to LSTM optimization and targeted towards addressing this problem.", "Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed.", "Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them.", "Our algorithm\\footnote{Our code is available at https://github.com/bhargav104/h-detach.", "} prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better.", "We show significant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modification of LSTM gradient on various benchmark datasets."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryf7ioRqFX", "target": ["Un algoritmo sencillo para mejorar la optimizaci\u00f3n y el manejo de las dependencias a largo plazo en LSTM", "El art\u00edculo introduce un algoritmo estoc\u00e1stico simple llamado h-detach que es espec\u00edfico para la optimizaci\u00f3n de LSTM y est\u00e1 dirigido a abordar este problema.", "Propone una sencilla modificaci\u00f3n del proceso de entrenamiento de la LSTM para facilitar la propagaci\u00f3n del gradiente a lo largo de los estados de la c\u00e9lula, o el \"camino temporal lineal\""]}
{"source": ["Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision.", "However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes.", "Further, they are also vulnerable to adversarial examples.", "We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set.", "We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization.", "As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples.", "To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness.", "We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.", "Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Skgge3R9FQ", "target": ["El entrenamiento adecuado de las CNN con la clase dustbin aumenta su robustez frente a los ataques de adversarios y su capacidad para hacer frente a las muestras fuera de distribuci\u00f3n.", "Este trabajo propone a\u00f1adir una etiqueta adicional para detectar muestras OOD y ejemplos adversos en modelos CNN.", "El art\u00edculo propone una clase adicional que incorpora im\u00e1genes naturales de distribuci\u00f3n externa e im\u00e1genes interpoladas para las muestras adversas y de distribuci\u00f3n externa en las CNN"]}
{"source": ["Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization.", "Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit.", "Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity.", "Although these techniques have been proven successful in terms of results, they seem to waste capacity.", "In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity.", "In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "ByJWeR1AW", "target": ["En una red neuronal convolucional profunda entrenada con un nivel suficiente de aumento de datos, optimizada por SGD, los regularizadores expl\u00edcitos (decaimiento de pesos y abandono) podr\u00edan no proporcionar ninguna mejora adicional de la generalizaci\u00f3n.", "Este trabajo propone el aumento de datos como alternativa a las t\u00e9cnicas de regularizaci\u00f3n com\u00fanmente utilizadas, y muestra que para unos pocos modelos/tareas de referencia se puede conseguir el mismo rendimiento de generalizaci\u00f3n utilizando \u00fanicamente el aumento de datos.", "Este trabajo presenta un estudio sistem\u00e1tico del aumento de datos en la clasificaci\u00f3n de im\u00e1genes con redes neuronales profundas, sugiriendo que el aumento de datos puede replicar algunos regularizadores comunes como el decaimiento de peso y el abandono."]}
{"source": ["Text editing on mobile devices can be a tedious process.", "To perform various editing operations, a user must repeatedly move his or her fingers between the text input area and the keyboard, making multiple round trips and breaking the flow of typing.", "In this work, we present Gedit, a system of on-keyboard gestures for convenient mobile text editing.", "Our design includes a ring gesture and flicks for cursor control, bezel gestures for mode switching, and four gesture shortcuts for copy, paste, cut, and undo.", "Variations of our gestures exist for one and two hands.", "We conducted an experiment to compare Gedit with the de facto touch+widget based editing interactions.", "Our results showed that Gedit\u2019s gestures were easy to learn, 24% and 17% faster than the de facto interactions for one- and two-handed use, respectively, and preferred by participants."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "CZ938F7zVF", "target": ["En este trabajo, presentamos Gedit, un sistema de gestos en el teclado para una c\u00f3moda edici\u00f3n de texto en el m\u00f3vil.", "Informa sobre el dise\u00f1o y la evaluaci\u00f3n de las t\u00e9cnicas de interacci\u00f3n de Gedit.", "Presenta un nuevo conjunto de gestos t\u00e1ctiles para realizar una transici\u00f3n fluida entre la entrada y la edici\u00f3n de texto en los dispositivos m\u00f3viles"]}
{"source": ["Deep learning achieves remarkable generalization capability with overwhelming number of model parameters.", "Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored.", "This paper attempts to provide an alternative understanding from the perspective of maximum entropy.", "We first derive two feature conditions that softmax regression strictly apply maximum entropy principle.", "DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle.", "The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "r1kj4ACp-", "target": ["Demostramos que la DNN es una soluci\u00f3n recursivamente aproximada al principio de m\u00e1xima entrop\u00eda.", "Presenta una derivaci\u00f3n que vincula una DNN con la aplicaci\u00f3n recursiva del ajuste de modelos de m\u00e1xima entrop\u00eda.", "El art\u00edculo pretende ofrecer una visi\u00f3n del aprendizaje profundo desde la perspectiva del principio de m\u00e1xima entrop\u00eda."]}
{"source": [" As people learn to navigate the world, autonomic nervous system (e.g., ``fight or flight) responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger.", "We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses.", "Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency.", "We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage."], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "SyNvti09KQ", "target": ["Presentamos un novedoso enfoque de aprendizaje por refuerzo que aprovecha una funci\u00f3n de recompensa intr\u00ednseca independiente de la tarea y entrenada con mediciones de pulso perif\u00e9rico que se correlacionan con las respuestas del sistema nervioso aut\u00f3nomo humano. ", "Propone un marco de aprendizaje por refuerzo basado en la reacci\u00f3n emocional humana en el contexto de la conducci\u00f3n aut\u00f3noma.", "Los autores proponen utilizar se\u00f1ales, como las respuestas auton\u00f3micas viscerales b\u00e1sicas que influyen en la toma de decisiones, dentro del marco de la RL, aumentando las funciones de recompensa de la RL con un modelo aprendido directamente de las respuestas del sistema nervioso humano.", "Propone utilizar se\u00f1ales fisiol\u00f3gicas para mejorar el rendimiento de los algoritmos de aprendizaje por refuerzo y construir una funci\u00f3n de recompensa intr\u00ednseca que sea menos dispersa mediante la medici\u00f3n de la amplitud del pulso card\u00edaco"]}
{"source": ["Deep convolutional neural networks (CNNs) are known to be robust against label noise on extensive datasets.", "However, at the same time, CNNs are capable of memorizing all labels even if they are random, which means they can memorize corrupted labels.", "Are CNNs robust or fragile to label noise?", "Much of researches focusing on such memorization uses class-independent label noise to simulate label corruption, but this setting is simple and unrealistic.", "In this paper, we investigate the behavior of CNNs under class-dependently simulated label noise, which is generated based on the conceptual distance between classes of a large dataset (i.e., ImageNet-1k).", "Contrary to previous knowledge, we reveal CNNs are more robust to such class-dependent label noise than class-independent label noise.", "We also demonstrate the networks under class-dependent noise situations learn similar representation to the no noise situation, compared to class-independent noise situations."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1xmqiAqFm", "target": ["\u00bfSon las CNN robustas o fr\u00e1giles al ruido de las etiquetas? Pr\u00e1cticamente, robustos.", "Los autores ponen a prueba la robustez de las CNN frente al ruido de etiquetas utilizando el \u00e1rbol ImageNet 1k de WordNet.", "Un an\u00e1lisis del rendimiento de los modelos de redes neuronales convolucionales cuando se introduce ruido dependiente e independiente de la clase", "Demuestra que las CNN son m\u00e1s robustas al ruido de las etiquetas relevantes para la clase y argumenta que el ruido del mundo real deber\u00eda ser relevante para la clase"]}
{"source": ["Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.", "Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.", "Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain.", "Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.\n"], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "H1xQVn09FX", "target": ["S\u00edntesis de audio de alta calidad con GANs", "Propone un enfoque que utiliza el marco GAN para generar audio mediante el modelado de magnitudes de registro y frecuencias instant\u00e1neas con suficiente resoluci\u00f3n de frecuencia en el dominio espectral. ", "Una estrategia para generar muestras de audio a partir de ruido con GANs, con cambios en la arquitectura y representaci\u00f3n necesarios para generar un audio convincente que contenga un c\u00f3digo latente interpretable.", "Presenta una idea sencilla para representar mejor los datos de audio de modo que puedan aplicarse modelos convolucionales como las redes generativas adversariales"]}
{"source": ["In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation.", "Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain.", "We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity.", "Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph."], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "HklZOfW0W", "target": ["Optimizaci\u00f3n de gr\u00e1ficos con filtrado de se\u00f1ales en el dominio de los v\u00e9rtices.", "El art\u00edculo investiga el aprendizaje de la matriz de adyacencia de un grafo no dirigido escasamente conectado con pesos de arista no negativos, utilizando un algoritmo de descenso de sub-gradiente proyectado.", "Desarrolla un novedoso esquema de retropropagaci\u00f3n en la matriz de adyacencia de un grafo de red neuronal"]}
{"source": ["The use of AR in an industrial context could help for the training of new operators.", "To be able to use an AR guidance system, we need a tool to quickly create a 3D representation of the assembly line and of its AR annotations.", "This tool should be very easy to use by an operator who is not an AR or VR specialist: typically the manager of the assembly line.", "This is why we proposed WAAT, a 3D authoring tool allowing user to quickly create 3D models of the workstations, and also test the AR guidance placement.", "WAAT makes on-site authoring possible, which should really help to have an accurate 3D representation of the assembly line.", "The verification of AR guidance should also be very useful to make sure everything is visible and doesn't interfere with technical tasks.", "In addition to these features, our future work will be directed in the deployment of WAAT into a real boiler assembly line to assess the usability of this solution."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "1qdNTwXpgE", "target": ["Este art\u00edculo describe una herramienta de autor\u00eda 3D para proporcionar RA en las l\u00edneas de montaje de la industria 4.0", "El documento aborda c\u00f3mo las herramientas de autor\u00eda de RA apoyan la formaci\u00f3n de los sistemas de la l\u00ednea de montaje y propone un enfoque", "Un sistema de guiado de RA para l\u00edneas de montaje industriales que permite la creaci\u00f3n de contenidos de RA in situ.", "Presenta un sistema que permite formar a los trabajadores de una f\u00e1brica de forma m\u00e1s eficiente mediante un sistema de realidad aumentada. "]}
{"source": ["Generative adversarial network (GAN) is one of the best known unsupervised learning techniques these days due to its superior ability to learn data distributions.", "In spite of its great success in applications, GAN is known to be notoriously hard to train.", "The tremendous amount of time it takes to run the training algorithm and its sensitivity to hyper-parameter tuning have been haunting researchers in this area.", "To resolve these issues, we need to first understand how GANs work.", "Herein, we take a step toward this direction by examining the dynamics of GANs.", "We relate a large class of GANs including the Wasserstein GANs to max-min optimization problems with the coupling term being linear over the discriminator.", "By developing new primal-dual optimization tools, we show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate.", "The same framework also applies to multi-task learning and distributional robust learning problems.", "We verify our analysis on numerical examples with both synthetic and real data sets.", "We hope our analysis shed light on future studies on the theoretical properties of relevant machine learning problems."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rylIy3R9K7", "target": ["Demostramos que, con una elecci\u00f3n adecuada del tama\u00f1o de los pasos, el algoritmo iterativo de primer orden ampliamente utilizado en el entrenamiento de GANs converger\u00eda de hecho a una soluci\u00f3n estacionaria con una tasa sublineal.", "Este trabajo utiliza GANs y el aprendizaje multitarea para proporcionar una garant\u00eda de convergencia para los algoritmos primal-dual en ciertos problemas min-max.", "Analiza la din\u00e1mica de aprendizaje de las GANs formulando el problema como un problema de optimizaci\u00f3n primal-dual asumiendo una clase limitada de modelos"]}
{"source": ["Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction.", "We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest.", "However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict.", "We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards).", "We call this consequentialist conditional cooperation.", "We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games.", "We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "BkabRiQpb", "target": ["Mostramos c\u00f3mo utilizar la RL profunda para construir agentes que puedan resolver dilemas sociales m\u00e1s all\u00e1 de los juegos matriciales.", "Aprender a jugar a juegos de suma general de dos jugadores con estado con informaci\u00f3n imperfecta", "Especifica una estrategia de activaci\u00f3n (CCC) y el algoritmo correspondiente, demostrando la convergencia a resultados eficientes en dilemas sociales sin necesidad de que los agentes observen las acciones de los dem\u00e1s."]}
{"source": ["In distributed training, the communication cost due to the transmission of gradients\n", "or the parameters of the deep model is a major bottleneck in scaling up the number\n", "of processing nodes.", "To address this issue, we propose dithered quantization for\n", "the transmission of the stochastic gradients and show that training with Dithered\n", "Quantized Stochastic Gradients (DQSG) is similar to the training with unquantized\n", "SGs perturbed by an independent bounded uniform noise, in contrast to the other\n", "quantization methods where the perturbation depends on the gradients and hence,\n", "complicating the convergence analysis.", "We study the convergence of training\n", "algorithms using DQSG and the trade off between the number of quantization\n", "levels and the training time.", "Next, we observe that there is a correlation among the\n", "SGs computed by workers that can be utilized to further reduce the communication\n", "overhead without any performance loss.", "Hence, we develop a simple yet effective\n", "quantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\n", "communication significantly without requiring the workers communicating extra\n", "information to each other.", "We prove that although NDQSG requires significantly\n", "less bits, it can achieve the same quantization variance bound as DQSG.", "Our\n", "simulation results confirm the effectiveness of training using DQSG and NDQSG\n", "in reducing the communication bits or the convergence time compared to the\n", "existing methods without sacrificing the accuracy of the trained model."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJxMM2C5K7", "target": ["El art\u00edculo propone y analiza dos esquemas de cuantificaci\u00f3n para la comunicaci\u00f3n de Gradientes Estoc\u00e1sticos en el aprendizaje distribuido que reducir\u00edan los costes de comunicaci\u00f3n en comparaci\u00f3n con el estado del arte, manteniendo la misma precisi\u00f3n.  ", "Los autores proponen aplicar una cuantificaci\u00f3n interpuesta a los gradientes estoc\u00e1sticos calculados mediante el proceso de entrenamiento, lo que mejora el error de cuantificaci\u00f3n y consigue resultados superiores a los de las l\u00edneas de base, y proponen un esquema anidado para reducir el coste de comunicaci\u00f3n.", "Los autores establecen una conexi\u00f3n entre la reducci\u00f3n de la comunicaci\u00f3n en la optimizaci\u00f3n distribuida y la cuantificaci\u00f3n vacilante, y desarrollan dos nuevos algoritmos de entrenamiento distribuido en los que la sobrecarga de comunicaci\u00f3n se reduce significativamente."]}
{"source": ["Deep neural networks have been shown to perform well in many classical machine learning problems, especially in image classification tasks.", "However, researchers have found that neural networks can be easily fooled, and they are surprisingly sensitive to small perturbations imperceptible to humans.  ", "Carefully crafted input images (adversarial examples) can force a well-trained neural network to provide arbitrary outputs.  ", "Including adversarial examples during training is a popular defense mechanism against adversarial attacks.", "In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework.", "We model the adversarial noise using a generative network, trained jointly with a classification discriminative network as a minimax game.", "We show empirically that our adversarial network approach works well against black box attacks, with performance on par with state-of-art methods such as ensemble adversarial training and adversarial training with projected gradient descent.\n"], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "S1lIMn05F7", "target": ["Entrenar conjuntamente una red generadora de ruido adversario con una red de clasificaci\u00f3n para proporcionar una mayor robustez frente a los ataques adversarios.", "Una soluci\u00f3n GAN para modelos profundos de clasificaci\u00f3n, frente a ataques de caja blanca y negra, que produce modelos robustos. ", "El art\u00edculo propone un mecanismo defensivo contra los ataques adversarios utilizando GANs con perturbaciones generadas utilizadas como ejemplos adversarios y un discriminador utilizado para distinguir entre ellos"]}
{"source": ["Deep learning has become the state of the art approach in many machine learning problems such as classification.", "It has recently been shown that deep learning is highly vulnerable to adversarial perturbations.", "Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians.", "Hence, in order to use deep learning without safety concerns a proper defense strategy is required.", "We propose to use ensemble methods as a defense strategy against adversarial perturbations.", "We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task.", "This makes ensemble methods an attractive defense strategy against adversarial attacks.", "We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rkA1f3NpZ", "target": ["Utilizaci\u00f3n de m\u00e9todos de conjunto como defensa frente a las perturbaciones adversarias contra las redes neuronales profundas.", "Este trabajo propone utilizar el ensamblaje como mecanismo de defensa adversarial.", "Se ha investigado emp\u00edricamente la robustez de diferentes conjuntos de redes neuronales profundas frente a los dos tipos de ataques, FGSM y BIM, en dos conjuntos de datos populares, MNIST y CIFAR10"]}
{"source": ["In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in order to utilize visual information in a dialogue system without image input.", "In research on Neural Machine Translation, there are studies that generate translated sentences using both images and sentences, and these studies show that visual information improves translation performance.", "However, it is not possible to use sentence generation algorithms using images for the dialogue systems since many text-based dialogue systems only accept text input.", "Our approach generates (associates) visual information from input text and generates response text using context vector  fusing associative visual information and sentence textual information.", "A comparative experiment between our proposed model and a model without association showed that our proposed model is generating useful sentences by associating visual information related to sentences.", "Furthermore, analysis experiment of visual association showed that our proposed model generates (associates) visual information effective for sentence generation."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HJ39YKiTb", "target": ["Propuesta del m\u00e9todo de generaci\u00f3n de frases basado en la fusi\u00f3n entre la informaci\u00f3n textual y la informaci\u00f3n visual asociada a la informaci\u00f3n textual", "Este trabajo describe un modelo de aprendizaje profundo para sistemas de di\u00e1logo que aprovecha la informaci\u00f3n visual.", "Este art\u00edculo propone un nuevo conjunto de datos para el di\u00e1logo fundamentado y hace una observaci\u00f3n computacional que podr\u00eda ayudar a razonar sobre la visi\u00f3n incluso cuando se realiza un di\u00e1logo basado en texto.", "Propone aumentar los enfoques tradicionales de generaci\u00f3n de frases/di\u00e1logos basados en el texto incorporando informaci\u00f3n visual mediante la recopilaci\u00f3n de un conjunto de datos consistentes tanto en texto como en im\u00e1genes o v\u00eddeos asociados"]}
{"source": ["Feedforward convolutional neural network has achieved a great success in many computer vision tasks.", "While it validly imitates the hierarchical structure of biological visual system, it still lacks one essential architectural feature: contextual recurrent connections with feedback, which widely exists in biological visual system.", "In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure.", "We found that such feedback connections could enable lower layers to ``rethink\" about their representations given the top-down contextual information.", "We carefully studied the components of this network, and showed its robustness and superiority over feedforward baselines in such tasks as noise image classification, partially occluded object recognition and fine-grained image classification.", "We believed this work could be an important step to help bridge the gap between computer vision models and real biological visual system."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkzyX3CcFQ", "target": ["proponemos una nueva red convolucional recurrente contextual con una propiedad robusta de aprendizaje visual ", "Este art\u00edculo introduce la conexi\u00f3n de retroalimentaci\u00f3n para mejorar el aprendizaje de caracter\u00edsticas mediante la incorporaci\u00f3n de informaci\u00f3n de contexto.", "El trabajo propone a\u00f1adir conexiones \"recurrentes\" dentro de una red de convoluci\u00f3n con mecanismo de gating."]}
{"source": ["Deep neural networks have led to a series of breakthroughs, dramatically improving the state-of-the-art in many domains.", "The techniques driving these advances, however, lack a formal method to account for model uncertainty.", "While the Bayesian approach to learning provides a solid theoretical framework to handle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.", "In this paper, we provide a practical approach to Bayesian learning that relies on a regularization technique found in nearly every modern network, batch normalization.", "We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty.", "Using our approach, it is possible to make meaningful uncertainty estimates using conventional architectures without modifying the network or the training procedure.", "Our approach is thoroughly validated in a series of empirical experiments on different tasks and using various measures, showing it to outperform baselines on a majority of datasets with strong statistical significance."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJlrSmbAZ", "target": ["Mostramos que el entrenamiento de una red profunda utilizando la normalizaci\u00f3n por lotes es equivalente a la inferencia aproximada en los modelos bayesianos, y demostramos c\u00f3mo este hallazgo nos permite hacer estimaciones \u00fatiles de la incertidumbre del modelo en las redes convencionales.", "Este art\u00edculo propone utilizar la normalizaci\u00f3n por lotes en el momento de la prueba para obtener la incertidumbre de la predicci\u00f3n, y muestra que la predicci\u00f3n de Monte Carlo en el momento de la prueba utilizando la norma por lotes es mejor que el abandono.", "Propone que el procedimiento de regularizaci\u00f3n llamado normalizaci\u00f3n por lotes puede entenderse como la realizaci\u00f3n de una inferencia bayesiana aproximada, que se comporta de forma similar al abandono de MC en t\u00e9rminos de las estimaciones de incertidumbre que produce."]}
{"source": ["Data-parallel neural network training is network-intensive, so gradient dropping was designed to exchange only large gradients.  ", "However, gradient dropping has been shown to slow convergence.  ", "We propose to improve convergence by having each node combine its locally computed gradient with the sparse global gradient exchanged over the network.", "We empirically confirm with machine translation tasks that gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping.", "We also show that gradient dropping with a local gradient update does not reduce the model's final quality."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "BkeSusCcYm", "target": ["Mejoramos el gradient dropping (una t\u00e9cnica que s\u00f3lo intercambia gradientes grandes en el entrenamiento distribuido) incorporando gradientes locales mientras se hace una actualizaci\u00f3n de los par\u00e1metros para reducir la p\u00e9rdida de calidad y mejorar a\u00fan m\u00e1s el tiempo de entrenamiento.", "Este trabajo propone 3 modos de combinar los gradientes locales y globales para utilizar mejor m\u00e1s nodos de computaci\u00f3n", "Examina el problema de la reducci\u00f3n de los requisitos de comunicaci\u00f3n para la aplicaci\u00f3n de las t\u00e9cnicas de optimizaci\u00f3n distribuida, en particular el SGD"]}
{"source": ["    We establish the relation between Distributional RL and the Upper Confidence Bound (UCB) approach to exploration.\n    ", "In this paper we show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB.", "This approach does not require counting and, therefore, generalizes well to the Deep RL.", "We also point to the asymmetry of the empirical densities estimated by the Distributional RL algorithms like QR-DQN.", "This observation leads to the reexamination of the variance's performance in the UCB type approach to exploration.", "We introduce truncated variance as an alternative estimator of the UCB and a novel algorithm based on it.", "We empirically show that newly introduced algorithm achieves better performance in multi-armed bandits setting.", "Finally, we extend this approach to high-dimensional setting and test it on the Atari 2600 games.", "New approach achieves better performance compared to QR-DQN in 26 of games, 13 ties out of 49 games."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1fNJhRqFX", "target": ["Exploraci\u00f3n mediante RL distributiva y varianza truncada.", "Presenta un m\u00e9todo de RL para gestionar las compensaciones de exploraci\u00f3n-explotaci\u00f3n mediante t\u00e9cnicas de UCB.", "Un m\u00e9todo para utilizar la distribuci\u00f3n aprendida por la Regresi\u00f3n Cuantil DQN para la exploraci\u00f3n, en lugar de la estrategia habitual de epsilon-greedy.", "Propone nuevos algoritmos (QUCB y QUCB+) para manejar el compromiso de exploraci\u00f3n en los Bandidos Multiarmados y m\u00e1s generalmente en el Aprendizaje por Refuerzo"]}
{"source": ["Good representations facilitate transfer learning and few-shot learning.", "Motivated by theories of language and communication that explain why communities with large number of speakers have, on average, simpler languages with more regularity, we cast the representation learning problem in terms of learning to communicate.", "Our starting  point sees traditional autoencoders as  a single encoder with a fixed decoder partner that must learn to communicate.", "Generalizing from there, we introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations.", "Our experiments show that increasing community sizes reduce idiosyncrasies in the learned codes, resulting in more invariant representations with increased reusability and structure."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HkzL4hR9Ym", "target": ["Motivados por las teor\u00edas del lenguaje y la comunicaci\u00f3n, presentamos los autocodificadores basados en la comunidad, en los que m\u00faltiples codificadores y decodificadores aprenden colectivamente representaciones estructuradas y reutilizables.", "Los autores abordan el problema del aprendizaje de la representaci\u00f3n, pretenden construir una representaci\u00f3n reutilizable y estructurada, argumentan que la coadaptaci\u00f3n entre el codificador y el decodificador en la EA tradicional produce una representaci\u00f3n pobre, e introducen autocodificadores basados en la comunidad.", "Este art\u00edculo presenta un marco de autocodificaci\u00f3n basado en la comunidad para abordar la coadaptaci\u00f3n de codificadores y decodificadores y pretende construir mejores representaciones."]}
{"source": ["Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt.", "Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks.", "Achieving these abilities in autonomous agents is an open problem.", "In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap.", "MetaMimic can learn both", "(i) policies for high-fidelity one-shot imitation of diverse novel skills, and", "(ii) policies that enable the agent to solve tasks more efficiently than the demonstrators.", "MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL.", "This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.\n", "The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJMjW3RqtX", "target": ["Presentamos MetaMimic, un algoritmo que toma como entrada un conjunto de datos de demostraci\u00f3n y produce (i) una pol\u00edtica de imitaci\u00f3n de alta fidelidad de una sola vez y (ii) una pol\u00edtica de tarea incondicional.", "El art\u00edculo examina el problema de la imitaci\u00f3n de una sola vez con alta precisi\u00f3n de imitaci\u00f3n, extendiendo DDPGfD para utilizar s\u00f3lo las trayectorias de estado.", "Este trabajo propone un enfoque para la imitaci\u00f3n de una sola vez con alta precisi\u00f3n, y aborda el problema com\u00fan de la exploraci\u00f3n en el aprendizaje de la imitaci\u00f3n.", "Presenta un m\u00e9todo de RL para el aprendizaje a partir de una demostraci\u00f3n de v\u00eddeo sin acceso a las acciones de los expertos"]}
{"source": ["Normalization methods are a central building block in the deep learning toolbox.", "They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.", "When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.", "As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.", "We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HyN-M2Rctm", "target": ["Presentamos un nuevo m\u00e9todo de normalizaci\u00f3n para redes neuronales profundas que es robusto a las multimodalidades en las distribuciones de caracter\u00edsticas intermedias.", "M\u00e9todo de normalizaci\u00f3n que aprende la distribuci\u00f3n multimodal en el espacio de caracter\u00edsticas", "Propone una generalizaci\u00f3n de la Normalizaci\u00f3n por Lotes bajo el supuesto de que la estad\u00edstica de las activaciones unitarias sobre los lotes y sobre las dimensiones espaciales no es unimodal"]}
{"source": ["Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving.", "However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations.", "In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation.", "Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation.", "Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method.", "Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "S1gUsoR9YX", "target": ["Proponemos un m\u00e9todo basado en la destilaci\u00f3n de conocimientos para aumentar la precisi\u00f3n de la traducci\u00f3n autom\u00e1tica neural multiling\u00fce.", "Un modelo de traducci\u00f3n autom\u00e1tica neural multiling\u00fce que primero entrena modelos separados para cada par de idiomas y luego realiza la destilaci\u00f3n.", "El objetivo de este trabajo es entrenar un modelo de traducci\u00f3n autom\u00e1tica aumentando la p\u00e9rdida de entrop\u00eda cruzada est\u00e1ndar con un componente de destilaci\u00f3n basado en modelos de profesores individuales (de un solo par de idiomas)."]}
{"source": ["What makes humans so good at solving seemingly complex video games?  ", "Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making.", "This paper investigates the role of human priors for solving video games.", "Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors.", "We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors.", "We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes.", "Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Hk91SGWR-", "target": ["Investigamos los distintos tipos de conocimientos previos que ayudan al aprendizaje humano y descubrimos que los conocimientos previos generales sobre los objetos desempe\u00f1an el papel m\u00e1s cr\u00edtico a la hora de guiar el juego humano.", "Los autores estudian, mediante un experimento, qu\u00e9 aspectos de los prejuicios humanos son los m\u00e1s importantes para el aprendizaje por refuerzo en los videojuegos.", "Los autores presentan un estudio sobre los prejuicios empleados por los humanos al jugar a los videojuegos y demuestran la existencia de una taxonom\u00eda de caracter\u00edsticas que afectan a la capacidad de completar las tareas del juego en distintos grados."]}
{"source": ["Driven by the need for parallelizable hyperparameter optimization methods, this paper studies \\emph{open loop} search methods: sequences that are predetermined and can be generated before a single configuration is evaluated.", "Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.\n", "In particular, we propose the use of $k$-determinantal point processes in  hyperparameter optimization via random search.", "Compared to conventional uniform random search where hyperparameter settings are sampled independently, a $k$-DPP promotes diversity.  ", "We describe an approach that transforms hyperparameter search spaces for efficient use with a $k$-DPP.", "In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from $k$-DPPs defined over spaces with a mixture of discrete and continuous dimensions.", "Our experiments show significant benefits over uniform random search  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyBbjW-RW", "target": ["Impulsados por la necesidad de m\u00e9todos de optimizaci\u00f3n de hiperpar\u00e1metros paralelizables y de bucle abierto, proponemos el uso de procesos puntuales k-determinantes en la optimizaci\u00f3n de hiperpar\u00e1metros mediante b\u00fasqueda aleatoria.", "Propone utilizar el k-DPP para seleccionar los puntos candidatos en las b\u00fasquedas de hiperpar\u00e1metros.", "Los autores proponen k-DPP como m\u00e9todo de bucle abierto para la optimizaci\u00f3n de hiperpar\u00e1metros y proporcionan su estudio emp\u00edrico y su comparaci\u00f3n con otros m\u00e9todos.", "Considera la b\u00fasqueda no secuencial y no informada de hiperpar\u00e1metros utilizando procesos puntuales determinantes, que son distribuciones de probabilidad sobre subconjuntos de un conjunto b\u00e1sico con la propiedad de que los subconjuntos con elementos m\u00e1s \"diversos\" tienen mayor probabilidad"]}
{"source": ["In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\n", "When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\n", "However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.\n", "In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\n", "We eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rye7IMbAZ", "target": ["En el aprendizaje inductivo por transferencia, el ajuste fino de las redes convolucionales preentrenadas supera sustancialmente el entrenamiento desde cero.", "Aborda el problema del aprendizaje de transferencia en redes profundas y propone tener un t\u00e9rmino de regularizaci\u00f3n que penalice la divergencia de la inicializaci\u00f3n.", "Propone un an\u00e1lisis sobre diferentes t\u00e9cnicas de regularizaci\u00f3n adaptativa para el aprendizaje profundo de transferencia, centr\u00e1ndose espec\u00edficamente en el uso de una condici\u00f3n L@-SP"]}
{"source": ["Artificial neural networks have opened up a world of possibilities in data science and artificial intelligence, but neural networks are cumbersome tools that grow with the complexity of the learning problem.", "We make contributions to this issue by considering a modified version of the fully connected layer we call a block diagonal inner product layer.", "These modified layers have weight matrices that are block diagonal, turning a single fully connected layer into a set of densely connected neuron groups.", "This idea is a natural extension of group, or depthwise separable, convolutional layers applied to the fully connected layers.", "Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries.", "This method condenses network storage and speeds up the run time without significant adverse effect on the testing accuracy, thus offering a new approach to improve network computation efficiency."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HyI5ro0pW", "target": ["Nos fijamos en las redes neuronales con capas de producto interno diagonal en bloque para mejorar la eficiencia.", "Este art\u00edculo propone que las capas internas de una red neuronal sean diagonales en bloque, y discute que las matrices diagonales en bloque son m\u00e1s eficientes que la poda y que las capas diagonales en bloque conducen a redes m\u00e1s eficientes.", "Sustituci\u00f3n de capas totalmente conectadas por capas totalmente conectadas diagonales en bloque"]}
{"source": ["One of the challenges in the study of generative adversarial networks is the instability of its training. \n", "In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\n", "Our new normalization technique is computationally light and easy to incorporate into existing implementations. \n", "We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1QRgziT-", "target": ["Proponemos una novedosa t\u00e9cnica de normalizaci\u00f3n de pesos llamada normalizaci\u00f3n espectral para estabilizar el entrenamiento del discriminador de los GANs.", "Este trabajo utiliza la regularizaci\u00f3n espectral para normalizar los objetivos del GAN, y el GAN resultante, denominado SN-GAN, garantiza esencialmente la propiedad Lipschitz del discriminador.", "Este trabajo propone la \"normalizaci\u00f3n espectral\", dando un buen paso adelante en la mejora del entrenamiento de los GANs."]}
{"source": ["Humans acquire complex skills by exploiting previously learned skills and making transitions between them.", "To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards.", "To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill.", "The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at.", "We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills.", "The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.", "We make our environments, primitive skills, and code public for further research at https://youngwoon.github.io/transition ."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rygrBhC5tQ", "target": ["Las pol\u00edticas de transici\u00f3n permiten a los agentes componer habilidades complejas mediante la conexi\u00f3n fluida de habilidades primitivas previamente adquiridas.", "Propone un esquema de transici\u00f3n a estados de estrato favorables para la ejecuci\u00f3n de opciones dadas en dominios continuos. Utiliza dos procesos de aprendizaje realizados simult\u00e1neamente.", "Presenta un m\u00e9todo para el aprendizaje de pol\u00edticas de transici\u00f3n de una tarea a otra con el objetivo de completar tareas complejas utilizando el estimador de proximidad de estado para recompensar la pol\u00edtica de transici\u00f3n.", "Propone un nuevo esquema de entrenamiento con una funci\u00f3n de recompensa auxiliar aprendida para optimizar las pol\u00edticas de transici\u00f3n que conectan el estado final de una macro acci\u00f3n/opci\u00f3n anterior con buenos estados de iniciaci\u00f3n de la siguiente macro acci\u00f3n/opci\u00f3n"]}
{"source": ["Gated recurrent units (GRUs) were inspired by the common gated recurrent unit, long short-term memory (LSTM), as a means of capturing temporal structure with less complex memory unit architecture.", "Despite their incredible success in tasks such as natural and artificial language processing, speech, video, and polyphonic music, very little is understood about the specific dynamic features representable in a GRU network.", "As a result, it is difficult to know a priori how successful a GRU-RNN will perform on a given data set.", "In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system.\n", "We found rich repertoire that includes stable limit cycles over time (nonlinear oscillations), multi-stable state transitions with various topologies, and homoclinic orbits.", "In addition, we show that any finite dimensional GRU cannot precisely replicate the dynamics of a ring attractor, or more generally, any continuous attractor, and is limited to finitely many isolated fixed points in theory.", "These findings were then experimentally verified in two dimensions by means of time series prediction."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1eiZnAqKm", "target": ["Clasificamos las caracter\u00edsticas din\u00e1micas que pueden y no pueden captar las c\u00e9lulas GRU en tiempo continuo, y verificamos nuestros hallazgos experimentalmente con la predicci\u00f3n de series temporales en k pasos. ", "Los autores analizan GRUs con tama\u00f1os ocultos de uno y dos como sistemas din\u00e1micos de tiempo continuo, afirmando que el poder expresivo de la representaci\u00f3n de estados ocultos puede proporcionar un conocimiento previo sobre el rendimiento de una GRU en un conjunto de datos determinado", "Este trabajo analiza las GRUs desde una perspectiva de sistemas din\u00e1micos, y muestra que las GRUs 2d pueden ser entrenadas para adoptar una variedad de puntos fijos y pueden aproximarse a los atractores de l\u00ednea, pero no pueden imitar un atractor de anillo.", "Convierte las ecuaciones de GRU en tiempo continuo y utiliza la teor\u00eda y los experimentos para estudiar las redes GRU de 1 y 2 dimensiones y mostrar todas las variedades de topolog\u00eda din\u00e1mica disponibles en estos sistemas"]}
{"source": ["Stacked hourglass network has become an important model for Human pose estimation.", "The estimation of human body posture depends on the global information of the keypoints type and the local information of the keypoints location.", "The consistent processing of inputs and constraints makes it difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass network.", "In this paper, we propose a Multi-Scale Stacked Hourglass (MSSH) network to high-light the differentiation capabilities of each Hourglass network for human pose estimation.  ", "The pre-processing network forms feature maps of different scales,and dispatch them to various locations of the stack hourglass network, where the small-scale features reach the front of stacked hourglass network, and large-scale features reach the rear of stacked hourglass network.   ", "And a new loss function is proposed for multi-scale stacked hourglass network.  ", "Different keypoints have different weight coefficients of loss function at different scales, and the keypoints weight coefficients are dynamically adjusted from the top-level hourglass network to the bottom-level hourglass network.  ", "Experimental results show that the pro-posed method is competitive with respect to the comparison algorithm on MPII and LSP datasets."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HkM3vjCcF7", "target": ["Las entradas diferenciadas provocan una diferenciaci\u00f3n funcional de la red, y la interacci\u00f3n de las funciones de p\u00e9rdida entre redes puede afectar al proceso de optimizaci\u00f3n.", "Una modificaci\u00f3n de la red original de reloj de arena para la estimaci\u00f3n de una sola pose que produce mejoras sobre la l\u00ednea de base original.", "Los autores ampl\u00edan una red de reloj de arena apilada con m\u00f3dulos inception-resnet-A y proponen un enfoque multiescala para la estimaci\u00f3n de la pose humana en im\u00e1genes RGB fijas."]}
{"source": ["We present a new unsupervised method for learning general-purpose sentence embeddings.\n", "Unlike existing methods which rely on local contexts, such as words\n", "inside the sentence or immediately neighboring sentences, our method selects, for\n", "each target sentence, influential sentences in the entire document based on a document\n", "structure.", "We identify a dependency structure of sentences using metadata\n", "or text styles.", "Furthermore, we propose a novel out-of-vocabulary word handling\n", "technique to model many domain-specific terms, which were mostly discarded by\n", "existing sentence embedding methods.", "We validate our model on several tasks\n", "showing 30% precision improvement in coreference resolution in a technical domain,\n", "and 7.5% accuracy increase in paraphrase detection compared to baselines."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1a37GWCZ", "target": ["Para entrenar una incrustaci\u00f3n de frases utilizando documentos t\u00e9cnicos, nuestro enfoque tiene en cuenta la estructura del documento para encontrar un contexto m\u00e1s amplio y manejar las palabras fuera del vocabulario.", "Presenta ideas para mejorar la inserci\u00f3n de la frase recurriendo a m\u00e1s contexto.", "Aprendizaje de representaciones de oraciones con informaci\u00f3n de dependencias de oraciones", "Ampl\u00eda la idea de formar una representaci\u00f3n no supervisada de las frases utilizada en el enfoque SkipThough utilizando un conjunto m\u00e1s amplio de pruebas para formar la representaci\u00f3n de una frase"]}
{"source": ["Neural network training relies on our ability to find ````````\"good\" minimizers of highly non-convex loss functions.", "It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better.", "However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood.\n\n", "In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.", "First, we introduce a simple ``\"filter normalization\" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions.", "Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HkmaTz-0W", "target": ["Exploramos la estructura de las funciones de p\u00e9rdida neuronales, y el efecto de los paisajes de p\u00e9rdida en la generalizaci\u00f3n, utilizando una serie de m\u00e9todos de visualizaci\u00f3n.", "Este art\u00edculo propone un m\u00e9todo para visualizar la funci\u00f3n de p\u00e9rdida de una NN y proporciona informaci\u00f3n sobre la capacidad de entrenamiento y la generalizaci\u00f3n de las NN.", "Investiga la no convexidad de la superficie de p\u00e9rdidas y las v\u00edas de optimizaci\u00f3n."]}
{"source": ["Deep models are state-of-the-art for many computer vision tasks including image classification and object detection.", "However, it has been shown that deep models are vulnerable to adversarial examples.", "We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping.", "We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust.", "Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks.", "We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training.", "The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1xOYoA5tQ", "target": ["Demostramos que aprovechando una codificaci\u00f3n de salida multidireccional, en lugar de la ampliamente utilizada codificaci\u00f3n de un solo disparo, podemos hacer que los modelos profundos sean m\u00e1s robustos a los ataques adversarios.", "Este trabajo propone reemplazar la capa final de entrop\u00eda cruzada entrenada con etiquetas de un solo punto en los clasificadores, codificando cada etiqueta como un vector de alta dimensi\u00f3n y entrenando el clasificador para minimizar la distancia L2 de la codificaci\u00f3n de la clase correcta.", "Los autores proponen un nuevo m\u00e9todo contra los ataques de adversarios que muestra una cantidad significativa de ganancias en comparaci\u00f3n con las l\u00edneas de base"]}
{"source": ["Existing approaches to neural machine translation condition each output word on previously generated outputs.", "We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference.", "Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher.", "We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English\u2013German and two WMT language pairs.", "By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English\u2013Romanian."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1l8BtlCb", "target": ["Introducimos el primer modelo NMT con decodificaci\u00f3n totalmente paralela, reduciendo la latencia de la inferencia en 10 veces.", "Este trabajo propone un decodificador no autorregresivo para el marco codificador-decodificador en el que la decisi\u00f3n de generar una palabra no depende de la decisi\u00f3n previa de las palabras generadas", "Este art\u00edculo describe un enfoque de decodificaci\u00f3n no autoregresiva para la traducci\u00f3n autom\u00e1tica neural con la posibilidad de una decodificaci\u00f3n m\u00e1s paralela que puede dar lugar a un aumento significativo de la velocidad.", "Propone la introducci\u00f3n de un conjunto de variables latentes para representar la fertilidad de cada palabra de origen para que la generaci\u00f3n de la frase objetivo no sea autorregresiva"]}
{"source": ["While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs.", "Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses.", "Can we somehow end this arms race?", "In this work, we study this problem for neural networks with one hidden layer.", "We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value.", "Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks.", "On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n"], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "Bys4ob-Rb", "target": ["Demostramos un m\u00e9todo certificable, entrenable y escalable para defenderse de ejemplos adversos.", "Propone una nueva defensa contra los ataques de seguridad a las redes neuronales con el modelo de ataque que emite un certificado de seguridad sobre el algoritmo.", "Deduce un l\u00edmite superior de la perturbaci\u00f3n adversarial para redes neuronales con una capa oculta"]}
{"source": ["We formulate an information-based optimization problem for supervised classification.", "For invertible neural networks, the control of these information terms is passed down to the latent features and parameter matrix in the last fully connected layer, given that mutual information is invariant under invertible map.  ", "We propose an objective function and prove that it solves the optimization problem.", "Our framework allows us to learn latent features in an more interpretable form while improving the classification performance.", "We perform extensive quantitative and qualitative experiments in comparison with the existing state-of-the-art classification models."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJgvg30ctX", "target": ["proponemos un regularizador que mejora el rendimiento de clasificaci\u00f3n de las redes neuronales", "los autores proponen entrenar un modelo desde el punto de maximizar la informaci\u00f3n mutua entre las predicciones y las salidas verdaderas, con un t\u00e9rmino de regularizaci\u00f3n que minimiza la informaci\u00f3n irrelevante mientras se aprende.", "Propone descomponer los par\u00e1metros en un mapa de caracter\u00edsticas invertible F y una transformaci\u00f3n lineal w en la \u00faltima capa para maximizar la informaci\u00f3n mutua I(Y, \\hat{T}) mientras se restringe la informaci\u00f3n irrelevante"]}
{"source": ["Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood.", "These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable.", "We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE).", "This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well.", "Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BkMqUiA5KX", "target": ["Este trabajo introduce un novedoso marco de modelizaci\u00f3n generativa que evita el colapso de las variables latentes y aclara el uso de ciertos factores ad-hoc en el entrenamiento de los Autoencoders Variacionales.", "El art\u00edculo propone resolver el problema de un autocodificador variacional que ignora las variables latentes.", "Este trabajo propone a\u00f1adir un autoencoder estoc\u00e1stico al modelo VAE original para abordar el problema de que el decodificador LSTM de un modelo ling\u00fc\u00edstico podr\u00eda ser demasiado fuerte para ignorar la informaci\u00f3n de la variable latente.", "Este trabajo presenta AutoGen, que combina un autoencoder generativo variacional con un modelo de reconstrucci\u00f3n de alta fidelidad basado en el autoencoder para utilizar mejor la representaci\u00f3n latente"]}
{"source": ["This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions.", "This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions.", "Previous works only calibrate the con\ufb01dent prediction of classi\ufb01ers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013).", "In contrast, this paper proposes a probabilistic way of directly estimating and \ufb01ne-tuning the decision boundary between seen and unseen classes.", "In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain.", "Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the \ufb01rst time, are introduced to uncover and \ufb01ne-tune the decision boundary of each domain.", "Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted con\ufb01dently.", "Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1GaLiAcY7", "target": [" Este trabajo estudia el problema de la divisi\u00f3n de dominios mediante la segmentaci\u00f3n de instancias extra\u00eddas de diferentes distribuciones probabil\u00edsticas.  ", "Este trabajo aborda el problema del reconocimiento de la novedad en el aprendizaje de conjuntos abiertos y el aprendizaje generalizado de disparos cero y propone una posible soluci\u00f3n", "Un enfoque para la separaci\u00f3n de dominios basado en el bootstrapping para identificar los umbrales de corte de similitud para las clases conocidas, seguido de una prueba de Kolmogorov-Smirnoff para refinar las zonas de in-distribuci\u00f3n bootstrapped.", "Propone introducir un nuevo dominio, el dominio incierto, para manejar mejor la divisi\u00f3n entre dominios vistos/no vistos en el aprendizaje de conjunto abierto y de tiro cero generalizado"]}
{"source": ["Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive.", "We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term.", "This potential is however not the original loss function in general.", "So SGD does perform variational inference, but for a different loss than the one used to compute the gradients.", "Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points.", "Instead, they resemble closed loops with deterministic components.", "We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension.", "We provide extensive empirical validation of these claims, proven in the appendix."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HyWrIgW0W", "target": ["El SGD realiza impl\u00edcitamente una inferencia variacional; el ruido del gradiente es altamente no isotr\u00f3pico, por lo que el SGD ni siquiera converge a los puntos cr\u00edticos de la p\u00e9rdida original", "Este documento proporciona un an\u00e1lisis variacional de la SGD como proceso de no equilibrio.", "Este art\u00edculo analiza la funci\u00f3n objetivo regularizada minimizada por el SGD est\u00e1ndar en el contexto de las redes neuronales, y proporciona una perspectiva de inferencia variacional utilizando la ecuaci\u00f3n de Fokker-Planck.", "Desarrolla una teor\u00eda para estudiar el impacto del ruido de gradiente estoc\u00e1stico para SGD, especialmente para modelos de redes neuronales profundas"]}
{"source": ["The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate.", "We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.", "In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.", "The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.", "Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.", "We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.", "Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.", "Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BkisuzWRW", "target": ["Los agentes pueden aprender a imitar \u00fanicamente demostraciones visuales (sin acciones) en el momento de la prueba despu\u00e9s de aprender de su propia experiencia sin ning\u00fan tipo de supervisi\u00f3n en el momento de la formaci\u00f3n.", "Este art\u00edculo propone un enfoque para el aprendizaje visual de cero disparos mediante el aprendizaje de funciones de habilidad param\u00e9tricas.", "Un art\u00edculo sobre la imitaci\u00f3n de una tarea presentada justo durante la inferencia, donde el aprendizaje se realiza de forma autosupervisada y durante el entrenamiento el agente explora tareas relacionadas pero diferentes.", "Propone un m\u00e9todo para eludir el problema de la costosa demostraci\u00f3n de expertos utilizando la exploraci\u00f3n aleatoria de un agente para aprender habilidades generalizables que puedan aplicarse sin un entrenamiento previo espec\u00edfico"]}
{"source": ["Distributional Semantics Models(DSM) derive word space from linguistic items\n", "in context.", "Meaning is obtained by defining a distance measure between vectors\n", "corresponding to lexical entities.", "Such vectors present several problems.", "This\n", "work concentrates on quality of word embeddings, improvement of word embedding\n", "vectors, applicability of a novel similarity metric used \u2018on top\u2019 of the\n", "word embeddings.", "In this paper we provide comparison between two methods\n", "for post process improvements to the baseline DSM vectors.", "The counter-fitting\n", "method which enforces antonymy and synonymy constraints into the Paragram\n", "vector space representations recently showed improvement in the vectors\u2019 capability\n", "for judging semantic similarity.", "The second method is our novel RESM\n", "method applied to GloVe baseline vectors.", "By applying the hubness reduction\n", "method, implementing relational knowledge into the model by retrofitting synonyms\n", "and providing a new ranking similarity definition RESM that gives maximum\n", "weight to the top vector component values we equal the results for the ESL\n", "and TOEFL sets in comparison with our calculations using the Paragram and Paragram\n", "+ Counter-fitting methods.", "For SIMLEX-999 gold standard since we cannot\n", "use the RESM the results using GloVe and PPDB are significantly worse compared\n", "to Paragram.", "Apparently, counter-fitting corrects hubness.", "The Paragram\n", "or our cosine retrofitting method are state-of-the-art results for the SIMLEX-999\n", "gold standard.", "They are 0.2 better for SIMLEX-999 than word2vec with sense\n", "de-conflation (that was announced to be state-of the-art method for less reliable\n", "gold standards).", "Apparently relational knowledge and counter-fitting is more important\n", "for judging semantic similarity than sense determination for words.", "It is to\n", "be mentioned, though that Paragram hyperparameters are fitted to SIMLEX-999\n", "results.", "The lesson is that many corrections to word embeddings are necessary\n", "and methods with more parameters and hyperparameters perform better.\n"], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyHmGyZCZ", "target": ["El art\u00edculo proporciona una descripci\u00f3n de un procedimiento para mejorar el modelo de espacio vectorial de palabras con una evaluaci\u00f3n de los modelos Paragram y GloVe para las pruebas de similitud.", "Este trabajo propone un nuevo algoritmo que ajusta los vectores de palabras de GloVe y luego utiliza una funci\u00f3n de similitud no euclidiana entre ellos.", "Los autores presentan observaciones sobre los puntos d\u00e9biles de los modelos de espacio vectorial existentes y enumeran un enfoque de 6 pasos para perfeccionar los vectores de palabras existentes"]}
{"source": ["Recurrent neural networks have achieved excellent performance in many applications.", "However, on portable devices with limited resources, the models are often too large to deploy.", "For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources.", "In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem.", "Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  ", "We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models.", "Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy.", "By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration.", "Both results beat the exiting quantization works with large margins.  ", "We extend our alternating quantization to image classification tasks.", "In both RNNs and feedforward neural networks, the method also achieves  excellent performance."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "S19dR9x0b", "target": ["Proponemos un nuevo m\u00e9todo de cuantificaci\u00f3n y lo aplicamos para cuantificar las RNN tanto para la compresi\u00f3n como para la aceleraci\u00f3n", "Este trabajo propone un m\u00e9todo de cuantificaci\u00f3n de m\u00faltiples bits para redes neuronales recurrentes.", "Una t\u00e9cnica para cuantificar las matrices de pesos de las redes neuronales, y un procedimiento de optimizaci\u00f3n alternativo para estimar el conjunto de k vectores binarios y coeficientes que mejor representan el vector original."]}
{"source": ["The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA).", "We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset.", "The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression.", "We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n"], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "rkGZuJb0b", "target": ["Sustituimos las capas totalmente conectadas de una red neuronal por el ansatz de renormalizaci\u00f3n del enredo a escala m\u00faltiple, un tipo de operaci\u00f3n cu\u00e1ntica que describe las correlaciones de largo alcance. ", "En el art\u00edculo los autores sugieren utilizar la t\u00e9cnica de tensorizaci\u00f3n MERA para comprimir las redes neuronales.", "Una nueva parametrizaci\u00f3n de los mapas lineales para su uso en redes neuronales, utilizando una factorizaci\u00f3n jer\u00e1rquica del mapa lineal que reduce el n\u00famero de par\u00e1metros, al tiempo que permite modelar interacciones relativamente complejas.", "Estudios de compresi\u00f3n de las capas de avance utilizando descomposiciones tensoriales de bajo rango y explorando una descomposici\u00f3n tipo \u00e1rbol"]}
{"source": ["Deep learning models have outperformed traditional methods in many fields such\n", "as natural language processing and computer vision.", "However, despite their\n", "tremendous success, the methods of designing optimal Convolutional Neural Networks\n", "(CNNs) are still based on heuristics or grid search.", "The resulting networks\n", "obtained using these techniques are often overparametrized with huge computational\n", "and memory requirements.", "This paper focuses on a structured, explainable\n", "approach towards optimal model design that maximizes accuracy while keeping\n", "computational costs tractable.", "We propose a single-shot analysis of a trained CNN\n", "that uses Principal Component Analysis (PCA) to determine the number of filters\n", "that are doing significant transformations per layer, without the need for retraining.\n", "It can be interpreted as identifying the dimensionality of the hypothesis space\n", "under consideration.", "The proposed technique also helps estimate an optimal number\n", "of layers by looking at the expansion of dimensions as the model gets deeper.\n", "This analysis can be used to design an optimal structure of a given network on\n", "a dataset, or help to adapt a predesigned network on a new dataset.", "We demonstrate\n", "these techniques by optimizing VGG and AlexNet networks on CIFAR-10,\n", "CIFAR-100 and ImageNet datasets."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJgzJh0qtQ", "target": ["Presentamos un an\u00e1lisis de una red neuronal entrenada para eliminar la redundancia e identificar la estructura \u00f3ptima de la red", "Este trabajo propone un conjunto de heur\u00edsticas para identificar una buena arquitectura de red neuronal, basada en el PCA de las activaciones de las unidades sobre el conjunto de datos", "Este art\u00edculo presenta un marco para optimizar las arquitecturas de las redes neuronales mediante la identificaci\u00f3n de filtros redundantes en las capas"]}
{"source": ["Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary\u2019s capability to conduct attacks on black-box networks.", "This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels.  ", "First, we define the threat model for these attacks:  our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim \u2019s deep learning  (DL) system is running and passively monitors the accesses of the target functions in the shared framework.  ", "Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique.", "Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim\u2019s entire network architecture.  ", "In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having only observed one forward propagation.", "Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting.", "From this meta-model,  we evaluate the importance of the observed attributes in the fingerprinting process.", "Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker\u2019s observations.", "Our empirical security analysis represents a step toward understanding the DNNs\u2019 vulnerability to cache side-channel attacks."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rk4Wf30qKQ", "target": ["Llevamos a cabo el primer an\u00e1lisis de seguridad en profundidad de los ataques de huellas dactilares de la DNN que explotan los canales laterales de la cach\u00e9, lo que representa un paso hacia la comprensi\u00f3n de la vulnerabilidad de la DNN a los ataques de canales laterales.", "Este art\u00edculo considera el problema de la huella digital de las arquitecturas de redes neuronales utilizando los canales laterales de la cach\u00e9, y discute las defensas de seguridad a trav\u00e9s de la obscuridad.", "Este trabajo realiza ataques de canal lateral de cach\u00e9 para extraer atributos de un modelo v\u00edctima e inferir su arquitectura, adem\u00e1s de mostrar que pueden lograr una precisi\u00f3n de clasificaci\u00f3n casi perfecta."]}
{"source": ["Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years.", "Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes.", "We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance.", "This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes.", "We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding.", "The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks.", "In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.\n"], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HyM7AiA5YX", "target": ["Proponemos el Entrenamiento de Objetivos Complementarios (COT), un nuevo paradigma de entrenamiento que optimiza tanto los objetivos primarios como los complementarios para aprender eficazmente los par\u00e1metros de las redes neuronales.", "Considera la posibilidad de aumentar el objetivo de entrop\u00eda cruzada con la maximizaci\u00f3n del objetivo de \"complemento\", que pretende neutralizar las probabilidades predichas de las clases distintas de las etiquetas de la verdad b\u00e1sica.", "Los autores proponen un objetivo secundario para la minimizaci\u00f3n de softmax basado en la evaluaci\u00f3n de la informaci\u00f3n obtenida de las clases incorrectas, lo que conduce a un nuevo enfoque de entrenamiento.", "Trata del entrenamiento de redes neuronales para tareas de clasificaci\u00f3n o generaci\u00f3n de secuencias utilizando la p\u00e9rdida de entrop\u00eda cruzada"]}
{"source": ["We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output.", "We extend softmax layer with an additional constant input.", "The corresponding additional output is able to represent the uncertainty of the network.", "The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets.", "We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rJxA-h05KQ", "target": ["Estimaci\u00f3n de la incertidumbre en una sola pasada hacia delante sin par\u00e1metros adicionales aprendibles.", "Un nuevo m\u00e9todo para calcular las estimaciones de incertidumbre de salida en las DNNs para problemas de clasificaci\u00f3n que se ajusta a los m\u00e9todos m\u00e1s avanzados para la estimaci\u00f3n de la incertidumbre y los supera en las tareas de detecci\u00f3n fuera de la distribuci\u00f3n.", "Los autores presentan el softmax inhibido, una modificaci\u00f3n del softmax mediante la adici\u00f3n de una activaci\u00f3n constante que proporciona una medida de incertidumbre. "]}
{"source": ["When deep learning is applied to sensitive data sets, many privacy-related implementation issues arise.", "These issues are especially evident in the healthcare, finance, law and government industries.", "Homomorphic encryption could allow a server to make inferences on inputs encrypted by a client, but to our best knowledge, there has been no complete implementation of common deep learning operations, for arbitrary model depths, using homomorphic encryption.", "This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption.", "As part of our implementation, we demonstrate Single and Multi-Layer Neural Networks, for the Wisconsin Breast Cancer dataset, as well as a Convolutional Neural Network for MNIST.", "Our results give promising directions for privacy-preserving representation learning, and the return of data control to users.\n\n"], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByCPHrgCW", "target": ["Hicimos un sistema rico en caracter\u00edsticas para el aprendizaje profundo con entradas cifradas, produciendo salidas cifradas, preservando la privacidad.", "Un marco para la inferencia de modelos privados de aprendizaje profundo utilizando esquemas FHE que admiten un bootstrapping r\u00e1pido y, por tanto, pueden reducir el tiempo de c\u00e1lculo.", "El art\u00edculo presenta un medio para evaluar una red neuronal de forma segura utilizando el cifrado homom\u00f3rfico."]}
{"source": ["In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.", "Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner.", "Hence, they provide an opportunity to explore theorem proving with human supervision.", "We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem.", "We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1xwKoR9Y7", "target": ["Presentamos un sistema llamado GamePad para explorar la aplicaci\u00f3n de m\u00e9todos de aprendizaje autom\u00e1tico a la demostraci\u00f3n de teoremas en el asistente de pruebas Coq.", "Este art\u00edculo describe un sistema para aplicar el aprendizaje autom\u00e1tico a la demostraci\u00f3n interactiva de teoremas, se centra en las tareas de predicci\u00f3n de t\u00e1cticas y evaluaci\u00f3n de posiciones, y muestra que un modelo neural supera a un SVM en ambas tareas.", "Propone que se utilicen t\u00e9cnicas de aprendizaje autom\u00e1tico para ayudar a construir pruebas en el prover de teoremas Coq."]}
{"source": ["Deep neural networks are usually huge, which significantly limits the deployment on low-end devices.", "In recent years, many\n", "weight-quantized models have  been proposed.", "They have small storage and fast inference, but training can still be time-consuming.", "This can be improved with distributed learning.", "To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. \n", "In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.\n", "We show  that", "(i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension;", "(ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and", "(iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization.", "Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryM_IoAqYX", "target": ["En este trabajo, estudiamos el entrenamiento eficiente de redes cuantificadas por peso con p\u00e9rdidas y gradiente cuantificado en un entorno distribuido, tanto te\u00f3rica como emp\u00edricamente.", "Este trabajo estudia las propiedades de convergencia de la cuantificaci\u00f3n de pesos con p\u00e9rdidas con diferentes precisiones de gradiente en el entorno distribuido, y proporciona un an\u00e1lisis de convergencia para la cuantificaci\u00f3n de pesos con gradientes de precisi\u00f3n total, cuantificados y cuantificados recortados.", "Los autores proponen un an\u00e1lisis del efecto de cuantificar simult\u00e1neamente los pesos y los gradientes en el entrenamiento de un modelo parametrizado en un entorno distribuido totalmente sincronizado."]}
{"source": ["Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task.", "In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them.", "To achieve Selfless Sequential Learning we study different regularization strategies and activation functions.", "We find that\n", "imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity.", "In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition.", "It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks.", "As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations,\n", "our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain.", "We combine our novel regularizer with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network.", "We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement on diverse datasets."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Bkxbrn0cYX", "target": ["Una estrategia de regularizaci\u00f3n para mejorar el rendimiento del aprendizaje secuencial", "Un enfoque novedoso, basado en la regularizaci\u00f3n, para el problema del aprendizaje secuencial utilizando un modelo de tama\u00f1o fijo que a\u00f1ade t\u00e9rminos adicionales a la p\u00e9rdida, fomentando la dispersi\u00f3n de la representaci\u00f3n y combatiendo el olvido catastr\u00f3fico.", "Este trabajo aborda el problema del olvido catastr\u00f3fico en el aprendizaje permanente proponiendo estrategias de aprendizaje regularizadas"]}
{"source": ["A Synaptic Neural Network (SynaNN) consists of synapses and neurons.", "Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear synapse function of excitatory and inhibitory channel probabilities.", "Introduced the concept of surprisal space and constructed a commutative diagram, we proved that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the topologically conjugate function of the inhibitory complementary probability 1-x in probability space.", "Furthermore, we found that the derivative of the synapse over the parameter in the surprisal space is equal to the negative Bose-Einstein distribution.", "In addition, we constructed a fully connected synapse graph (tensor) as a synapse block of a synaptic neural network.", "Moreover, we proved the gradient formula of a cross-entropy loss function over parameters, so synapse learning can work with the gradient descent and backpropagation algorithms.", "In the proof-of-concept experiment, we performed an MNIST training and testing on the MLP model with synapse network as hidden layers."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryGpEiAcFQ", "target": ["Una red neuronal sin\u00e1ptica con gr\u00e1fico de sinapsis y aprendizaje que tiene la caracter\u00edstica de conjugaci\u00f3n topol\u00f3gica y distribuci\u00f3n de Bose-Einstein en el espacio de sorpresa.  ", "Los autores proponen una red neuronal h\u00edbrida compuesta por un grafo de sinapsis que puede integrarse en una red neuronal est\u00e1ndar", "Presenta un modelo de red neuronal de inspiraci\u00f3n biol\u00f3gica basado en los canales i\u00f3nicos excitatorios e inhibitorios de las membranas de las c\u00e9lulas reales"]}
{"source": ["Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs.", "Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far.", "In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework.", "Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "SJd0EAy0b", "target": ["Modelos de incrustaci\u00f3n de gr\u00e1ficos generalizados", "Un enfoque de incrustaci\u00f3n de grafos de conocimiento generalizado que aprende las incrustaciones bas\u00e1ndose en tres objetivos simult\u00e1neos diferentes, y rinde a la par o incluso supera los enfoques existentes del estado del arte.", "Aborda la tarea de aprender incrustaciones de grafos multirrelacionales mediante una red neuronal", "Propone un nuevo m\u00e9todo, GEN, para calcular incrustaciones de grafos multirelacionales, en particular que las llamadas E-Cells y R-Cells pueden responder a consultas de la forma (h,r,?),(?r,t), y (h,?,t)"]}
{"source": ["We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning.", "The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages.", "At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset.", "MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement.", "We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods.", "We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance.", "Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set.", "Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BywyFQlAW", "target": ["El Aprendizaje Curricular Minimax es un m\u00e9todo de ense\u00f1anza de m\u00e1quinas que implica el aumento de la dureza deseable y la reducci\u00f3n programada de la diversidad.", " Un enfoque de aprendizaje curricular que utiliza una funci\u00f3n de conjunto submodular que captura la diversidad de los ejemplos elegidos durante el entrenamiento. ", "El art\u00edculo presenta el aprendizaje curricular MiniMax como un enfoque para el entrenamiento adaptativo de modelos proporcion\u00e1ndole diferentes subconjuntos de datos. "]}
{"source": ["Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference.", "However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases.", "In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes?", "How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships?", "To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.", "For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density.", "For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples.", "In experiments, we scale Bayesian inference on up to a billion genetic measurements.", "We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyELrEeAb", "target": ["Modelos impl\u00edcitos aplicados a la causalidad y la gen\u00e9tica", "Los autores proponen utilizar el modelo impl\u00edcito para abordar el problema de la asociaci\u00f3n de todo el genoma.", "Este trabajo propone soluciones para los problemas de los estudios de asociaci\u00f3n de todo el genoma de confusi\u00f3n debido a la estructura de la poblaci\u00f3n y la posible presencia de interacciones no lineales entre diferentes partes del genoma, y tiende un puente entre la gen\u00e9tica estad\u00edstica y el ML.", "Presenta un modelo generativo no lineal para GWAS que modela la estructura de la poblaci\u00f3n donde las no linealidades se modelan usando redes neuronales como aproximadores de funciones no lineales y la inferencia se realiza usando inferencia variacional sin verosimilitud"]}
{"source": ["\nFew-shot learning trains image classifiers over datasets with few examples per category. \n", "It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. \n", "Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories.", "In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.\n", "For a new category, even though its images are not seen by the model, some objects may appear in the training images.", "Hence, object-level relation is useful for inferring the relation of images from unseen categories.", "Consequently, our model generalizes well for new categories without fine-tuning.\n", "Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkzcvoA9YX", "target": ["Aprendizaje de pocos disparos explotando la relaci\u00f3n a nivel de objeto para aprender la relaci\u00f3n a nivel de imagen (similitud)", "Este art\u00edculo aborda el problema del aprendizaje de pocos disparos proponiendo un enfoque basado en la incrustaci\u00f3n que aprende a comparar las caracter\u00edsticas a nivel de objeto entre los ejemplos del conjunto de apoyo y de consulta", "Propone un m\u00e9todo de aprendizaje de pocas tomas que explota la relaci\u00f3n a nivel de objeto entre las diferentes im\u00e1genes, basado en la b\u00fasqueda de vecinos cercanos, y concatena los mapas de caracter\u00edsticas de dos im\u00e1genes de entrada en un solo mapa de caracter\u00edsticas"]}
{"source": ["Word embeddings are widely used in machine learning based natural language processing systems.", "It is common to use pre-trained word embeddings which provide benefits such as reduced training time and improved overall performance.", "There has been a recent interest in applying natural language processing techniques to programming languages.", "However, none of this recent work uses pre-trained embeddings on code tokens.", "Using extreme summarization as the downstream task, we show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\\% improvement in test loss, 4\\% improvement in F1 scores, and resistance to over-fitting.", "We also show that the choice of language used for the embeddings does not have to match that of the task to achieve these benefits and that even embeddings pre-trained on human languages provide these benefits to programming languages.   "], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "H1glKiCqtm", "target": ["Los investigadores que exploran las t\u00e9cnicas de procesamiento del lenguaje natural aplicadas al c\u00f3digo fuente no utilizan ninguna forma de incrustaci\u00f3n preentrenada, nosotros demostramos que deber\u00edan hacerlo.", "Este art\u00edculo se propone comprender si el preentrenamiento de las incrustaciones de palabras para el c\u00f3digo del lenguaje de programaci\u00f3n mediante el uso de modelos de lenguaje similares a los de la PNL tiene un impacto en la tarea de resumen de c\u00f3digo extremo.", "Este trabajo muestra c\u00f3mo el preentrenamiento de vectores de palabras utilizando corpus de c\u00f3digo conduce a representaciones que son m\u00e1s adecuadas que las representaciones inicializadas y entrenadas al azar para la predicci\u00f3n de nombres de funciones/m\u00e9todos"]}
{"source": ["Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data.", "These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network).", "In these two-player games, a reward is always received at the end of the game.", "However, the Rubik\u2019s Cube has only a single solved state, and episodes are not guaranteed to terminate.", "This poses a major problem for these API algorithms since they rely on the reward received at the end of the game.", "We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away.", "Autodidactic Iteration is able to learn how to solve the Rubik\u2019s Cube and the 15-puzzle without relying on human data.", "Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves \u2014 less than or equal to solvers that employ human domain knowledge."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Hyfn2jCcKm", "target": ["Resolvemos el cubo de Rubik con aprendizaje por refuerzo puro", "Soluci\u00f3n a la resoluci\u00f3n del cubo de Rubik utilizando el aprendizaje por refuerzo (RL) con la b\u00fasqueda en \u00e1rbol de Monte-Carlo (MCTS) a trav\u00e9s de la iteraci\u00f3n autodid\u00e1ctica. ", "Este trabajo resuelve el cubo de Rubik utilizando un m\u00e9todo de iteraci\u00f3n de pol\u00edtica aproximada llamado iteraci\u00f3n autodid\u00e1ctica, superando el problema de las recompensas dispersas mediante la creaci\u00f3n de su propio sistema de recompensas.", "Introduce un algoritmo RL profundo para resolver el cubo de Rubik que maneja el enorme espacio de estados y la recompensa muy dispersa del cubo de Rubik"]}
{"source": ["Answering compositional questions requiring multi-step reasoning is challenging for current models.", "We introduce an end-to-end differentiable model for interpreting questions, which is inspired by formal approaches to semantics.", "Each span of text is represented by a denotation in a knowledge graph, together with a vector that captures ungrounded aspects of meaning.", "Learned composition modules recursively combine constituents, culminating in a grounding for the complete sentence which is an answer to the question.", "For example, to interpret \u2018not green\u2019, the model will represent \u2018green\u2019 as a set of entities, \u2018not\u2019 as a trainable ungrounded vector, and then use this vector to parametrize a composition function to perform a complement operation.", "For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent.", "We show the model can learn to represent a variety of challenging semantic operators, such as quantifiers, negation, disjunctions and composed relations on a synthetic question answering task.", "The model also generalizes well to longer sentences than seen in its training data, in contrast to LSTM and RelNet baselines.", "We will release our code."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkaqxm-0b", "target": ["Describimos un modelo diferenciable de extremo a extremo para la GC que aprende a representar tramos de texto en la pregunta como denotaciones en el grafo de conocimiento, mediante el aprendizaje de m\u00f3dulos neuronales para la composici\u00f3n y la estructura sint\u00e1ctica de la frase.", "Este art\u00edculo presenta un modelo para responder a preguntas visuales que puede aprender tanto los par\u00e1metros como los predictores de estructura para una red neuronal modular, sin estructuras supervisadas ni asistencia de un analizador sint\u00e1ctico.", "Propone entrenar un modelo de respuesta a preguntas a partir de las respuestas \u00fanicamente y una KB mediante el aprendizaje de \u00e1rboles latentes que capturan la sintaxis y aprenden la sem\u00e1ntica de las palabras"]}
{"source": ["Deep learning software demands reliability and performance.", "However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter.", "We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM.", "Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity.", "With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryG6xZ-RZ", "target": ["Presentamos una novedosa infraestructura de compilaci\u00f3n que resuelve las deficiencias de los marcos de aprendizaje profundo existentes.", "Propuesta para pasar de la generaci\u00f3n de c\u00f3digo ad-hoc en los motores de aprendizaje profundo a las mejores pr\u00e1cticas de compiladores y lenguajes.", "Este trabajo presenta un marco de compilaci\u00f3n que permite definir lenguajes espec\u00edficos de dominio para sistemas de aprendizaje profundo, y define etapas de compilaci\u00f3n que pueden aprovechar optimizaciones est\u00e1ndar y optimizaciones especializadas para redes neuronales.", "Este trabajo introduce un DLVM para aprovechar los aspectos del compilador de un tensor"]}
{"source": ["In this work, we focus on the problem of grounding language by training an agent\n", "to follow a set of natural language instructions and navigate to a target object\n", "in a 2D grid environment.", "The agent receives visual information through raw\n", "pixels and a natural language instruction telling what task needs to be achieved.\n", "Other than these two sources of information, our model does not have any prior\n", "information of both the visual and textual modalities and is end-to-end trainable.\n", "We develop an attention mechanism for multi-modal fusion of visual and textual\n", "modalities that allows the agent to learn to complete the navigation tasks and also\n", "achieve language grounding.", "Our experimental results show that our attention\n", "mechanism outperforms the existing multi-modal fusion mechanisms proposed in\n", "order to solve the above mentioned navigation task.", "We demonstrate through the\n", "visualization of attention weights that our model learns to correlate attributes of\n", "the object referred in the instruction with visual representations and also show\n", "that the learnt textual representations are semantically meaningful as they follow\n", "vector arithmetic and are also consistent enough to induce translation between instructions\n", "in different natural languages.", "We also show that our model generalizes\n", "effectively to unseen scenarios and exhibit zero-shot generalization capabilities.\n", "In order to simulate the above described challenges, we introduce a new 2D environment\n", "for an agent to jointly learn visual and textual modalities"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJPSN3gRW", "target": ["Arquitectura basada en la atenci\u00f3n para el aprendizaje del lenguaje mediante el aprendizaje por refuerzo en un nuevo entorno cuadriculado 2D personalizable  ", "El art\u00edculo aborda el problema de la navegaci\u00f3n dada una instrucci\u00f3n y propone un enfoque para combinar la informaci\u00f3n textual y visual a trav\u00e9s de un mecanismo de atenci\u00f3n", "Este trabajo considera el problema de seguir instrucciones en lenguaje natural dada una visi\u00f3n en primera persona de un entorno a priori desconocido, y propone un m\u00e9todo de arquitectura neuronal.", "Estudia el problema de la navegaci\u00f3n hacia un objeto objetivo en un entorno de cuadr\u00edcula 2D siguiendo una descripci\u00f3n en lenguaje natural dada y recibiendo informaci\u00f3n visual como p\u00edxeles en bruto."]}
{"source": [" Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention.", "Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs.", "We propose a new Q\\&A architecture called QANet, which does not require recurrent networks:  Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions.", "On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models.", "The speed-up gain allows us to train the model with much more data.", "We hence combine our model with data generated by backtranslation from a neural machine translation model. \n", "On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B14TlG-RW", "target": ["Una arquitectura simple, compuesta por convoluciones y atenci\u00f3n, logra resultados a la par de los modelos recurrentes mejor documentados.", "Un m\u00e9todo de aumento de datos r\u00e1pido y de alto rendimiento basado en la par\u00e1frasis y un modelo de comprensi\u00f3n de lectura no recurrente que utiliza s\u00f3lo convoluciones y atenci\u00f3n.", "Este trabajo propone aplicar CNNs+m\u00f3dulos de autoatenci\u00f3n en lugar de LSTMs y mejorar el entrenamiento del modelo de RC con par\u00e1frasis de pasajes generadas por un modelo neural de par\u00e1frasis para mejorar el rendimiento de la RC.", "Este trabajo presenta un modelo de comprensi\u00f3n lectora que utiliza convoluciones y atenci\u00f3n y propone aumentar los datos de entrenamiento adicionales mediante la par\u00e1frasis basada en la traducci\u00f3n autom\u00e1tica neural."]}
{"source": ["Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images.", "However, a number of problems of recent interest have created a demand for models that can analyze spherical images.", "Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.", "A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.\n\n", "In this paper we introduce the building blocks for constructing spherical CNNs.", "We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant.", "The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.", "We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Hkbd5xZRb", "target": ["Introducimos las CNN esf\u00e9ricas, una red convolucional para se\u00f1ales esf\u00e9ricas, y la aplicamos al reconocimiento de modelos 3D y a la regresi\u00f3n de la energ\u00eda molecular.", "El art\u00edculo propone un marco para construir redes convolucionales esf\u00e9ricas basado en una novedosa s\u00edntesis de varios conceptos existentes", "Este trabajo se centra en c\u00f3mo ampliar las redes neuronales convolucionales para que tengan invariancia esf\u00e9rica incorporada, y adapta herramientas del an\u00e1lisis arm\u00f3nico no abeliano para lograr este objetivo.", "Los autores desarrollan un novedoso esquema de representaci\u00f3n de datos esf\u00e9ricos desde la base"]}
{"source": ["We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks.", "Our approach works by training a neural network to mimic the fitness function of a design optimization task and then, using the differential nature of the neural network, perform gradient decent to maximize the fitness.", "We demonstrate this methods effectiveness by designing an optimized heat sink and both 2D and 3D airfoils that maximize the lift drag ratio under steady state flow conditions.", "We highlight that our method has two distinct benefits over other automated design approaches.", "First, evaluating the neural networks prediction of fitness can be orders of magnitude faster then simulating the system of interest.", "Second, using gradient decent allows the design space to be searched much more efficiently then other gradient free methods.", "These two strengths work together to overcome some of the current shortcomings of automated design."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByaQIGg0-", "target": ["Un m\u00e9todo para realizar el dise\u00f1o automatizado de objetos del mundo real, como disipadores de calor y perfiles de alas, que hace uso de redes neuronales y descenso de gradiente.", "Red neuronal (parametrizaci\u00f3n y predicci\u00f3n) y descenso de gradiente (back propogation) para el dise\u00f1o autom\u00e1tico de tareas de ingenier\u00eda. ", "Este trabajo introduce el uso de una red profunda para aproximar el comportamiento de un sistema f\u00edsico complejo, y luego dise\u00f1ar dispositivos \u00f3ptimos optimizando esta red con respecto a sus entradas."]}
{"source": ["Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results.", "However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization.", "We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy.", "Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions.", "We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits.", "In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BkA7gfZAb", "target": [" Proponemos una versi\u00f3n dual de la distancia adversarial log\u00edstica para la alineaci\u00f3n de caracter\u00edsticas y mostramos que produce iteraciones de pasos de gradiente m\u00e1s estables que el objetivo min-max.", "El documento aborda la fijaci\u00f3n de las GAN a nivel computacional", "Este trabajo estudia una formulaci\u00f3n dual de una p\u00e9rdida adversarial basada en un l\u00edmite superior de la p\u00e9rdida log\u00edstica, y convierte el problema est\u00e1ndar min max de entrenamiento adversarial en un \u00fanico problema de minimizaci\u00f3n.", "Propone reformular el objetivo del punto de equilibrio de GAN (para un discriminador de regresi\u00f3n log\u00edstica) como un problema de minimizaci\u00f3n mediante la dualizaci\u00f3n del objetivo de m\u00e1xima probabilidad para la regresi\u00f3n log\u00edstica regularizada"]}
{"source": ["  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized.", "Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective.", "In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations.", "Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies.", "Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs.", "Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution.", "These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations.", "We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude).", "Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Sk6fD5yCb", "target": ["implementaci\u00f3n de redes neuronales binarias con rendimiento computacional de \u00faltima generaci\u00f3n", "El art\u00edculo presenta una biblioteca escrita en C/CUDA que cuenta con todas las funcionalidades necesarias para la propagaci\u00f3n hacia delante de las BCNN", "Este trabajo se basa en Binary-NET y lo ampl\u00eda a las arquitecturas CNN, proporciona optimizaciones que mejoran la velocidad del paso hacia delante y ofrece un c\u00f3digo optimizado para Binary CNN."]}
{"source": ["Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization.", "In this paper, we propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization.", "We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task.", "Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set.", "We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient based training.", "The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1GaAjRcF7", "target": ["Proponemos un algoritmo de selecci\u00f3n de subconjuntos que se puede entrenar con m\u00e9todos basados en el gradiente, pero que logra un rendimiento casi \u00f3ptimo mediante la optimizaci\u00f3n submodular.", "Propone un modelo basado en redes neuronales que integra la funci\u00f3n submodular combinando la t\u00e9cnica de optimizaci\u00f3n basada en el gradiente con el marco submodular denominado 'Differentiable Greedy Network' (DGN).", "Propone una red neuronal que tiene como objetivo seleccionar un subconjunto de elementos (por ejemplo, seleccionar k frases que est\u00e1n mayormente relacionadas con una reclamaci\u00f3n de un conjunto de documentos recuperados)"]}
{"source": ["The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years.", "In spite of the advance, clustering with representation learning has been limited to flat-level categories, which oftentimes involves cohesive clustering with a focus on instance relations.", "To overcome the limitations of flat clustering, we introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space.", "Specifically, we place a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies under the variational autoencoder framework, and to adopt the generative process of a hierarchical-versioned Gaussian mixture model.", "Compared with a few prior works focusing on unifying representation learning and hierarchical clustering, HCRL is the first model to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components.", "This generation process enables more meaningful separations and mergers of clusters via branches in a hierarchy.", "In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features.", "We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1ERcs09KQ", "target": ["Introducimos el aprendizaje de representaci\u00f3n jer\u00e1rquica agrupada (HCRL), que optimiza simult\u00e1neamente el aprendizaje de representaci\u00f3n y la agrupaci\u00f3n jer\u00e1rquica en el espacio de incrustaci\u00f3n.", "El documento propone utilizar el CRP anidado como modelo de agrupaci\u00f3n en lugar de un modelo tem\u00e1tico", "Presenta un nuevo m\u00e9todo de agrupaci\u00f3n jer\u00e1rquica sobre un espacio de incrustaci\u00f3n en el que se aprenden simult\u00e1neamente el espacio de incrustaci\u00f3n y la agrupaci\u00f3n jer\u00e1rquica"]}
{"source": ["We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers.", "Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry.", "Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers.", "This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition).", "We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJeapjA5FX", "target": ["Desarrollamos un marco de aumento del aprendizaje estad\u00edstico-geom\u00e9trico no supervisado para las redes neuronales profundas con el fin de hacerlas robustas a los ataques adversarios.", "Transfiere las redes neuronales profundas tradicionales a calsificadores robustos adversarios utilizando GRNs", "Propone una defensa basada en distribuciones de caracter\u00edsticas condicionales de clase para convertir las redes neuronales profundas en clasificadores robustos"]}
{"source": ["Reinforcement learning in environments with large state-action spaces is challenging, as exploration can be highly inefficient.", "Even if the dynamics are simple, the optimal policy can be combinatorially hard to discover.", "In this work, we propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces.", "The key idea is to model a stochastic policy as a hierarchical latent variable model, which can learn low-dimensional structure in the state-action space, and to define exploration by sampling from the low-dimensional latent space.", "This approach enables lower sample complexity, while preserving policy expressivity.", "In order to make learning tractable, we derive a joint learning and exploration strategy by combining hierarchical variational inference with actor-critic learning.", "The benefits of our learning approach are that", "1) it is principled,", "2) simple to implement,", "3) easily scalable to settings with many actions and", "4) easily composable with existing deep learning approaches.", "We demonstrate the effectiveness of our approach on learning a deep centralized multi-agent policy, as multi-agent environments naturally have an exponentially large state-action space.", "In this setting, the latent hierarchy implements a form of multi-agent coordination during exploration and execution (MACE).", "We demonstrate empirically that MACE can more efficiently learn optimal policies in challenging multi-agent games with a large number (~20) of agents, compared to conventional baselines.", "Moreover, we show that our hierarchical structure leads to meaningful agent coordination."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyunpgbR-", "target": ["Hacer m\u00e1s eficiente el aprendizaje por refuerzo profundo en grandes espacios estado-acci\u00f3n utilizando la exploraci\u00f3n estructurada con pol\u00edticas jer\u00e1rquicas profundas.", "Un m\u00e9todo para coordinar el comportamiento de los agentes mediante el uso de pol\u00edticas que tienen una estructura latente compartida, un m\u00e9todo de optimizaci\u00f3n de pol\u00edticas variacional para optimizar las pol\u00edticas coordinadas, y una derivaci\u00f3n de la actualizaci\u00f3n jer\u00e1rquica y variacional de los autores.", "Este trabajo sugiere una innovaci\u00f3n algor\u00edtmica consistente en variables latentes jer\u00e1rquicas para la exploraci\u00f3n coordinada en entornos multiagente"]}
{"source": ["Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance.", "Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks.", "However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks.", "We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks.", "In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR.", "Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size.", "This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data.", "Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent.", "Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryfMLoCqtQ", "target": ["Aportamos muchas ideas sobre la generalizaci\u00f3n de las redes neuronales a partir del caso lineal te\u00f3ricamente manejable.", "Los autores estudian un modelo sencillo de redes lineales para comprender el aprendizaje de generalizaci\u00f3n y transferencia"]}
{"source": ["We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training in this work, which is believed to play a critical role in addressing the gradient vanishing/explosion problem.", "Specifically, by analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation.", "As a result, the gradient vanishing/explosion problem is avoided.", "Furthermore, we use the same analysis to discuss the tradeoff between depth and width of a residual network and demonstrate that shallower yet wider resnets have stronger learning performance than deeper yet thinner resnets."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "r1Kr3TyAb", "target": ["La normalizaci\u00f3n por lotes mantiene la varianza del gradiente a lo largo del entrenamiento, estabilizando as\u00ed la optimizaci\u00f3n.", "Este trabajo analiz\u00f3 el efecto de la normalizaci\u00f3n de lotes en la retropropagaci\u00f3n de gradiente en redes residuales"]}
{"source": ["To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories.", "These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments.", "Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes.", "We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryxSrhC9KX", "target": ["Los juicios del comportamiento humano se utilizan para obtener representaciones escasas e interpretables de los objetos que se generalizan a otras tareas", "Este art\u00edculo describe un experimento a gran escala sobre las representaciones humanas de objetos/sem\u00e1ticas y un modelo de dichas representaciones.", "Este trabajo desarrolla un nuevo sistema de representaci\u00f3n de objetos a partir del entrenamiento en datos recogidos de juicios humanos de im\u00e1genes impares.", "Un nuevo enfoque para aprender un espacio sem\u00e1ntico escaso, positivo e interpretable que maximiza los juicios de similitud humana mediante el entrenamiento para maximizar espec\u00edficamente la predicci\u00f3n de los juicios de similitud humana."]}
{"source": ["We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. \n\n", "We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers.", "The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. \n\n", "The reformulation system is trained end-to-end to maximize answer quality using policy gradient.", "We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!.", "The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks.\n\nWe also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1CChZ-CZ", "target": ["Proponemos un agente que se sit\u00faa entre el usuario y un sistema de respuesta de preguntas de caja negra y que aprende a reformular las preguntas para obtener las mejores respuestas posibles", "Este trabajo propone una respuesta activa a las preguntas mediante un enfoque de aprendizaje por refuerzo que aprende a reformular las preguntas de manera que ofrezcan las mejores respuestas posibles.", "Describe claramente c\u00f3mo los investigadores dise\u00f1aron y entrenaron activamente dos modelos para la reformulaci\u00f3n de preguntas y la selecci\u00f3n de respuestas durante los episodios de respuesta a preguntas"]}
{"source": ["Most deep latent factor models choose simple priors for simplicity, tractability\n", "or not knowing what prior to use.", "Recent studies show that the choice of\n", "the prior may have a profound effect on the expressiveness of the model,\n", "especially when its generative network has limited capacity.", "In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n", "(AAEs).", "We introduce the notion of code generators to transform manually selected\n", "simple priors into ones that can better characterize the data distribution.", "Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\n", "AAEs in both supervised and unsupervised settings.", "Lastly, we present its\n", "ability to do cross-domain translation in a  text-to-image synthesis task."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJSr0GZR-", "target": ["Aprendizaje de prebendas para autocodificadores adversariales", "Propone una extensi\u00f3n sencilla de los autocodificadores adversariales para la generaci\u00f3n de im\u00e1genes condicionales.", "Se centra en los autocodificadores adversarios e introduce una red de generadores de c\u00f3digo para transformar una prioridad simple en una que, junto con el generador, pueda ajustarse mejor a la distribuci\u00f3n de los datos"]}
{"source": ["In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs).", "GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer.", "In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data.", "Attempts have been made for utilizing GANs with word embeddings for text generation.", "This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures.", "The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SkGMOi05FQ", "target": ["Generaci\u00f3n de texto utilizando incrustaciones de frases a partir de vectores de salto de pensamiento con la ayuda de redes adversariales generativas.", "Describe la aplicaci\u00f3n de las redes generativas adversariales para el modelado de datos textuales con la ayuda de vectores de pensamiento de esqu\u00ed y los experimentos con diferentes sabores de GAN para dos conjuntos de datos diferentes."]}
{"source": ["The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models.", "It works in a streaming fashion and avoids backtracking through past activations and inputs.", "UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  ", "Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed.", "On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  ", "On synthetic tasks where truncated BPTT is shown to diverge, UORO converges.", "For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n"], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJQDjk-0b", "target": ["Introduce una estimaci\u00f3n de gradiente en l\u00ednea, insesgada y f\u00e1cilmente implementable para modelos recurrentes.", "Los autores introducen un enfoque novedoso para el aprendizaje en l\u00ednea de los par\u00e1metros de las redes neuronales recurrentes a partir de secuencias largas que supera la imitaci\u00f3n de la retropropagaci\u00f3n truncada a trav\u00e9s del tiempo", "Este trabajo aborda el entrenamiento en l\u00ednea de las RNNs de una forma basada en principios, y propone una modificaci\u00f3n de la RTRL y el uso de un enfoque hacia adelante para el c\u00e1lculo del gradiente."]}
{"source": ["We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution) labels, given the joint distribution between those low- and high-resolution labels.", "This method involves a novel loss function that minimizes the distance between a distribution determined by a set of model outputs and the corresponding distribution given by low-resolution labels over the same set of outputs.", "This setup does not require that the high-resolution classes match the low-resolution classes and can be used in high-resolution semantic segmentation tasks where high-resolution labeled data is not available.", "Furthermore, our proposed method is able to utilize both data with low-resolution labels and any available high-resolution labels, which we show improves performance compared to a network trained only with the same amount of high-resolution data.\n", "We test our proposed algorithm in a challenging land cover mapping task to super-resolve labels at a 30m resolution to a separate set of labels at a 1m resolution.", "We compare our algorithm with models that are trained on high-resolution data and show that", "1) we can achieve similar performance using only low-resolution data; and", "2) we can achieve better performance when we incorporate a small amount of high-resolution data in our training.", "We also test our approach on a medical imaging problem, resolving low-resolution probability maps into high-resolution segmentation of lymphocytes with accuracy equal to that of fully supervised models."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkxwShA9Ym", "target": ["Superresoluci\u00f3n de etiquetas gruesas en etiquetas a nivel de p\u00edxel, aplicada a im\u00e1genes a\u00e9reas y escaneos m\u00e9dicos.", "Un m\u00e9todo para superar las etiquetas de segmentaci\u00f3n de baja resoluci\u00f3n si se conoce la distribuci\u00f3n conjunta de las etiquetas de baja y alta resoluci\u00f3n."]}
{"source": ["We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions.", "Our approach assumes that the two datasets are sampled from a common latent space, i.e., they measure equivalent systems.", "Thus, we expect there to exist a natural (albeit unknown) alignment of the data manifolds associated with the intrinsic geometry of these datasets, which are perturbed by measurement artifacts in the sampling process.", "Importantly, we do not assume any individual correspondence (partial or complete) between data points.", "Instead, we rely on our assumption that a subset of data features have correspondence across datasets.", "We leverage this assumption to estimate relations between intrinsic manifold dimensions, which are given by diffusion map coordinates over each of the datasets.", "We compute a correlation matrix between diffusion coordinates of the datasets by considering graph (or manifold) Fourier coefficients of corresponding data features.", "We then orthogonalize this correlation matrix to form an isometric transformation between the diffusion maps of the datasets.", "Finally, we apply this transformation to the diffusion coordinates and construct a unified diffusion geometry of the datasets together.", "We show that this approach successfully corrects misalignment artifacts, and allows for integrated data."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkGNrnC9FQ", "target": ["Proponemos un m\u00e9todo para alinear las caracter\u00edsticas latentes aprendidas de diferentes conjuntos de datos utilizando correlaciones arm\u00f3nicas.", "Propone utilizar las correspondencias de rasgos para preformar la alineaci\u00f3n de colectores entre lotes de datos de las mismas muestras para evitar la recogida de mediciones ruidosas."]}
{"source": ["Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments.", "When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment.", "However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations.", "Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. \n", "We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure.", "Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance.", "We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. \n", "We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena.", "Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. \n\n", "A video is available at: www.youtube.com/watch?v=eei6Rgom3YY"], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJgWl3A5YX", "target": ["La evoluci\u00f3n de la forma del cuerpo en agentes controlados por RL mejora su rendimiento (y ayuda al aprendizaje)", "Algoritmo PEOM que incorpora el valor Shapley para acelerar la evoluci\u00f3n identificando la contribuci\u00f3n de cada parte del cuerpo"]}
{"source": ["Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never.", "Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy.", "In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes.", "Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe.", "This score acts as a penalty on the Q-learning objective.", "Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions.", "Our analysis also shows robustness to classification errors.", "Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "B16yEqkCZ", "target": ["Dar forma a la recompensa con motivaci\u00f3n intr\u00ednseca para evitar estados catastr\u00f3ficos y mitigar el olvido catastr\u00f3fico.", "Un algoritmo RL que combina el algoritmo DQN con un modelo de miedo entrenado en paralelo para predecir estados catastr\u00f3ficos.", "El art\u00edculo estudia el olvido catastr\u00f3fico en la RL, haciendo hincapi\u00e9 en las tareas en las que un DQN es capaz de aprender a evitar los eventos catastr\u00f3ficos siempre que evite el olvido."]}
{"source": ["Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks.", "Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere S^2 or a unit ball B^3---entails unique challenges.", "In this work, we propose a novel `\"volumetric convolution\" operation that can effectively convolve arbitrary functions in B^3.", "We develop a theoretical framework for \"volumetric convolution\" based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks.", "Furthermore, our formulation leads to derivation of a  novel formula to measure the symmetry of a function in B^3 around an arbitrary axis, that is useful in 3D shape analysis tasks.", "We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkfhIo0qtQ", "target": ["Un nuevo operador de convoluci\u00f3n para el aprendizaje autom\u00e1tico de la representaci\u00f3n dentro de la bola unitaria", "Este trabajo est\u00e1 relacionado con los recientes art\u00edculos sobre la CNN esf\u00e9rica y la red equivariante SE(n) y extiende las ideas anteriores a los datos volum\u00e9tricos en la bola unitaria.", "Propone el uso de convoluciones volum\u00e9tricas en redes convolutivas para el aprendizaje de la bola unitaria y discute la metodolog\u00eda y los resultados del proceso."]}
{"source": ["Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent\u2019s learning through trial-and-error.", "For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large.", "Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions.", "We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from.", "Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions.", "In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively.", "We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions.", "We evaluate the ability of our agent to generalize to new instructions onWorld of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions.", "The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJemQ209FQ", "target": ["Entrenamos pol\u00edticas de aprendizaje por refuerzo utilizando el aumento de la recompensa, el aprendizaje curricular y el metaaprendizaje para navegar con \u00e9xito por las p\u00e1ginas web.", "Desarrolla un m\u00e9todo de aprendizaje curricular para entrenar a un agente RL a navegar por una web, basado en la idea de descomponer una instrucci\u00f3n en m\u00faltiples sub-instrucciones."]}
{"source": ["Labeled text classification datasets are typically only available in a few select languages.", "In order to train a model for e.g news categorization in a language $L_t$ without a suitable text classification dataset there are two options.", "The first option is to create a new labeled dataset by hand, and the second option is to transfer label information from an existing labeled dataset in a source language $L_s$ to the target language $L_t$. In this paper we propose a method for sharing label information across languages by means of a language independent text encoder.", "The encoder will give almost identical representations to multilingual versions of the same text.", "This means that labeled data in one language can be used to train a classifier that works for the rest of the languages.", "The encoder is trained independently of any concrete classification task and can therefore subsequently be used for any classification task.  ", "We show that it is possible to obtain good performance even in the case where only a comparable corpus of texts is available."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1XXq6lRW", "target": ["Clasificaci\u00f3n de textos en varias lenguas mediante codificaci\u00f3n universal", "Este art\u00edculo propone un enfoque para la clasificaci\u00f3n de textos multiling\u00fces mediante el uso de corpus comparables.", "Aprender incrustaciones multiling\u00fces y entrenar un clasificador utilizando datos etiquetados en la lengua de origen para abordar el aprendizaje de un categorizador de texto multiling\u00fce sin informaci\u00f3n etiquetada en la lengua de destino"]}
{"source": ["Syntax is a powerful abstraction for language understanding.", "Many downstream tasks require segmenting input text into meaningful constituent chunks (e.g., noun phrases or entities); more generally, models for learning semantic representations of text benefit from integrating syntax in the form of parse trees (e.g., tree-LSTMs).", "Supervised parsers have traditionally been used to obtain these trees, but lately interest has increased in unsupervised methods that induce syntactic representations directly from unlabeled text.", "To this end, we propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree.", "Unlike many prior approaches, DIORA does not rely on supervision from auxiliary downstream tasks and is thus not constrained to particular domains.", "Furthermore, competing approaches do not learn explicit phrase representations along with tree structures, which limits their applicability to phrase-based tasks.", "Extensive experiments on unsupervised parsing, segmentation, and phrase clustering demonstrate the efficacy of our method.", "DIORA achieves the state of the art in unsupervised parsing (46.9 F1) on the benchmark WSJ dataset."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJeq43AqF7", "target": ["En este trabajo proponemos los autocodificadores recursivos internos-externos profundos (DIORA), un m\u00e9todo totalmente no supervisado para descubrir la sintaxis mientras se aprenden simult\u00e1neamente representaciones para los constituyentes descubiertos. ", "Un modelo neural de \u00e1rbol latente entrenado con un objetivo de autocodificaci\u00f3n que alcanza el estado del arte en el an\u00e1lisis sint\u00e1ctico no supervisado de constituyentes y captura la estructura sint\u00e1ctica mejor que otros modelos de \u00e1rbol latente.", "El art\u00edculo propone un modelo de an\u00e1lisis sint\u00e1ctico de dependencias no supervisado (inducci\u00f3n de \u00e1rboles latentes) que se basa en una combinaci\u00f3n del algoritmo inside-outside con el modelado neuronal (autocodificadores recursivos). "]}
{"source": ["Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training.", "There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled.", "But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training.", "We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias.", "We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons.", "We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area.", "We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "H1MczcgR-", "target": ["Investigamos el sesgo en el objetivo de meta-optimizaci\u00f3n de horizonte corto.", "Este trabajo propone un modelo y un problema simplificados para demostrar el sesgo de horizonte corto de la meta-optimizaci\u00f3n de la tasa de aprendizaje.", "Este trabajo estudia la cuesti\u00f3n de la retropropagaci\u00f3n truncada para la meta-optimizaci\u00f3n a trav\u00e9s de una serie de experimentos sobre un problema de juguete"]}
{"source": ["Mainstream captioning models often follow a sequential structure to generate cap-\n", "tions, leading to issues such as introduction of irrelevant semantics, lack of diversity\n", "in the generated captions, and inadequate generalization performance.", "In this paper,\n", "we present an alternative paradigm for image captioning, which factorizes the\n", "captioning procedure into two stages: (1) extracting an explicit semantic represen-\n", "tation from the given image; and (2) constructing the caption based on a recursive\n", "compositional procedure in a bottom-up manner.", "Compared to conventional ones,\n", "our paradigm better preserves the semantic content through an explicit factorization\n", "of semantics and syntax.", "By using the compositional generation procedure, caption\n", "construction follows a recursive structure, which naturally fits the properties of\n", "human language.", "Moreover, the proposed compositional procedure requires less\n", "data to train, generalizes better, and yields more diverse captions."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SJxyZ81IYQ", "target": ["una forma jer\u00e1rquica y compositiva de generar subt\u00edtulos", "Este art\u00edculo presenta un m\u00e9todo m\u00e1s interpretable para el subtitulado de im\u00e1genes."]}
{"source": ["While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data.", "Measures for characterizing and monitoring structural properties, however, have not been developed.", "In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs.", "To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization.", "Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByxkijC5FQ", "target": ["Desarrollamos una nueva medida de complejidad topol\u00f3gica para las redes neuronales profundas y demostramos que captura sus propiedades m\u00e1s destacadas.", "Este trabajo propone la noci\u00f3n de persistencia neuronal, una medida topol\u00f3gica para asignar puntuaciones a las capas totalmente conectadas de una red neuronal.", "El art\u00edculo propone analizar la complejidad de una red neuronal utilizando su homolog\u00eda persistente cero."]}
{"source": ["Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs.", "Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models.", "However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls.", "In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models.", "First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations.", "These examples successfully evade a defense that only considers a small ball around an input instance.", "Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes.", "We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples.", "Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples.", "Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BkpiPMbA-", "target": ["Observar los l\u00edmites de la decisi\u00f3n en torno a una entrada le da m\u00e1s informaci\u00f3n que un peque\u00f1o barrio fijo", "Los autores presentan un novedoso ataque para generar ejemplos adversos en el que atacan a los clasificadores creados clasificando aleatoriamente L2 peque\u00f1as perturbaciones", "Un nuevo enfoque para generar ataques adversos a una red neuronal, y un m\u00e9todo para defender una red neuronal de esos ataques."]}
{"source": ["Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  ", "We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  ", "Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  ", "We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  ", "We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SJIA6ZWC-", "target": ["Entrenamos una red neuronal para que produzca pesos aproximadamente \u00f3ptimos en funci\u00f3n de los hiperpar\u00e1metros.", "Hiperredes para la optimizaci\u00f3n de hiperpar\u00e1metros en redes neuronales."]}
{"source": ["Estimating covariances between financial assets plays an important role in risk management.", "In practice, when the sample size is small compared to the number of variables, the empirical estimate is known to be very unstable.", "Here, we propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM).", "Our estimator can be considered as a non-linear extension of standard factor models with readily interpretable parameters reminiscent of market betas.", "Furthermore, our Bayesian treatment naturally shrinks the sample covariance matrix towards a more structured matrix given by the prior and thereby systematically reduces estimation errors.", "Finally, we discuss some financial applications of the GP-LVM model."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryEquiR9KX", "target": ["Estimaci\u00f3n de la matriz de covarianza de los activos financieros con modelos de variables latentes de proceso gaussiano", "Ilustra c\u00f3mo el modelo de variable latente de proceso gaussiano (GP-LVM) puede sustituir a los modelos cl\u00e1sicos de factores lineales para la estimaci\u00f3n de las matrices de covarianza en los problemas de optimizaci\u00f3n de carteras.", "Este trabajo utiliza GPLVMs est\u00e1ndar para modelar la estructura de covarianza y una representaci\u00f3n de espacio latente de las series temporales financieras del S&P500, para optimizar las carteras y predecir los valores perdidos.", "Este trabajo propone utilizar un GPLVM para modelar los rendimientos financieros"]}
{"source": ["We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution.", "In particular, we contrast the results from various well-known techniques for training GANs when the discriminator is near-optimal and updated multiple times per update to the generator.", "As an alternative, we propose an additional method to train GANs by explicitly modeling the discriminator's output as a bi-modal Gaussian distribution over the real/fake indicator variables.", "In order to do this, we train the Gaussian classifier to match the target bi-modal distribution implicitly through meta-adversarial training.", "We observe that our new method, when trained together with a strong discriminator, provides meaningful, non-vanishing gradients."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkeZRGbRW", "target": ["Introducimos el aprendizaje meta-adversarial, una nueva t\u00e9cnica para regularizar los GANs, y proponemos un m\u00e9todo de entrenamiento controlando expl\u00edcitamente la distribuci\u00f3n de salida del discriminador.", "El art\u00edculo propone un aprendizaje adversarial de regularizaci\u00f3n de la varianza para el entrenamiento de GANs con el fin de garantizar que el gradiente del generador no se desvanezca"]}
{"source": ["We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent\u2019s policy can be used to aid efficient exploration.", "The parameters of the noise are learned with gradient descent along with the remaining network weights.  ", "NoisyNet is straightforward to implement and adds little computational overhead.", "We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "rywHCPkAW", "target": ["Se puede utilizar un agente de aprendizaje por refuerzo profundo con ruido param\u00e9trico a\u00f1adido a sus pesos para ayudar a la exploraci\u00f3n eficiente.", "Este art\u00edculo presenta las NoisyNets, redes neuronales cuyos par\u00e1metros est\u00e1n perturbados por una funci\u00f3n de ruido param\u00e9trico, que obtienen una mejora sustancial del rendimiento respecto a los algoritmos de aprendizaje de refuerzo profundo de referencia.", "Nuevo m\u00e9todo de exploraci\u00f3n para la RL profunda mediante la inyecci\u00f3n de ruido en los pesos de las redes profundas, adoptando el ruido diversas formas"]}
{"source": ["Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment.", "Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent.", "We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently.", "The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization.", "Active Neural Localizer is trained end-to-end with reinforcement learning.", "We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine.", "The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations.", "We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ry6-G_66b", "target": ["\"Active Neural Localizer\", una red neuronal totalmente diferenciable que aprende a localizar de forma eficiente utilizando el aprendizaje profundo por refuerzo.", "Este trabajo formula el problema de localizaci\u00f3n en un mapa conocido utilizando una red de creencias como un problema de RL donde el objetivo del agente es minimizar el n\u00famero de pasos para localizarse a s\u00ed mismo.", "Este es un art\u00edculo claro e interesante que construye una red parametrizada para seleccionar acciones para un robot en un entorno simulado"]}
{"source": ["Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy.", "Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency.", "Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time.", "However, they could only achieve inferior accuracy compared with ART models.", "To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model.", "We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models.", "Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks.", "In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines.", "It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "r1gGpjActQ", "target": ["Desarrollamos un algoritmo de entrenamiento para modelos de traducci\u00f3n autom\u00e1tica no autorregresivos, que consigue una precisi\u00f3n comparable a la de los modelos base autorregresivos fuertes, pero un orden de magnitud m\u00e1s r\u00e1pido en la inferencia.  ", "Destila el conocimiento de los estados ocultos intermedios y los pesos de atenci\u00f3n para mejorar la traducci\u00f3n autom\u00e1tica neuronal no autorregresiva.", "Propone aprovechar el modelo autorregresivo bien entrenado para informar de los estados ocultos y la alineaci\u00f3n de palabras de los modelos de traducci\u00f3n autom\u00e1tica neuronal no autorregresiva."]}
{"source": ["Artificial neural networks are built on the basic operation of linear combination and non-linear activation function.", "Theoretically this structure can approximate any continuous function with three layer architecture.", "But in practice learning  the parameters of such network can be hard.", "Also the choice of activation function can greatly impact the performance of the network.", "In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non-linear activation function.", "To this end we are proposing the use of elementary  morphological operations (dilation and erosion) as the basic operation in neurons.", "We show that these networks (Denoted as Morph-Net) with morphological operations can approximate any smooth function requiring less number of parameters than what is necessary for normal neural networks.", "The results show that our network perform favorably when compared with similar structured network.", "We have carried out our experiments on  MNIST, Fashion-MNIST, CIFAR10 and CIFAR100."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SyxknjC9KQ", "target": ["Utilizando la operaci\u00f3n mofol\u00f3gica (dilataci\u00f3n y erosi\u00f3n) hemos definido una clase de red que puede aproximar cualquier funci\u00f3n continua. ", "Este trabajo propone sustituir las unidades est\u00e1ndar RELU/tanh por una combinaci\u00f3n de operaciones de dilataci\u00f3n y erosi\u00f3n, observando que el nuevo operador crea m\u00e1s hiperplanos y tiene m\u00e1s poder expresivo.", "Los autores presentan Morph-Net, una red neuronal de una sola capa en la que el mapeo se realiza mediante dilataci\u00f3n y erosi\u00f3n morfol\u00f3gica."]}
{"source": ["With the rapidly scaling up of deep neural networks (DNNs), extensive research studies on network model compression such as weight pruning have been performed for efficient deployment.", "This work aims to advance the compression beyond the weights to the activations of DNNs.", "We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning.", "Through the learning on the different importance of neuron responses and connections, the generated network, namely IPnet, balances the sparsity between activations and weights and therefore further improves execution efficiency.", "The feasibility and effectiveness of IPnet are thoroughly evaluated through various network models with different activation functions and on different datasets.", "With <0.5% disturbance on the testing accuracy, IPnet saves 71.1% ~ 96.35% of computation cost, compared to the original dense models with up to 5.8x and 10x reductions in activation and weight numbers, respectively."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyevnsCqtQ", "target": ["Este trabajo avanza la compresi\u00f3n de la DNN m\u00e1s all\u00e1 de los pesos a las activaciones integrando la poda de activaci\u00f3n con la poda de pesos. ", "Un m\u00e9todo integral de compresi\u00f3n de modelos que maneja tanto la poda de pesos como la de activaci\u00f3n, lo que conduce a un c\u00e1lculo m\u00e1s eficiente de la red y a una reducci\u00f3n efectiva del n\u00famero de multiplicaciones y acumulaciones.", "Este art\u00edculo presenta un enfoque novedoso para reducir el coste computacional de las redes neuronales profundas integrando la poda de activaci\u00f3n junto con la poda de pesos y muestra que las t\u00e9cnicas comunes de poda exclusiva de pesos aumentan el n\u00famero de activaciones no nulas despu\u00e9s de ReLU."]}
{"source": ["The Variational Auto Encoder (VAE) is a popular generative \nlatent variable model that is often \napplied for representation learning.\n", "Standard VAEs assume continuous valued \nlatent variables and are trained by maximization\nof the evidence lower bound (ELBO).", "Conventional methods obtain a \ndifferentiable estimate of the ELBO with reparametrized sampling and\noptimize it with Stochastic Gradient Descend (SGD).", "However, this is not possible if \nwe want to train VAEs with discrete valued latent variables, \nsince reparametrized sampling is not possible.", "Till now, there\nexist no simple solutions to circumvent this problem.\n", "In this paper, we propose an easy method to train VAEs \nwith binary or categorically valued latent representations.", "Therefore, we use a differentiable\nestimator for the ELBO which is based on importance sampling.", "In experiments, we verify the approach and\ntrain two different VAEs architectures with Bernoulli and \nCategorically distributed latent representations on two different benchmark\ndatasets.\t"], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkNSOjR9Y7", "target": ["Proponemos un m\u00e9todo sencillo para entrenar Auto Codificadores Variacionales (VAE) con representaciones latentes discretas, utilizando el muestreo de importancia", "Introducci\u00f3n de una distribuci\u00f3n de muestreo de importancia y uso de muestras de la distribuci\u00f3n para calcular la estimaci\u00f3n ponderada por importancia del gradiente", "Este trabajo propone utilizar el muestreo importante para optimizar la VAE con variables latentes discretas."]}
{"source": ["Distributed computing can significantly reduce the training time of neural networks.", "Despite its potential, however, distributed training has not been widely adopted: scaling the training process is difficult, and existing SGD methods require substantial tuning of hyperparameters and learning schedules to achieve sufficient accuracy when increasing the number of workers.", "In practice, such tuning can be prohibitively expensive given the huge number of potential hyperparameter configurations and the effort required to test each one.\n    \n", "We propose DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead.", "DANA estimates the future value of model parameters by adapting Nesterov Accelerated Gradient to a distributed setting, and so mitigates the effect of gradient staleness, one of the main difficulties in scaling SGD to more workers.\n\n", "Evaluation on three state-of-the-art network architectures and three datasets shows that DANA scales as well as or better than existing work without having to tune any hyperparameters or tweak the learning schedule.", "For example, DANA achieves 75.73% accuracy on ImageNet when training ResNet-50 with 16 workers, similar to the non-distributed baseline."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SkGQujR5FX", "target": ["Un nuevo algoritmo as\u00edncrono distribuido de SGD que alcanza la precisi\u00f3n m\u00e1s avanzada en las arquitecturas existentes sin ning\u00fan ajuste o sobrecarga adicional.", "Propone una mejora de los enfoques ASGD existentes a escala media utilizando el impulso con SGD para el entrenamiento as\u00edncrono a trav\u00e9s de un grupo de trabajadores distribuidos.", "Este art\u00edculo aborda el problema del estancamiento del gradiente frente al rendimiento paralelo en el entrenamiento distribuido del aprendizaje profundo, y propone un enfoque para estimar los futuros par\u00e1metros del modelo en las esclavas para reducir los efectos de la latencia de la comunicaci\u00f3n."]}
{"source": ["This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training.", "The approach employs additional modules called local critic networks besides the main network model to be trained, which are used to obtain error gradients without complete feedforward and backward propagation processes.", "We propose a cascaded learning strategy for these local networks.", "In addition, the approach is also useful from multi-model perspectives, including structural optimization of neural networks, computationally efficient progressive inference, and ensemble classification for performance improvement.", "Experimental results show the effectiveness of the proposed approach and suggest guidelines for determining appropriate algorithm parameters."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1x-LjAcKX", "target": ["Proponemos un nuevo algoritmo de aprendizaje de redes neuronales profundas, que desbloquea la dependencia por capas de la retropropagaci\u00f3n.", "Un paradigma de entrenamiento alternativo para los DNIs en el que el m\u00f3dulo auxiliar se entrena para aproximarse directamente a la salida final del modelo original, ofreciendo beneficios secundarios.", "Describe un m\u00e9todo de entrenamiento de redes neuronales sin bloqueo de actualizaciones."]}
{"source": ["\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs.", "Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  ", "However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory.", "We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness.", "ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation.", "We check the viability of ARTBP on two tasks.", "First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably.", "Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n"], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rkrWCJWAW", "target": ["Proporciona una versi\u00f3n no sesgada de la retropropagaci\u00f3n truncada mediante el muestreo de las longitudes de truncamiento y la reponderaci\u00f3n correspondiente.", "Propone m\u00e9todos de determinaci\u00f3n estoc\u00e1stica de los puntos de truncamiento en la retropropagaci\u00f3n en el tiempo.", "Una nueva aproximaci\u00f3n a la retropropagaci\u00f3n en el tiempo para superar las cargas computacionales y de memoria que surgen al tener que aprender de secuencias largas."]}
{"source": ["Graph convolutional networks (GCNs) have been widely used for classifying graph nodes in the semi-supervised setting.\n", "Previous works have shown that GCNs are vulnerable to the perturbation on adjacency and feature matrices of existing nodes.", "However, it is unrealistic to change the connections of  existing nodes in many applications, such as existing users in social networks.", "In this paper, we investigate methods attacking GCNs by adding fake nodes.", "A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones.", "In additional, we introduce a discriminator to classify fake nodes from real nodes, and propose a Greedy-GAN algorithm to simultaneously update the discriminator and the attacker, to make fake nodes indistinguishable to the real ones.  ", "Our non-targeted attack decreases the accuracy of GCN down to 0.10, and our targeted attack reaches a success rate of 0.99 for attacking the whole datasets, and 0.94 on average for attacking a single node."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rke8ZhCcFQ", "target": ["ataque no dirigido y dirigido a la GCN mediante la adici\u00f3n de nodos falsos", "Los autores proponen una nueva t\u00e9cnica adversarial para a\u00f1adir nodos \"falsos\" para enga\u00f1ar a un clasificador basado en GCN"]}
{"source": ["Transfer learning aims to solve the data sparsity for a specific domain by applying information of another domain.", "Given a sequence (e.g. a natural language sentence), the transfer learning, usually enabled by recurrent neural network (RNN), represent the sequential information transfer.", "RNN uses a chain of repeating cells to model the sequence data.", "However, previous studies of neural network based transfer learning simply transfer the information across the whole layers, which are unfeasible for seq2seq and sequence labeling.", "Meanwhile, such layer-wise transfer learning mechanisms also lose the fine-grained cell-level information from the source domain.\n\n", "In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer.", "ART is in a recurrent manner that different cells share the same parameters.", "Besides transferring the corresponding information at the same position, ART transfers information from all collocated words in the source domain.", "This strategy enables ART to capture the word collocation across domains in a more flexible way.", "We conducted extensive experiments on both sequence labeling tasks (POS tagging, NER) and sentence classification (sentiment analysis).", "ART outperforms the state-of-the-arts over all experiments.\n"], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByldlhAqYQ", "target": ["Aprendizaje de transferencia para la secuencia a trav\u00e9s del aprendizaje para alinear la informaci\u00f3n a nivel de c\u00e9lula a trav\u00e9s de los dominios.", "El art\u00edculo propone utilizar RNN/LSTM con alineaci\u00f3n de colocaci\u00f3n como m\u00e9todo de aprendizaje de representaci\u00f3n para el aprendizaje de transferencia/adaptaci\u00f3n al dominio en PNL."]}
{"source": ["Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world.", "We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution.", "Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function.", "To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state.", "Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SJGvns0qK7", "target": ["Formulamos la incertidumbre del modelo en el Aprendizaje por Refuerzo como un Proceso de Decisi\u00f3n de Markov continuo y adaptativo de Bayes y presentamos un m\u00e9todo para la optimizaci\u00f3n pr\u00e1ctica y escalable de la pol\u00edtica bayesiana.", "Utilizando un enfoque bayesiano, hay un mejor equilibrio entre la exploraci\u00f3n y la explotaci\u00f3n en RL"]}
{"source": ["For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic.\n", "We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model.", "In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions.", "The resulting benchmarks cannot be ``won'' by training set memorization, while still being perceptually correlated and computable only from samples.", "We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.\n"], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkxKH2AcFm", "target": ["Argumentamos que los puntos de referencia de GAN deben requerir una gran muestra del modelo para penalizar la memorizaci\u00f3n e investigamos si las divergencias de la red neuronal tienen esta propiedad.", "Los autores proponen un criterio para evaluar la calidad de las muestras producidas por una Red Generativa Adversarial."]}
{"source": ["Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data.", "In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation.", "The dialogue acts are generally designed and reveal how people engage in social chat.", "Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation.", "With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Bym0cU1CZ", "target": ["generaci\u00f3n de di\u00e1logos de dominio abierto con actos de di\u00e1logo", "Los autores utilizan una t\u00e9cnica de supervisi\u00f3n a distancia para a\u00f1adir etiquetas de actos de di\u00e1logo como factor condicionante para generar respuestas en di\u00e1logos de dominio abierto", "El art\u00edculo describe una t\u00e9cnica para incorporar actos de di\u00e1logo en agentes conversacionales neuronales"]}
{"source": ["We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics.", "Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed.", "Specifically, in typical cases, it seems possible to build infinitely many alternative mappings  from every target mapping.", "This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.\n\n", "We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity.", "A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique.", "The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.\n\n", "Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping.", "These are verified extensively.", "In addition, a new mapping algorithm is proposed and shown to lead to better mapping results."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1VjBebR-", "target": ["Nuestra hip\u00f3tesis es que, dados dos dominios, el mapeo de menor complejidad que tiene una discrepancia baja se aproxima al mapeo objetivo.", "El art\u00edculo aborda el problema del aprendizaje de mapeos entre diferentes dominios sin ninguna supervisi\u00f3n, enunciando tres conjeturas.", "Demuestra que en el aprendizaje no supervisado sobre datos no alineados es posible aprender el mapeo entre dominios utilizando s\u00f3lo GAN sin una p\u00e9rdida de reconstrucci\u00f3n."]}
{"source": ["We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming.", "This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions."], "source_labels": [0, 1], "rouge_scores": [], "paper_id": "HJgeEh09KQ", "target": ["Perfeccionamos los resultados de sobreaproximaci\u00f3n de los verificadores incompletos utilizando solucionadores MILP para demostrar m\u00e1s propiedades de robustez que el estado del arte. ", "Presenta un verificador que obtiene la mejora de la precisi\u00f3n de los verificadores incompletos y la escalabilidad de los verificadores completos utilizando la sobreparametrizaci\u00f3n, la programaci\u00f3n lineal entera mixta y la relajaci\u00f3n de la programaci\u00f3n lineal.", "Una estrategia mixta para obtener una mayor precisi\u00f3n en las verificaciones de robustez de las redes neuronales feed-forward con funciones de activaci\u00f3n lineales a trozos, logrando una mayor precisi\u00f3n que los verificadores incompletos y una mayor escalabilidad que los verificadores completos."]}
{"source": ["A distinct commonality between HMMs and RNNs is that they both learn hidden representations for sequential data.", "In addition, it has been noted that the backward computation of the Baum-Welch algorithm for HMMs is a special case of the back-propagation algorithm used for neural networks (Eisner (2016)).", "Do these observations suggest that, despite their many apparent differences, HMMs are a special case of RNNs?", "In this paper, we show that that is indeed the case, and investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization.", "In particular, we investigate three key design factors\u2014independence assumptions between the hidden states and the observation, the placement of softmaxes, and the use of non-linearities\u2014in order to pin down their empirical effects.", "We present a comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rJxEso0osm", "target": ["\u00bfSon los HMM un caso especial de las RNN? Investigamos una serie de transformaciones arquitect\u00f3nicas entre los HMM y las RNN, tanto a trav\u00e9s de derivaciones te\u00f3ricas como de la hibridaci\u00f3n emp\u00edrica, y aportamos nuevas ideas.", "Este art\u00edculo explora si los HMMs son un caso especial de RNNs utilizando el modelado del lenguaje y el etiquetado POS"]}
{"source": ["Deep neural networks have been tremendously successful in a number of tasks.\n", "One of the main reasons for this is their capability to automatically\n", "learn representations of data in levels of abstraction,\n", "increasingly disentangling the data as the internal transformations are applied.\n", "In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\n", "This makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\n", "The proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\n", "Our approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ByzvHagA-", "target": ["Proponemos un novedoso m\u00e9todo de regularizaci\u00f3n que penaliza la covarianza entre las dimensiones de las capas ocultas de una red.", "Este trabajo presenta un mecanismo de regularizaci\u00f3n que penaliza la covarianza entre todas las dimensiones en la representaci\u00f3n latente de una red neuronal para desentra\u00f1ar la representaci\u00f3n latente"]}
{"source": ["This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning.", "Trained with datasets whose labels are randomly shuffled except for one class of interest, a neural network learns class-wise parameter values, and remolds itself from a feature sorter into feature filters, each of which discerns objects belonging to one of the classes only.", "Classification of an input can be inferred from the maximum response of the filters.", "A multiple check with multiple versions of filters can diminish fluctuation and yields better performance.", "This scheme of discerning, maximum response and multiple check is a method of general viability to improve performance of feedforward networks, and the filter training itself is a promising feature abstraction procedure.", "In contrast to the direct sorting, the scheme mimics the classification process mediated by a series of one component picking."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "r1gKNs0qYX", "target": ["El esquema propuesto imita el proceso de clasificaci\u00f3n mediado por una serie de picking de un componente.", "Un m\u00e9todo para aumentar la precisi\u00f3n de las redes profundas en tareas de clasificaci\u00f3n multiclase aparentemente mediante una reducci\u00f3n de la clasificaci\u00f3n multiclase a binaria.", "Un novedoso procedimiento de clasificaci\u00f3n de discernimiento, respuesta m\u00e1xima y comprobaci\u00f3n m\u00faltiple para mejorar la precisi\u00f3n de las redes mediocres y mejorar las redes feedforward."]}
{"source": ["A long-held conventional wisdom states that larger models train more slowly when using gradient descent.", "This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step.", "In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\n", "We design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal.", "Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n"], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "S1lPShAqFm", "target": ["Emp\u00edricamente se muestra que los modelos m\u00e1s grandes se entrenan en menos pasos de entrenamiento, porque todos los factores en el recorrido del espacio de pesos mejoran.", "Este trabajo muestra que las RNN m\u00e1s anchas mejoran la velocidad de convergencia cuando se aplican a problemas de PNL, y por extensi\u00f3n el efecto de aumentar las anchuras en las redes neuronales profundas sobre la convergencia de la optimizaci\u00f3n", "Este art\u00edculo caracteriza el impacto de la sobreparametrizaci\u00f3n en el n\u00famero de iteraciones que tarda un algoritmo en converger, y presenta otras observaciones emp\u00edricas sobre los efectos de la sobreparametrizaci\u00f3n en el entrenamiento de redes neuronales."]}
{"source": ["Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research.", "Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes.", "In this work, we consider a recently identified class of bugs called variable-misuse bugs.", "The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction.", "We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs.", "We present multi-headed pointer networks for this purpose, with one head each for localization and repair.", "The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByloJ20qtm", "target": ["Redes de punteros de varias cabezas para aprender conjuntamente a localizar y reparar los fallos de uso de las variables", "Propone un modelo basado en LSTM con punteros para descomponer el problema de VarMisuse en m\u00faltiples pasos.", "Este art\u00edculo presenta un modelo basado en LSTM para la detecci\u00f3n y reparaci\u00f3n del fallo VarMisuse, y demuestra mejoras significativas en comparaci\u00f3n con enfoques anteriores en varios conjuntos de datos."]}
{"source": ["Classification and clustering have been studied separately in machine learning and computer vision.", "Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution.", "We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints.", "Instead, we emphasize on the compositionality of the real world structures and objects.", "In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms.", "The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance.", "This, by no means, suggests that other methods do not hold merits.", "For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters)."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Skvin0GWM", "target": ["Clustering de tipo humano con CNNs", "El art\u00edculo valida la idea de que las redes neuronales convolucionales profundas podr\u00edan aprender a agrupar los datos de entrada mejor que otros m\u00e9todos de agrupaci\u00f3n al observar su capacidad para interpretar el contexto de cada punto de entrada gracias a un gran campo de visi\u00f3n.", "Este trabajo combina el aprendizaje profundo para la representaci\u00f3n de caracter\u00edsticas con la tarea de agrupaci\u00f3n no supervisada de tipo humano."]}
{"source": ["Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features.", "Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  ", "This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models.", "We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  ", "In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  ", "We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory.", "We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "S1E3Ko09F7", "target": ["Desarrollamos dos algoritmos de complejidad lineal para la interpretaci\u00f3n de modelos agn\u00f3sticos basados en el valor de Shapley, en los escenarios en los que la contribuci\u00f3n de las caracter\u00edsticas al objetivo est\u00e1 bien aproximada por una factorizaci\u00f3n estructurada en grafos.", "El art\u00edculo propone dos aproximaciones al valor de Shapley utilizado para generar puntuaciones de caracter\u00edsticas para la interpretabilidad.", "Este art\u00edculo propone dos m\u00e9todos para la puntuaci\u00f3n de la importancia de las caracter\u00edsticas en funci\u00f3n de la instancia utilizando los valores de Shapely, y proporciona dos m\u00e9todos eficientes para calcular los valores de Shapely aproximados cuando existe una estructura conocida que relaciona las caracter\u00edsticas."]}
{"source": ["According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations.", "This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct.", "However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks.", "In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities.", "We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data.", "Using a 1-hot output code drastically decreases the number of local codes on the hidden layer.", "The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks.", "This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex.", "The findings highlight how local codes should not be dismissed out of hand."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJXOfZ-AZ", "target": ["Se han encontrado c\u00f3digos locales en las redes neuronales feed-forward", "Un m\u00e9todo para determinar hasta qu\u00e9 punto las neuronas individuales de una capa oculta de un MLP codifican un c\u00f3digo localista, que se estudia para diferentes representaciones de entrada.", "Estudia el desarrollo de representaciones localistas en las capas ocultas de las redes neuronales feed-forward."]}
{"source": ["Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.", "Existing approaches however primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in relational databases, such as text, images, and numerical values.", "In our approach, we propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities.", "We extend existing datasets to create two novel benchmarks, YAGO-10-plus and MovieLens-100k-plus, that contain additional relations such as textual descriptions and images of the original entities.", "We demonstrate that our model utilizes the additional information effectively to provide further gains in accuracy.", "Moreover, we test our learned multimodal embeddings by using them to predict missing multimodal attributes."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "By03VlJGG", "target": ["Ampliaci\u00f3n de la modelizaci\u00f3n relacional para soportar datos multimodales mediante codificadores neuronales.", "Este trabajo propone realizar la predicci\u00f3n de enlaces en las Bases de Conocimiento complementando las entidades originales con informaci\u00f3n multimodal, y presenta un modelo capaz de codificar todo tipo de informaci\u00f3n a la hora de puntuar las triplas.", "El art\u00edculo trata sobre la incorporaci\u00f3n de informaci\u00f3n de diferentes modalidades en los enfoques de predicci\u00f3n de enlaces"]}
{"source": ["An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. \n", "In this work, we propose a two-stage method to learn Sparse Structured Ensembles (SSEs) for neural networks.\n", "In the first stage, we run SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters.", "In the second stage, we apply weight-pruning to each sampled network and then perform retraining over the remained connections.\n", "In this way of learning SSEs with SG-MCMC and pruning, we not only achieve high prediction accuracy since SG-MCMC enhances exploration of the model-parameter space, but also reduce memory and computation cost significantly in both training and testing of NN ensembles.\n", "This is thoroughly evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.\n", "For example, in LSTM based language modeling (LM), we obtain 21\\% relative reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has only 30\\% of model parameters and 70\\% of computations in total, as compared to the baseline large LSTM LM.\n", "To the best of our knowledge, this work represents the first methodology and empirical study of integrating SG-MCMC, group sparse prior and network pruning together for learning NN ensembles."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1uOhfb0W", "target": ["Proponemos un m\u00e9todo novedoso que integra el muestreo SG-MCMC, el grupo sparse prior y la poda de la red para aprender el Sparse Structured Ensemble (SSE) con un rendimiento mejorado y un coste significativamente menor que los m\u00e9todos tradicionales. ", "Los autores proponen un procedimiento para generar un conjunto de modelos estructurados dispersos", "Un nuevo marco para el entrenamiento de redes neuronales de conjunto que utiliza m\u00e9todos SG-MCMC dentro del aprendizaje profundo, y luego aumenta la eficiencia computacional mediante sparsity+pruning de grupo.", "Este trabajo explora el uso de FNN y LSTMs para hacer que el promedio de modelos bayesianos sea m\u00e1s factible computacionalmente y mejorar el rendimiento promedio del modelo."]}
{"source": ["This paper introduces a new framework for data efficient and versatile learning.", "Specifically:\n", "1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction.", "ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. \n", "2) We introduce \\Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass.", "\\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.\n", "3) We evaluate \\Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time.", "The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkxStoC5F7", "target": ["Nuevo marco para el meta-aprendizaje que unifica y ampl\u00eda una amplia clase de m\u00e9todos de aprendizaje de pocos disparos existentes. Consigue un gran rendimiento en los puntos de referencia de aprendizaje de pocos disparos sin requerir la inferencia iterativa en tiempo de prueba.   ", "Este trabajo aborda el aprendizaje de pocos disparos desde el punto de vista de la inferencia probabil\u00edstica, logrando el estado del arte a pesar de una configuraci\u00f3n m\u00e1s simple que muchos competidores"]}
{"source": ["In recent years, softmax together with its fast approximations has become the de-facto loss function for deep neural networks with multiclass predictions.", "However, softmax is used in many problems that do not fully fit the multiclass framework and where the softmax assumption of mutually exclusive outcomes can lead to biased results.", "This is often the case for applications such as language modeling, next event prediction and matrix factorization, where many of the potential outcomes are not mutually exclusive, but are more likely to be independent conditionally on the state.", "To this end, for the set of problems with positive and unlabeled data, we propose a relaxation of the original softmax formulation, where, given the observed state, each of the outcomes are conditionally independent but share a common set of negatives.", "Since we operate in a regime where explicit negatives are missing, we create an adversarially-trained model of negatives and derive a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS).", "We show empirically the advantages of our newly introduced negative sampling scheme by pluging it in the Word2Vec algorithm and benching it extensively against other negative sampling schemes on both language modeling and matrix factorization tasks and show large lifts in performance."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rkx0g3R5tX", "target": ["Definici\u00f3n de una p\u00e9rdida softmax parcialmente mutuamente excluyente para datos positivos y aplicaci\u00f3n de un esquema de muestreo basado en la cooperaci\u00f3n", "Este trabajo presenta el Muestreo de Importancia Cooperativo para resolver el problema de la suposici\u00f3n mutuamente excluyente de que el softmax tradicional est\u00e1 sesgado cuando las muestras negativas no est\u00e1n expl\u00edcitamente definidas", "Este trabajo propone m\u00e9todos PMES para relajar la suposici\u00f3n de resultado exclusivo en la p\u00e9rdida softmax, demostrando el m\u00e9rito emp\u00edrico en la mejora de los modelos de incrustaci\u00f3n del tipo word2vec."]}
{"source": ["Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention.", "Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos.", "In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation.", "Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video.", "The idea is to then train the student to minimize", " (i)  the difference between the final representation computed by the student and the teacher and/or", "(ii) the difference between the distributions predicted by the teacher and the student.", "This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification.", "We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "H1GWAoRcKX", "target": ["Marco de trabajo profesor-estudiante para la clasificaci\u00f3n eficiente de v\u00eddeos utilizando menos fotogramas ", "El art\u00edculo propone una idea para destilar, a partir de un modelo de clasificaci\u00f3n de v\u00eddeo completo, un peque\u00f1o modelo que s\u00f3lo recibe un n\u00famero menor de fotogramas.", "Los autores presentan una red profesor-alumno para resolver el problema de clasificaci\u00f3n de v\u00eddeos, proponiendo algoritmos de entrenamiento en serie y en paralelo con el fin de reducir los costes computacionales."]}
{"source": ["Deep generative models have achieved impressive success in recent years.", "Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively.", "This paper aims to establish formal connections between GANs and VAEs through a new formulation of them.", "We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively.", "The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way.", "For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples.", "Experiments show generality and effectiveness of the transfered techniques."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rylSzl-R-", "target": ["Una visi\u00f3n estad\u00edstica unificada de la amplia clase de modelos generativos profundos ", "El art\u00edculo desarrolla un marco que interpreta los algoritmos GAN como una forma de inferencia variacional sobre un modelo generativo que reconstruye una variable indicadora de si una muestra pertenece a la verdadera de las distribuciones generativas de datos."]}
{"source": ["Deep neural networks have demonstrated promising prediction and classification performance on many healthcare applications.", "However, the interpretability of those models are often lacking.", "On the other hand, classical interpretable models such as rule lists or decision trees do not lead to the same level of accuracy as deep neural networks and can often be too complex to interpret (due to the potentially large depth of rule lists).", "In this work, we present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes.", "The resulting prototype neural network provides  accurate prediction, and the prediction can be easily explained by  prototype and its guiding rule lists.", "Thanks to the prediction power of neural networks, the rule lists from\t\t\t\t prototypes are more concise and hence provide better interpretability.", "On two real-world electronic healthcare records (EHR) datasets, PEARL consistently outperforms all baselines across both datasets, especially achieving performance improvement over conventional rule learning by up to 28% and over prototype learning by up to 3%.", "Experimental results also show the resulting interpretation of PEARL is  simpler than the standard rule learning."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "r1gnQ20qYX", "target": ["un m\u00e9todo que combina el aprendizaje de listas de reglas y el aprendizaje de prototipos ", "Presenta un nuevo marco de predicci\u00f3n interpretable, que combina el aprendizaje basado en reglas, el aprendizaje de prototipos y las NN, que es particularmente aplicable a los datos longitudinales.", "Este trabajo tiene como objetivo abordar la falta de interpretabilidad de los modelos de aprendizaje profundo, y proponer Prototype lEArning via Rule Lists (PEARL), que combina el aprendizaje de reglas y el aprendizaje de prototipos para lograr una clasificaci\u00f3n m\u00e1s precisa y hace que la tarea de interpretabilidad sea m\u00e1s sencilla."]}
{"source": ["Generative Adversarial Networks (GANs) are powerful tools for realistic image generation.", "However, a major drawback of GANs is that they are especially hard to train, often requiring large amounts of data and long training time.", "In this paper we propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space using similar approaches in \\cite{deligan}.", "The structure of the latent space we consider in this paper is modeled as a mixture of Gaussians, whose parameters are learned in the training process.", "Furthermore, to improve stability and efficiency, we use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence.", "We show by experiments that the Deli-Fisher GAN performs better than DCGAN, WGAN, and the Fisher GAN as measured by inception score."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyMuaiAqY7", "target": ["Este trabajo propone una nueva Red Adversarial Generativa que es m\u00e1s estable, m\u00e1s eficiente y produce mejores im\u00e1genes que las del status-quo ", "Este documento combina Fisher-GAN y Deli-GAN", "Este trabajo combina Deli-GAN, que tiene una distribuci\u00f3n previa de mezcla en el espacio latente, y Fisher GAN, que utiliza Fisher IPM en lugar de JSD como objetivo."]}
{"source": ["Recent work on encoder-decoder models for sequence-to-sequence mapping has shown that integrating both temporal and spatial attentional mechanisms into neural networks increases the performance of the system substantially.", "We report on a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection for multi-sensor setups.", "This network called the sensor transformation attention network (STAN) is evaluated in scenarios which include the presence of natural noise or synthetic dynamic noise.", "We demonstrate how the attentional signal responds dynamically to changing noise levels and sensor-specific noise, leading to reduced word error rates (WERs) on both audio and visual tasks using TIDIGITS and GRID; and also on CHiME-3, a multi-microphone real-world noisy dataset.", "The improvement grows as more channels are corrupted as demonstrated on the CHiME-3 dataset.", "Moreover, the proposed STAN architecture naturally introduces a number of advantages including ease of removing sensors from existing architectures, attentional interpretability, and increased robustness to a variety of noise environments."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Bk346Ok0W", "target": ["Presentamos una arquitectura de red modular multisensor con un mecanismo de atenci\u00f3n que permite la selecci\u00f3n din\u00e1mica de sensores en datos ruidosos del mundo real de CHiME-3.", "Una arquitectura neuronal gen\u00e9rica capaz de aprender la atenci\u00f3n que debe prestarse a los distintos canales de entrada en funci\u00f3n de la calidad relativa de cada sensor con respecto a los dem\u00e1s.", " Considera el uso de la atenci\u00f3n para la selecci\u00f3n de sensores o canales con resultados en TIDIGITS y GRID que muestran un beneficio de la atenci\u00f3n sobre la concatenaci\u00f3n de caracter\u00edsticas."]}
{"source": ["Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints.", "Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection.", "To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud.", "The local neural network (NN) is used to generate the feature representations.", "To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs.", "The cloud NN is then trained based on the extracted intermediate representations for the target learning task.", "We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task.", "Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage.", "The efficiency and effectiveness of PrivyNet are demonstrated with CIFAR-10 dataset."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJcjQTJ0W", "target": ["Para permitir el entrenamiento de las DNN en la nube y proteger simult\u00e1neamente la privacidad de los datos, proponemos aprovechar las representaciones de datos intermedias, lo que se consigue dividiendo las DNN y despleg\u00e1ndolas por separado en plataformas locales y en la nube.", "Este trabajo propone una t\u00e9cnica para privatizar los datos mediante el aprendizaje de una representaci\u00f3n de caracter\u00edsticas que es dif\u00edcil de utilizar para la reconstrucci\u00f3n de im\u00e1genes, pero \u00fatil para la clasificaci\u00f3n de las mismas."]}
{"source": ["Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data.", "In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data.", "RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator.", "In the case of RCGANs, both of these RNNs are conditioned on auxiliary information.", "We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series.", "We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa.", "We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data.", "This is demonstrated on digit classification from \u2018serialised\u2019 MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit.", "We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1ZZTfZAW", "target": ["GANs recurrentes condicionales para la generaci\u00f3n de secuencias m\u00e9dicas de valor real, mostrando nuevos enfoques de evaluaci\u00f3n y un an\u00e1lisis emp\u00edrico de la privacidad.", "Propone el uso de datos sint\u00e9ticos generados por GANs como reemplazo de los datos de identificaci\u00f3n personal en el entrenamiento de modelos ML para aplicaciones sensibles a la privacidad", "Los autores proponen una nueva arquitectura GAN recurrente que genera secuencias de dominio continuo, y la eval\u00faan en varias tareas sint\u00e9ticas y en una tarea de datos de series temporales de la UCI.", "Propone utilizar RGANs y RCGANs para generar secuencias sint\u00e9ticas de datos reales."]}
{"source": ["Emphasis effects \u2013 visual changes that make certain elements more\n", "prominent \u2013 are commonly used in information visualization to draw\n", "the user\u2019s attention or to indicate importance.", "Although theoretical\n", "frameworks of emphasis exist (that link visually diverse emphasis\n", "effects through the idea of visual prominence compared to background\n", "elements), most metrics for predicting how emphasis effects\n", "will be perceived by users come from abstract models of human\n", "vision which may not apply to visualization design.", "In particular,\n", "it is difficult for designers to know, when designing a visualization,\n", "how different emphasis effects will compare and what level of one\n", "effect is equivalent to what level of another.", "To address this gap,\n", "we carried out two studies that provide empirical evidence about\n", "how users perceive different emphasis effects, using three visual\n", "variables (colour, size, and blur/focus) and eight strength levels.\n", "Results from gaze tracking, mouse clicks, and subjective responses\n", "show that there are significant differences between visual variables\n", "and between levels, and allow us to develop an initial understanding\n", "of perceptual equivalence.", "We developed a model from the data in\n", "our first study, and used it to predict the results in the second; the\n", "model was accurate, with high correlations between predictions and\n", "real values.", "Our studies and empirical models provide valuable new\n", "information for designers who want to understand and control how\n", "emphasis effects will be perceived by users."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "NxvF-PleYy", "target": ["Nuestros estudios y modelos emp\u00edricos aportan informaci\u00f3n nueva y valiosa para los dise\u00f1adores que desean comprender y controlar c\u00f3mo perciben los usuarios los efectos del \u00e9nfasis", "Este art\u00edculo examina qu\u00e9 tipo de resaltado visual se percibe m\u00e1s r\u00e1pidamente en la visualizaci\u00f3n de datos y c\u00f3mo se comparan los distintos m\u00e9todos de resaltado entre s\u00ed", "Dos estudios sobre la eficacia de los efectos de \u00e9nfasis, uno que eval\u00faa los niveles de diferencias \u00fatiles, y otro m\u00e1s aplicado que utiliza visualizaciones reales diferentes para una investigaci\u00f3n m\u00e1s v\u00e1lida desde el punto de vista ecol\u00f3gico."]}
{"source": ["Memory Network based models have shown a remarkable progress on the task of relational reasoning.\n", "Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \n", "Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\n", "We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \n", "We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \n", "As a result, our model is as simple as RN but the computational complexity is reduced to linear time.\n", "It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByquB-WC-", "target": ["Una arquitectura de razonamiento sencilla basada en la red de memoria (MemNN) y la red de relaciones (RN), que reduce la complejidad temporal en comparaci\u00f3n con la RN y logra un resultado de vanguardia en la GC basada en la historia bAbI y el di\u00e1logo bAbI.", "Introduce la Red de Memoria Relacionada (RMN), una mejora de las Redes de Relaci\u00f3n (RN)."]}
{"source": ["We investigate in this paper the architecture of deep convolutional networks.", "Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN.", "We show that this arrangement is an efficient way to significantly reduce the number of parameters while at the same time improving the performance.", "The use of branches brings an additional form of regularization.", "In addition to splitting the parameters into parallel branches, we propose a tighter coupling of these branches by averaging their log-probabilities.", "The tighter coupling favours the learning of better representations, even at the level of the individual branches, as compared to when each branch is trained independently.", "We refer to this branched architecture as \"coupled ensembles\".", "The approach is very generic and can be applied with almost any neural network architecture.", "With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks.", "For the same parameter budget, DenseNet-BC has an error rate of 3.46%, 17.18%, and 1.8% respectively.  ", "With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hk2MHt-3-", "target": ["Demostramos que dividir una red neuronal en ramas paralelas mejora el rendimiento y que el acoplamiento adecuado de las ramas mejora a\u00fan m\u00e1s el rendimiento.", "El trabajo propone una reconfiguraci\u00f3n del modelo de CNN existente en el estado del arte utilizando una nueva arquitectura de ramificaci\u00f3n, con un mejor rendimiento.", "Este documento muestra las ventajas de ahorro de par\u00e1metros del ensamblaje acoplado.", "Presenta una arquitectura de red profunda que procesa los datos utilizando m\u00faltiples ramas paralelas y combina las posteriores de estas ramas para calcular las puntuaciones finales."]}
{"source": ["Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few.", "Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices.", "To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly.", "Yet, this acceleration comes at the cost of a larger error.", "The NICE method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy.", "This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and 3 -bit activations.", "We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HyfyN30qt7", "target": ["Combina la inyecci\u00f3n de ruido, la cuantificaci\u00f3n gradual y el aprendizaje de sujeci\u00f3n de la activaci\u00f3n para lograr una cuantificaci\u00f3n de 3,4 y 5 bits de \u00faltima generaci\u00f3n", "Propone inyectar ruido durante el entrenamiento y sujetar los valores de los par\u00e1metros en una capa, as\u00ed como la salida de activaci\u00f3n en la cuantificaci\u00f3n de la red neuronal.", "Un m\u00e9todo para la cuantificaci\u00f3n de redes neuronales profundas para la clasificaci\u00f3n y la regresi\u00f3n, utilizando la inyecci\u00f3n de ruido, la sujeci\u00f3n con activaciones m\u00e1ximas aprendidas y la cuantificaci\u00f3n gradual de bloques para obtener un rendimiento igual o mejor que los m\u00e9todos del estado de la t\u00e9cnica."]}
{"source": ["In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks.", "Approaches that transfer information contained only in the final parameters of a source model will therefore struggle.", "Instead, transfer learning at at higher level of abstraction is needed.", "We propose Leap, a framework that achieves this by transferring knowledge across learning processes.", "We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta-learning objective that minimizes the expected length of this path.", "Our framework leverages only information obtained during training and can be computed on the fly at negligible cost.", "We demonstrate that our framework outperforms competing methods, both in meta-learning and transfer learning, on a set of computer vision tasks.", "Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding reinforcement learning environments (Atari) that involve millions of gradient steps."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HygBZnRctX", "target": ["Proponemos Leap, un marco que transfiere el conocimiento a trav\u00e9s de los procesos de aprendizaje minimizando la distancia esperada que el proceso de formaci\u00f3n recorre en la superficie de p\u00e9rdida de una tarea.", "El art\u00edculo propone un nuevo objetivo de meta-aprendizaje para superar los enfoques m\u00e1s avanzados cuando se trata de colecciones de tareas que presentan una diversidad sustancial entre las tareas"]}
{"source": ["Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned.", "Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.", "We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones.", "DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance.", "When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy.", "The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains.", "We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryj0790hb", "target": ["Una alternativa al aprendizaje por transferencia que aprende m\u00e1s r\u00e1pido, requiere muchos menos par\u00e1metros (3-13 %), suele conseguir mejores resultados y conserva con precisi\u00f3n el rendimiento en tareas antiguas.", "M\u00f3dulos de control para el aprendizaje incremental en conjuntos de datos de clasificaci\u00f3n de im\u00e1genes"]}
{"source": ["High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications.", "This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including", "1) channel-wise scale factors of weights, especially for depthwise convolution,", "2) Winograd convolution, and", "3) topology-wise 8-bit support.", "We experiment the techniques on top of a widely-used deep learning framework.", "The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining.", "We perform a systematical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution.", "We show that the inference throughput\n", "and latency are improved by 1.6X and 1.5X respectively with minimal within 0.6%1to no loss in accuracy from FP32 baseline.", "We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks.", "All the code and models will be publicly available soon."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SklzIjActX", "target": ["Presentamos una t\u00e9cnica general para la inferencia de baja precisi\u00f3n de 8 bits de las redes neuronales convolucionales. ", "Este trabajo dise\u00f1a un sistema para cuantificar autom\u00e1ticamente los modelos preentrenados de la CNN"]}
{"source": ["Recent approaches have successfully demonstrated the benefits of learning the parameters of shallow networks in hyperbolic space.", "We extend this line of work by imposing hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures.", "By only changing the geometry of embedding of object representations, we can use the embedding space more efficiently without increasing the number of parameters of the model.", "Mainly as the number of objects grows exponentially for any semantic distance from the query, hyperbolic geometry  --as opposed to Euclidean geometry-- can encode those objects without having any interference.", "Our method shows improvements in generalization on neural machine translation on WMT'14 (English to German), learning on graphs (both on synthetic and real-world graph tasks) and visual question answering (CLEVR) tasks while keeping the neural representations compact."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJxHsjRqFQ", "target": ["Proponemos incorporar sesgos inductivos y operaciones procedentes de la geometr\u00eda hiperb\u00f3lica para mejorar el mecanismo de atenci\u00f3n de las redes neuronales.", "Este trabajo sustituye la similitud punto-producto utilizada en los mecanismos de atenci\u00f3n por la distancia hiperb\u00f3lica negativa, y la aplica al modelo Transformer existente, a las redes de atenci\u00f3n gr\u00e1fica y a las redes de relaci\u00f3n", "Los autores proponen un enfoque novedoso para mejorar la atenci\u00f3n relacional cambiando las funciones de emparejamiento y agregaci\u00f3n para utilizar la geometr\u00eda hiperb\u00f3lica. "]}
{"source": ["We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies.", "We also formulate a zero-sum dynamic game for designing robust deep reinforcement learning policies.", "Our approach mitigates the brittleness of policies when agents are trained in a simulated environment and are later exposed to the real world where it is hazardous to employ RL policies.", "This framework for training deep RL policies involve a zero-sum  dynamic game against an adversarial agent, where the goal is to drive the system dynamics to a saddle region.", "Using a variant of the guided policy search algorithm, our agent learns to adopt robust policies that require less samples for learning the dynamics and performs better than the GPS algorithm.", "Without loss of generality, we demonstrate that deep RL policies trained in this fashion will be maximally robust to a ``worst\" possible adversarial disturbances."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkc_hGb0Z", "target": ["Este art\u00edculo demuestra c\u00f3mo la teor\u00eda de control H-infinito puede ayudar a dise\u00f1ar mejor las pol\u00edticas profundas robustas para los taks de los motores de los robots", "Propone incorporar elementos de control robusto a la investigaci\u00f3n de la pol\u00edtica guiada para concebir un m\u00e9todo resistente a las perturbaciones y al desajuste del modelo.", "El art\u00edculo presenta un m\u00e9todo para evaluar la sensibilidad y robustez de las pol\u00edticas de RL profunda, y propone un enfoque de juego din\u00e1mico para el aprendizaje de pol\u00edticas robustas."]}
{"source": ["Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers.", "In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary.", "Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models).", "We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved.", "Under such conditions, we prove the existence of small universal perturbations.", "Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByrZyglCb", "target": ["An\u00e1lisis de la vulnerabilidad de los clasificadores a las perturbaciones universales y relaci\u00f3n con la curvatura de la frontera de decisi\u00f3n.", "El art\u00edculo proporciona un interesante an\u00e1lisis que relaciona la geometr\u00eda de los l\u00edmites de decisi\u00f3n del clasificador con peque\u00f1as perturbaciones adversarias universales.", "Este art\u00edculo analiza las perturbaciones universales, es decir, las perturbaciones que pueden inducir a error a un clasificador entrenado si se a\u00f1aden a la mayor\u00eda de los puntos de datos de entrada.", "El art\u00edculo desarrolla modelos que intentan explicar la existencia de perturbaciones universales que enga\u00f1an a las redes neuronales"]}
{"source": ["Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning.", "However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration.", "Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines.", "However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task.", "In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill.", "Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly.", "In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HkgSEnA5KQ", "target": ["Proponemos un m\u00e9todo de meta-aprendizaje para la correcci\u00f3n interactiva de pol\u00edticas con lenguaje natural.", "Este art\u00edculo proporciona un marco de metaaprendizaje que muestra c\u00f3mo aprender nuevas tareas en una configuraci\u00f3n interactiva. Cada tarea se aprende a trav\u00e9s de una configuraci\u00f3n de aprendizaje por refuerzo, y luego la tarea se actualiza mediante la observaci\u00f3n de nuevas instrucciones.", "Este trabajo ense\u00f1a a los agentes a completar tareas mediante instrucciones en lenguaje natural en un proceso iterativo."]}
{"source": ["Deep generative models such as Generative Adversarial Networks (GANs) and\n", "Variational Auto-Encoders (VAEs) are important tools to capture and investigate\n", "the properties of complex empirical data.", "However, the complexity of their inner\n", "elements makes their functionment challenging to assess and modify.", "In this\n", "respect, these architectures behave as black box models.", "In order to better\n", "understand the function of such networks, we analyze their modularity based on\n", "the counterfactual manipulation of their internal variables.", "Our experiments on the\n", "generation of human faces with VAEs and GANs support that modularity between\n", "activation maps distributed over channels of generator architectures is achieved\n", "to some degree, can be used to better understand how these systems operate and allow meaningful transformations of the generated images without further training.\n", "erate and edit the content of generated images."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Byldr3RqKX", "target": ["Investigamos la modularidad de los modelos generativos profundos.", "El art\u00edculo proporciona una forma de investigar la estructura modular del modelo generativo profundo, con el concepto clave de distribuir sobre canales de arquitecturas generadoras."]}
{"source": ["Relational databases store a significant amount of the worlds data.", "However, accessing this data currently requires users to understand a query language such as SQL.", "We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.", "Our model uses rewards from in the loop query execution over the database to learn a policy to generate the query, which contains unordered parts that are less suitable for optimization via cross entropy loss.", "Moreover, Seq2SQL leverages the structure of SQL to prune the space of generated queries and significantly simplify the generation problem.", "In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables fromWikipedia that is an order of magnitude larger than comparable datasets.", "By applying policy based reinforcement learning with a query execution environment to WikiSQL, Seq2SQL outperforms a state-of-the-art semantic parser, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Syx6bz-Ab", "target": ["Presentamos Seq2SQL, que traduce las preguntas a consultas SQL utilizando las recompensas de la ejecuci\u00f3n de las consultas en l\u00ednea, y WikiSQL, un conjunto de datos de tablas/preguntas/consultas SQL \u00f3rdenes de magnitud mayores que los conjuntos de datos existentes.", "Un nuevo conjunto de datos de an\u00e1lisis sint\u00e1ctico que se centra en la generaci\u00f3n de SQL a partir del lenguaje natural mediante un modelo basado en el aprendizaje por refuerzo"]}
{"source": ["We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks.", "We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness.", "We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis.", "Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components.", "We show that models learnt with our approach perform remarkably well against a wide-range of attacks.", "Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkMk9j0qYm", "target": ["El modelado de ruido en la entrada durante el entrenamiento discriminativo mejora la robustez adversarial. Proponer una m\u00e9trica de evaluaci\u00f3n de la robustez adversarial basada en el PCA", "Este trabajo propone, ExL, un m\u00e9todo de entrenamiento adversarial que utiliza ruido multiplicador y que se muestra \u00fatil para defenderse de los ataques de caja negra en tres conjuntos de datos.", "Este trabajo incluye el ruido multiplicativo N en los datos de entrenamiento para lograr la robustez adversarial, cuando se entrena tanto en los par\u00e1metros del modelo theta como en el propio ruido."]}
{"source": ["We propose a method which can visually explain the classification decision of deep neural networks (DNNs).", "There are many proposed methods in machine learning and computer vision seeking to clarify the decision of machine learning black boxes, specifically DNNs.  ", "All of these methods try to gain insight into why the network \"chose class A\" as an answer.", "Humans, when searching for explanations, ask two types of questions.", "The first question is, \"Why did you choose this answer?", "\"", "The second question asks, \"Why did you not choose answer B over A?\"", "The previously proposed methods are either not able to provide the latter directly or efficiently.\n\n", "We introduce a method capable of answering the second question both directly and efficiently.", "In this work, we limit the inputs to be images.", "In general, the proposed method generates explanations in the input space of any model capable of efficient evaluation and gradient evaluation.", "We provide results, showing the superiority of this approach for gaining insight into the inner representation of machine learning models."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyNmRiCqtm", "target": ["Un m\u00e9todo para responder a \"\u00bfpor qu\u00e9 no la clase B?\" para explicar las redes profundas", "El art\u00edculo propone un enfoque para proporcionar explicaciones visuales contrastivas para las redes neuronales profundas."]}
{"source": ["We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping.", "We provide theoretical and numerical results on the inverse of ReLU-layers.", "First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation.", "Next, we move to robustness via analyzing local effects on the inverse.", "To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyxYEoA5FX", "target": ["Analizamos la invertibilidad de las redes neuronales profundas estudiando las preim\u00e1genes de las capas ReLU y la estabilidad de la inversa.", "Este trabajo estudia el volumen de preimagen de la activaci\u00f3n de una red ReLU en una determinada capa, y se basa en la linealidad a trozos de la funci\u00f3n de avance de una red ReLU. ", "Este trabajo presenta un an\u00e1lisis de la invariabilidad inversa de las redes ReLU y proporciona l\u00edmites superiores a los valores singulares de una red de trenes."]}
{"source": ["While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs.", "This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important.", "Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks.", "In this paper we investigate how this approach scales as we increase the computational budget given to the defender.", "We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model.", "Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HJguLo0cKQ", "target": ["El entrenamiento adversarial de los conjuntos proporciona una robustez frente a los ejemplos adversarios mayor que la observada en los modelos entrenados adversariamente y en los conjuntos entrenados independientemente.", " Propone entrenar un conjunto de modelos de forma conjunta, donde en cada paso de tiempo se incorpora al aprendizaje un conjunto de ejemplos adversos para el propio conjunto."]}
{"source": ["Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer.", "To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm.", "A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks.", "A function block may be any neural network \u2013 for example a fully-connected or a convolutional layer.", "Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached.", "In this way the routing network dynamically composes different function blocks for each input.", "We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks.", "We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets.", "Our experiments demonstrate a significant improvement in accuracy, with sharper convergence.", "In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks.", "On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time.\n"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ry8dvM-R-", "target": ["redes de enrutamiento: un nuevo tipo de red neuronal que aprende a enrutar adaptativamente su entrada para el aprendizaje multitarea", "El documento sugiere utilizar una red modular con un controlador que tome decisiones, en cada paso de tiempo, sobre el siguiente n\u00f3dulo a aplicar.", "El art\u00edculo presenta una formulaci\u00f3n novedosa para el aprendizaje de la arquitectura \u00f3ptima de una red neuronal en un marco de aprendizaje multitarea mediante el uso del aprendizaje de refuerzo multiagente para encontrar una pol\u00edtica, y muestra la mejora con respecto a las arquitecturas codificadas con capas compartidas."]}
{"source": ["We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero.", "Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization.", "AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization.", "However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function.", "We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero.", "We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters.", "We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid.", "The parameters of the distribution over the gates can then be jointly optimized with the original network parameters.", "As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way.", "We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1Y8hhg0b", "target": ["Mostramos c\u00f3mo optimizar la norma L_0 esperada de los modelos param\u00e9tricos con el descenso de gradiente e introducimos una nueva distribuci\u00f3n que facilita el hard gating.", "Los autores introducen un enfoque basado en el gradiente para minimizar una funci\u00f3n objetivo con una penalizaci\u00f3n L0 dispersa para ayudar a aprender redes neuronales dispersas"]}
{"source": ["Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches.", "These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer.", "Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models.", "This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small.", "This in turn allows a room for designing more innovative propagation layers.", "Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph.", "The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions.", "In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods.", "By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJg4YGWRb", "target": ["Proponemos una nueva arquitectura de red neuronal gr\u00e1fica interpretable basada en la atenci\u00f3n que supera a las redes neuronales gr\u00e1ficas actuales en conjuntos de datos de referencia est\u00e1ndar", "Los autores proponen dos extensiones de las GCN, eliminando las no linealidades intermedias del c\u00e1lculo de las GCN y a\u00f1adiendo un mecanismo de atenci\u00f3n en la capa de agregaci\u00f3n.", "El art\u00edculo propone un algoritmo de aprendizaje semi-supervisado para la clasificaci\u00f3n de nodos de grafos, inspirado en las redes neuronales de grafos."]}
{"source": ["Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimensionality of data can be much lower than the ambient dimensionality.", "We argue that this discrepancy may contribute to the difficulties in training generative models.", "We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space.", "The resulting method, perceptual generative autoencoder (PGA), is then incorporated with maximum likelihood or variational autoencoder (VAE) objective to train the generative model.", "With maximum likelihood, PGA generalizes the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities.", "When combined with VAE, PGA can generate sharper samples than vanilla VAE."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "rkxkr8UKuN", "target": ["Un marco para el entrenamiento de modelos generativos basados en autoencoders, con p\u00e9rdidas no adversariales y arquitecturas de redes neuronales no restringidas.", "Este trabajo utiliza autocodificadores para realizar la correspondencia de distribuciones en un espacio de alta dimensi\u00f3n."]}
{"source": ["The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\n", "Euclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\n", "We address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\n", "We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\n", "Empirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\n", "We discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\n", "On a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset.", "We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\n", "points in Spearman rank correlation on similarity tasks\n", "and 3.4 points on analogy accuracy.\n"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJxeWnCcF7", "target": ["Los espacios de incrustaci\u00f3n de colectores de productos con curvatura heterog\u00e9nea ofrecen representaciones mejoradas en comparaci\u00f3n con los espacios de incrustaci\u00f3n tradicionales para una variedad de estructuras.", "Propone un m\u00e9todo de reducci\u00f3n de la dimensionalidad que incrusta los datos en una variedad de productos de variedades esf\u00e9ricas, euclidianas e hiperb\u00f3licas. El algoritmo se basa en hacer coincidir las distancias geod\u00e9sicas en el colector producto con las distancias de los gr\u00e1ficos."]}
{"source": ["Synthesizing user-intended programs from a small number of input-output exam-\n", "ples is a challenging problem with several important applications like spreadsheet\n", "manipulation, data wrangling and code refactoring.", "Existing synthesis systems\n", "either completely rely on deductive logic techniques that are extensively hand-\n", "engineered or on purely statistical models that need massive amounts of data, and in\n", "general fail to provide real-time synthesis on challenging benchmarks.", "In this work,\n", "we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\n", "that combines the best of both symbolic logic techniques and statistical models.\n", "Thus, it produces programs that satisfy the provided specifications by construction\n", "and generalize well on unseen examples, similar to data-driven systems.", "Our\n", "technique effectively utilizes the deductive search framework to reduce the learning\n", "problem of the neural component to a simple supervised learning setup.", "Further,\n", "this allows us to both train on sparingly available real-world data and still leverage\n", "powerful recurrent neural network encoders.", "We demonstrate the effectiveness\n", "of our method by evaluating on real-world customer scenarios by synthesizing\n", "accurate programs with up to 12\u00d7 speed-up compared to state-of-the-art systems."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rywDjg-RW", "target": ["Integramos m\u00e9todos simb\u00f3licos (deductivos) y estad\u00edsticos (basados en la neurona) para permitir la s\u00edntesis de programas en tiempo real con una generalizaci\u00f3n casi perfecta a partir de un ejemplo de entrada-salida.", "El art\u00edculo presenta un enfoque de branch-and-bound para aprender buenos programas en el que se utiliza un LSTM para predecir qu\u00e9 ramas del \u00e1rbol de b\u00fasqueda deber\u00edan conducir a buenos programas", "Propone un sistema que sintetiza programas a partir de un \u00fanico ejemplo que generaliza mejor que el estado de la t\u00e9cnica anterior"]}
{"source": ["Variational auto-encoders\n (VAEs) offer a tractable approach when performing approximate inference in otherwise intractable generative models.", "However, standard VAEs often produce latent codes that are disperse and lack interpretability, thus making the resulting representations unsuitable for auxiliary tasks (e.g. classi\ufb01cation) and human interpretation.", "We address these issues by merging ideas from variational auto-encoders and sparse coding, and propose to explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution.", "We derive the evidence lower bound using a discrete mixture recognition function thereby making approximate posterior inference as computational ef\ufb01cient as in the standard VAE case.", "With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models.", "We show that these sparse representations are advantageous over standard VAE representations on two benchmark classi\ufb01cation tasks (MNIST and Fashion-MNIST) by demonstrating improved classi\ufb01cation accuracy and signi\ufb01cantly increased robustness to the number of latent dimensions.", "Furthermore, we demonstrate qualitatively that the sparse elements capture subjectively understandable sources of variation."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SkeJ6iR9Km", "target": ["Exploramos la intersecci\u00f3n entre las VAE y la codificaci\u00f3n dispersa.", "Este trabajo propone una extensi\u00f3n de los VAE con priores y posteriors dispersos para aprender representaciones interpretables dispersas."]}
{"source": ["A widely observed phenomenon in deep learning is the degradation problem: increasing\n", "the depth of a network leads to a decrease in performance on both test and training data.", "Novel architectures such as ResNets and Highway networks have addressed this issue by introducing various flavors of skip-connections or gating mechanisms.", "However, the degradation problem persists in the context of plain feed-forward networks.", "In this work we propose a simple method to address this issue.", "The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers.", "This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner.", "We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to greatly decrease the degradation effect (compared to plain networks) and is often competitive with ResNets."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJQPG5lR-", "target": ["La eliminaci\u00f3n progresiva de las conexiones de salto de una manera principista evita la degradaci\u00f3n en las redes profundas feed-forward.", "Los autores presentan una nueva estrategia de entrenamiento, VAN, para entrenar redes feed-forward muy profundas sin conexiones de salto", "El art\u00edculo introduce una arquitectura que interpola linealmente entre ResNets y redes profundas de vainilla sin conexiones de salto."]}
{"source": ["Deep learning is becoming more widespread in its application due to its power in solving complex classification problems.", "However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications.", "This work addresses the problem by proposing methods {\\em Weight Reduction Quantisation} for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight.", "Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem.", "Our method that mini-batch SVRG with $\\ell$1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates.", "Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60$\\times$ without affecting their test accuracy."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Sk0pHeZAW", "target": ["Compresi\u00f3n de redes neuronales profundas desplegadas en dispositivos embebidos. ", "Los autores presentan un algoritmo de entrenamiento basado en el SVRG regularizado l-1 que es capaz de forzar que muchos pesos de la red sean 0.", "Este trabajo reduce los requisitos de memoria."]}
{"source": ["It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment.", "For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus.", "Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment.", "However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain.", "In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment.", "Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm.", "To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training.", "Despite the novel features of these objects the model is able to infer the latent representations for them.", "Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hy8hkYeRb", "target": ["Un algoritmo de aprendizaje basado en la codificaci\u00f3n predictiva para construir modelos de redes neuronales profundas del cerebro", "El documento considera el aprendizaje de una red neuronal generativa utilizando una configuraci\u00f3n de codificaci\u00f3n predictiva"]}
{"source": ["In this paper, we propose deep convolutional generative adversarial networks (DCGAN) that learn to produce a 'mental image' of the input image as internal representation of a certain category of input data distribution.  ", "This mental image is what the DCGAN 'imagines' that the input image might look like under ideal conditions.  ", "The mental image contains a version of the input that is iconic, without any peculiarities that do not contribute to the ideal representation of the input data distribution within a category.", "A DCGAN learns this association by training an encoder to capture salient features from the original image and a decoder to convert salient features into its associated mental image representation.  ", "Our new approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns features that are useful for recognizing entire classes of objects, and that this in turn has the benefit of helping single and zero shot recognition.  ", "We demonstrate our approach on object instance recognition and handwritten digit recognition tasks."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rynniUpQM", "target": ["El reconocimiento de instancias de objetos con autocodificadores adversariales se realiz\u00f3 con un nuevo objetivo de \"imagen mental\" que es una representaci\u00f3n can\u00f3nica de la imagen de entrada.", "El art\u00edculo propone un m\u00e9todo para aprender caracter\u00edsticas para el reconocimiento de objetos que es invariante a varias transformaciones del objeto, sobre todo la pose del objeto.", "Este trabajo investig\u00f3 la tarea de reconocimiento de unos pocos disparos a trav\u00e9s de una imagen mental generada como representaci\u00f3n intermedia dada la imagen de entrada."]}
{"source": ["An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large number of interactions with the environment in order to master a skill.", "The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task.", "We present a framework that combines techniques in \\textit{formal methods} with \\textit{hierarchical reinforcement learning} (HRL).", "The set of techniques we provide allows for the convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards using any RL methods and is able to construct new skills from existing ones without additional learning.", "We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJgVaG-Ab", "target": ["Combinar la l\u00f3gica temporal con el aprendizaje por refuerzo jer\u00e1rquico para la composici\u00f3n de habilidades", "El art\u00edculo ofrece una estrategia para construir un MDP de producto a partir de un MDP original y el aut\u00f3mata asociado a una f\u00f3rmula LTL.", "Propone unir la l\u00f3gica temporal con el aprendizaje de refuerzo jer\u00e1rquico para simplificar la composici\u00f3n de habilidades."]}
{"source": ["The tremendous memory and computational complexity of Convolutional Neural Networks (CNNs) prevents the inference deployment on resource-constrained systems.", "As a result, recent research focused on CNN optimization techniques, in particular quantization, which allows weights and activations of layers to be represented with just a few bits while achieving impressive prediction performance.", "However, aggressive quantization techniques still fail to achieve full-precision prediction performance on state-of-the-art CNN architectures on large-scale classification tasks.", "In this work we propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs.", "Our weight quantization scheme is based on trainable scaling factors and a nested-means clustering strategy which is robust to weight updates and therefore exhibits good convergence properties.", "The flexibility of nested-means clustering enables exploration of various n-ary weight representations with the potential of high parameter compression.", "For activations, we propose a linear quantization strategy that takes the statistical properties of batch normalization into account.", "We demonstrate the effectiveness of our approach using state-of-the-art models on ImageNet."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HylDpoActX", "target": ["Proponemos un esquema de cuantificaci\u00f3n para los pesos y las activaciones de las redes neuronales profundas. Esto reduce la huella de memoria sustancialmente y acelera la inferencia.", "Compresi\u00f3n de modelos CNN y aceleraci\u00f3n de la inferencia mediante cuantificaci\u00f3n."]}
{"source": ["Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently.", "This means that we must not only specify what to do, but also the much larger space of what not to do.", "It is easy to forget these preferences, since these preferences are already satisfied in our environment.", "This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want.", "We can therefore use this implicit preference information from the state to fill in the blanks.", "We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties.", "We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.", "Our code can be found at https://github.com/HumanCompatibleAI/rlsp."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkevMnRqYQ", "target": ["Cuando un robot se despliega en un entorno en el que los humanos han actuado, el estado del entorno ya est\u00e1 optimizado para lo que los humanos quieren, y podemos utilizarlo para inferir las preferencias humanas.", "Los autores proponen aumentar la funci\u00f3n de recompensa expl\u00edcita de un agente RL con recompensas/costes auxiliares inferidos del estado inicial y un modelo de la din\u00e1mica del estado", "Este trabajo propone una forma de inferir la informaci\u00f3n impl\u00edcita en el estado inicial utilizando IRL y combinar la recompensa inferida con una recompensa especificada."]}
{"source": ["Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other.", "In our work we present a novel, systematic, unifying taxonomy to categorize existing methods.", "We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures.", "We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on.", "We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories.", "This helps revealing links and fundamental similarities between them.", "Finally, we include practical recommendations both for users and for developers of new regularization methods."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SkHkeixAW", "target": ["Categorizaci\u00f3n sistem\u00e1tica de los m\u00e9todos de regularizaci\u00f3n para el aprendizaje profundo, revelando sus similitudes.", "Intenta construir una taxonom\u00eda para las t\u00e9cnicas de regularizaci\u00f3n empleadas en el aprendizaje profundo."]}
{"source": ["Deep neural networks are surprisingly efficient at solving practical tasks,\n", "but the theory behind this phenomenon is only starting to catch up with\n", "the practice.", "Numerous works show that depth is the key to this efficiency.\n", "A certain class of deep convolutional networks \u2013 namely those that correspond\n", "to the Hierarchical Tucker (HT) tensor decomposition \u2013 has been\n", "proven to have exponentially higher expressive power than shallow networks.\n", "I.e. a shallow network of exponential width is required to realize\n", "the same score function as computed by the deep architecture.", "In this paper,\n", "we prove the expressive power theorem (an exponential lower bound on\n", "the width of the equivalent shallow network) for a class of recurrent neural\n", "networks \u2013 ones that correspond to the Tensor Train (TT) decomposition.\n", "This means that even processing an image patch by patch with an RNN\n", "can be exponentially more efficient than a (shallow) convolutional network\n", "with one hidden layer.", "Using theoretical results on the relation between\n", "the tensor decompositions we compare expressive powers of the HT- and\n", "TT-Networks.", "We also implement the recurrent TT-Networks and provide\n", "numerical evidence of their expressivity."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1WRibb0Z", "target": ["Demostramos la eficiencia exponencial de las redes neuronales de tipo recurrente sobre las redes superficiales.", "Los autores comparan la complejidad de las redes de tren tensorial con las redes estructuradas por descomposici\u00f3n CP"]}
{"source": ["Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN).", "In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior.", "Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference.", "Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution.", "Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1l7bnR5Ym", "target": ["Un novedoso tratamiento probabil\u00edstico para GAN con garant\u00eda te\u00f3rica.", "Este trabajo propone un GAN bayesiano que tiene garant\u00edas te\u00f3ricas de convergencia a la distribuci\u00f3n real y pone verosimilitudes sobre el generador y el discriminador con logaritmos proporcionales a las funciones objetivo del GAN tradicional."]}
{"source": ["In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and applies a \\emph{small perturbation} to  $\\bfx$ to produce another point $\\bfx'$ that $F$ classifies \\emph{incorrectly}.", " In this paper, we propose taking into account \\emph{the inherent confidence information} produced by models when studying adversarial perturbations, where a natural measure of ``confidence'' is \\|F(\\bfx)\\|_\\infty$ (i.e. how confident $F$ is about its prediction?)", ". Motivated by a thought experiment based on the manifold assumption, we propose a ``goodness property'' of models which states that \\emph{confident regions of a good model should be well separated}.", "We give formalizations of this property and examine existing robust training objectives in view of them.", "Interestingly, we find that a recent objective by Madry et al. encourages training a model that satisfies well our formal version of the goodness property, but has a weak control of points that are wrong but with low confidence.", "However, if Madry et al.'s model is indeed a good solution to their objective, then good and bad points are now distinguishable and we can try to embed uncertain points back to the closest confident region to get (hopefully) correct predictions.", "We thus propose embedding objectives and algorithms, and perform an empirical study using this method.", "Our experimental results are encouraging: Madry et al.'s model wrapped with our embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining good generalization behavior.\n"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hk-FlMbAZ", "target": ["Defensa contra las perturbaciones adversas de las redes neuronales a partir de la suposici\u00f3n de la matriz ", "El manuscrito propone dos funciones objetivo basadas en la suposici\u00f3n del m\u00faltiple como mecanismos de defensa contra los ejemplos adversos.", "Defensa contra ataques adversarios basados en la suposici\u00f3n de datos naturales"]}
{"source": ["Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space.", "Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method.", "In DSO-NAS, we provide a novel model pruning view to NAS problem.", "In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations.", "Next, we impose sparse regularizations to prune useless connections in the architecture.", "Lastly, we derive an efficient and theoretically sound optimization method to solve it.", "Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet.", "Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84%, while on the ImageNet dataset DSO-NAS achieves 25.4% test error under 600M FLOPs with 8 GPUs in 18 hours."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryxjH3R5KQ", "target": ["b\u00fasqueda de arquitecturas neuronales de una sola vez mediante la optimizaci\u00f3n directa de la dispersi\u00f3n", "Presenta un m\u00e9todo de b\u00fasqueda de arquitectura en el que se eliminan las conexiones con regularizaci\u00f3n dispersa.", "Este trabajo propone la Optimizaci\u00f3n Directa Sparse, que es un m\u00e9todo para obtener arquitecturas neuronales en problemas espec\u00edficos, a un coste computacional razonable.", "Este trabajo propone un m\u00e9todo de b\u00fasqueda de arquitecturas neuronales basado en una optimizaci\u00f3n directa dispersa"]}
{"source": ["Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning.", "One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices.", "This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks.", "The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels.", "The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  ", "We validate both methods through experiments on convolutional and recurrent architectures.", "We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction.", "In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n"], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "S1XolQbRW", "target": ["Obtiene la precisi\u00f3n m\u00e1s avanzada para redes cuantificadas y poco profundas aprovechando la destilaci\u00f3n. ", "Propone modelos peque\u00f1os y de bajo coste combinando destilaci\u00f3n y cuantificaci\u00f3n para experimentos de visi\u00f3n y traducci\u00f3n autom\u00e1tica neural", "Este trabajo presenta un marco de uso del modelo de maestro para ayudar a la compresi\u00f3n para el modelo de aprendizaje profundo en el contexto de la compresi\u00f3n del modelo."]}
{"source": ["Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation.", "However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful.", "In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation.", "We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side.", "Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Bkl1uWb0Z", "target": ["mejorar la NMT con \u00e1rboles latentes", "Este art\u00edculo describe un m\u00e9todo para inducir estructuras de dependencia del lado de la fuente al servicio de la traducci\u00f3n autom\u00e1tica neural."]}
{"source": ["Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards.", "We explore a method to improve the sample efficiency when we have access to demonstrations.", "Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task.", "Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state.", "Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency.", "This includes reward shaping, behavioral cloning, and reverse curriculum generation."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "H1xk8jAqKQ", "target": ["Aprenda trabajando hacia atr\u00e1s a partir de una sola demostraci\u00f3n, incluso una ineficiente, y haga que el agente haga progresivamente m\u00e1s de la resoluci\u00f3n por s\u00ed mismo.", "Este trabajo presenta un m\u00e9todo para aumentar la eficiencia de los m\u00e9todos RL de recompensa dispersa a trav\u00e9s de un curr\u00edculo hacia atr\u00e1s en las demostraciones de los expertos. ", "El art\u00edculo presenta una estrategia para resolver tareas de recompensa dispersas con RL mediante el muestreo de estados iniciales a partir de demostraciones."]}
{"source": ["Episodic memory is a psychology term which refers to the ability to recall specific events from the past.", "We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful.", "Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states.", "The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on.", "Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory.", "Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.\n"], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "ByJDAIe0b", "target": ["Memoria externa para el aprendizaje de refuerzo en l\u00ednea basado en la estimaci\u00f3n de gradientes sobre una novedosa t\u00e9cnica de muestreo de dep\u00f3sitos.", "El art\u00edculo propone un enfoque modificado de la RL, en el que el agente mantiene una \"memoria epis\u00f3dica\" adicional y utiliza una \"red de consulta\" que se basa en el estado actual."]}
{"source": ["We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation.", "Our decomposition leads to an interesting phenomenon that the variance does not necessarily increase when more parameters are included in Boltzmann machines, while the bias always decreases.", "Our result gives a theoretical evidence of the generalization ability of deep learning architectures because it provides the possibility of increasing the representation power with avoiding the variance inflation."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "rkMt1bWAZ", "target": ["Conseguimos la descomposici\u00f3n sesgo-varianza para las m\u00e1quinas de Boltzmann utilizando una formulaci\u00f3n geom\u00e9trica de la informaci\u00f3n.", "El objetivo de este trabajo es analizar la eficacia y la generalizabilidad del aprendizaje profundo presentando un an\u00e1lisis te\u00f3rico de la descomposici\u00f3n sesgo-varianza para modelos jer\u00e1rquicos, concretamente las m\u00e1quinas de Boltzmann  ", "El documento llega a la conclusi\u00f3n principal de que es posible reducir tanto el sesgo como la varianza en un modelo jer\u00e1rquico."]}
{"source": ["Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network.  ", "Following recent work in simplifying these networks with model pruning and a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs.  ", "We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout.  ", "With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a density of 30%.  ", "Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x, enabling larger networks to help advance the state-of-the-art.  ", "We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkxF5RgC-", "target": ["Combinando la poda de la red y los n\u00facleos persistentes en una implementaci\u00f3n de red pr\u00e1ctica, r\u00e1pida y precisa.", "Este trabajo introduce las RNN dispersas persistentes, un mecanismo para a\u00f1adir la poda al trabajo existente de almacenar los pesos de las RNN en un chip."]}
{"source": ["Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy.", "Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources.", "Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists.", "In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed.", "In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm.", "The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process.", "Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate.", "Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1D8MPxA-", "target": ["Presentamos un nuevo m\u00e9todo de poda y un formato de matriz dispersa para permitir un alto \u00edndice de compresi\u00f3n y un proceso de decodificaci\u00f3n de \u00edndices en paralelo.", "Los autores utilizan la codificaci\u00f3n Viterbi para comprimir dr\u00e1sticamente el \u00edndice de la matriz dispersa de un grafo podado, reduciendo una de las principales cargas de memoria y acelerando la inferencia en el entorno paralelo."]}
{"source": ["Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL).", "It is also a requirement for its deployment in real-world scenarios.", "This paper proposes a novel framework for efficient multi-task reinforcement learning.", "Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.", "This enables agents to continually acquire new skills during different stages of training.", "Each learned task corresponds to a human language description.", "Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices.", "In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills.", "We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJJQVZW0b", "target": ["Una novedosa red pol\u00edtica jer\u00e1rquica que puede reutilizar habilidades previamente aprendidas junto con y como subcomponentes de nuevas habilidades descubriendo las relaciones subyacentes entre habilidades.", "El objetivo de este trabajo es aprender pol\u00edticas jer\u00e1rquicas utilizando una estructura de pol\u00edtica recursiva regulada por una gram\u00e1tica temporal estoc\u00e1stica", "Este trabajo propone un enfoque para el aprendizaje de pol\u00edticas jer\u00e1rquicas en un contexto de aprendizaje permanente mediante el apilamiento de pol\u00edticas y el uso de una pol\u00edtica expl\u00edcita de \"cambio\"."]}
{"source": ["Embeddings are a fundamental component of many modern machine learning and natural language processing models.\n", "Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models.\n", "State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging.\n", "In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology.\n", "This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces.\n", "We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study.", "The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Skz3Q2CcFX", "target": ["Proponemos utilizar la proyecci\u00f3n de f\u00f3rmulas algebraicas vectoriales expl\u00edcitas como una forma alternativa de visualizar los espacios de incrustaci\u00f3n espec\u00edficamente adaptados a las tareas de an\u00e1lisis orientadas a objetivos y supera al t-SNE en nuestro estudio de usuarios.", "An\u00e1lisis de los espacios de incrustaci\u00f3n de forma no param\u00e9trica (basada en ejemplos_)"]}
{"source": ["Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers.  ", "Towardsthis end, a typical strategy is to apply data augmentation to enlarge the trainingset.   ", "However,  standard  data  augmentation  is  essentially  a  brute-force  strategywhich is inefficient,  as it performs all the pre-defined transformations  to everytraining sample.", "In this paper, we propose a principled approach to train networkswith  significantly  improved  resistance  to  large  variations  between  training  andtesting data.  ", "This is achieved by embedding a learnable transformation moduleinto the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al.,2018), which is a convolutional neural network (CNN) classifier empowered withgenerative capabilities.  ", "Our approach alternatively synthesizes pseudo-negativesamples with learned transformations and enhances the classifier by retraining itwith synthesized samples.  ", "Experimental results verify that our approach signif-icantly improves the ability of deep networks to resist large variations betweentraining and testing data and achieves classification accuracy improvements onseveral benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SyG1QnRqF7", "target": ["Proponemos un enfoque basado en principios que dota a los clasificadores de la capacidad de resistir las grandes variaciones entre los datos de entrenamiento y los de prueba de una manera inteligente y eficiente.", "Utilizaci\u00f3n del aprendizaje introspectivo para gestionar las variaciones de los datos en el momento de la prueba", "Este trabajo sugiere el uso de redes de transformaci\u00f3n aprendidas, incrustadas dentro de redes introspectivas para mejorar el rendimiento de la clasificaci\u00f3n con ejemplos sintetizados."]}
{"source": ["It is well known that it is possible to construct \"adversarial examples\"\n", "for neural networks: inputs which are misclassified by the network\n", "yet indistinguishable from true data.", "We propose a simple\n", "modification to standard neural network architectures, thermometer\n", "encoding, which significantly increases the robustness of the network to\n", "adversarial examples.", "We demonstrate this robustness with experiments\n", "on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that\n", "models with thermometer-encoded inputs consistently have higher accuracy\n", "on adversarial examples, without decreasing generalization.\n", "State-of-the-art accuracy under the strongest known white-box attack was \n", "increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10.\n", "We explore the properties of these networks, providing evidence\n", "that thermometer encodings help neural networks to\n", "find more-non-linear decision boundaries."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S18Su--CW", "target": ["La discretizaci\u00f3n de la entrada permite la robustez frente a los ejemplos adversos", "Los autores presentan un estudio en profundidad de la discretizaci\u00f3n/cuantificaci\u00f3n de la entrada como defensa contra los ejemplos adversos"]}
{"source": ["Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.\n", "Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.\n", "These bounds tend to depend on the dimension of the model $d$ in that the number of bits needed to achieve a particular error bound increases as $d$ increases.\n", "This is undesirable because a motivating application for low-precision training is large-scale models, such as deep learning, where $d$ can be huge.\n", "In this paper, we prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic, which lets us better understand what affects the convergence of these algorithms as parameters scale.\n", "Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "ryeX-nC9YQ", "target": ["demostramos l\u00edmites independientes de la dimensi\u00f3n para algoritmos de entrenamiento de baja precisi\u00f3n", "Este art\u00edculo analiza las condiciones en las que la convergencia de los modelos de entrenamiento con pesos de baja precisi\u00f3n no depende de la dimensi\u00f3n del modelo."]}
{"source": ["We consider the problem of exploration in meta reinforcement learning.", "Two new meta reinforcement learning algorithms are suggested: E-MAML and ERL2.", "Results are presented on a novel environment we call 'Krazy World'  and a set of maze environments.", "We show E-MAML and ERL2 deliver better performance on tasks where exploration is important."], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "Skk3Jm96W", "target": ["Modificaciones en MAML y RL2 que deber\u00edan permitir una mejor exploraci\u00f3n. ", "El art\u00edculo propone un truco de ampliaci\u00f3n de las funciones objetivo para impulsar la exploraci\u00f3n en meta-RL sobre dos algoritmos meta-RL recientes"]}
{"source": ["We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs.", "The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions.", "We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively.", "Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "ryxhB3CcK7", "target": ["Un modelo simb\u00f3lico neuronal probabil\u00edstico con un espacio program\u00e1tico latente, para responder a preguntas m\u00e1s interpretables", "Este trabajo propone un modelo discreto y estructurado de variables latentes para la respuesta a preguntas visuales que implica la generalizaci\u00f3n y el razonamiento composicional con una ganancia significativa en el rendimiento y la capacidad."]}
{"source": ["The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network.", "In recent years, several techniques have been proposed for training networks that are robust to such examples; and each time stronger attacks have been devised, demonstrating the shortcomings of existing defenses.", "This highlights a key difficulty in designing an effective defense: the inability to assess a network's robustness against future attacks.", "We propose to address this difficulty through formal verification techniques.", "We construct ground truths: adversarial examples with a provably-minimal distance from a given input point.", "We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths; and also of defense techniques, by computing the distance to the ground truths before and after the defense is applied, and measuring the improvement.", "We use this technique to assess recently suggested attack and defense techniques.\n"], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Hki-ZlbA-", "target": ["Utilizamos la verificaci\u00f3n formal para evaluar la eficacia de las t\u00e9cnicas para encontrar ejemplos adversos o para defenderse de ellos.", "Este trabajo propone un m\u00e9todo para calcular ejemplos adversarios con una distancia m\u00ednima a las entradas originales.", "Los autores proponen emplear ejemplos de distancia m\u00ednima demostrable como herramienta para evaluar la robustez de una red entrenada.", "El documento describe un m\u00e9todo para generar ejemplos adversarios que tienen una distancia m\u00ednima con el ejemplo de entrenamiento utilizado para generarlos"]}
{"source": ["This paper introduces a new framework for open-domain question answering in which the retriever and the reader \\emph{iteratively interact} with each other.", "The framework is agnostic to the architecture of the machine reading model provided it has \\emph{access} to the token-level hidden representations of the reader.", "The retriever uses fast nearest neighbor search that allows it to scale to corpora containing millions of paragraphs.", "A gated recurrent unit updates the query at each step conditioned on the \\emph{state} of the reader and the \\emph{reformulated} query is used to re-rank the paragraphs by the retriever.", "We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.", "Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures (\\drqa and \\bidaf) on various large open-domain datasets ---\\tqau, \\quasart, \\searchqa, and \\squado\\footnote{Code and pretrained models are available at \\url{https://github.com/rajarshd/Multi-Step-Reasoning}}."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkfPSh05K7", "target": ["El recuperador de p\u00e1rrafos y el lector autom\u00e1tico interact\u00faan entre s\u00ed mediante el aprendizaje por refuerzo para obtener grandes mejoras en conjuntos de datos de dominio abierto", "El art\u00edculo introduce un nuevo marco de interacci\u00f3n bidireccional entre el recuperador de documentos y el lector para la respuesta a preguntas de dominio abierto con la idea del \"estado del lector\" del lector al recuperador.", "El art\u00edculo propone un modelo de lectura autom\u00e1tica extractiva de varios documentos, compuesto por 3 partes distintas y un algoritmo."]}
{"source": ["Many imaging tasks require global information about all pixels in an image.", "Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and down-sampled into a single output.", "But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs.", "To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost.", "This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution.", "SUNets leverage the information globalization power of u-nets in a deeper net- work architectures that is capable of handling the complexity of natural images.", "SUNets perform extremely well on semantic segmentation tasks using a small number of parameters."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJgFcj0qKX", "target": ["Presenta una nueva arquitectura que aprovecha el poder de globalizaci\u00f3n de la informaci\u00f3n de las u-nets en una red m\u00e1s profunda y que se desempe\u00f1a bien en todas las tareas sin ning\u00fan tipo de campanas y silbatos.", "Una arquitectura de red para la segmentaci\u00f3n sem\u00e1ntica de im\u00e1genes, basada en la composici\u00f3n de una pila de arquitecturas U-Net b\u00e1sicas, que reduce el n\u00famero de par\u00e1metros y mejora los resultados.", "Se propone una arquitectura U-Net apilada para la segmentaci\u00f3n de im\u00e1genes."]}
{"source": ["Asking questions is an important ability for a chatbot.", "This paper focuses on question generation.", "Although there are existing works on question generation based on a piece of descriptive text, it remains to be a very challenging problem.", "In the paper, we propose a new question generation problem, which also requires the input of a target topic in addition to a piece of descriptive text.", "The key reason for proposing the new problem is that in practical applications, we found that useful questions need to be targeted toward some relevant topics.", "One almost never asks a random question in a conversation.", "Due to the fact that given a descriptive text, it is often possible to ask many types of questions, generating a question without knowing what it is about is of limited use.", "To solve the problem, we propose a novel neural network that is able to generate topic-specific questions.", "One major advantage of this model is that it can be trained directly using a question-answering corpus without requiring any additional annotations like annotating topics in the questions or answers.", "Experimental results show that our model outperforms the state-of-the-art baseline."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rk3pnae0b", "target": ["Proponemos una red neuronal capaz de generar preguntas sobre temas espec\u00edficos.", "Presenta un enfoque basado en redes neuronales para generar preguntas sobre temas espec\u00edficos con la motivaci\u00f3n de que las preguntas sobre temas son m\u00e1s significativas en las aplicaciones pr\u00e1cticas.", "Propone un m\u00e9todo de generaci\u00f3n basado en temas utilizando un LSTM para extraer temas mediante una t\u00e9cnica de codificaci\u00f3n en dos etapas"]}
{"source": ["Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option\n", "to restore voluntary movements after paralysis.", "These devices are based on the\n", "ability to extract information about movement intent from neural signals recorded\n", "using multi-electrode arrays chronically implanted in the motor cortices of the\n", "brain.", "However, the inherent loss and turnover of recorded neurons requires repeated\n", "recalibrations of the interface, which can potentially alter the day-to-day\n", "user experience.", "The resulting need for continued user adaptation interferes with\n", "the natural, subconscious use of the BMI.", "Here, we introduce a new computational\n", "approach that decodes movement intent from a low-dimensional latent representation\n", "of the neural data.", "We implement various domain adaptation methods\n", "to stabilize the interface over significantly long times.", "This includes Canonical\n", "Correlation Analysis used to align the latent variables across days; this method\n", "requires prior point-to-point correspondence of the time series across domains.\n", "Alternatively, we match the empirical probability distributions of the latent variables\n", "across days through the minimization of their Kullback-Leibler divergence.\n", "These two methods provide a significant and comparable improvement in the performance\n", "of the interface.", "However, implementation of an Adversarial Domain\n", "Adaptation Network trained to match the empirical probability distribution of the\n", "residuals of the reconstructed neural signals outperforms the two methods based\n", "on latent variables, while requiring remarkably few data points to solve the domain\n", "adaptation problem."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hyx6Bi0qYm", "target": ["Implementamos una red de adaptaci\u00f3n de dominio adversarial para estabilizar una interfaz cerebro-m\u00e1quina fija frente a cambios graduales en las se\u00f1ales neuronales registradas.", "Describe un nuevo enfoque para la interfaz cerebro-m\u00e1quina implantada con el fin de abordar el problema de la calibraci\u00f3n y el cambio de covarianza. ", "Los autores definen un IMC que utiliza un autocodificador y luego abordan el problema de la deriva de los datos en el IMC."]}
{"source": ["Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds.", "To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words.", "While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so.", "Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions.", "For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world.", "We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge.", "We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "ByZmGjkA-", "target": ["Analizar y comprender c\u00f3mo los agentes de las redes neuronales aprenden a comprender un lenguaje sencillo en tierra", "Los autores conectan los m\u00e9todos experimentales psicol\u00f3gicos para entender c\u00f3mo la caja negra de los m\u00e9todos de aprendizaje profundo resuelve los problemas.", "Este trabajo presenta un an\u00e1lisis de los agentes que aprenden el lenguaje de base mediante el aprendizaje por refuerzo en un entorno sencillo que combina la instrucci\u00f3n verbal con la informaci\u00f3n visual"]}
{"source": ["Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future.", "In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos.", "Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future.", "Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJe10iC5K7", "target": ["Aprender las partes del objeto, la estructura jer\u00e1rquica y la din\u00e1mica observando c\u00f3mo se mueven", "Propone un modelo de aprendizaje no supervisado que aprende a desentra\u00f1ar los objetos en partes, predecir la estructura jer\u00e1rquica de las partes y, bas\u00e1ndose en las partes desentra\u00f1adas y la jerarqu\u00eda, predecir el movimiento."]}
{"source": ["A successful application of convolutional architectures is to increase the resolution of single low-resolution images -- a image restoration task called super-resolution (SR).", "Naturally, SR is of value to resource constrained devices like mobile phones, electronic photograph frames and televisions to enhance image quality.", "However, SR demands perhaps the most extreme amounts of memory and compute operations of any mainstream vision task known today, preventing SR from being deployed to devices that require them.", "In this paper, we perform a early systematic study of system resource efficiency for SR, within the context of a variety of architectural and low-precision approaches originally developed for discriminative neural networks.", "We present a rich set of insights, representative SR architectures, and efficiency trade-offs; for example, the prioritization of ways to compress models to reach a specific memory and computation target and techniques to compact SR models so that they are suitable for DSPs and FPGAs.", "As a result of doing so, we manage to achieve better and comparable performance with previous models in the existing literature, highlighting the practicality of using existing efficiency techniques in SR tasks.", "Collectively, we believe these results provides the foundation for further research into the little explored area of resource efficiency for SR."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BkgGmh09FQ", "target": ["Construimos una comprensi\u00f3n de las t\u00e9cnicas de eficiencia de recursos en Super-Resoluci\u00f3n", "El art\u00edculo propone una evaluaci\u00f3n emp\u00edrica detallada de las compensaciones logradas por varias redes neuronales convolucionales en el problema de la superresoluci\u00f3n.", "Este trabajo propone mejorar la eficiencia de los recursos del sistema para las redes de superresoluci\u00f3n."]}
{"source": ["Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy.", "In this work we present properties of neural networks that complement this aspect of expressivity.", "By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior.", "Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples.", "We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior.", "Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1gR2sC9FX", "target": ["Investigamos las redes ReLU en el dominio de Fourier y demostramos un comportamiento peculiar.", "An\u00e1lisis de Fourier de la red ReLU, encontrando que est\u00e1n sesgados hacia el aprendizaje de baja frecuencia ", "Este trabajo tiene aportaciones te\u00f3ricas y emp\u00edricas sobre el tema de los coeficientes de Fourier de las redes neuronales"]}
{"source": ["Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering.", "Many metric learning methods represent the input as a single point in the embedding space.", "Often the distance between points is used as a proxy for match confidence.", "However, this can fail to represent uncertainty which can arise when the input is ambiguous, e.g., due to occlusion or blurriness.", "This work addresses this issue and explicitly models the uncertainty by \u201chedging\u201d the location of each input in the embedding space.", "We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle (Alemi et al., 2016; Achille & Soatto, 2018).", "Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of \u201chedging its bets\u201d across the embedding space upon encountering ambiguous inputs.", "This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure which is correlated with downstream performance."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1xQQhAqKX", "target": ["El art\u00edculo propone utilizar distribuciones de probabilidad en lugar de puntos para tareas de incrustaci\u00f3n de instancias como el reconocimiento y la verificaci\u00f3n.", "El art\u00edculo propone una alternativa a la incrustaci\u00f3n de puntos actual y una t\u00e9cnica para entrenarlos.", "El trabajo propone un modelo que utiliza incertidumbres-embeddings para extender el aprendizaje profundo a las aplicaciones bayesianas"]}
{"source": ["Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  ", "While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  ", "Despite its success, this approach has notable drawbacks.", "Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \n", "We present two new techniques to address these problems.  ", "First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network.", "Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  ", "Both the contraction and regression weights are learned end-to-end by backpropagation.", "By imposing low rank on both, we use significantly fewer parameters.  ", "Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "S16FPMgRZ", "target": ["Proponemos capas de contracci\u00f3n tensorial y de regresi\u00f3n tensorial de bajo rango para preservar y aprovechar la estructura multilineal en toda la red, lo que supone un enorme ahorro de espacio con un impacto m\u00ednimo en el rendimiento.", "Este trabajo propone nuevas arquitecturas de capas de redes neuronales que utilizan una representaci\u00f3n de bajo rango de los tensores", "Este trabajo incorpora la descomposici\u00f3n tensorial y la regresi\u00f3n tensorial en la CNN utilizando una nueva capa de regresi\u00f3n tensorial."]}
{"source": ["We explore ways of incorporating bilingual dictionaries to enable semi-supervised\n", "neural machine translation.", "Conventional back-translation methods have shown\n", "success in leveraging target side monolingual data.", "However, since the quality of\n", "back-translation models is tied to the size of the available parallel corpora, this\n", "could adversely impact the synthetically generated sentences in a low resource\n", "setting.", "We propose a simple data augmentation technique to address both this\n", "shortcoming.", "We incorporate widely available bilingual dictionaries that yield\n", "word-by-word translations to generate synthetic sentences.", "This automatically\n", "expands the vocabulary of the model while maintaining high quality content.", "Our\n", "method shows an appreciable improvement in performance over strong baselines."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1ecYsqSuN", "target": ["Utilizamos diccionarios biling\u00fces para aumentar los datos de la traducci\u00f3n autom\u00e1tica neural", "Este art\u00edculo investiga el uso de diccionarios biling\u00fces para crear fuentes sint\u00e9ticas para los datos monoling\u00fces del lado de destino con el fin de mejorar los modelos NMT entrenados con peque\u00f1as cantidades de datos paralelos."]}
{"source": ["Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity.", "One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning.", "In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus.", "Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward.", "We propose a new curiosity method which uses episodic memory to form the novelty bonus.", "To determine the bonus, the current observation is compared with the observations in memory.", "Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics.", "This allows us to overcome the known \"couch-potato\" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences.", "We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo.", "In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM.", "In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.", "The code is available at https://github.com/google-research/episodic-curiosity/."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkeK3s0qKQ", "target": ["Proponemos un novedoso modelo de curiosidad basado en la memoria epis\u00f3dica y en las ideas de alcanzabilidad que nos permite superar los conocidos problemas de \"couch-potato\" de los trabajos anteriores.", "Propone dar bonificaciones de exploraci\u00f3n en los algoritmos de RL dando mayores bonificaciones a las observaciones que son padre en los pasos del entorno.", "Los autores proponen un bono de exploraci\u00f3n que tiene como objetivo ayudar en los problemas de RL de recompensa dispersa y considera muchos experimentos en entornos 3D complejos"]}
{"source": ["We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task.", "We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''.", "Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "SkZxCk-0Z", "target": ["Introducimos un nuevo conjunto de datos de vinculaciones l\u00f3gicas con el fin de medir la capacidad de los modelos para capturar y explotar la estructura de las expresiones l\u00f3gicas frente a una tarea de predicci\u00f3n de vinculaciones.", "El art\u00edculo propone un nuevo modelo para utilizar modelos profundos para detectar la vinculaci\u00f3n l\u00f3gica como un producto de funciones continuas sobre mundos posibles.", "Propone un nuevo modelo dise\u00f1ado para el aprendizaje autom\u00e1tico con predicci\u00f3n de vinculaci\u00f3n l\u00f3gica."]}
{"source": ["Deep convolutional neural network (DCNN) based supervised learning is a widely practiced approach for large-scale image classification.  ", "However, retraining these large networks to accommodate new, previously unseen data demands high computational time and energy requirements.", "Also, previously seen training samples may not be available at the time of retraining.", "We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network.", "Our proposed methodology is inspired by transfer learning techniques, although it does not forget previously learned classes.", "An updated network for learning new set of classes is formed using previously learned convolutional layers (shared from initial part of base network) with addition of few newly added convolutional kernels included in the later layers of the network.", "We evaluated the proposed scheme on several recognition applications.", "The classification accuracy achieved by our approach is comparable to the regular incremental learning approach (where networks are updated with new training samples only, without any network sharing)."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hy-lXyDWG", "target": ["El art\u00edculo trata de una nueva metodolog\u00eda de eficiencia energ\u00e9tica para el aprendizaje incremental", "Propone el procedimiento de aprendizaje incremental como aprendizaje por transferencia.", "El art\u00edculo presenta un m\u00e9todo para entrenar redes neuronales convolucionales profundas de forma incremental, en el que los datos est\u00e1n disponibles en peque\u00f1os lotes durante un per\u00edodo de tiempo.", "Presenta una aproximaci\u00f3n al aprendizaje incremental de clase utilizando redes profundas proponiendo tres estrategias de aprendizaje diferentes en el enfoque final/mejor."]}
{"source": ["Recurrent neural networks (RNNs) are widely used to model sequential data but\n", "their non-linear dependencies between sequence elements prevent parallelizing\n", "training over sequence length.", "We show the training of RNNs with only linear\n", "sequential dependencies can be parallelized over the sequence length using the\n", "parallel scan algorithm, leading to rapid training on long sequences even with\n", "small minibatch size.", "We develop a parallel linear recurrence CUDA kernel and\n", "show that it can be applied to immediately speed up training and inference of\n", "several state of the art RNN architectures by up to 9x.", " We abstract recent work\n", "on linear RNNs into a new framework of linear surrogate RNNs and develop a\n", "linear surrogate model for the long short-term memory unit, the GILR-LSTM, that\n", "utilizes parallel linear recurrence.", " We extend sequence learning to new\n", "extremely long sequence regimes that were previously out of reach by\n", "successfully training a GILR-LSTM on a synthetic sequence classification task\n", "with a one million timestep dependency.\n"], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyUNwulC-", "target": ["utilizar la exploraci\u00f3n paralela para paralelizar las redes neuronales recurrentes lineales. entrenar el modelo con una longitud de 1 mill\u00f3n de dependencias", "Propone acelerar la RNN aplicando el m\u00e9todo de Blelloch.", "Los autores proponen un algoritmo paralelo para RNNs Lineales Sustitutos, que produce mejoras de velocidad sobre las implementaciones existentes de Quasi-RNN, SRU y LSTM."]}
{"source": ["Neural text generation models are often autoregressive language models or seq2seq models.", "Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks.", "These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality.", "Language models are typically trained via maximum likelihood and most often with teacher forcing.", "Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time.", "We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation.", "GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them.", "We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context.", "We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "ByOExmWAb", "target": ["GAN de lenguaje natural para rellenar el espacio en blanco", "Este trabajo propone generar texto utilizando GANs.", "Generaci\u00f3n de muestras de texto mediante GAN y un mecanismo para rellenar las palabras que faltan condicionado por el texto circundante"]}
{"source": ["Parametric texture models have been applied successfully to synthesize artificial images.", "Psychophysical studies show that under defined conditions observers are unable to differentiate between model-generated and original natural textures.", "In industrial applications the reverse case is of interest: a texture analysis system should decide if human observers are able to discriminate between a reference and a novel texture.", "For example, in case of inspecting decorative surfaces the de- tection of visible texture anomalies without any prior knowledge is required.", "Here, we implemented a human-vision-inspired novelty detection approach.", "Assuming that the features used for texture synthesis are important for human texture percep- tion, we compare psychophysical as well as learnt texture representations based on activations of a pretrained CNN in a novelty detection scenario.", "Additionally, we introduce a novel objective function to train one-class neural networks for novelty detection and compare the results to standard one-class SVM approaches.", "Our experiments clearly show the differences between human-vision-inspired texture representations and learnt features in detecting visual anomalies.", "Based on a dig- ital print inspection scenario we show that psychophysical texture representations are able to outperform CNN-encoded features."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJEOOsCqKm", "target": ["Comparaci\u00f3n de representaciones de textura psicof\u00edsicas y codificadas por CNN en una aplicaci\u00f3n de detecci\u00f3n de novedades con una red neuronal de una clase.", "Este art\u00edculo se centra en la detecci\u00f3n de novedades y muestra que las representaciones psicof\u00edsicas pueden superar a las caracter\u00edsticas del codificador VGG en alguna parte de esta tarea", "Este trabajo considera la detecci\u00f3n de anomal\u00edas en las texturas y propone una funci\u00f3n de p\u00e9rdida original.", "Propone entrenar dos detectores de anomal\u00edas a partir de tres modelos diferentes para detectar anomal\u00edas perceptivas en texturas visuales."]}
{"source": ["In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues.", "To address these problems, we study a new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector.", "We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regularized SC.", "Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance."], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1kjEuHpZ", "target": ["Proponemos un nuevo tipo de enfoque de regularizaci\u00f3n que fomenta el no solapamiento en el aprendizaje de la representaci\u00f3n, en aras de mejorar la interpretabilidad y reducir el sobreajuste.", "El art\u00edculo introduce un regularizador matricial para inducir simult\u00e1neamente tanto la dispersi\u00f3n como la ortogonalidad aproximada.", "El art\u00edculo estudia un m\u00e9todo de regularizaci\u00f3n para promover la dispersi\u00f3n y reducir el solapamiento entre los soportes de los vectores de peso en las representaciones aprendidas para mejorar la interpretabilidad y evitar el sobreajuste", "El art\u00edculo propone un nuevo enfoque de regularizaci\u00f3n que fomenta simult\u00e1neamente que los vectores de peso (W) sean dispersos y ortogonales entre s\u00ed."]}
{"source": ["Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  ", "Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\n", "We prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data.", "We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\n", "This result leads to a new way of initializing gate biases in LSTMs and GRUs.", "Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n"], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "SJcKhk-Ab", "target": ["Demuestra que los mecanismos de compuerta proporcionan invariabilidad a las transformaciones temporales. Introduce y prueba una nueva inicializaci\u00f3n para las LSTMs a partir de esta idea.", "El art\u00edculo relaciona el dise\u00f1o de las redes recurrentes y su efecto en la forma en que la red reacciona a las transformaciones del tiempo, y lo utiliza para desarrollar un sencillo esquema de inicializaci\u00f3n del sesgo."]}
{"source": ["Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.", "A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.", "In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}.", "In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.", "We call this approach ``learning to teach''.", "In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).", "The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.", "To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding)."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HJewuJWCZ", "target": ["Proponemos y comprobamos la eficacia de aprender a ense\u00f1ar, un nuevo marco para guiar autom\u00e1ticamente el proceso de aprendizaje autom\u00e1tico.", "Este art\u00edculo se centra en la \"ense\u00f1anza autom\u00e1tica\" y propone aprovechar el aprendizaje por refuerzo definiendo la recompensa como la rapidez con la que aprende el alumno y utilizando el gradiente de la pol\u00edtica para actualizar los par\u00e1metros del profesor", "Los autores definen un modelo de aprendizaje profundo compuesto por cuatro componentes: un modelo de alumno, un modelo de profesor, una funci\u00f3n de p\u00e9rdida y un conjunto de datos. ", "Sugiere un marco de \"aprender a ense\u00f1ar\", correspondiente a las opciones sobre los datos que se presentan al alumno."]}
{"source": ["We present DL2, a system for training and querying neural networks with logical constraints.", "The key idea is to translate these constraints into a differentiable loss with desirable mathematical properties and to then either train with this loss in an iterative manner or to use the loss for querying the network for inputs subject to the constraints.", "We empirically demonstrate that DL2 is effective in both training and querying scenarios, across a range of constraints and data sets."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "H1faSn0qY7", "target": ["Una p\u00e9rdida diferenciable para las restricciones l\u00f3gicas de entrenamiento y consulta de las redes neuronales.", "Un marco para convertir las consultas sobre par\u00e1metros y pares de entrada y salida de las redes neuronales en funciones de p\u00e9rdida diferenciables y un lenguaje declarativo asociado para especificar estas consultas", "Este trabajo aborda el problema de combinar enfoques l\u00f3gicos con redes neuronales traduciendo una f\u00f3rmula l\u00f3gica en una funci\u00f3n de p\u00e9rdida no negativa para una red neuronal."]}
{"source": ["Genetic algorithms have been widely used in many practical optimization problems.\n", "Inspired by natural selection, operators, including mutation, crossover\n", "and selection, provide effective heuristics for search and black-box optimization.\n", "However, they have not been shown useful for deep reinforcement learning, possibly\n", "due to the catastrophic consequence of parameter crossovers of neural networks.\n", "Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm\n", "for sample-efficient deep policy optimization.", "GPO uses imitation learning\n", "for policy crossover in the state space and applies policy gradient methods for mutation.\n", "Our experiments on MuJoCo tasks show that GPO as a genetic algorithm\n", "is able to provide superior performance over the state-of-the-art policy gradient\n", "methods and achieves comparable or higher sample efficiency."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByOnmlWC-", "target": ["Enfoque basado en algoritmos gen\u00e9ticos para optimizar las pol\u00edticas de las redes neuronales profundas", "Los autores presentan un algoritmo para el entrenamiento de conjuntos de redes de pol\u00edticas que mezcla regularmente diferentes pol\u00edticas del conjunto.", "Este trabajo propone un m\u00e9todo de optimizaci\u00f3n de pol\u00edticas inspirado en un algoritmo gen\u00e9tico, que imita los operadores de mutaci\u00f3n y cruce sobre redes de pol\u00edticas."]}
{"source": ["To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights.", "One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping.", "Despite its empirical success, little is understood about why the straight-through gradient method works.\n", "Building upon a novel observation that the straight-through gradient method is in fact identical to the well-known Nesterov\u2019s dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant , that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method.", "ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness.", "For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization.", "We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyzMyhCcK7", "target": ["Un marco de principios para la cuantificaci\u00f3n de modelos utilizando el m\u00e9todo del gradiente proximal, con evaluaci\u00f3n emp\u00edrica y an\u00e1lisis de convergencia te\u00f3rica.", "Propone el m\u00e9todo ProxQuant para entrenar redes neuronales con pesos cuantificados.", "Propone resolver las redes binarias y sus variantes utilizando el descenso de gradiente proximal."]}
{"source": ["Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions.", "However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., \u201cnear\u201d linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\n", "Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss.", "We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units.", "We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hkfmn5n6W", "target": ["Los m\u00ednimos locales \"malos\" desaparecen en una red neuronal multicapa: una prueba con supuestos m\u00e1s razonables que antes", "En las redes con una sola capa oculta, el volumen de m\u00ednimos locales sub\u00f3ptimos disminuye exponencialmente en comparaci\u00f3n con los m\u00ednimos globales.", "Este trabajo pretende responder a por qu\u00e9 los algoritmos est\u00e1ndar basados en SGD sobre redes neuronales convergen a soluciones \"buenas\"."]}
{"source": ["Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations.", "An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications.", "Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness, and several state-of-the-art defenses are shown to be robust against transferable adversarial examples.", "In this paper, we identify the attention shift phenomenon, which may hinder the transferability of adversarial examples to the defense models.", "It indicates that the defenses rely on different discriminative regions to make predictions compared with normally trained models.", "Therefore, we propose an attention-invariant attack method to generate more transferable adversarial examples.", "Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method.", "Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the defense techniques."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BJzVUj0qtQ", "target": ["Proponemos un m\u00e9todo de ataque invariante de la atenci\u00f3n para generar ejemplos adversarios m\u00e1s transferibles para los ataques de caja negra, que pueden enga\u00f1ar a las defensas del estado de la t\u00e9cnica con una alta tasa de \u00e9xito.", "El documento propone una nueva forma de superar las defensas del estado del arte contra los ataques adversarios a la CNN.", "Este trabajo sugiere que el \"cambio de atenci\u00f3n\" es una propiedad clave detr\u00e1s del fracaso de los ataques adversarios a la transferencia y propone un m\u00e9todo de ataque invariante de la atenci\u00f3n"]}
{"source": ["We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(d\\log{K})$ memory while only requiring $O(K\\log{K} + d\\log{K})$ operation for inference.", "MACH is the first generic $K$-classification algorithm, with provably theoretical guarantees, which requires $O(\\log{K})$ memory without any assumption on the relationship between classes.", "MACH uses universal hashing to reduce classification with a large number of classes to few independent classification task with very small (constant) number of classes.", "We provide theoretical quantification of accuracy-memory tradeoff by showing the first connection between extreme classification and heavy hitters.", "With MACH we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU (12GB), with the classification accuracy of 19.28\\%, which is the best-reported accuracy on this dataset.", "Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (320 GB model size) and achieves 9\\% accuracy.  ", "In contrast, MACH can achieve 9\\% accuracy with 480x reduction in the model size (of mere 0.6GB).", "With MACH, we also demonstrate complete training of fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "r1RQdCg0W", "target": ["C\u00f3mo entrenar 100.000 clases en una sola GPU", "Propone un m\u00e9todo de hashing eficiente MACH para la aproximaci\u00f3n de softmax en el contexto de un gran espacio de salida, que ahorra tanto memoria como computaci\u00f3n.", "Un m\u00e9todo para el esquema de clasificaci\u00f3n de problemas que implican un gran n\u00famero de clases en un entorno multiclase demostrado en los conjuntos de datos ODP e Imagenet-21K", "El art\u00edculo presenta un esquema basado en hashing para reducir la memoria y el tiempo de c\u00e1lculo para la clasificaci\u00f3n de K v\u00edas cuando K es grande"]}
{"source": ["Gradient-based optimization is the foundation of deep learning and reinforcement learning.\n", "Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy.", "We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\n", "These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings.", "We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic.", "We also demonstrate this framework for training discrete latent-variable models."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SyzKd1bCW", "target": ["Presentamos un m\u00e9todo general para la estimaci\u00f3n insesgada de gradientes de funciones de caja negra de variables aleatorias. Aplicamos este m\u00e9todo a la inferencia variacional discreta y al aprendizaje por refuerzo. ", "Sugiere un nuevo enfoque para realizar el descenso de gradiente para la optimizaci\u00f3n de la caja negra o el entrenamiento de modelos de variables latentes discretas."]}
{"source": ["Do GANS (Generative Adversarial Nets) actually learn the target distribution?", "The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time.", "A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size.", "It showed that the training objective can approach its optimum value even if the generated distribution has very low support.", "In other words, the training objective is unable to prevent mode collapse.", "The current paper makes two contributions.", "(1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability.", "Using this  evidence is presented that well-known GANs approaches do learn distributions of fairly low support.  ", "(2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue.", "Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse.", "More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJehNfW0-", "target": ["Proponemos un estimador del tama\u00f1o de soporte de la distribuci\u00f3n aprendida de los GANs para demostrar que efectivamente sufren de colapso de modo, y demostramos que los GANs codificadores-decodificadores tampoco evitan el problema.", "El art\u00edculo trata de estimar experimentalmente el tama\u00f1o del soporte de las soluciones producidas por GANs t\u00edpicas. ", "Este trabajo propone una nueva prueba inteligente basada en la paradoja de cumplea\u00f1os para medir la diversidad en la muestra generada, con resultados de experimentos que se interpretan como que el colapso de modos es fuerte en una serie de modelos generativos de \u00faltima generaci\u00f3n.", "El art\u00edculo utiliza la paradoja del cumplea\u00f1os para mostrar que algunas arquitecturas GAN generan distribuciones con un soporte bastante bajo."]}
{"source": ["Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed.", "Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail.", "However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language.", "In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks.", "We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation.", "We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "H1BLjgZCb", "target": ["Proponemos un marco para generar adversarios naturales contra clasificadores de caja negra tanto para dominios visuales como textuales, haciendo la b\u00fasqueda de adversarios en el espacio sem\u00e1ntico latente.", "Propone un m\u00e9todo para la creaci\u00f3n de ejemplos de adversarios sem\u00e1nticos.", "Propone un marco para generar ejemplos adversarios naturales mediante la b\u00fasqueda de adversarios en un espacio latente de representaci\u00f3n de datos densos y continuos "]}
{"source": ["Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017).  ", "It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized.", "The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well.", "In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs.", "This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form.", "We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HyMTkQZAb", "target": ["Ampliamos el m\u00e9todo K-FAC a las RNN desarrollando una nueva familia de aproximaciones de Fisher.", "Los autores extienden el m\u00e9todo K-FAC a las RNNs y presentan 3 formas de aproximar F, mostrando resultados de optimizaci\u00f3n en 3 conjuntos de datos, que superan a ADAM tanto en n\u00famero de actualizaciones como en tiempo de computaci\u00f3n.", "Propone extender el m\u00e9todo de optimizaci\u00f3n de Curvatura Apropiada del factor Kronecker al entorno de las redes neuronales recurrentes.", "Los autores presentan un m\u00e9todo de segundo orden dise\u00f1ado espec\u00edficamente para las RNN"]}
{"source": ["Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution.", "In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights.", "We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors.", "In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.\n"], "source_labels": [0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByGuynAct7", "target": ["El modelo generativo para los n\u00facleos de las redes neuronales convolucionales, que act\u00faa como una distribuci\u00f3n a priori mientras se entrena en nuevos conjuntos de datos.", "Un m\u00e9todo para modelar redes neuronales convolucionales utilizando un m\u00e9todo de Bayes.", "Propone la \"prioridad de peso profundo\": la idea es obtener una prioridad en un conjunto de datos auxiliar y luego utilizar esa prioridad sobre los filtros CNN para iniciar la inferencia para un conjunto de datos de inter\u00e9s.", "Este trabajo explora el aprendizaje de priores informativos para modelos de redes neuronales convolucionales con dominios de problemas similares mediante el uso de autocodificadores para obtener un prior expresivo sobre los pesos filtrados de las redes entrenadas."]}
{"source": ["The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements.  ", "Motivated by the potential for an in-the-field cell sorting detector, we examine a Synechocystis sp.", "PCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures.  ", "We use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background.", "Further, we use the classification accuracy to guide a data-driven, iterative feature selection method, allowing the design neural networks requiring 90% fewer input features with little accuracy degradation."], "source_labels": [0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HkanP0lRW", "target": ["Aplicamos t\u00e9cnicas de aprendizaje profundo a la segmentaci\u00f3n de im\u00e1genes hiperespectrales y al muestreo iterativo de caracter\u00edsticas.", "Propone un esquema codicioso para seleccionar un subconjunto de caracter\u00edsticas espectrales altamente correlacionadas en una tarea de clasificaci\u00f3n.", "El art\u00edculo explora el uso de redes neuronales para la clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes hipersepctrales (HSI) de c\u00e9lulas.", "Clasificaci\u00f3n de c\u00e9lulas e implementaci\u00f3n de la segmentaci\u00f3n celular basada en t\u00e9cnicas de aprendizaje profundo con reducci\u00f3n de caracter\u00edsticas de entrada"]}
{"source": ["Data Interpretation is an important part of Quantitative Aptitude exams and requires an individual to answer questions grounded in plots such as bar charts, line graphs, scatter plots, \\textit{etc}.", "Recently, there has been an increasing interest in building models which can perform this task by learning from datasets containing triplets of the form \\{plot, question, answer\\}.", "Two such datasets have been proposed in the recent past which contain plots generated from synthetic data with limited", "(i) $x-y$ axes variables", "(ii) question templates and", "(iii) answer vocabulary and hence do not adequately capture the challenges posed by this task.", "To overcome these limitations of existing datasets, we introduce a new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators.", "First, the plots in our dataset contain a wide variety of realistic $x$-$y$ variables such as CO2 emission, fertility rate, \\textit{etc.", "} extracted from  real word data sources such as World Bank, government sites, \\textit{etc}.", "Second, the questions in our dataset are more complex as they are based on templates extracted from interesting questions asked by a crowd of workers using a fraction of these plots.", "Lastly, the answers in our dataset are not restricted to a small vocabulary and a large fraction of the answers seen at test time are not present in the training vocabulary.", "As a result, existing models for Visual Question Answering which largely use end-to-end models in a multi-class classification framework cannot be used for this task.", "We establish initial results on this dataset and emphasize the complexity of the task using a multi-staged modular pipeline with various sub-components to", "(i) extract relevant data from the plot and convert it to a semi-structured table", "(ii) combine the question with this table and use compositional semantic parsing to arrive at a logical form from which the answer can be derived.", "We believe that such a modular framework is the best way to go forward as it would enable the research community to independently make progress on all the sub-tasks involved in plot question answering."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SygeznA9YX", "target": ["Creamos un nuevo conjunto de datos para la interpretaci\u00f3n de los datos sobre las parcelas y tambi\u00e9n proponemos una l\u00ednea de base para la misma.", "Los autores proponen un procedimiento para resolver el problema DIP que implica el aprendizaje a partir de conjuntos de datos que contienen tripletas de la forma {parcela, pregunta, respuesta}", "Propone un algoritmo que puede interpretar los datos mostrados en los gr\u00e1ficos cient\u00edficos."]}
{"source": ["Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing.", "Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model.", "PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule.", "Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well.", "Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train.", "Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation.", "Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting.", "In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs.", "In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJJ23bW0b", "target": ["Mejora de las redes neuronales recurrentes de predicci\u00f3n del estado mediante caracter\u00edsticas aleatorias ortogonales", "Propone mejorar las prestaciones de las Redes Neuronales Recurrentes de Estado Predicitivo considerando Caracter\u00edsticas Aleatorias Ortogonales.", "El trabajo aborda el problema del entrenamiento de redes neuronales recurrentes de estado predictivo y realiza dos aportaciones."]}
{"source": ["Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing.", "This creates a fundamental quality- versus-quantity trade-off in the learning process.", "Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data?", "We argue that if the learner could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds.", "To this end, we propose \u201cfidelity-weighted learning\u201d (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data.", "FWL modulates the parameter updates to a student network (trained on the task we care about) on a per-sample basis according to the posterior confidence of its label-quality estimated by a teacher (who has access to the high-quality labels).", "Both student and teacher are learned from the data.", "We evaluate FWL on two tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of strong and weak labels, and leads to better task-dependent data representations."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1X0mzZCW", "target": ["Proponemos el Aprendizaje Ponderado por Fidelidad, un enfoque semi-supervisado de profesor-estudiante para el entrenamiento de redes neuronales utilizando datos d\u00e9bilmente etiquetados.", "Este trabajo sugiere un enfoque para el aprendizaje con supervisi\u00f3n d\u00e9bil utilizando un conjunto de datos limpio y otro con ruido y asumiendo una red de profesores y alumnos", "El art\u00edculo trata de entrenar modelos de redes neuronales profundas con pocas muestras de entrenamiento etiquetadas.", "Los autores proponen un enfoque para el entrenamiento de modelos de aprendizaje profundo para situaciones en las que no hay suficientes datos anotados fiables."]}
{"source": [" Online learning has attracted great attention due to the increasing demand for systems that have the ability of learning and evolving.", "When the data to be processed is also high dimensional and dimension reduction is necessary for visualization or prediction enhancement, online dimension reduction will play an essential role.", "The purpose of this paper is to propose new online learning approaches for supervised dimension reduction.", "Our first algorithm is motivated by adapting the sliced inverse regression (SIR), a pioneer and effective algorithm for supervised dimension reduction, and making it implementable in an incremental manner.", "The new algorithm, called incremental sliced inverse regression (ISIR), is able to update the subspace of significant factors with intrinsic lower dimensionality fast and efficiently when new observations come in.", "We also refine the algorithm by using an overlapping technique  and develop an incremental overlapping sliced inverse regression (IOSIR) algorithm.", "We verify the effectiveness and efficiency of both algorithms by simulations and real data applications."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "B1MUroRct7", "target": ["Propusimos dos nuevos enfoques, la regresi\u00f3n inversa incremental rebanada y la regresi\u00f3n inversa incremental superpuesta rebanada, para implementar la reducci\u00f3n de dimensi\u00f3n supervisada de una manera de aprendizaje en l\u00ednea.", "Estudia el problema de la reducci\u00f3n de dimensi\u00f3n suficiente y propone un algoritmo de regresi\u00f3n inversa incremental rebanada.", "Este trabajo propone un algoritmo de aprendizaje en l\u00ednea para la reducci\u00f3n supervisada de la dimensi\u00f3n, llamado regresi\u00f3n inversa incremental rebanada"]}
{"source": ["This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage such as hundreds of kilo-Bytes.", "The main idea of the proposed model is to recursively recycle data storage of weights (parameters) during training.", "This enables a device with a given storage constraint to train and instantiate a neural network classifier with a larger number of weights on a chip, achieving better classification accuracy.", "Such efficient use of on-chip storage reduces off-chip storage accesses, improving energy-efficiency and speed of training.", "We verified the proposed training model with deep and convolutional neural network classifiers on the MNIST and voice activity detection benchmarks.", "For the deep neural network, our model achieves data storage requirement of as low as 2 bits/weight, whereas the conventional binary neural network learning models require data storage of 8 to 32 bits/weight.", "With the same amount of data storage, our model can train a bigger network having more weights, achieving 1% less test error than the conventional binary neural network learning model.", "To achieve the similar classification error, the conventional binary neural network model requires 4\u00d7 more data storage for weights than our proposed model.", "For the convolution neural network classifier, the proposed model achieves 2.4% less test error for the same on-chip storage or 6\u00d7 storage savings to achieve the similar accuracy.\n"], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rkONG0xAW", "target": ["Proponemos un modelo de aprendizaje que permite a la DNN aprender con s\u00f3lo 2 bits/peso, lo que es especialmente \u00fatil para el aprendizaje en el dispositivo", "Propone un m\u00e9todo para discretizar una NN de forma incremental para mejorar la memoria y el rendimiento."]}
{"source": ["Within-class variation in a high-dimensional dataset can be modeled as being on a low-dimensional manifold due to the constraints of the physical processes producing that variation (e.g., translation, illumination, etc.).", "We desire a method for learning a representation of the manifolds induced by identity-preserving transformations that can be used to increase robustness, reduce the training burden, and encourage interpretability in machine learning tasks.", "In particular, what is needed is a representation of the transformation manifold that can robustly capture the shape of the manifold from the input data, generate new points on the manifold, and extend transformations outside of the training domain without significantly increasing the error.", "Previous work has proposed algorithms to efficiently learn analytic operators (called transport operators) that define the process of transporting one data point on a manifold to another.  ", "The main contribution of this paper is to define two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data.", "The first method uses this representation in a novel randomized approach to transfer learning that employs the learned generative model to map out unseen regions of the data space.", "These results are shown through demonstrations of transfer learning in a data augmentation task for few-shot image classification.", "The second method use of transport operators for injecting specific transformations into new data examples which allows for realistic image animation and informed data augmentation.  ", "These results are shown on stylized constructions using the classic swiss roll data structure and in demonstrations of transfer learning in a data augmentation task for few-shot image classification.", "We also propose the use of transport operators for injecting transformations into new data examples which allows for realistic image animation."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJL6pz-CZ", "target": ["El aprendizaje de los operadores de transporte en las variedades constituye una valiosa representaci\u00f3n para realizar tareas como el aprendizaje por transferencia.", "Utiliza un marco de aprendizaje de diccionario para aprender operadores de transporte m\u00faltiple en d\u00edgitos aumentados de USPS.", "El trabajo considera el marco de aprendizaje de operadores de transporte de colectores de Culpepper y Olshausen (2009), y lo interpreta como la obtenci\u00f3n de una estimaci\u00f3n MAP bajo un modelo generativo probabil\u00edstico."]}
{"source": ["The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry.", "We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts.", "If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts.", "This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain.", "SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner.", "Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations.", "Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa.", "It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations.", "Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkN2Il-RZ", "target": ["Presentamos un modelo neuronal variacional para el aprendizaje de conceptos visuales compositivos guiados por el lenguaje.", "Propone una novedosa arquitectura de red neuronal que aprende los conceptos de los objetos combinando un beta-VAE y un SCAN.", "Este trabajo introduce un modelo basado en VAE para traducir entre im\u00e1genes y texto, con su representaci\u00f3n latente bien adaptada a la aplicaci\u00f3n de operaciones simb\u00f3licas, lo que les da un lenguaje m\u00e1s expresivo para el muestreo de im\u00e1genes a partir del texto. ", "Este trabajo propone un nuevo modelo denominado SCAN (Symbol-Concept Association Network) para el aprendizaje jer\u00e1rquico de conceptos y permite la generalizaci\u00f3n a nuevos conceptos compuestos a partir de conceptos existentes utilizando operadores l\u00f3gicos."]}
{"source": ["Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses.", "In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations.", "The neural topic component encodes information from the source sentence to build a global \u201ctopic\u201d distribution over words, which is then consulted by the seq2seq model to improve generation at each time step.", "The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations.", "In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n"], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "S1GUgxgCW", "target": ["Modelo conversacional de temas latentes, un h\u00edbrido de seq2seq y modelo neural de temas para generar respuestas m\u00e1s diversas e interesantes.", "Este trabajo propone la combinaci\u00f3n de un modelo tem\u00e1tico y un modelo conversacional seq2seq", "Propone un modelo conversacional con informaci\u00f3n t\u00f3pica combinando el modelo seq2seq con modelos neurales de temas y muestra que el modelo propuesto supera a algunos de los modelos de referencia seq2seq y a otra variante de modelo de variable latente de seq2seq.", "El art\u00edculo aborda la cuesti\u00f3n de la perdurabilidad de la actualidad en los modelos de conversaci\u00f3n y propone un modelo que es una combinaci\u00f3n de un modelo neural de temas y un sistema de di\u00e1logo basado en seq2seq. "]}
{"source": ["Most of the existing Graph Neural Networks (GNNs) are the mere extension of the Convolutional Neural Networks (CNNs) to graphs.", "Generally, they consist of several steps of message passing between the nodes followed by a global indiscriminate feature pooling function.", "In many data-sets, however, the nodes are unlabeled or their labels provide no information about the similarity between the nodes and the locations of the nodes in the graph.", "Accordingly, message passing may not propagate helpful information throughout the graph.", "We show that this conventional approach can fail to learn to perform even simple graph classification tasks.", "We alleviate this serious shortcoming of the GNNs by making them a two step method.", "In the first of the proposed approach, a graph embedding algorithm is utilized to obtain a continuous feature vector for each node of the graph.", "The embedding algorithm represents the graph as a point-cloud in the embedding space.", "In the second step, the GNN is applied to the point-cloud representation of the graph provided by the embedding method.", "The GNN learns to perform the given task by inferring the topological structure of the graph encoded in the spatial distribution of the embedded vectors.", "In addition, we extend the proposed approach to the graph clustering problem and a new architecture for graph clustering is proposed.", "Moreover, the spatial representation of the graph is utilized to design a graph pooling algorithm.", "We turn the problem of graph down-sampling into a column sampling problem, i.e., the sampling algorithm selects a subset of the nodes whose feature vectors preserve the spatial distribution of all the feature vectors.", "We apply the proposed approach to several popular benchmark data-sets and it is shown that the proposed geometrical approach strongly improves the state-of-the-art result for several data-sets.", "For instance, for the PTC data-set, we improve the state-of-the-art result for more than 22 %."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Hkes0iR9KX", "target": ["El problema del an\u00e1lisis de gr\u00e1ficos se transforma en un problema de an\u00e1lisis de nubes de puntos. ", "Propone una red GNN profunda para problemas de clasificaci\u00f3n de grafos utilizando su capa de agrupaci\u00f3n de grafos adaptativa.", "Los autores proponen un m\u00e9todo de aprendizaje de representaciones para grafos"]}
{"source": ["Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs.", "Such adversarial examples can mislead DNNs to produce adversary-selected results.\n", "Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. \n", "In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. \n", "For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses.  \n", "We apply AdvGAN in both semi-whitebox and black-box attack settings.", "In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks.", "In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly.\n", "Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks.", "Our attack  has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HknbyQbC-", "target": ["Proponemos generar un ejemplo adversarial basado en redes generativas adversariales en un entorno de caja semiblanca y caja negra.", "Describe AdvGAN, un GAN condicional m\u00e1s p\u00e9rdida adversarial, y eval\u00faa AdvGAN en escenarios de caja semiblanca y caja negra, informando de los resultados del estado del arte.", "Este trabajo propone una forma de generar ejemplos adversarios que enga\u00f1en a los sistemas de clasificaci\u00f3n y gana el reto mnista de MadryLab."]}
{"source": ["This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set.", "Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training.", "We empirically demonstrate that:", "a) deep autoencoder models generalize much better than the shallow ones,", "b) non-linear activation functions with negative parts are crucial for training deep models, and", "c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting.", "We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering.", "The new algorithm significantly speeds up training and improves model performance.", "Our code is publicly available."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkNQeiRpb", "target": ["Este art\u00edculo demuestra c\u00f3mo entrenar autocodificadores profundos de principio a fin para lograr resultados de SoA en un conjunto de datos de Netflix divididos en el tiempo.", "Este art\u00edculo presenta un modelo de autocodificador profundo para la predicci\u00f3n de valoraciones que supera a otros enfoques del estado del arte en el conjunto de datos de premios de Netflix. ", "Propone utilizar un EA profundo para realizar tareas de predicci\u00f3n de calificaci\u00f3n en sistemas de recomendaci\u00f3n.", "Los autores presentan un modelo para recomendaciones m\u00e1s precisas de Netflix que demuestra que un autocodificador profundo puede superar a modelos m\u00e1s complejos basados en RNN que tienen informaci\u00f3n temporal. "]}
{"source": ["Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.", "However, their inherently sequential computation makes them slow to train.", "Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.", "Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.", "We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.", "UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.", "We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.", "In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete.", "Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HyzdRiR9Y7", "target": ["Introducimos el Transformador Universal, un modelo de secuencia recurrente paralelo en tiempo que supera a los Transformadores y LSTMs en una amplia gama de tareas de secuencia a secuencia, incluyendo la traducci\u00f3n autom\u00e1tica.", "Propone un nuevo modelo UT, basado en el modelo Transformer, con recurrencia a\u00f1adida y detenci\u00f3n din\u00e1mica de la recurrencia.", "Este trabajo ampl\u00eda Transformer aplicando recursivamente un bloque de autoatenci\u00f3n de varias cabezas, en lugar de apilar varios bloques en el Transformer vainilla."]}
{"source": ["We present a framework for interpretable continual learning (ICL).", "We show that explanations of previously performed tasks can be used to improve performance on future tasks.", "ICL generates a good explanation of a finished task, then uses this to focus attention on what is important when facing a new task.", "The ICL idea is general and may be applied to many continual learning approaches.", "Here we focus on the variational continual learning framework to take advantage of its flexibility and efficacy in overcoming catastrophic forgetting.", "We use saliency maps to provide explanations of performed tasks and propose a new metric to assess their quality.", "Experiments show that ICL achieves state-of-the-art results in terms of overall continual learning performance as measured by average classification accuracy, and also in terms of its explanations, which are assessed qualitatively and quantitatively using the proposed metric."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "S1g9N2A5FX", "target": ["El art\u00edculo desarrolla un marco interpretable de aprendizaje continuo en el que las explicaciones de las tareas terminadas se utilizan para mejorar la atenci\u00f3n del alumno durante las tareas futuras, y en el que tambi\u00e9n se propone una m\u00e9trica de explicaci\u00f3n. ", "Los autores proponen un marco para el aprendizaje continuo basado en las explicaciones de las clasificaciones realizadas de las tareas previamente aprendidas", "Este trabajo propone una extensi\u00f3n del marco de aprendizaje continuo utilizando el aprendizaje continuo variacional existente como m\u00e9todo base con el peso de la evidencia."]}
{"source": ["The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017).", "On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10).", "In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware.", "In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations.", "The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator.", "We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput.", "To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision"], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "H135uzZ0-", "target": ["Canalizaci\u00f3n de entrenamiento de precisi\u00f3n mixta utilizando enteros de 16 bits en HW de prop\u00f3sito general; Precisi\u00f3n SOTA para CNN de clase ImageNet; Mejor precisi\u00f3n reportada para la tarea de clasificaci\u00f3n ImageNet-1K con cualquier entrenamiento de precisi\u00f3n reducida;", "Este trabajo muestra que una implementaci\u00f3n cuidadosa del c\u00e1lculo din\u00e1mico en punto fijo de precisi\u00f3n mixta puede lograr una precisi\u00f3n de vanguardia utilizando un modelo de aprendizaje profundo de precisi\u00f3n reducida con una representaci\u00f3n de enteros de 16 bits", "Propone un esquema de \"punto fijo din\u00e1mico\" que comparte la parte del exponente para un tensor y desarrolla procedimientos para hacer computaci\u00f3n NN con este formato y lo demuestra para el entrenamiento de precisi\u00f3n limitada."]}
{"source": ["In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model.", "The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed.", "Key ingredients for the acoustic model are Gated Linear Units and high dropout.", "We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n"], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "Hyig0zb0Z", "target": ["Un modelo ac\u00fastico ConvNet basado en letras da lugar a un proceso de reconocimiento del habla sencillo y competitivo.", "Este trabajo aplica las redes neuronales convolucionales cerradas al reconocimiento del habla, utilizando el criterio de entrenamiento ASG."]}
{"source": ["Generative adversarial networks (GANs) are a powerful framework for generative tasks.", "However, they are difficult to train and tend to miss modes of the true data generation process.", "Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space.", "We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations.", "Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes.", "We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ry0WOxbRZ", "target": ["Un marco GAN noval que utiliza caracter\u00edsticas invariantes de la transformaci\u00f3n para aprender representaciones ricas y generadores fuertes.", "Propone un objetivo GAN modificado que consiste en un t\u00e9rmino GAN cl\u00e1sico y un t\u00e9rmino de codificaci\u00f3n invariante.", "Este art\u00edculo presenta el IVE-GAN, un modelo que introduce un codificador en el marco de la Red Generativa Adversarial."]}
{"source": ["We propose a method for learning the dependency structure between latent variables in deep latent variable models.  ", "Our general modeling and inference framework combines the complementary strengths of deep generative models and probabilistic graphical models.", "In particular, we express the latent variable space of a variational autoencoder (VAE) in terms of a Bayesian network with a learned, flexible dependency structure.  ", "The network parameters, variational parameters as well as the latent topology are optimized simultaneously with a single objective.  ", "Inference is formulated via a sampling procedure that produces expectations over latent variable structures and incorporates top-down and bottom-up reasoning over latent variable values.  ", "We validate our framework in extensive experiments on MNIST, Omniglot, and CIFAR-10.", "Comparisons to state-of-the-art structured variational autoencoder baselines show improvements in terms of the expressiveness of the learned model."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJgsCjCqt7", "target": ["Proponemos un m\u00e9todo para el aprendizaje de la estructura de dependencia latente en autocodificadores variacionales.", "Utiliza una matriz de variables aleatorias binarias para capturar las dependencias entre las variables latentes en un modelo generativo profundo jer\u00e1rquico.", "Este trabajo presenta un enfoque VAE en el que se aprende una estructura de dependencia sobre la variable latente durante el entrenamiento.", "Los autores proponen aumentar el espacio latente de un VAE con una estructura autorregresiva, para mejorar la expresividad tanto de la red de inferencia como de la previa latente"]}
{"source": ["Many real-world time series, such as in activity recognition, finance, or climate science, have changepoints where the system's structure or parameters change.", "Detecting changes is important as they may indicate critical events.", "However, existing methods for changepoint detection face challenges when (1) the patterns of change cannot be modeled using simple and predefined metrics, and (2) changes can occur gradually, at multiple time-scales.", "To address this, we show how changepoint detection can be treated as a supervised learning problem, and propose a new deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales.", "Our proposed method, pyramid recurrent neural network (PRNN), is designed to be scale-invariant, by incorporating wavelets and pyramid analysis techniques from multi-scale signal processing.", "Through experiments on synthetic and real-world datasets, we show that PRNN can detect abrupt and gradual changes with higher accuracy than the state of the art and can extrapolate to detect changepoints at novel timescales that have not been seen in training."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HkGTwjCctm", "target": ["Introducimos una arquitectura de red neuronal invariable en escala para la detecci\u00f3n de puntos de cambio en series temporales multivariantes.", "El art\u00edculo aprovecha el concepto de la transformada wavelet dentro de una arquitectura profunda para resolver la detecci\u00f3n de puntos de cambio.", "Este trabajo propone una red neuronal basada en una pir\u00e1mide y la aplica a se\u00f1ales 1D con procesos subyacentes que ocurren en diferentes escalas de tiempo donde la tarea es la detecci\u00f3n de puntos de cambio"]}
{"source": ["We demonstrate how to learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning.", "We focus on backtracking search algorithms for quantified Boolean logics, which already can solve formulas of impressive size - up to 100s of thousands of variables.", "The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way.", "For challenging problems, the heuristic learned through our approach reduces execution time by a factor of 10 compared to the existing handwritten heuristics."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkeyZhC9F7", "target": ["RL encuentra mejores heur\u00edsticas para los algoritmos de razonamiento automatizado.", "Tiene como objetivo aprender una heur\u00edstica para un algoritmo de b\u00fasqueda de rastreo utilizando el aprendizaje por refuerzo y propone un modelo que hace uso de redes neuronales gr\u00e1ficas para producir incrustaciones de literales y cl\u00e1usulas, y utilizarlas para predecir la calidad de cada literal para decidir la probabilidad de cada acci\u00f3n.", "El trabajo propone un enfoque para el aprendizaje autom\u00e1tico de la heur\u00edstica de selecci\u00f3n de variables para QBF utilizando el aprendizaje profundo"]}
{"source": ["We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data.", "We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is.", "Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution.", "We use this procedure to assess the performance of several modern generative adversarial network architectures.", "We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  ", "We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByuI-mW0W", "target": ["Eval\u00fae si su GAN est\u00e1 haciendo realmente algo m\u00e1s que memorizar los datos de entrenamiento.", "Tiene como objetivo proporcionar una medida/prueba de calidad para las GANs y propone evaluar la aproximaci\u00f3n actual de una distribuci\u00f3n aprendida por una GAN utilizando la distancia Wasserstein entre dos distribuciones hechas de una suma de Diracs como rendimiento de referencia. ", "En este trabajo se propone un procedimiento para evaluar el rendimiento de las GANs mediante la reconsideraci\u00f3n de la clave de observaci\u00f3n, utilizando el procedimiento para probar y mejorar las GANs actuales"]}
{"source": ["The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance.", "Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU).", "Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains.", "In this work, we propose to leverage automatic search techniques to discover new activation functions.", "Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions.", "We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function.", "Our experiments show that the best discovered activation function, f(x) = x * sigmoid(beta * x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets.", "For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2.", "The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkBYYyZRZ", "target": ["Utilizamos t\u00e9cnicas de b\u00fasqueda para descubrir nuevas funciones de activaci\u00f3n, y nuestra mejor funci\u00f3n de activaci\u00f3n descubierta, f(x) = x * sigmoide(beta * x), supera a ReLU en una serie de tareas dif\u00edciles como ImageNet.", "Propone un enfoque basado en el aprendizaje por refuerzo para encontrar la no linealidad mediante la b\u00fasqueda de combinaciones a partir de un conjunto de operadores unarios y binarios.", "Este trabajo utiliza el aprendizaje por refuerzo para buscar la combinaci\u00f3n de un conjunto de funciones unarias y binarias que dan como resultado una nueva funci\u00f3n de activaci\u00f3n", "El autor utiliza el aprendizaje por refuerzo para encontrar nuevas funciones de activaci\u00f3n potenciales a partir de un rico conjunto de posibles candidatos. "]}
{"source": ["Successful training of convolutional neural networks is often associated with suffi-\n", "ciently deep architectures composed of high amounts of features.", "These networks\n", "typically rely on a variety of regularization and pruning techniques to converge\n", "to less redundant states.", "We introduce a novel bottom-up approach to expand\n", "representations in fixed-depth architectures.", "These architectures start from just a\n", "single feature per layer and greedily increase width of individual layers to attain\n", "effective representational capacities needed for a specific task.", "While network\n", "growth can rely on a family of metrics, we propose a computationally efficient\n", "version based on feature time evolution and demonstrate its potency in determin-\n", "ing feature importance and a networks\u2019 effective capacity.", "We demonstrate how\n", "automatically expanded architectures converge to similar topologies that benefit\n", "from lesser amount of parameters or improved accuracy and exhibit systematic\n", "correspondence in representational complexity with the specified task.", "In contrast\n", "to conventional design patterns with a typical monotonic increase in the amount of\n", "features with increased depth, we observe that CNNs perform better when there is\n", "more learnable parameters in intermediate, with falloffs to earlier and later layers."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkffVjUaW", "target": ["Un algoritmo ascendente que ampl\u00eda las CNNs comenzando con una caracter\u00edstica por capa hasta arquitecturas con suficiente capacidad de representaci\u00f3n.", "Propone ajustar din\u00e1micamente la profundidad del mapa de caracter\u00edsticas de una red neuronal totalmente convolucional, formulando una medida de autorresistencia y potenciando el rendimiento.", "Introduce una sencilla m\u00e9trica basada en la correlaci\u00f3n para medir si los filtros de las redes neuronales se utilizan de forma eficaz, como indicador de la capacidad efectiva.", "Pretende abordar el problema de b\u00fasqueda de la arquitectura de aprendizaje profundo mediante la adici\u00f3n y eliminaci\u00f3n incremental de canales en las capas intermedias de la red."]}
{"source": ["Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation).", "Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way.", "In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient.", "Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network.", "In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation.", "This feed-forward network learns to approximate the state of the fixed-point using a local learning rule.", "After training, we can simply use this initializing network for inference, resulting in a learned feedforward network.", "Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation.", "This shows how we might go about training deep networks without using backpropagation."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1GMDsR5tm", "target": ["Entrenamos una red feedforward sin backprop utilizando un modelo basado en la energ\u00eda para proporcionar objetivos locales", "El objetivo de este trabajo es acelerar el procedimiento de inferencia iterativa en los modelos basados en la energ\u00eda entrenados con Propagaci\u00f3n de Equilibrio (PE), proponiendo entrenar una red feedforward para predecir un punto fijo de la \"red de equilibrio\". ", "Entrenamiento de una red independiente para inicializar las redes recurrentes entrenadas con propagaci\u00f3n de equilibrio "]}
{"source": ["We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation.", "A single object may have many attributes which when altered do not change the identity of the object itself.", "Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses.", "The attribute of wearing glasses can be changed without changing the identity of the person.", "However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task.", "Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses').", "We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute.", "We refer to this specific synthesis process as image attribute manipulation.", "We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJfRpoA9YX", "target": ["Aprenda las representaciones de las im\u00e1genes que factorizan un solo atributo.", "Este trabajo se basa en los GANs VAE condicionales para permitir la manipulaci\u00f3n de atributos en el proceso de s\u00edntesis.", "Este trabajo propone un modelo generativo para aprender la representaci\u00f3n que puede separar la identidad de un objeto de un atributo, y ampl\u00eda el autoencoder adversarial a\u00f1adiendo una red auxiliar."]}
{"source": ["Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames.", "These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive.", "We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points.", "For example, our model can \"jump\" and directly sample frames at the end of the video, without sampling intermediate frames.", "Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity.", "We also apply our framework to a 3D scene reconstruction dataset.", "Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain.", "Reconstructions and videos are available at https://bit.ly/2O4Pc4R.\n"], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1gQ5sRcFm", "target": ["Presentamos un modelo para la reconstrucci\u00f3n 3D consistente y la predicci\u00f3n de v\u00eddeo con saltos, por ejemplo, produciendo fotogramas de imagen en m\u00faltiples pasos de tiempo en el futuro sin generar fotogramas intermedios.", "Este trabajo propone un m\u00e9todo general para el modelado de datos indexados mediante la codificaci\u00f3n de la informaci\u00f3n del \u00edndice junto con la observaci\u00f3n en una red neuronal, y luego decodificar la condici\u00f3n de la observaci\u00f3n en el \u00edndice objetivo.", "Propone utilizar un VAE que codifique el v\u00eddeo de entrada de forma invariable a la permutaci\u00f3n para predecir los fotogramas futuros de un v\u00eddeo."]}
{"source": ["The ADAM optimizer is exceedingly popular in the deep learning community.", "Often it works very well, sometimes it doesn\u2019t.", "Why?", "We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance.", "We  disentangle these two aspects and analyze them in isolation, shedding light on ADAM \u2019s inner workings.", "Transferring the \"variance adaptation\u201d to momentum- SGD gives rise to a novel method, completing the practitioner\u2019s toolbox for problems where ADAM fails."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1EwLkW0W", "target": ["An\u00e1lisis del popular optimizador Adam", "El documento trata de mejorar a Adam bas\u00e1ndose en la adaptaci\u00f3n de la varianza con el impulso proponiendo dos algoritmos", "Este trabajo analiza la invariabilidad de la escala y la forma particular de la tasa de aprendizaje utilizada en Adam, argumentando que la actualizaci\u00f3n de Adam es una combinaci\u00f3n de una actualizaci\u00f3n de se\u00f1ales y una tasa de aprendizaje basada en la varianza.", "El art\u00edculo divide el algoritmo ADAM en dos componentes: direcci\u00f3n estoc\u00e1stica en signo de gradiente y paso a paso adaptativo con varianza relativa, y se proponen dos algoritmos para probar cada uno de ellos."]}
{"source": ["We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.", "The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation.", "However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space.", "In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices.", "Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power.", "Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm.", "We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1uxsye0Z", "target": ["Proponemos un marco novedoso para ajustar de forma adaptativa las tasas de abandono de la red neuronal profunda bas\u00e1ndonos en un l\u00edmite de complejidad de Rademacher.", "Los autores relacionan los par\u00e1metros de abandono con un l\u00edmite de la complejidad Rademacher de la red", "Relaciona la complejidad de la capacidad de aprendizaje de las redes con las tasas de abandono en la retropropagaci\u00f3n."]}
{"source": ["Sensor fusion is a key technology that integrates various sensory inputs to allow for robust decision making in many applications such as autonomous driving and robot control.", "Deep neural networks have been adopted for sensor fusion in a body of recent studies.", "Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolu- tional neural networks (CNN).", "In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group-level and feature- level fusion weights.", "Using driving mode prediction and human activity recogni- tion datasets, we demonstrate the significant performance improvements brought by the proposed gated architectures and also their robustness in the presence of sensor noise and failures.\n"], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Syeil309tX", "target": ["Se proponen arquitecturas de aprendizaje profundo optimizadas para la fusi\u00f3n de sensores.", "Los autores mejoran varias limitaciones de la arquitectura b\u00e1sica de negaci\u00f3n proponiendo una arquitectura de fusi\u00f3n cerrada de grano m\u00e1s grueso y una arquitectura de fusi\u00f3n cerrada en dos etapas", "Propone dos arquitecturas de aprendizaje profundo con compuerta para la fusi\u00f3n de sensores y, al tener las caracter\u00edsticas agrupadas, demuestra un mejor rendimiento, especialmente en presencia de ruido aleatorio de los sensores y de fallos."]}
{"source": ["We develop a mean field theory for batch normalization in fully-connected feedforward neural networks.", "In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization.", "Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function.", "Indeed, batch normalization itself is the cause of gradient explosion.", "As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations.", "While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections.", "Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range.", "Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyMDXnCcF7", "target": ["La normalizaci\u00f3n por lotes provoca la explosi\u00f3n de los gradientes en las redes vanilla feedforward.", "Desarrolla una teor\u00eda de campo medio para la normalizaci\u00f3n de lotes (BN) en redes totalmente conectadas con pesos inicializados aleatoriamente.", "Proporciona una perspectiva din\u00e1mica de la red neuronal profunda utilizando la evoluci\u00f3n de la matriz de covarianza junto con las capas."]}
{"source": ["We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability.  ", "Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations.", "Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "HJMC_iA5tm", "target": ["Entrenamos una red de grafos para predecir la satisfabilidad booleana y demostramos que aprende a buscar soluciones, y que las soluciones que encuentra pueden descodificarse a partir de sus activaciones.", "El art\u00edculo describe una arquitectura de red neuronal general para predecir la satisfacci\u00f3n", "Este art\u00edculo presenta la arquitectura NeuroSAT que utiliza una red neuronal profunda de paso de mensajes para predecir la satisfabilidad de las instancias CNF"]}
{"source": ["Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain.", "Traffic forecasting is one canonical example of such learning task.", "The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting.", "To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow.", "Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling.", "We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines"], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SJiHXGWAZ", "target": ["Un modelo de secuencia neural que aprende a pronosticar en un gr\u00e1fico dirigido.", "El art\u00edculo propone la arquitectura de la red neuronal convolucional difusa para el problema de previsi\u00f3n del tr\u00e1fico espacio-temporal", "Propone la construcci\u00f3n de un modelo de previsi\u00f3n del tr\u00e1fico mediante un proceso de difusi\u00f3n para redes neuronales recurrentes convolucionales para abordar la autocorrelaci\u00f3n s\u00e1pido-temporal."]}
{"source": ["Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge.", "Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior.", "In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution.", "We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates.", "The key idea is to train the model to output high uncertainty for data points outside of the training distribution.", "NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs.", "NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training.", "Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning.", "We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkgxasA5Ym", "target": ["Entrenamos a las redes neuronales para que sean inciertas en las entradas ruidosas para evitar predicciones excesivamente seguras fuera de la distribuci\u00f3n de entrenamiento.", "Presenta un enfoque para obtener estimaciones de incertidumbre para las predicciones de las redes neuronales que tiene un buen rendimiento cuando se cuantifica la incertidumbre predictiva en puntos que est\u00e1n fuera de la distribuci\u00f3n de entrenamiento.", "El art\u00edculo considera el problema de la estimaci\u00f3n de la incertidumbre de las redes neuronales y propone utilizar el enfoque bayesiano con un previo contrastivo de ruido"]}
{"source": ["Convolutional neural networks (CNNs) were inspired by human vision and, in some settings, achieve a performance comparable to human object recognition.", "This has lead to the speculation that both systems use similar mechanisms to perform recognition.", "In this study, we conducted a series of simulations that indicate that there is a fundamental difference between human vision and CNNs: while object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias.", "We teased apart the type of features selected by the model by modifying the CIFAR-10 dataset so that, in addition to containing objects with shape, the images concurrently contained non-shape features, such as a noise-like mask.", "When trained on these modified set of images, the model did not show any bias towards selecting shapes as features.", "Instead it relied on whichever feature allowed it to perform the best prediction -- even when this feature was a noise-like mask or a single predictive pixel amongst 50176 pixels.", "We also found that regularisation methods, such as batch normalisation or Dropout, did not change this behaviour and neither did past or concurrent experience with images from other datasets."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ByePUo05K7", "target": ["Este estudio pone de manifiesto una diferencia clave entre la visi\u00f3n humana y las CNN: mientras que el reconocimiento de objetos en los humanos se basa en el an\u00e1lisis de la forma, las CNN no tienen ese sesgo de forma.", "Busca establecer, mediante una serie de experimentos bien dise\u00f1ados, que las CNNs entrenadas para la clasificaci\u00f3n de im\u00e1genes no codifican sesgos de forma como la visi\u00f3n humana.", "Este art\u00edculo destaca el hecho de que las CNNs no necesariamente aprender\u00e1n a reconocer objetos bas\u00e1ndose en su forma y muestra que sobrepasar\u00e1n las caracter\u00edsticas basadas en el ruido."]}
{"source": ["The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks.", "Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate.", "We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views.", "We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object.", "Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space.", "This approach allows us to generate realistic samples corresponding to various objects in a high variety of views.", "Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content.", "Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations.", "Our model can be used on problems with a huge, potentially infinite, number of categories.", "We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "ryRh0bb0Z", "target": ["Describimos un novedoso modelo generativo multivista que puede generar m\u00faltiples vistas del mismo objeto, o m\u00faltiples objetos en la misma vista sin necesidad de etiquetar las vistas.", "Este art\u00edculo presenta un m\u00e9todo basado en GAN para la generaci\u00f3n de im\u00e1genes que intenta separar las variables latentes que describen el contenido de la imagen de las que describen las propiedades de la vista.", "Este trabajo propone una arquitectura GAN que pretende descomponer la distribuci\u00f3n subyacente de una clase particular en \"contenido\" y \"vista\".", "Propone un nuevo modelo generativo basado en la Red Generativa Adversarial (GAN) que desentra\u00f1a el contenido y la vista de los objetos sin supervisi\u00f3n de la vista y extiende la GAN en un modelo generativo condicional que toma una imagen de entrada y genera diferentes vistas del objeto en la imagen de entrada. "]}
{"source": ["The huge size of deep networks hinders their use in small computing devices.", "In this paper, we consider compressing the network by weight quantization.", "We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization.", "Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network."], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "BkrSv0lA-", "target": ["Se propone un algoritmo de cuantificaci\u00f3n del peso que tiene en cuenta las p\u00e9rdidas y que considera directamente su efecto sobre las mismas.", "Propone un m\u00e9todo de compresi\u00f3n de la red mediante la ternarizaci\u00f3n de pesos. ", "El art\u00edculo propone un nuevo m\u00e9todo para entrenar DNNs con pesos cuantificados, mediante la inclusi\u00f3n de la cuantificaci\u00f3n como una restricci\u00f3n en un algoritmo proximal cuasi-Newton, que aprende simult\u00e1neamente un escalado para los valores cuantificados.", "El art\u00edculo ampl\u00eda el esquema de binarizaci\u00f3n por peso con p\u00e9rdidas a la terarizaci\u00f3n y a la cuantificaci\u00f3n arbitraria de m bits y demuestra su prometedor rendimiento."]}
{"source": ["In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments.", "The options framework provides formalism for such abstraction over sequences of decisions.  ", "However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable.", "Indeed, it is preferable to learn options directly from interaction with the environment.", "Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  ", "In this work we develop a novel policy gradient method for the automatic learning of policies with options.  ", "This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels.", "Experimental results show that the options learned can be interpreted.", "Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJIgf7bAZ", "target": ["Desarrollamos un novedoso m\u00e9todo de gradiente de pol\u00edticas para el aprendizaje autom\u00e1tico de pol\u00edticas con opciones utilizando un paso de inferencia diferenciable.", "El documento presenta una nueva t\u00e9cnica de gradiente de pol\u00edtica para el aprendizaje de opciones, en la que una sola muestra puede utilizarse para actualizar todas las opciones.", "Propone un m\u00e9todo fuera de pol\u00edtica para el aprendizaje de opciones en problemas continuos complejos."]}
{"source": ["The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data.", "The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000).", "The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "ByxF-nAqYX", "target": ["Selecci\u00f3n de caracter\u00edsticas no supervisada mediante la captura de la estructura lineal local de los datos", "Propone una selecci\u00f3n de caracter\u00edsticas localmente lineal y no supervisada.", "El art\u00edculo propone el m\u00e9todo LLUFS para la selecci\u00f3n de caracter\u00edsticas."]}
{"source": ["Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills.", "Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\"", "In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences.", "We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods.", "We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task.", "However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly.", "We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H18WqugAb", "target": ["Utilizando una sencilla tarea de navegaci\u00f3n basada en el lenguaje, estudiamos las capacidades de composici\u00f3n de las modernas redes recurrentes seq2seq.", "Este art\u00edculo se centra en las capacidades de composici\u00f3n de aprendizaje de disparo cero de las modernas RNN de secuencia a secuencia y expone las deficiencias de las actuales arquitecturas de RNN de secuencia a secuencia.", "El art\u00edculo analiza las capacidades de composici\u00f3n de las RNN, en concreto, la capacidad de generalizaci\u00f3n de las RNN en subconjuntos aleatorios de comandos SCAN, en comandos SCAN m\u00e1s largos, y de composici\u00f3n sobre comandos primitivos. ", "Los autores presentan un nuevo conjunto de datos que facilita el an\u00e1lisis de un caso de aprendizaje Seq2Seq"]}
{"source": ["This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.", "First, we demonstrate how  Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning.", "Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.", "We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems.", "The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1xiOjC9F7", "target": ["Abordamos el problema del aprendizaje de similitudes para objetos estructurados con aplicaciones en particular en la seguridad inform\u00e1tica, y proponemos un nuevo modelo de redes de coincidencia de grafos que destaca en esta tarea.", "Los autores introducen una red de emparejamiento de grafos para la recuperaci\u00f3n y emparejamiento de objetos estructurados en grafos.", "Los autores atacan el problema del emparejamiento de grafos proponiendo una extensi\u00f3n de las redes de incrustaci\u00f3n de grafos", "Los autores presentan dos m\u00e9todos para el aprendizaje de una puntuaci\u00f3n de similitud entre pares de grafos y muestran lo beneficioso de introducir ideas de la correspondencia de grafos en las redes neuronales de grafos."]}
{"source": ["Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language.", "In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.", "As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder, and we show that neither an autoregressive decoder nor an RNN decoder is required.  ", "We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.", "Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks.", "We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Bk7wvW-C-", "target": ["Proponemos un modelo codificador-decodificador RNN-CNN para el aprendizaje r\u00e1pido de la representaci\u00f3n de frases sin supervisi\u00f3n.", "Modificaciones del marco de pensamiento saltado para el aprendizaje de incrustaciones de oraciones.", "Este trabajo presenta un nuevo dise\u00f1o h\u00edbrido de codificador RNN y decodificador CNN para su uso en el preentrenamiento, que no requiere un decodificador autorregresivo cuando se preentrenan codificadores.", "Los autores ampl\u00edan Skip-thought decodificando s\u00f3lo una frase objetivo mediante un decodificador CNN."]}
{"source": ["Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs).", "VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function.", "GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data.", "The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems.", "In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on.", "In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework.", "Our numerical results on several datasets demonstrate consistent trends with the proposed theory."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BygMAiRqK7", "target": ["Un enfoque estad\u00edstico para calcular las probabilidades de las muestras en las Redes Generativas Adversariales", "Demostrar que el WGAN con regularizaci\u00f3n entr\u00f3pica maximiza un l\u00edmite inferior en la probabilidad de la distribuci\u00f3n de los datos observados.", "Los autores afirman que es posible aprovechar el l\u00edmite superior de un transporte \u00f3ptimo regularizado por la entrop\u00eda para obtener una medida de \"probabilidad de la muestra\"."]}
{"source": ["We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations.", "Our contribution is threefold.", "First, geomstats allows the flexible modeling of many a machine learning problem through an efficient and extensively unit-tested implementations of these manifolds, as well as the set of useful Riemannian metrics, exponential and logarithm maps that we provide.", "Moreover, the wide choice of loss functions and our implementation of the corresponding gradients allow fast and easy optimization over manifolds.", "Finally, geomstats is the only package to provide a unified framework for Riemannian geometry, as the operations implemented in geomstats are available with different computing backends (numpy,tensorflow and keras), as well as with a GPU-enabled mode\u2013-thus considerably facilitating the application of Riemannian geometry in machine learning.", "In this paper, we present geomstats through a review of the utility and advantages of manifolds in machine learning, using the concrete examples that they span to show the efficiency and practicality of their implementation using our package"], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rklXaoAcFX", "target": ["Presentamos geomstats, un eficiente paquete de Python para la modelizaci\u00f3n y optimizaci\u00f3n riemanniana sobre colectores compatible con numpy y tensorflow .", "El art\u00edculo presenta el paquete de software geomstats, que proporciona un uso sencillo de las variedades y m\u00e9tricas riemannianas dentro de los modelos de aprendizaje autom\u00e1tico", "Propone un paquete de Python para la optimizaci\u00f3n y las aplicaciones en variedades riemannianas y destaca las diferencias entre el paquete Geomstats y otros paquetes.", "Presenta una caja de herramientas geom\u00e9tricas, Geomstats, para el aprendizaje autom\u00e1tico en variedades riemannianas."]}
{"source": ["We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference.", "The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices.", "However, most of the previous studies optimize for inference while neglect training or even complicate it.", "Training is far more intractable, since", "(i) the neurons dominate the memory cost rather than the weights in inference;", "(ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid;", "(iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity.", "To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimensionreduction search and obtains the BN compatibility via a double-mask selection.", "Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1goBoR9F7", "target": ["Construimos un grafo din\u00e1mico disperso mediante una b\u00fasqueda de reducci\u00f3n de dimensiones para reducir los costes de computaci\u00f3n y memoria tanto en el entrenamiento como en la inferencia de la DNN.", "Los autores proponen utilizar el gr\u00e1fico de c\u00e1lculo disperso din\u00e1mico para reducir el coste de memoria y tiempo de c\u00e1lculo en la red neuronal profunda (DNN).", "Este trabajo propone un m\u00e9todo para acelerar el entrenamiento y la inferencia de las redes neuronales profundas utilizando la poda din\u00e1mica del grafo de c\u00e1lculo."]}
{"source": ["Efficient exploration remains a major challenge for reinforcement learning.", "One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic.", "Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting.", "Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning.", "As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning.", "The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise.", "We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Byx83s09Km", "target": ["Desarrollamos una extensi\u00f3n pr\u00e1ctica del Muestreo Dirigido a la Informaci\u00f3n para el Aprendizaje por Refuerzo, que tiene en cuenta la incertidumbre param\u00e9trica y la heteroscedasticidad en la distribuci\u00f3n de retorno para la exploraci\u00f3n.", "Los autores proponen una forma de extender el muestreo dirigido por la informaci\u00f3n al aprendizaje por refuerzo, combinando dos tipos de incertidumbre para obtener una estrategia de exploraci\u00f3n sencilla basada en IDS. ", "Este trabajo investiga los enfoques de exploraci\u00f3n sofisticada para el aprendizaje por refuerzo construidos sobre el muestreo directo de informaci\u00f3n y sobre el aprendizaje por refuerzo distribucional"]}
{"source": ["We address the problem of learning structured policies for continuous control.", "In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions.", "In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph.", "Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent.", "In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments.", "We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer.", "We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n"], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1sqHMZCb", "target": ["Utilizaci\u00f3n de redes neuronales gr\u00e1ficas para modelar la informaci\u00f3n estructural de los agentes con el fin de mejorar la pol\u00edtica y la transferibilidad ", "M\u00e9todo de representaci\u00f3n y aprendizaje de pol\u00edticas estructuradas para tareas de control continuo mediante redes neuronales gr\u00e1ficas", "La presentaci\u00f3n propone la incorporaci\u00f3n de una estructura adicional en los problemas de aprendizaje por refuerzo, en particular la estructura de la morfolog\u00eda del agente", "Proponer una aplicaci\u00f3n de las Redes Neuronales Gr\u00e1ficas al aprendizaje de pol\u00edticas de control de robots \"ciempi\u00e9s\" de diferentes longitudes."]}
{"source": ["Real-world tasks are often highly structured.", "Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL).", "However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task.", "In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization.", "Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space.", "To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling.  \n", "In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method.", "This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy.  ", "Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "Hyl_vjC5KQ", "target": ["Este trabajo presenta un marco de aprendizaje por refuerzo jer\u00e1rquico basado en pol\u00edticas de opciones deterministas y en la maximizaci\u00f3n de la informaci\u00f3n mutua. ", "Propone un algoritmo HRL que intenta aprender las opciones que maximizan su informaci\u00f3n mutua con la densidad estado-acci\u00f3n bajo la pol\u00edtica \u00f3ptima.", "Este trabajo propone un sistema HRL en el que la informaci\u00f3n mutua de la variable latente y de los pares estado-acci\u00f3n es aproximadamente maximizada.", "Propone un criterio que pretende maximizar la informaci\u00f3n mutua entre las opciones y los pares estado-acci\u00f3n y muestra emp\u00edricamente que las opciones aprendidas descomponen el espacio estado-acci\u00f3n pero no el espacio estado. "]}
{"source": ["Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables.", "However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications.", "To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD).", "Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction.", "This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.", "We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths.", "Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs.", "We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkEqro0ctQ", "target": ["Introducimos y validamos las interpretaciones locales jer\u00e1rquicas, la primera t\u00e9cnica para buscar y mostrar autom\u00e1ticamente las interacciones importantes para las predicciones individuales realizadas por LSTMs y CNNs.", "Un nuevo enfoque para explicar las predicciones de las redes neuronales mediante el aprendizaje de representaciones jer\u00e1rquicas de grupos de caracter\u00edsticas de entrada y su contribuci\u00f3n a la predicci\u00f3n final", "Extiende un m\u00e9todo de interpretaci\u00f3n de caracter\u00edsticas existente para LSTMs a DNNs m\u00e1s gen\u00e9ricas e introduce un clustering jer\u00e1rquico de las caracter\u00edsticas de entrada y las contribuciones de cada cluster a la predicci\u00f3n final.", "Este trabajo propone una extensi\u00f3n jer\u00e1rquica de la descomposici\u00f3n contextual."]}
{"source": ["Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression.", "PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint.", "We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer.", "Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively.", "In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio.", "Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkl42iA5t7", "target": ["Proponemos un m\u00e9todo f\u00e1cil de aplicar, pero eficaz, para la compresi\u00f3n de redes neuronales. PFA explota la correlaci\u00f3n intr\u00ednseca entre las respuestas de los filtros dentro de las capas de la red para recomendar una huella de red m\u00e1s peque\u00f1a.", "Propone podar las redes convolucionales analizando la correlaci\u00f3n observada entre los filtros de una misma capa, expresada por el espectro de valores propios de su matriz de covarianza.", "Este trabajo introduce un enfoque para comprimir las redes neuronales observando la correlaci\u00f3n de las respuestas de los filtros en cada capa mediante dos estrategias.", "Este trabajo propone un m\u00e9todo de compresi\u00f3n basado en el an\u00e1lisis espectral"]}
{"source": ["We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering.", "In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer.", "Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set.", "Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data.", "We show that the improved performance is due to the increased diversity of reformulation strategies."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkMhusC5Y7", "target": ["Agentes de reformulaci\u00f3n de consultas m\u00faltiples y diversas entrenados con aprendizaje de refuerzo para mejorar los motores de b\u00fasqueda.", "Parelizaci\u00f3n del m\u00e9todo de conjunto en el aprendizaje de refuerzo para la reformulaci\u00f3n de consultas, acelerando el entrenamiento y mejorando la diversidad de las freformulaciones aprendidas", "Los autores proponen entrenar a m\u00faltiples agentes distintos, cada uno sobre un subconjunto diferente del conjunto de entrenamiento.", "Los autores proponen un enfoque de conjunto para la reformulaci\u00f3n de consultas"]}
{"source": ["Network Embeddings (NEs) map the nodes of a given network into $d$-dimensional Euclidean space $\\mathbb{R}^d$.", "Ideally, this mapping is such that 'similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if 'similar' means being 'more likely to be connected') or classification (if 'similar' means 'being more likely to have the same label').", "In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes.\n\n", "A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc.", "To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \\emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.).", "We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently.\n\n", "We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity.", "Finally, we illustrate the potential of CNE for network visualization."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "ryepUj0qtX", "target": ["Introducimos un m\u00e9todo de incrustaci\u00f3n de redes que tiene en cuenta la informaci\u00f3n previa sobre la red, lo que permite obtener un rendimiento emp\u00edrico superior.", "El trabajo propuso utilizar una distribuci\u00f3n a priori para restringir la incrustaci\u00f3n de la red, para la formulaci\u00f3n este trabajo utiliz\u00f3 distribuciones gaussianas muy restringidas.", "Propone el aprendizaje de incrustaciones de nodos sin supervisi\u00f3n considerando las propiedades estructurales de las redes."]}
{"source": ["This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients.", "This class, which we refer to as the ''``Adam-type'', includes the popular algorithms such as Adam, AMSGrad, AdaGrad.", "Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving  non-convex problems remains an open question.", "In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order   $O(\\log{T}/\\sqrt{T})$ for non-convex stochastic optimization.", "Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum).", "We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge.", "Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily  monitor the progress of algorithms and determine their convergence behavior."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "H1x-x309tm", "target": ["Analizamos la convergencia de los algoritmos de tipo Adam y proporcionamos condiciones suaves suficientes para garantizar su convergencia, tambi\u00e9n mostramos que violar las condiciones puede hacer que un algoritmo diverja.", "Presenta un an\u00e1lisis de convergencia en el entorno no convexo para una familia de algoritmos de optimizaci\u00f3n.", "Este trabajo investiga la condici\u00f3n de convergencia de los optimizadores tipo Adam en los problemas de optimizaci\u00f3n no convexos sin restricciones."]}
{"source": ["This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data.", "These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data.", "The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required.", "This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture.", "Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rkhxwltab", "target": ["El autocodificador de pesos atados con la funci\u00f3n abs como funci\u00f3n de activaci\u00f3n, aprende a hacer la clasificaci\u00f3n en la direcci\u00f3n hacia adelante y la regresi\u00f3n en la direcci\u00f3n hacia atr\u00e1s debido a la funci\u00f3n de coste especialmente definida.", "El art\u00edculo propone utilizar la funci\u00f3n de activaci\u00f3n del valor absoluto en una arquitectura de autoencoder con un t\u00e9rmino adicional de aprendizaje supervisado en la funci\u00f3n objetivo", "Este trabajo introduce una red reversible con valor absoluto utilizado como funci\u00f3n de activaci\u00f3n"]}
{"source": ["Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step.", "Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages.", "Similarly, pre-processing introduces an additional source of error.", "To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018].", "Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions.", "TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task.", "TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively.", "Furthermore, we observe a significant increase in sample efficiency.", "With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset.", "We open-source our trained models, experiments, and source code."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJgrxbqp67", "target": ["Proponemos un modelo de extracci\u00f3n de relaciones basado en Transformer que utiliza representaciones ling\u00fc\u00edsticas preentrenadas en lugar de caracter\u00edsticas ling\u00fc\u00edsticas expl\u00edcitas.", "Presenta un modelo de extracci\u00f3n de relaciones basado en transformadores que aprovecha el preentrenamiento en texto no etiquetado con un objetivo de modelado del lenguaje.", "Este art\u00edculo describe una novedosa aplicaci\u00f3n de las redes de transformaci\u00f3n para la extracci\u00f3n de relaciones.", "El art\u00edculo presenta una arquitectura basada en transformadores para la extracci\u00f3n de relajaci\u00f3n, evaluada en dos conjuntos de datos."]}
{"source": ["Neural networks have recently had a lot of success for many tasks.", "However, neural\n", "network architectures that perform well are still typically designed manually\n", "by experts in a cumbersome trial-and-error process.", "We propose a new method\n", "to automatically search for well-performing CNN architectures based on a simple\n", "hill climbing procedure whose operators apply network morphisms, followed\n", "by short optimization runs by cosine annealing.", "Surprisingly, this simple method\n", "yields competitive results, despite only requiring resources in the same order of\n", "magnitude as training a single network.", "E.g., on CIFAR-10, our method designs\n", "and trains networks with an error rate below 6% in only 12 hours on a single GPU;\n", "training for one day reduces this error further, to almost 5%."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SySaJ0xCZ", "target": ["Proponemos un m\u00e9todo sencillo y eficaz para la b\u00fasqueda de arquitecturas de redes neuronales convolucionales.", "Propone un m\u00e9todo de b\u00fasqueda de arquitecturas neuronales que consigue una precisi\u00f3n cercana al estado del arte en CIFAR10 y requiere muchos menos recursos computacionales.", "Presenta un m\u00e9todo para buscar arquitecturas de redes neuronales al mismo tiempo que se entrena, lo que ahorra dr\u00e1sticamente el tiempo de entrenamiento y de b\u00fasqueda de arquitecturas.", "Propone una variante de b\u00fasqueda de arquitecturas neuronales utilizando morfismos de red para definir un espacio de b\u00fasqueda utilizando arquitecturas CNN completando la tarea de clasificaci\u00f3n de im\u00e1genes CIFAR"]}
{"source": ["We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks.\n", "We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph.\n", "Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective.", "GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph.", "Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties.", "We discover that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them.", "Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction as well as promising results on node classification, even though not specifically trained for these tasks."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H15RufWAW", "target": ["Uso de GANs para generar grafos mediante paseos aleatorios.", "Los autores propusieron un modelo generativo de paseos aleatorios sobre grafos que permite el aprendizaje agn\u00f3stico del modelo, el ajuste controlable, la generaci\u00f3n de grafos conjuntos", "Propone una formulaci\u00f3n WGAN para la generaci\u00f3n de grafos basada en paseos aleatorios utilizando incrustaciones de nodos y una arquitectura LSTM para el modelado."]}
{"source": ["The ability of a classifier to recognize unknown inputs is important for many classification-based systems.", "We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes.", "We propose a method based on the Generative Adversarial Networks (GAN) framework.", "We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector.", "We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection.", "Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Hy7EPh10W", "target": ["Proponemos resolver un problema de clasificaci\u00f3n simult\u00e1nea y de detecci\u00f3n de novedades en el marco del GAN.", "Propone un GAN para unificar la clasificaci\u00f3n y la detecci\u00f3n de novedades.", "El art\u00edculo presenta un m\u00e9todo de detecci\u00f3n de novedades basado en un GAN multiclase que se entrena para obtener im\u00e1genes generadas a partir de una mezcla de las distribuciones nominal y de novedades.", "El art\u00edculo propone un GAN para la detecci\u00f3n de novedades utilizando un generador de mezclas con p\u00e9rdida de coincidencia de caracter\u00edsticas"]}
{"source": ["  Verifying a person's identity based on their voice is a challenging, real-world problem in biometric security.", "A crucial requirement of such speaker verification systems is to be domain robust.", "Performance should not degrade even if speakers are talking in languages not seen during training.", "To this end, we present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks.", "We combine adversarial training with an angular margin loss function, which encourages the speaker embedding model to be discriminative by directly optimizing for cosine similarity between classes.", "We are able to beat a strong baseline system using a cosine distance classifier and a simple score-averaging strategy.", "Our results also show that models with adversarial adaptation perform significantly better than unadapted models.", "In an attempt to better understand this behavior, we quantitatively measure the degree of invariance induced by our proposed methods using Maximum Mean Discrepancy and Frechet distances.", "Our analysis shows that our proposed adversarial speaker embedding models significantly reduce the distance between source and target data distributions, while performing similarly on the former and better on the latter."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Byx4xH3is7", "target": ["El rendimiento de la verificaci\u00f3n del hablante puede mejorarse significativamente adaptando el modelo a los datos del dominio utilizando redes adversariales generativas. Adem\u00e1s, la adaptaci\u00f3n puede realizarse de forma no supervisada.", "Proponer una serie de variantes de GAN en la tarea de reconocimiento de hablantes en la condici\u00f3n de desajuste de dominio."]}
{"source": ["Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting is still a major challenge.", "In the following paper we address the task of disentanglement and introduce a new state-of-the-art approach called Non-synergistic variational Autoencoder (Non-Syn VAE).", "Our model draws inspiration from population coding, where the notion of synergy arises when we describe the encoded information by neurons in the form of responses from the stimuli.", "If those responses convey more information together than separate as independent sources of encoding information, they are acting synergetically.", "By penalizing the synergistic mutual information within the latents we encourage information independence and by doing that disentangle the latent factors.", "Notably, our approach could be added to the VAE framework easily, where the new ELBO function is still a lower bound on the log likelihood.", "In addition, we qualitatively compare our model with Factor VAE and show that this one implicitly minimises the synergy of the latents."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "Skl3M20qYQ", "target": ["Minimizaci\u00f3n de la informaci\u00f3n mutua sin\u00e9rgica dentro de las latentes y los datos para la tarea de desentra\u00f1amiento utilizando el marco VAE.", "Propone una nueva funci\u00f3n objetivo para el aprendizaje de representaciones dientangulares en un marco variacional minimizando la sinergia de la informaci\u00f3n proporcionada.", "Los autores pretenden entrenar una VAE que haya desentra\u00f1ado las representaciones latentes de una manera \"sin\u00e9rgica\" m\u00e1xima. ", "Este trabajo propone un nuevo enfoque para reforzar el desentra\u00f1amiento en las VAE utilizando un t\u00e9rmino que penaliza la informaci\u00f3n mutua sin\u00e9rgica entre las variables latentes."]}
{"source": ["   Metric embeddings are   immensely useful representations of associations between entities   (images, users, search queries, words, and more).  ", "Embeddings are learned by  optimizing a loss objective of the general form of a sum over example associations.", "Typically, the optimization uses stochastic gradient updates over minibatches of examples that are arranged  independently at random.", "In this work, we propose the use of {\\em structured arrangements} through randomized {\\em microbatches} of examples that are more likely to include similar ones.", "We make a principled argument for the properties of our arrangements  that accelerate the training and present efficient algorithms to generate microbatches that respect the marginal  distribution of training examples.  ", "Finally, we observe experimentally that our structured arrangements accelerate training by 3-20\\%.", "Structured arrangements emerge as a powerful and novel performance knob for SGD that is independent and complementary to other SGD  hyperparameters and thus is a candidate for wide deployment."], "source_labels": [0, 0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "r1erRoCqtX", "target": ["Acelerar el SGD disponiendo los ejemplos de forma diferente", "El art\u00edculo presenta un m\u00e9todo para mejorar la tasa de convergencia del Descenso Gradiente Estoc\u00e1stico para el aprendizaje de incrustaciones agrupando muestras de entrenamiento similares.", "Propone una estrategia de muestreo no uniforme para construir minibatches en SGD para la tarea de aprender incrustaciones para asociaciones de objetos."]}
{"source": ["Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data.", "One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words.", "In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  ", "We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method.", "For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus.", "In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJ7yZ2P6-", "target": ["Combinar la informaci\u00f3n entre la incrustaci\u00f3n de palabras preestablecida y la representaci\u00f3n de palabras espec\u00edfica de la tarea para resolver el problema de la falta de vocabulario", "Este art\u00edculo propone un enfoque para mejorar la predicci\u00f3n de incrustaci\u00f3n fuera de vocabulario para la tarea de modelar conversaciones de di\u00e1logo con ganancias considerables sobre las l\u00edneas de base.", "Propone combinar las incrustaciones de palabras externas preentrenadas y las incrustaciones de palabras preentrenadas en los datos de entrenamiento, manteni\u00e9ndolas como dos vistas.", "Propone un m\u00e9todo para ampliar la cobertura de las incrustaciones de palabras preentrenadas para hacer frente al problema de OOV que surge al aplicarlas a conjuntos de datos conversacionales y aplica nuevas variantes del modelo basado en LSTM a la tarea de selecci\u00f3n de respuestas en el modelado de di\u00e1logos."]}
{"source": ["Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks.", "Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks.", "However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice.", "Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process.", "The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations).", "In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance.", "This allows us to train neural networks to perform optimization faster than well tuned first-order methods.", "Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods.", "We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJxwAo09KQ", "target": ["Analizamos los problemas al entrenar optimizadores aprendidos, abordamos esos problemas mediante la optimizaci\u00f3n variacional utilizando dos estimadores de gradiente complementarios, y entrenamos optimizadores que son 5 veces m\u00e1s r\u00e1pidos en tiempo de reloj de pared que los optimizadores de referencia (por ejemplo, Adam).", "Este trabajo utiliza la optimizaci\u00f3n no rodada para aprender redes neuronales de optimizaci\u00f3n.", "Este trabajo aborda el problema del aprendizaje de un optimizador, concretamente los autores se centran en la obtenci\u00f3n de gradientes m\u00e1s limpios a partir del procedimiento de entrenamiento desenrollado.", "Presenta un m\u00e9todo para \"aprender un optimizador\" utilizando una Optimizaci\u00f3n Variacional para la p\u00e9rdida del optimizador \"externo\" y propone la idea de combinar tanto el gradiente reparametrizado como el estimador de la funci\u00f3n de puntuaci\u00f3n para el Objetivo Variacional y los pondera utilizando una f\u00f3rmula de producto de Gauss para la media."]}
{"source": ["Asynchronous distributed gradient descent algorithms for training of deep neural\n", "networks are usually considered as inefficient, mainly because of the Gradient delay\n", "problem.", "In this paper, we propose a novel asynchronous distributed algorithm\n", "that tackles this limitation by well-thought-out averaging of model updates, computed\n", "by workers.", "The algorithm allows computing gradients along the process\n", "of gradient merge, thus, reducing or even completely eliminating worker idle time\n", "due to communication overhead, which is a pitfall of existing asynchronous methods.\n", "We provide theoretical analysis of the proposed asynchronous algorithm,\n", "and show its regret bounds.", "According to our analysis, the crucial parameter for\n", "keeping high convergence rate is the maximal discrepancy between local parameter\n", "vectors of any pair of workers.", "As long as it is kept relatively small, the\n", "convergence rate of the algorithm is shown to be the same as the one of a sequential\n", "online learning.", "Furthermore, in our algorithm, this discrepancy is bounded\n", "by an expression that involves the staleness parameter of the algorithm, and is\n", "independent on the number of workers.", "This is the main differentiator between\n", "our approach and other solutions, such as Elastic Asynchronous SGD or Downpour\n", "SGD, in which that maximal discrepancy is bounded by an expression that\n", "depends on the number of workers, due to gradient delay problem.", "To demonstrate\n", "effectiveness of our approach, we conduct a series of experiments on image\n", "classification task on a cluster with 4 machines, equipped with a commodity communication\n", "switch and with a single GPU card per machine.", "Our experiments\n", "show a linear scaling on 4-machine cluster without sacrificing the test accuracy,\n", "while eliminating almost completely worker idle time.", "Since our method allows\n", "using commodity communication switch, it paves a way for large scale distributed\n", "training performed on commodity clusters."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1lo3sC9KX", "target": ["Un m\u00e9todo para un entrenamiento distribuido as\u00edncrono eficiente de modelos de aprendizaje profundo junto con l\u00edmites de arrepentimiento te\u00f3ricos.", "El art\u00edculo propone un algoritmo para restringir el estancamiento en el SGD as\u00edncrono y proporciona un an\u00e1lisis te\u00f3rico", "Propone un algoritmo h\u00edbrido para eliminar el retraso del gradiente de los m\u00e9todos as\u00edncronos."]}
{"source": ["Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs.", "However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks.", "This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks.", "We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise.", "Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference.", "Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches.", "As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryetZ20ctX", "target": ["Dise\u00f1amos una novedosa metodolog\u00eda de cuantificaci\u00f3n para optimizar conjuntamente la eficiencia y la robustez de los modelos de aprendizaje profundo.", "Propone un esquema de regularizaci\u00f3n para proteger las redes neuronales cuantificadas de los ataques de los adversarios utilizando un filtrado constante de Lipschitz de la entrada-salida de las capas internas."]}
{"source": ["Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks.", "However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies.", "In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.", "We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph.", "This model can also be encouraged to perform fewer state updates through a budget constraint.", "We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models.", "Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HkwVAXyCW", "target": ["Una modificaci\u00f3n para las arquitecturas RNN existentes que les permite omitir las actualizaciones de estado conservando el rendimiento de las arquitecturas originales.", "Propone el modelo Skip RNN que permite a una red recurrente omitir selectivamente la actualizaci\u00f3n de su estado oculto para algunas entradas, lo que permite reducir el c\u00e1lculo en el momento de la prueba.", "Propone un nuevo modelo de RNN en el que tanto la entrada como la actualizaci\u00f3n de estado de las c\u00e9lulas recurrentes se saltan de forma adaptativa durante algunos pasos de tiempo."]}
{"source": ["We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers.", "Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement.", "Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step.", "Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network.", "This estimate has the same size and is similar to the momentum variable that is commonly used in SGD.", "No estimate of the Hessian is maintained.\n", "We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle.", "We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning.", "We also show our optimiser's generality by testing on a large set of randomly-generated architectures."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Sygx4305KQ", "target": ["Un r\u00e1pido solucionador de segundo orden para el aprendizaje profundo que funciona en problemas a escala de ImageNet sin ajuste de hiperpar\u00e1metros", "Elecci\u00f3n de la direcci\u00f3n utilizando un \u00fanico paso de descenso de gradiente \"hacia el paso Newton\" a partir de una estimaci\u00f3n original, y luego tomando esta direcci\u00f3n en lugar del gradiente original", "Un nuevo m\u00e9todo aproximado de optimizaci\u00f3n de segundo orden con bajo coste computacional que sustituye el c\u00e1lculo de la matriz hessiana por un \u00fanico paso de gradiente y una estrategia de arranque en caliente."]}
{"source": ["The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development.", "However, to push this idea towards practical implementation, we need better models and better ways of training.", "We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function.", "We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes.", "With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms."], "source_labels": [0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "ByxBFsRqYm", "target": ["Modelo basado en la atenci\u00f3n y entrenado con REINFORCE con una l\u00ednea de base de despliegue codicioso para aprender heur\u00edsticas con resultados competitivos en TSP y otros problemas de enrutamiento", "Presenta un enfoque basado en la atenci\u00f3n para el aprendizaje de una pol\u00edtica para resolver TSP y otros problemas de optimizaci\u00f3n combinatoria de tipo de ruta.", "Este trabajo trata de aprender heur\u00edsticas para resolver problemas de optimizaci\u00f3n combinatoria"]}
{"source": ["We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters.", "While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks.", "Finally, we show its effectiveness on two distinct tasks."], "source_labels": [1, 0, 0], "rouge_scores": [], "paper_id": "H1OQukZ0-", "target": ["Un algoritmo para optimizar los hiperpar\u00e1metros de regularizaci\u00f3n durante el entrenamiento", "El art\u00edculo propone una forma de reiniciar y en cada actualizaci\u00f3n de lambda y un procedimiento de recorte de y para mantener la estabilidad del sistema din\u00e1mico.", "Propone un algoritmo para la optimizaci\u00f3n de hiperpar\u00e1metros que puede ser visto como una extensi\u00f3n de Franceschi 2017 donde algunas estimaciones son reiniciadas en caliente para aumentar la estabilidad del m\u00e9todo.", "Propone una extensi\u00f3n de un m\u00e9todo existente para optimizar los hiperpar\u00e1metros de regularizaci\u00f3n."]}
{"source": ["Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks.", "However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation.", "We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models.", "We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n"], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "ByJHuTgA-", "target": ["Demostrar que los LSTM son tan buenos o mejores que las innovaciones recientes para el LM y que la evaluaci\u00f3n de los modelos es a menudo poco fiable.", "Este art\u00edculo describe una validaci\u00f3n exhaustiva de los modelos ling\u00fc\u00edsticos de palabras y caracteres basados en LSTM, lo que conduce a un resultado significativo en el modelado del lenguaje y a un hito en el aprendizaje profundo."]}
{"source": ["  Residual and skip connections play an important role in many current\n  generative models.", "Although their theoretical and numerical advantages\n  are understood, their role in speech enhancement systems has not been\n  investigated so far.", "When performing spectral speech enhancement,\n  residual connections are very similar in nature to spectral subtraction,\n  which is the one of the most commonly employed speech enhancement approaches.\n  ", "Highway networks, on the other hand, can be seen as a combination of spectral\n  masking and spectral subtraction.", "However, when using deep neural networks, such operations would\n  normally happen in a transformed spectral domain, as opposed to traditional speech\n  enhancement where all operations are often done directly on the spectrum.\n  ", "In this paper, we aim to investigate the role of residual and highway\n  connections in deep neural networks for speech enhancement, and verify whether\n  or not they operate similarly to their traditional, digital signal processing\n  counterparts.", "We visualize the outputs of such connections, projected back to\n  the spectral domain, in models trained for speech denoising, and show that while\n  skip connections do not necessarily improve performance with regards to the\n  number of parameters, they make speech enhancement models more interpretable."], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rkzeXBDos7", "target": ["Mostramos c\u00f3mo el uso de las conexiones de salto puede hacer que los modelos de mejora del habla sean m\u00e1s interpretables, ya que hace que utilicen mecanismos similares a los que se han explorado en la literatura de DSP.", "Los autores proponen incorporar los bloques Residual, Highway y Masking dentro de un pipeline totalmente convolucional para entender c\u00f3mo se realiza la inferencia iterativa de la salida y el enmascaramiento en una tarea de mejora del habla", "Los autores interpretan las conexiones viales, residuales y de enmascaramiento. ", "Los autores generan su propio habla ruidosa a\u00f1adiendo artificialmente ruido de un conjunto de datos de ruido bien establecido a un conjunto de datos de habla limpia menos conocido."]}
{"source": ["Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data.", "Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient.", "With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications?", "We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates.", "We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances.", "Combining these two innovations, the resulting method is highly efficient and robust.", "On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1l08oAct7", "target": ["Un m\u00e9todo para eliminar la varianza del gradiente y afinar autom\u00e1ticamente las priores para un entrenamiento eficaz de las redes neuronales bayesianas", "Propone un nuevo enfoque para realizar la inferencia variacional determinista para BNN feed-forward con funciones de activaci\u00f3n no lineales espec\u00edficas mediante la aproximaci\u00f3n de los momentos de las capas.", "El art\u00edculo considera un enfoque puramente determinista para el aprendizaje de aproximaciones posteriores variacionales para redes neuronales bayesianas."]}
{"source": ["Skills learned through (deep) reinforcement learning often generalizes poorly\n", "across tasks and re-training is necessary when presented with a new task.", "We\n", "present a framework that combines techniques in formal methods with reinforcement\n", "learning (RL) that allows for the convenient specification of complex temporal\n", "dependent tasks with logical expressions and construction of new skills from existing\n", "ones with no additional exploration.", "We provide theoretical results for our\n", "composition technique and evaluate on a simple grid world simulation as well as\n", "a robotic manipulation task."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkfwpiA9KX", "target": ["Enfoque de un m\u00e9todo formal para la composici\u00f3n de habilidades en tareas de aprendizaje por refuerzo", "El art\u00edculo combina la RL y las restricciones expresadas mediante f\u00f3rmulas l\u00f3gicas estableciendo una automatizaci\u00f3n a partir de f\u00f3rmulas scTLTL.", "Propone un m\u00e9todo que ayuda a construir la pol\u00edtica a partir de las subtareas aprendidas en el tema de la combinaci\u00f3n de tareas RL con f\u00f3rmulas de l\u00f3gica temporal lineal."]}
{"source": ["The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.\n", "This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.\n", "Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.\n", "Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.\n"], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "rJl8FoRcY7", "target": ["Derivaci\u00f3n de una formulaci\u00f3n general de un VAE multimodal a partir de la log-verosimilitud marginal conjunta.", "Propone un VAE multimodal con un l\u00edmite variacional derivado de la regla de la cadena.", "Este trabajo propone un objetivo, M^2VAE, para las VAE multimodales, que se supone que aprende una representaci\u00f3n del espacio latente m\u00e1s significativa."]}
{"source": ["We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models.", "Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions.", "We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning.", "We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJ8c3f-0b", "target": ["Nos basamos en la autocodificaci\u00f3n secuencial de Monte Carlo, obtenemos nuevos conocimientos te\u00f3ricos y desarrollamos un procedimiento de entrenamiento mejorado basado en esos conocimientos.", "El art\u00edculo propone una versi\u00f3n del entrenamiento tipo IWAE que utiliza el SMC en lugar del cl\u00e1sico muestreo de importancia.", "Este trabajo propone la autocodificaci\u00f3n de Monte Carlo secuencial (SMC), ampliando el marco VAE a un nuevo objetivo de Monte Carto basado en SMC. "]}
{"source": ["A key component for many reinforcement learning agents is to learn a value function, either for policy evaluation or control.", "Many of the algorithms for learning values, however, are designed for linear function approximation---with a fixed basis or fixed representation.", "Though there have been a few sound extensions to nonlinear function approximation, such as nonlinear gradient temporal difference learning, these methods have largely not been adopted, eschewed in favour of simpler but not sound methods like temporal difference learning and Q-learning.", "In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale.", "The approach facilitates the use of algorithms developed for the linear setting, such as data-efficient least-squares methods, eligibility traces and the myriad of recently developed linear policy evaluation algorithms, to provide nonlinear value estimates.", "We prove convergence for TTNs, with particular care given to ensure convergence of the fast linear component under potentially dependent features provided by the learned representation.", "We empirically demonstrate the benefits of TTNs, compared to other nonlinear value function approximation algorithms, both for policy evaluation and control.    "], "source_labels": [0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "rJleN20qK7", "target": ["Proponemos una arquitectura para el aprendizaje de funciones de valor que permite el uso de cualquier algoritmo de evaluaci\u00f3n de pol\u00edticas lineales en t\u00e1ndem con el aprendizaje de caracter\u00edsticas no lineales.", "El trabajo propone un marco de dos escalas de tiempo para el aprendizaje de la funci\u00f3n de valor y una representaci\u00f3n de estado en conjunto con aproximadores no lineales.", "Este trabajo propone redes de dos escalas temporales (TTN) y demuestra la convergencia de este m\u00e9todo utilizando m\u00e9todos de aproximaci\u00f3n estoc\u00e1stica de dos escalas temporales. ", "Este trabajo presenta una red de dos escalas (TTN) que permite utilizar m\u00e9todos lineales para aprender valores. "]}
{"source": ["Large-scale Long Short-Term Memory (LSTM) cells are often the building blocks of many state-of-the-art algorithms for tasks in Natural Language Processing (NLP).", "However, LSTMs are known to be computationally inefficient because the memory capacity of the models depends on the number of parameters, and the inherent recurrence that models the temporal dependency is not parallelizable.", "In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance (and sometimes even gain).", "To show the effectiveness of our method across different tasks, we examine two settings:", "1) compressing core LSTM layers in Language Models,", "2) compressing biLSTM layers of ELMo~\\citep{ELMo} and evaluate in three downstream NLP tasks (Sentiment Analysis, Textual Entailment, and Question Answering).", "The latter is particularly interesting as embeddings from large pre-trained biLSTM Language Models are often used as contextual word representations.", "Finally, we discover that matrix factorization performs better in general, additive recurrence is often more important than multiplicative recurrence, and we identify an interesting correlation between matrix norms and compression performance.\n\n"], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BylahsR9tX", "target": ["Proponemos algoritmos de factorizaci\u00f3n matricial (MF) de bajo rango, sencillos pero eficaces, para acelerar el tiempo de ejecuci\u00f3n, ahorrar memoria y mejorar el rendimiento de las LSTM.", "Propone acelerar LSTM utilizando MF como estrategia de compresi\u00f3n post-procesamiento y lleva a cabo extensos experimentos para mostrar el rendimiento."]}
{"source": ["Manipulation and re-use of images in scientific publications is a recurring problem, at present lacking a scalable solution.  ", "Existing tools for detecting image duplication are mostly manual or semi-automated, despite the fact that generating data for a learning-based approach is straightforward, as we here illustrate.", "This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, scale, perspective transform, histogram adjustment, partial erasing, and compression artifacts.", "We propose a solution based on a 3-branch Siamese Convolutional Neural Network.", "The ConvNet model is trained to map images into a 128-dimensional space, where the Euclidean distance between duplicate (respectively, unique) images is no greater (respectively, greater) than 1.", "Our results suggest that such an approach can serve as tool to improve surveillance of the published and in-peer-review literature for image manipulation.", "We also show that as a byproduct the network learns useful representations for semantic segmentation, with performance comparable to that of domain-specific models."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJxY_oCqKQ", "target": ["Una m\u00e9trica forense para determinar si una imagen dada es una copia (con posible manipulaci\u00f3n) de otra imagen de un conjunto de datos dado.", "Introduce la red siamesa para identificar im\u00e1genes duplicadas y copiadas/modificadas, que puede utilizarse para mejorar la vigilancia de la literatura publicada y en revisi\u00f3n por pares.", "El art\u00edculo presenta una aplicaci\u00f3n de redes convolucionales profundas para la tarea de detecci\u00f3n de im\u00e1genes duplicadas", "Este trabajo aborda el problema de encontrar im\u00e1genes duplicadas/casi duplicadas de publicaciones biom\u00e9dicas y propone una CNN est\u00e1ndar y funciones de p\u00e9rdida y las aplica a este campo."]}
{"source": ["Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space.", "The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training.", "In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data.", "Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training.  ", "Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously.", "We  demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "SJahqJZAW", "target": ["Entrenamiento estable de GAN en altas dimensiones mediante el uso de un conjunto de discriminadores, cada uno con una visi\u00f3n de baja dimensi\u00f3n de las muestras generadas", "El art\u00edculo propone estabilizar el entrenamiento de GAN utilizando un conjunto de discriminadores, cada uno de los cuales trabaja sobre una proyecci\u00f3n aleatoria de los datos de entrada, para proporcionar la se\u00f1al de entrenamiento para el modelo generador.", "El art\u00edculo propone un m\u00e9todo de entrenamiento GAN para mejorar la estabilidad del entrenamiento. ", "El art\u00edculo propone un nuevo enfoque para el entrenamiento de GAN, que proporciona gradientes estables para entrenar el generador."]}
{"source": ["We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short).", "Inclusion relations among N-balls implicitly encode subordinate relations among categories.", "The similarity measurement in terms of the cosine function is enriched by category information.", "Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved.", "A new benchmark data set is created for validating the category of unknown words.", "Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words.", "Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}."], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJlWOj0qF7", "target": ["mostramos un m\u00e9todo geom\u00e9trico para codificar perfectamente la informaci\u00f3n del \u00e1rbol de categor\u00edas en las incrustaciones de palabras preentrenadas.", "El art\u00edculo propone la incrustaci\u00f3n de N bolas para datos taxon\u00f3micos, donde una N bola es un par de un vector centroide y el radio desde el centro.", "El art\u00edculo presenta un m\u00e9todo para modificar las incrustaciones vectoriales existentes de objetos categ\u00f3ricos (como las palabras), para convertirlas en incrustaciones de bolas que siguen jerarqu\u00edas.", "Se centra en ajustar las incrustaciones de palabras preentrenadas para que respeten la relaci\u00f3n hipernimia/hiponimia mediante una encapsulaci\u00f3n adecuada de n-ball."]}
{"source": ["For the challenging semantic image segmentation task the best performing models\n", "have traditionally combined the structured modelling capabilities of Conditional\n", "Random Fields (CRFs) with the feature extraction power of CNNs.", "In more recent\n", "works however, CRF post-processing has fallen out of favour.", "We argue that this\n", "is mainly due to the slow training and inference speeds of CRFs, as well as the\n", "difficulty of learning the internal CRF parameters.", "To overcome both issues we\n", "propose to add the assumption of conditional independence to the framework of\n", "fully-connected CRFs.", "This allows us to reformulate the inference in terms of\n", "convolutions, which can be implemented highly efficiently on GPUs.Doing so\n", "speeds up inference and training by two orders of magnitude.", "All parameters of\n", "the convolutional CRFs can easily be optimized using backpropagation.", "Towards\n", "the goal of facilitating further CRF research we have made our implementations\n", "publicly available."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1xEwsR9FX", "target": ["Proponemos los CRFs convolucionales como una alternativa r\u00e1pida, potente y entrenable a los CRFs totalmente conectados.", "Los autores sustituyen el gran paso de filtrado de la red permuto\u00e9drica por un n\u00facleo convolucional que var\u00eda espacialmente y muestran que la inferencia es m\u00e1s eficiente y el entrenamiento m\u00e1s sencillo. ", "Propone realizar el paso de mensajes en un CRF de n\u00facleo gaussiano truncado utilizando un n\u00facleo definido y el paso de mensajes paralelizado en la GPU."]}
{"source": ["Deep Learning NLP domain lacks procedures for the analysis of model robustness.", "In this paper we propose a framework which validates robustness of any Question Answering model through model explainers.", "We propose that output of a robust model should be invariant to alterations that do not change its semantics.", "We test this property by manipulating question in two ways: swapping important question word for", "1) its semantically correct synonym and", "2) for word vector that is close in embedding space.", "We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME).", "With these two steps we compare state-of-the-art Q&A models.", "We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input.", "We can choose architecture that is more immune to attacks and thus more robust and stable in production environment.", "Morevoer, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure.", "Our findings help to understand which models are more stable and how they can be improved.", "In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "SJlgRqjssQ", "target": ["Proponemos un enfoque agn\u00f3stico del modelo para la validaci\u00f3n de la solidez del sistema de preguntas y respuestas y demostramos los resultados en los modelos de preguntas y respuestas m\u00e1s avanzados.", "Aborda el problema de la solidez ante la informaci\u00f3n adversa en la respuesta a preguntas.", "Mejora de la robustez de la comprensi\u00f3n/respuesta autom\u00e1tica de preguntas."]}
{"source": ["In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution.", "In our model, we propose an adjustment component that collects all the generated data points from the generators, learns the boundary between each pair of generators, and provides error to separate the support of each of the generated distributions.", "To overcome the instability in a multiplayer game, a shrinkage adjustment component method is introduced to gradually reduce the boundary between generators during the training procedure.", "To address the linearly growing training time problem in a multiple generators model, we propose a method to train the generators in parallel.", "This means that our work can be scaled up to large parallel computation frameworks.", "We present an efficient loss function for the discriminator, an effective adjustment component, and a suitable generator.", "We also show how to introduce the decay factor to stabilize the training procedure.", "We have performed extensive experiments on synthetic datasets, MNIST, and CIFAR-10.", "These experiments reveal that the error provided by the adjustment component could successfully separate the generated distributions and each of the generators can stably learn a part of the real distribution even if only a few modes are contained in the real distribution."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJHcpW-CW", "target": ["multigenerador para capturar Pdata, resolver la competencia y el problema de un solo golpe", "Propone GANs paralelos para evitar el colapso de modos en GANs a trav\u00e9s de una combinaci\u00f3n de m\u00faltiples generadores d\u00e9biles. "]}
{"source": ["We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images.", "The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images.", "We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image.", "However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects.", "Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyYYPdg0-", "target": ["Segmentaci\u00f3n de im\u00e1genes d\u00e9bilmente supervisada mediante la estructura compositiva de las im\u00e1genes y los modelos generativos.", "Este trabajo crea una representaci\u00f3n en capas para aprender mejor la segmentaci\u00f3n a partir de im\u00e1genes no etiquetadas.", "Este trabajo propone un modelo generativo basado en GAN que descompone las im\u00e1genes en m\u00faltiples capas, donde el objetivo del GAN es distinguir las im\u00e1genes reales de las im\u00e1genes formadas por la combinaci\u00f3n de las capas.", "Este trabajo propone una arquitectura de red neuronal en torno a la idea de la composici\u00f3n de la escena en capas"]}
{"source": ["Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models.", "We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples.", "In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples.", "Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly.", "Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "H1lug3R5FX", "target": ["Presentamos un marco geom\u00e9trico para demostrar las garant\u00edas de robustez y destacamos la importancia de la codimensi\u00f3n en los ejemplos adversos. ", "Este trabajo ofrece un an\u00e1lisis te\u00f3rico de los ejemplos adversariales, mostrando que existe un compromiso entre la robustez en diferentes normas, el entrenamiento adversarial es ineficiente en cuanto a la muestra, y el clasificador del vecino m\u00e1s cercano puede ser robusto bajo ciertas condiciones."]}
{"source": ["Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  ", "Unfortunately, they are also very brittle and easily falter when presented with noisy data.", "In this paper, we confront NMT models with synthetic and natural sources of noise.", "We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.", "We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.", "We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "BJ8vJebC-", "target": ["CharNMT es fr\u00e1gil", "Este trabajo investiga el impacto del ruido a nivel de caracteres en 4 sistemas neuronales de traducci\u00f3n autom\u00e1tica diferentes", "Este art\u00edculo investiga emp\u00edricamente el rendimiento de los sistemas NMT a nivel de caracteres frente al ruido a nivel de caracteres, tanto sintetizado como natural.", "Este art\u00edculo investiga el impacto de la entrada ruidosa en la traducci\u00f3n autom\u00e1tica y prueba formas de hacer que los modelos NMT sean m\u00e1s robustos"]}
{"source": ["As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning.", "However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way.", "We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such.", "The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve.", "Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches.", "Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case.", "Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator."], "source_labels": [0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1Lc-Gb0Z", "target": ["Aprendemos redes profundas de unidades de umbral duro estableciendo los objetivos de las unidades ocultas mediante optimizaci\u00f3n combinatoria y los pesos mediante optimizaci\u00f3n convexa, lo que da como resultado un mejor rendimiento en ImageNet.", "El art\u00edculo explica y generaliza los enfoques para el aprendizaje de redes neuronales con activaci\u00f3n dura.", "Este trabajo examina el problema de la optimizaci\u00f3n de redes profundas de unidades de umbral duro.", "El art\u00edculo analiza el problema de la optimizaci\u00f3n de las redes neuronales con umbral duro y propone una soluci\u00f3n novedosa al mismo con una colecci\u00f3n de heur\u00edsticas/aproximaciones."]}
{"source": ["The robust and efficient recognition of visual relations in images is a hallmark of biological vision.", "Here, we argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations.", "Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs).", "The networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity.", "We further show that another type of feedforward network, called a relational network (RN), which was shown to successfully solve seemingly difficult visual question answering (VQA) problems on the CLEVR datasets, suffers similar limitations.", "Motivated by the comparable success of biological vision, we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning."], "source_labels": [0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HymuJz-A-", "target": ["Utilizando un nuevo reto controlado de relaciones visuales, mostramos que las tareas de igual-diferencia ponen a prueba la capacidad de las CNNs; argumentamos que las relaciones visuales pueden resolverse mejor utilizando estrategias de atenci\u00f3n-mnemot\u00e9cnica.", "Demuestra que las redes neuronales convolucionales y relacionales no resuelven los problemas de relaciones visuales entrenando las redes con datos de relaciones visuales generados artificialmente. ", "Este art\u00edculo explora c\u00f3mo las CNN y las redes relacionales actuales no reconocen las relaciones visuales en las im\u00e1genes."]}
{"source": ["Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations.", "Previous work has shown that the tracker can be trained in a simulator via reinforcement learning and deployed in real-world scenarios.", "However, during training, such a method requires manually specifying the moving path of the target object to be tracked, which cannot ensure the tracker\u2019s generalization on the unseen object moving patterns.", "To learn a robust tracker for VAT, in this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT.", "In AD-VAT, both the tracker and the target are approximated by end-to-end neural networks, and are trained via RL in a dueling/competitive manner: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker.", "They are asymmetric in that the target is aware of the tracker, but not vice versa.", "Specifically, besides its own observation, the target is fed with the tracker\u2019s observation and action, and learns to predict the tracker\u2019s reward as an auxiliary task.", "We show that such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker.", "To stabilize the training, we also propose a novel partial zero-sum reward for the tracker/target.", "The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios.", "For supplementary videos, see: https://www.youtube.com/playlist?list=PL9rZj4Mea7wOZkdajK1TsprRg8iUf51BS", "\n The code is available at https://github.com/zfw1226/active_tracking_rl"], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkgYmhR9KX", "target": ["Proponemos AD-VAT, donde el rastreador y el objeto objetivo, vistos como dos agentes aprendibles, son oponentes y pueden mejorarse mutuamente durante el entrenamiento.", "Este trabajo pretende abordar el problema del seguimiento visual activo con un mecanismo de entrenamiento en el que el rastreador y el objetivo sirven como oponentes mutuos", "Este art\u00edculo presenta una sencilla tarea de RL profunda multiagente en la que un rastreador en movimiento intenta seguir a un objetivo en movimiento.", "Propone una novedosa funci\u00f3n de recompensa - \"suma cero parcial\", que s\u00f3lo fomenta la competencia entre rastreadores y objetivos cuando est\u00e1n cerca y penaliza cuando est\u00e1n demasiado lejos."]}
{"source": ["Identifying the hypernym relations that hold between words is a fundamental task in NLP.", "Word embedding methods have recently shown some capability to encode hypernymy.", "However, such methods tend not to explicitly encode the hypernym hierarchy that exists between words.", "In this paper, we propose a method to learn a hierarchical word embedding in a speci\ufb01c order to capture the hypernymy.", "To learn the word embeddings, the proposed method considers not only the hypernym relations that exists between words on a taxonomy, but also their contextual information in a large text corpus.", "The experimental results on a supervised hypernymy detection and a newly-proposed hierarchical path completion tasks show the ability of the proposed method to encode the hierarchy.", "Moreover, the proposed method outperforms previously proposed methods for learning word and hypernym-speci\ufb01c word embeddings on multiple benchmarks."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "S1xf-W5paX", "target": ["Presentamos un m\u00e9todo para aprender conjuntamente un Embedding Jer\u00e1rquico de Palabras (HWE) utilizando un corpus y una taxonom\u00eda para identificar las relaciones de hipernimia entre las palabras.", "El art\u00edculo presenta un m\u00e9todo para aprender conjuntamente incrustaciones de palabras utilizando estad\u00edsticas de co-ocurrencia, as\u00ed como incorporando informaci\u00f3n jer\u00e1rquica de las redes sem\u00e1nticas.", "En este trabajo se propone un m\u00e9todo de aprendizaje conjunto de hiper\u00f3nimos a partir de datos de texto bruto y de taxonom\u00eda supervisada. ", "Este trabajo propone a\u00f1adir una medida de diferencia de \"inclusi\u00f3n distributiva\" al objetivo GloVE con el fin de representar las relaciones de hipernimia."]}
{"source": ["While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures.", "Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance.", "Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule.", "Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems."], "source_labels": [0, 0, 1, 0], "rouge_scores": [], "paper_id": "BJ8lbVAfz", "target": ["integraci\u00f3n de la autoorganizaci\u00f3n y el aprendizaje supervisado en una red neuronal jer\u00e1rquica", "El art\u00edculo analiza el aprendizaje en una red neuronal de tres capas, en la que la capa intermedia est\u00e1 organizada topogr\u00e1ficamente, e investiga la interacci\u00f3n entre el aprendizaje no supervisado y el supervisado jer\u00e1rquico en el contexto biol\u00f3gico.", "Una variante supervisada del mapa autoorganizado (SOM) de Kohonen, pero en la que la capa de salida lineal se sustituye con el error cuadrado por una capa softmax con entrop\u00eda cruzada.", "Propone un modelo que utiliza neuronas ocultas con funci\u00f3n de activaci\u00f3n autoorganizada, cuyas salidas alimentan un clasificador con funci\u00f3n de salida softmax. "]}
{"source": ["Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision.", "To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation.", "First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks.", "We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator.", "In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50.", "We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJx94o0qYX", "target": ["autopista de precisi\u00f3n; un concepto generalizado de flujo de informaci\u00f3n de alta precisi\u00f3n para la cuantificaci\u00f3n de sub 4 bits ", "Investiga el problema de la cuantificaci\u00f3n de las redes neuronales empleando una autopista de precisi\u00f3n de extremo a extremo para reducir el error de cuantificaci\u00f3n acumulado y permitir una precisi\u00f3n ultrabaja en las redes neuronales profundas. ", "Este trabajo estudia m\u00e9todos para mejorar el rendimiento de las redes neuronales cuantificadas", "Este trabajo propone mantener un alto flujo de activaci\u00f3n/gradiente en dos tipos de estructuras de redes, ResNet y LSTM."]}
{"source": ["The vast majority of natural sensory data is temporally redundant.", "For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values.  ", "Typically, deep learning algorithms take no advantage of this redundancy to reduce computations.  ", "This can be an obscene waste of energy.  ", "We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data.  ", "We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation.  ", "Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain.  ", "We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers.  "], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HkZy-bW0-", "target": ["Un algoritmo para entrenar redes neuronales de forma eficiente en datos temporalmente redundantes.", "El art\u00edculo describe un esquema de codificaci\u00f3n neuronal para el aprendizaje basado en picos en redes neuronales profundas", "Este art\u00edculo presenta un m\u00e9todo de aprendizaje basado en picos que tiene como objetivo reducir el c\u00e1lculo necesario durante el aprendizaje y la prueba al clasificar datos temporales redundantes.", "Este trabajo aplica una versi\u00f3n de codificaci\u00f3n predictiva del esquema de codificaci\u00f3n Sigma-Delta para reducir la carga computacional de una red de aprendizaje profundo, combinando los tres componentes de una manera no vista anteriormente."]}
{"source": ["Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. To do so, IB identifies an intermediate \"bottleneck\" variable T that has low mutual information I(X;T) and high mutual information I(Y;T).", "The \"IB curve\" characterizes the set of bottleneck variables that achieve maximal I(Y;T) for a given I(X;T), and is typically explored by maximizing the \"IB Lagrangian\", I(Y;T) - \u03b2I(X;T).", "In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of \u03b2; (2) there are \"uninteresting\" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal.", "We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way.", "To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases.", "We demonstrate the three caveats on the MNIST dataset."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rke4HiAcY7", "target": ["El cuello de botella de la informaci\u00f3n se comporta de forma sorprendente cuando la salida es una funci\u00f3n determinista de la entrada.", "Sostiene que la mayor\u00eda de los problemas reales de clasificaci\u00f3n muestran una relaci\u00f3n determinista de este tipo entre las etiquetas de clase y las entradas X y explora varias cuestiones que se derivan de tales patolog\u00edas.", "Explora los problemas que surgen cuando se aplican los conceptos de bottlenext de informaci\u00f3n a los modelos de aprendizaje supervisado determinista", "Los autores aclaran varios comportamientos contraintuitivos del m\u00e9todo del cuello de botella de informaci\u00f3n para el aprendizaje supervisado de una regla determinista."]}
{"source": ["We prove, under two sufficient conditions, that idealised models can have no adversarial examples.", "We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these.", "We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice.", "We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting.", "This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well.", "Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant."], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1eZRiC9YX", "target": ["Demostramos que las redes neuronales bayesianas idealizadas pueden no tener ejemplos adversos, y damos pruebas emp\u00edricas con BNN del mundo real.", "El art\u00edculo estudia la robustez adversarial de los clasificadores bayesianos y establece dos condiciones que demuestran que son suficientes para que los \"modelos idealizados\" en \"conjuntos de datos idealizados\" no tengan ejemplos adversariales", "El art\u00edculo plantea una clase de clasificadores bayesianos discriminativos que no tienen ejemplos adversarios."]}
{"source": ["Deep neural networks are susceptible to adversarial attacks.", "In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer.", "Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker.", "We introduce attacks that instead reprogram the target model to perform a task chosen by the attacker without the attacker needing to specify or compute the desired output for each test-time input.", "This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary\u2014even if the model was not trained to do this task.", "These perturbations can thus be considered a program for the new task.", "We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "Syx_Ss05tm", "target": ["Introducimos el primer caso de ataques adversarios que reprograman el modelo objetivo para realizar una tarea elegida por el atacante, sin que \u00e9ste tenga que especificar o calcular la salida deseada para cada entrada en tiempo de prueba.", "Los autores presentan un novedoso esquema de ataque adversario en el que una red neuronal es reutilizada para realizar una tarea diferente a la que fue entrenada originalmente", "En este trabajo se propone la \"reprogramaci\u00f3n adversarial\" de redes neuronales bien entrenadas y fijas y se demuestra que la reprogramaci\u00f3n adversarial es menos eficaz en redes no entrenadas.", "El art\u00edculo ampl\u00eda la idea de los \"ataques adversarios\" en el aprendizaje supervisado de las NNs a un replanteamiento completo de la soluci\u00f3n de una red entrenada."]}
{"source": ["As shown in recent research, deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data.", "This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization.", "This leads to the crucial question of how generalization gap should be predicted from the training data and network parameters.", "In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap.", "Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary.", "We find that it is necessary to use margin distributions at multiple layers of a deep network.", "On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap.", "In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum).\n", "Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "HJlQfnCqKX", "target": ["Desarrollamos un nuevo esquema para predecir la brecha de generalizaci\u00f3n en redes profundas con alta precisi\u00f3n.", "Los autores sugieren utilizar un margen geom\u00e9trico y una distribuci\u00f3n de m\u00e1rgenes por capas para predecir la brecha de generalizaci\u00f3n.", "Emp\u00edricamente se muestra una interesante conexi\u00f3n entre las estad\u00edsticas de margen propuestas y la brecha de generalizaci\u00f3n, que puede utilizarse para proporcionar algunas ideas prescriptivas hacia la comprensi\u00f3n de la generalizaci\u00f3n en las redes neuronales profundas. "]}
{"source": ["We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned.", "Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks.", "Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems.", "We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.", "While our focus is theoretical, we also present experiments that illustrate our theoretical findings."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkMnHjC5YQ", "target": ["Proponemos un algoritmo para recuperar de forma demostrable los par\u00e1metros (pesos convolucionales y de salida) de una red convolucional con parches superpuestos.", "Este trabajo estudia el aprendizaje te\u00f3rico de las redes neuronales convolucionales de una capa, lo que da como resultado un algoritmo de aprendizaje y garant\u00edas demostrables utilizando el algoritmo.", "Este trabajo ofrece un nuevo algoritmo para el aprendizaje de una red neuronal de dos capas que implica un \u00fanico filtro convolucional y un vector de pesos para diferentes ubicaciones."]}
{"source": ["Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data.", "Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem.", "A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping.", "Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training.", "We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable.", "These arguments are supported by experimental results on several data sets."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1hYRMbCW", "target": ["Un nuevo t\u00e9rmino de regularizaci\u00f3n puede mejorar su entrenamiento de wasserstein gans", "El trabajo propone un esquema de regularizaci\u00f3n para Wasserstein GAN basado en la relajaci\u00f3n de las restricciones de la constante Lipschitz de 1.", "El art\u00edculo trata de la regularizaci\u00f3n/penalizaci\u00f3n en el ajuste de GANs, cuando se basa en una m\u00e9trica L_1 Wasserstein."]}
{"source": ["We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. \n", "The approach is based on the gradient operator induced by optimal transport, which connects the geometry of sample space and parameter space in implicit deep generative models.", "From this theory, we obtain an easy-to-implement regularizer for the parameter updates.", "Our experiments demonstrate that this method improves the speed and stability in training GANs in terms of wall-clock time and Fr\\'echet Inception Distance (FID) learning curves."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "Bye5OiR5F7", "target": ["Proponemos el m\u00e9todo proximal de Wasserstein para el entrenamiento de GANs. ", "Propone un nuevo procedimiento GAN que tiene en cuenta los puntos generados en la iteraci\u00f3n anterior y actualiza el generador para que se realice l veces.", "Considera el aprendizaje de gradiente natural en el aprendizaje GAN, donde se emplea la estructura riemanniana inducida por la distancia Wasserstein-2.", "El art\u00edculo pretende utilizar el gradiente natural inducido por la distancia Wasserstein-2 para entrenar el generador en GAN y los autores proponen el operador proximal de Wasserstein como regularizaci\u00f3n."]}
{"source": ["Influence diagrams provide a modeling and inference framework for sequential decision problems, representing the probabilistic knowledge by a Bayesian network and the preferences of an agent by utility functions over the random variables and decision variables.\n", "MDPs and POMDPS, widely used for planning under uncertainty can also be represented by influence diagrams.\n", "The time and space complexity of computing the maximum expected utility (MEU) and its maximizing policy is exponential in the induced width of the underlying graphical model, which is often prohibitively large due to the growth of the information set under the sequence of decisions.\n", "In this paper, we develop a weighted mini-bucket approach for bounding the MEU.", " These bounds can be used as a stand-alone approximation that can be improved as a function of a controlling i-bound parameter", ".\nThey can also be used as heuristic  functions to guide search, especially for planning \n", "such as MDPs and POMDPs.\n", "We evaluate the scheme empirically against state-of-the-art, thus illustrating its potential.\n"], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1ls4-DpvN", "target": ["Este trabajo introduce una funci\u00f3n heur\u00edstica basada en la eliminaci\u00f3n para la toma de decisiones secuenciales, adecuada para guiar los algoritmos de b\u00fasqueda AND/OR para resolver diagramas de influencia.", "generaliza la heur\u00edstica de inferencia de los minibuckets a los diagramas de influencia."]}
{"source": ["Probabilistic Neural Networks deal with various sources of stochasticity: input noise, dropout, stochastic neurons, parameter uncertainties modeled as random variables, etc.\n", "In this paper we revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity.", "In contrast, standard NNs propagate only point estimates, discarding the uncertainty.\n", "Methods propagating also the variance have been proposed by several authors in different context.", "The view presented here attempts to clarify the assumptions and derivation behind such methods, relate them to classical NNs and broaden their scope of applicability.\n", "The main technical contributions are new approximations for the distributions of argmax and max-related transforms, which allow for fully analytic uncertainty propagation in networks with softmax and max-pooling layers as well as leaky ReLU activations.\n", "We evaluate the accuracy of the approximation and suggest a simple calibration.", "Applying the method to networks with dropout allows for faster training and gives improved test likelihoods without the need of sampling."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SkMuPjRcKQ", "target": ["Aproximaci\u00f3n de la media y la varianza de la salida de la NN en caso de entrada ruidosa / abandono / par\u00e1metros inciertos. Aproximaciones anal\u00edticas para las capas argmax, softmax y max.", "Los autores se centran en el problema de la propagaci\u00f3n de la incertidumbre DNN", "Este trabajo revisa la propagaci\u00f3n feed-forward de la media y la varianza en las neuronas, abordando el problema de la propagaci\u00f3n de la incertidumbre a trav\u00e9s de las capas max-pooling y softmax."]}
{"source": ["Generative Adversarial Networks are one of the leading tools in generative modeling, image editing and content creation. \n", "However, they are hard to train as they require a delicate balancing act between two deep networks fighting a never ending duel.", "Some of the most promising adversarial models today minimize a Wasserstein objective.", "It is smoother and more stable to optimize.", "In this paper, we show that the Wasserstein distance is just one out of a large family of objective functions that yield these properties.", "By making the discriminator of a GAN robust to adversarial attacks we can turn any GAN objective into a smooth and stable loss.", "We experimentally show that any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively.", "The training additionally becomes more robust to suboptimal choices of hyperparameters, model architectures, or objective functions."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "HJE6X305Fm", "target": ["Un discriminador que no se deja enga\u00f1ar f\u00e1cilmente por el ejemplo adversario hace que el entrenamiento del GAN sea m\u00e1s robusto y conduce a un objetivo m\u00e1s suave.", "Este trabajo propone una nueva forma de estabilizar el proceso de entrenamiento de GAN mediante la regularizaci\u00f3n del Discriminador para que sea robusto a los ejemplos adversos.", "El art\u00edculo propone una forma sistem\u00e1tica de entrenar GANs con t\u00e9rminos de regularizaci\u00f3n de robustez, lo que permite un entrenamiento m\u00e1s suave de los GANs. ", "Presenta la idea de que haciendo un discriminador robusto a las perturbaciones adversarias el objetivo del GAN puede hacerse suave lo que resulta en mejores resultados tanto visualmente como en t\u00e9rminos de FID."]}
{"source": ["We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\n", "First, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\n", "Second, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\n", "Third, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\n", "The resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\n", "Like a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "By-IifZRW", "target": ["Modelamos la funci\u00f3n de activaci\u00f3n de cada neurona como un Proceso Gaussiano y la aprendemos junto con el peso con Inferencia Variacional.", "Proponer la colocaci\u00f3n de priores de procesos gaussianos en la forma funcional de cada funci\u00f3n de activaci\u00f3n en la red neuronal para aprender la forma de las funciones de activaci\u00f3n."]}
{"source": ["Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice.", "In this paper, we bridge the gap between these good empirical results \n", "and the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks.", "More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\n", "bounded width and small depth can approximate a deep ReLU network in which the dense matrices are\n", "of low rank.", "Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks.", "We support our experimental results with thorough experiments on a large, real world video classification problem."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkeUG30cFQ", "target": ["Proporcionamos un estudio te\u00f3rico de las propiedades de las redes ReLU circulantes-diagonales profundas y demostramos que son aproximadores universales de anchura limitada.", "El art\u00edculo propone utilizar matrices circulantes y diagonales para acelerar el c\u00e1lculo y reducir los requisitos de memoria en las redes neuronales.", "Este trabajo demuestra que las redes ReLU diagonales-circulantes de anchura limitada (DC-ReLU) son aproximadores universales."]}
{"source": ["Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility.", "However, manual remote piloting of a drone is prone to errors.", "In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections.", "Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest.", "The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy.", "A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "BviYjfnIk", "target": ["StarHopper es una novedosa interfaz de pantalla t\u00e1ctil para la navegaci\u00f3n eficiente y flexible de drones con c\u00e1mara centrada en objetos", "Los autores esbozan una nueva interfaz de control de drones StarHopper que han desarrollado, combinando el pilotaje automatizado y el manual en una nueva interfaz de navegaci\u00c3\u00b3n h\u00c3brida y se deshace de la suposici\u00c3\u00b3n de que el objeto objetivo ya est\u00c3\u00a1 en el FOV del dron mediante el uso de una c\u00c3\u00a1mara superior adicional.", "Este art\u00edculo presenta StarHopper, un sistema de navegaci\u00f3n semiautom\u00e1tica de drones en el contexto de la inspecci\u00f3n remota.", "Presenta StarHopper, una aplicaci\u00f3n que utiliza t\u00e9cnicas de visi\u00f3n por ordenador con entrada t\u00e1ctil para apoyar el pilotaje de drones con un enfoque centrado en el objeto."]}
{"source": ["Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations.", "RNN can capture long-range dependency but is hard to parallelize and not time-efficient.", "CNN focuses on local dependency but does not perform well on some tasks.", "SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length.", "In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding.", "It requires as little memory as RNN but with all the merits of SAN.", "Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency.", "Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required.", "Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information.", "On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1cWzoxA-", "target": ["Una red de autoatenci\u00f3n para la codificaci\u00f3n de secuencias sin RNN/CNN con un peque\u00f1o consumo de memoria, un c\u00e1lculo altamente paralelizable y un rendimiento de vanguardia en varias tareas de PNL", "Propone aplicar la autoatenci\u00f3n en dos niveles para limitar el requisito de memoria en los modelos basados en la atenci\u00f3n con un impacto insignificante en la velocidad.", "Este art\u00edculo presenta el modelo de autoatenci\u00f3n de bloque bidireccional como codificador de prop\u00f3sito general para varias tareas de modelado de secuencias en PNL"]}
{"source": ["End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document.", "In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents.", "The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query.", "We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input.", "On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "Syl7OsRqY7", "target": ["Un nuevo modelo de vanguardia para la respuesta a preguntas con m\u00faltiples evidencias utilizando atenci\u00f3n jer\u00e1rquica de grano grueso y fino.", "Propone un m\u00e9todo de control de calidad multisalto basado en dos m\u00f3dulos separados (m\u00f3dulos de grano grueso y de grano fino).", "Este art\u00edculo propone una interesante arquitectura de red de atenci\u00f3n de grano grueso y fino para abordar la respuesta a preguntas con m\u00faltiples evidencias", "Se centra en la garant\u00eda de calidad de varias opciones y propone un marco de puntuaci\u00f3n de grueso a fino."]}
{"source": ["Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures.", "In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework.", "We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.", "We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018).", "We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HyexAiA5Fm", "target": ["Proponemos una nueva metodolog\u00eda para el transporte \u00f3ptimo desequilibrado utilizando redes generativas adversarias.", "Los autores consideran el problema de transporte \u00f3ptimo no equilibrado entre dos medidas con diferente masa total utilizando un algoritmo estoc\u00e1stico min-max y un escalado local", "Los autores proponen un enfoque para estimar el transporte \u00f3ptimo desequilibrado entre las medidas muestreadas que escala bien en la dimensi\u00f3n y en el n\u00famero de muestras.", "El art\u00edculo introduce una formulaci\u00f3n est\u00e1tica para el transporte \u00f3ptimo desequilibrado mediante el aprendizaje simult\u00e1neo de un mapa de transporte T y un factor de escala xi."]}
{"source": ["Extracting saliency maps, which indicate parts of the image important to classification, requires many tricks to achieve satisfactory performance when using classifier-dependent methods.", "Instead, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance.", "We observe that the proposed approach extracts higher quality saliency maps and outperforms existing weakly-supervised localization techniques, setting the new state of the art result on the ImageNet dataset."], "source_labels": [0, 0, 1], "rouge_scores": [], "paper_id": "BJxbYoC9FQ", "target": ["Proponemos un nuevo m\u00e9todo de extracci\u00f3n de mapas de saliencia que permite extraer mapas de mayor calidad.", "Propone un m\u00e9todo agn\u00f3stico de clasificaci\u00f3n para la extracci\u00f3n de mapas de saliencia.", "Este art\u00edculo presenta un nuevo extractor de mapas de saliencia que parece mejorar los resultados del estado del arte.", "Los autores argumentan que cuando un mapa de saliencia extra\u00eddo depende directamente de un modelo, entonces podr\u00eda no ser \u00fatil para un clasificador diferente, y sugiere un esquema para aproximar la soluci\u00f3n."]}
{"source": ["Unsupervised image-to-image translation has gained considerable attention due to the recent impressive progress based on generative adversarial networks (GANs).", "However, previous methods often fail in challenging cases, in particular, when an image has multiple target instances and a translation task involves significant changes in shape, e.g., translating pants to skirts in fashion images.", "To tackle the issues, we propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration.", "The proposed method translates both an image and the corresponding set of instance attributes while maintaining the permutation invariance property of the instances.", "To this end, we introduce a context preserving loss that encourages the network to learn the identity function outside of target instances.", "We also propose a sequential mini-batch inference/training technique that handles multiple instances with a limited GPU memory and enhances the network to generalize better for multiple instances.", "Our comparative evaluation demonstrates the effectiveness of the proposed method on different image datasets, in particular, in the aforementioned challenging cases.", "Code and results are available in https://github.com/sangwoomo/instagan"], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "ryxwJhC9YX", "target": ["Proponemos un m\u00e9todo novedoso para incorporar el conjunto de atributos de instancia para la traducci\u00f3n de imagen a imagen.", "Este art\u00edculo propone un m\u00e9todo -InstaGAN- que se basa en CycleGAN teniendo en cuenta la informaci\u00f3n de las instancias en forma de m\u00e1scaras de segmentaci\u00f3n por instancia, con resultados que se comparan favorablemente con CycleGAN y otras l\u00edneas de base.", " Propone a\u00f1adir m\u00e1scaras de segmentaci\u00f3n conscientes de la instancia para el problema de la traducci\u00f3n de imagen a imagen no emparejada."]}
{"source": ["Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime  where classical learning theory would instead predict that they would severely overfit.  ", "While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit.  ", "In this paper, we provide a new explanation.", "By applying a very general probability-complexity bound recently derived from  algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions.", "We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks trained on CIFAR10 and MNIST.\n", "As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems.\n", "This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than  the more conventional prior over parameter space.  ", "If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets.  ", "By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood,  we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rye4g3AqFm", "target": ["El mapa par\u00e1metro-funci\u00f3n de las redes profundas est\u00e1 enormemente sesgado; esto puede explicar por qu\u00e9 se generalizan. Utilizamos PAC-Bayes y procesos gaussianos para obtener l\u00edmites no vac\u00edos.", "El art\u00edculo estudia la capacidad de generalizaci\u00f3n de las redes neuronales profundas, con la ayuda de la teor\u00eda de aprendizaje PAC-Bayesiana y de intuiciones respaldadas emp\u00edricamente.", "Este art\u00edculo propone una explicaci\u00f3n de los comportamientos de generalizaci\u00f3n de las grandes redes neuronales sobreparametrizadas afirmando que el mapa par\u00e1metro-funci\u00f3n en las redes neuronales est\u00e1 sesgado hacia funciones \"simples\" y el comportamiento de generalizaci\u00f3n ser\u00e1 bueno si el concepto objetivo tambi\u00e9n es \"simple\"."]}
{"source": ["We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\n", "While the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\n", "Learning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\n", "We choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters.", "In the variational E-step,\n", "the latent states are then", " \noptimized according to a tractable free-energy objective", ". Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\n", "individuals as the (log) joint probabilities given by the used generative model.\n\n", "As a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches).", "Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood.", "In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SyjjD1WRb", "target": ["Presentamos el EM Evolutivo como un novedoso algoritmo para el entrenamiento no supervisado de modelos generativos con variables latentes binarias que conecta \u00edntimamente el EM variacional con la optimizaci\u00f3n evolutiva", "El trabajo presenta una combinaci\u00f3n de computaci\u00f3n evolutiva y EM variacional para modelos con variables latentes binarias representadas mediante una aproximaci\u00f3n basada en part\u00edculas", "El art\u00edculo intenta integrar estrechamente los algoritmos de entrenamiento de maximizaci\u00f3n de expectativas con los algoritmos evolutivos."]}
{"source": ["While deep neural networks have achieved groundbreaking prediction results in many tasks, there is a class of data where existing architectures are not optimal -- sequences of probability distributions.", "Performing forward prediction on sequences of distributions has many important applications.", "However, there are two main challenges in designing a network model for this task.", "First, neural networks are unable to encode distributions compactly as each node encodes just a real value.", "A recent work of Distribution Regression Network (DRN) solved this problem with a novel network that encodes an entire distribution in a single node, resulting in improved accuracies while using much fewer parameters than neural networks.", "However, despite its compact distribution representation, DRN does not address the second challenge, which is the need to model time dependencies in a sequence of distributions.", "In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN.", "The combination of compact distribution representation and shared weights architecture across time steps makes RDRN suitable for modeling the time dependencies in a distribution sequence.", "Compared to neural networks and DRN, RDRN achieves the best prediction performance while keeping the network compact."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SJlp8sA5Y7", "target": ["Proponemos un modelo de red recurrente eficiente para la predicci\u00f3n a futuro sobre distribuciones variables en el tiempo.", "Este art\u00edculo propone un m\u00e9todo para crear redes neuronales que mapean distribuciones hist\u00f3ricas en distribuciones y aplica el m\u00e9todo a varias tareas de predicci\u00f3n de distribuci\u00f3n.", "Propone una Red de Regresi\u00f3n de Distribuci\u00f3n Recurrente que utiliza una arquitectura recurrente sobre un modelo anterior de Red de Regresi\u00f3n de Distribuci\u00f3n.", "Este art\u00edculo trata de la regresi\u00f3n sobre las distribuciones de probabilidad estudiando las distribuciones que var\u00edan en el tiempo en un entorno de red neuronal recurrente"]}
{"source": ["We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.", "By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront.", "In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.", "Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."], "source_labels": [0, 0, 0, 1], "rouge_scores": [], "paper_id": "rJXMpikCZ", "target": ["Un enfoque novedoso para el procesamiento de datos estructurados en forma de grafos mediante redes neuronales, aprovechando la atenci\u00f3n sobre la vecindad de un nodo. Obtiene resultados de vanguardia en tareas de redes de citaci\u00f3n transductivas y en una tarea de interacci\u00f3n prote\u00edna-prote\u00edna inductiva.", "Este trabajo propone un nuevo m\u00e9todo para clasificar los nodos de un grafo, que puede ser utilizado en escenarios semi-supervisados y en un grafo completamente nuevo. ", "El art\u00edculo presenta una arquitectura de redes neuronales para operar con datos estructurados en forma de grafos, denominada Graph Attention Networks.", "Proporciona una discusi\u00f3n justa y casi completa de los enfoques del estado del arte para aprender representaciones vectoriales para los nodos de un gr\u00e1fico."]}
{"source": ["While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints.", "Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics.", "Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task.", "In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning.", "Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network.", "In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model.", "In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer.", "The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network.", "Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network.", "Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network.", "We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1hcZZ-AW", "target": ["Un novedoso enfoque basado en el aprendizaje por refuerzo para comprimir redes neuronales profundas con destilaci\u00f3n de conocimientos", "Este trabajo propone utilizar el aprendizaje por refuerzo en lugar de una heur\u00edstica predefinida para determinar la estructura del modelo comprimido en el proceso de destilaci\u00f3n del conocimiento", "Introduce una forma de principio de compresi\u00f3n de red a red, que utiliza gradientes de pol\u00edtica para optimizar dos pol\u00edticas que comprimen un maestro fuerte en un modelo de estudiante fuerte pero m\u00e1s peque\u00f1o."]}
{"source": ["Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss.", "However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples.", "In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss.", "We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset.", "Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "HJxyAjRcFX", "target": ["Demostramos que el colapso del modo en los GANs condicionales se atribuye en gran medida a un desajuste entre la p\u00e9rdida de reconstrucci\u00f3n y la p\u00e9rdida del GAN e introducimos un conjunto de nuevas funciones de p\u00e9rdida como alternativas para la p\u00e9rdida de reconstrucci\u00f3n.", "El art\u00edculo propone una modificaci\u00f3n del objetivo tradicional del GAN condicional para promover la generaci\u00f3n diversa y multimodal de im\u00e1genes. ", "Este trabajo propone una alternativa a los errores L1/L2 que se utilizan para aumentar las p\u00e9rdidas adversariales al entrenar GANs condicionales."]}
{"source": ["Generative models are important tools to capture and investigate the properties of complex empirical data.", "Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data.", "Does learning the parameters of both architectures obey the same rules?", "We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other.", "Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference.", "In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n"], "source_labels": [1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SySisz-CW", "target": ["Utilizamos la inferencia causal para caracterizar la arquitectura de los modelos generativos", "Este art\u00edculo examina la naturaleza de los filtros convolucionales en el codificador y el decodificador de un VAE, y en el generador y el discriminador de un GAN.", "Este trabajo explota el principio de causalidad para cuantificar c\u00f3mo se adaptan los pesos de las capas sucesivas entre s\u00ed."]}
{"source": ["Many deep reinforcement learning approaches use graphical state representations,\n", "this means visually distinct games that share the same underlying structure cannot\n", "effectively share knowledge.", "This paper outlines a new approach for learning\n", "underlying game state embeddings irrespective of the visual rendering of the game\n", "state.", "We utilise approaches from multi-task learning and domain adaption in\n", "order to place visually distinct game states on a shared embedding manifold.", "We\n", "present our results in the context of deep reinforcement learning agents."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJB7fkWR-", "target": ["Un enfoque para aprender un espacio de incrustaci\u00f3n compartido entre juegos visualmente distintos.", "Un nuevo enfoque para el aprendizaje de la estructura subyacente de los juegos visualmente distintos que combina capas convolucionales para el procesamiento de las im\u00e1genes de entrada, Asynchronous Advantage Actor Critic para el aprendizaje de refuerzo profundo y el enfoque adversarial para forzar la representaci\u00f3n de incrustaci\u00f3n para ser independiente de la representaci\u00f3n visual de los juegos", "Presenta un m\u00e9todo para aprender una pol\u00edtica sobre juegos visualmente distintos adaptando el aprendizaje por refuerzo profundo.", "Este art\u00edculo analiza una arquitectura de agente que utiliza una representaci\u00f3n compartida para entrenar m\u00faltiples tareas con diferentes estad\u00edsticas visuales a nivel de sprite"]}
{"source": ["We study discrete time dynamical systems governed by the state equation $h_{t+1}=\u03d5(Ah_t+Bu_t)$.", "Here A,B are weight matrices, \u03d5 is an activation function, and $u_t$ is the input data.", "This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks.", "We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$.", "We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size.", "Our results apply to increasing activations whose derivatives are bounded away from zero.", "The analysis is based on", "i) an SGD convergence result with nonlinear activations and", "ii) careful statistical characterization of the state vector.", "Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkeMHjR9Ym", "target": ["Estudiamos la ecuaci\u00f3n de estado de una red neuronal recurrente. Demostramos que el SGD puede aprender eficientemente la din\u00e1mica desconocida a partir de pocas observaciones de entrada/salida bajo supuestos adecuados.", "El art\u00edculo estudia sistemas din\u00e1micos de tiempo discreto con una ecuaci\u00f3n de estado no lineal, demostrando que la ejecuci\u00f3n del SGD en una trayectoria de longitud fija da una convergencia logar\u00edtmica.", "Este trabajo considera el problema del aprendizaje de un sistema din\u00e1mico no lineal en el que la salida es igual al estado. ", "Este trabajo estudia la capacidad del SGD para aprender la din\u00e1mica de un sistema lineal y la activaci\u00f3n no lineal."]}
{"source": ["Although deep neural networks show their extraordinary power in various tasks, they are not feasible for deploying such large models on embedded systems due to high computational cost and storage space limitation.", "The recent work knowledge distillation (KD) aims at transferring model knowledge from a well-trained teacher model to a small and fast student model which can significantly help extending the usage of large deep neural networks on portable platform.", "In this paper, we show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks through approximating neuron manifold of powerful teacher network.", "To make this, we propose several novel methods for learning neuron manifold from DNN model.", "Empowered with neuron manifold knowledge, our experiments show the great improvement across a variety of DNN architectures and training data.", "Compared with other KD methods, our Neuron Manifold Transfer (NMT) has best transfer ability of the learned features."], "source_labels": [0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SJlYcoCcKX", "target": ["Un nuevo m\u00e9todo de destilaci\u00f3n de conocimientos para el aprendizaje por transferencia", "El trabajo introduce un m\u00e9todo de destilaci\u00f3n del conocimiento utilizando el concepto de colector de neuronas propuesto. ", "Propone un m\u00e9todo de destilaci\u00f3n del conocimiento en el que se toma el colector neuronal como conocimiento transferido."]}
{"source": ["We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime.", "Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization.", "At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models.", "Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively.", "We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters.", "Lastly we visualize and discuss the learned features of slimmable networks.", "Code and models are available at: https://github.com/JiahuiYu/slimmable_networks"], "source_labels": [1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1gMCsAqY7", "target": ["Presentamos un m\u00e9todo sencillo y general para entrenar una \u00fanica red neuronal ejecutable con diferentes anchuras (n\u00famero de canales en una capa), lo que permite realizar compensaciones instant\u00e1neas y adaptables de precisi\u00f3n y eficiencia en tiempo de ejecuci\u00f3n.", "El art\u00edculo propone la idea de combinar modelos de diferentes tama\u00f1os en una red compartida, lo que mejora en gran medida el rendimiento de la detecci\u00f3n", "Este trabajo entrena un \u00fanico ejecutable de red con diferentes anchuras."]}
{"source": ["Measuring visual (dis)similarity between two or more instances within a data distribution is a fundamental task in many applications, specially in image retrieval.", "Theoretically, non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured by the similarity model.", "In this work, we analyze a simple approach for deep learning networks to be used as an approximation of non-metric similarity functions and we study how these models generalize across different image retrieval datasets."], "source_labels": [0, 0, 1], "rouge_scores": [], "paper_id": "Skvd-myR-", "target": ["Red de similitud para aprender una estimaci\u00f3n de similitud visual no m\u00e9trica entre un par de im\u00e1genes", "Los autores proponen el aprendizaje de la medida de similitud visual y obtienen con ello una mejora en conjuntos de datos muy conocidos de Oxford y Par\u00eds para la recuperaci\u00f3n de im\u00e1genes.", "El documento argumenta que es m\u00e1s adecuado utilizar distancias no m\u00e9tricas en lugar de distancias m\u00e9tricas."]}
{"source": ["Training a model to perform a task typically requires a large amount of data from the domains in which the task will be applied.\n", "However, it is often the case that data are abundant in some domains but scarce in others.", "Domain adaptation deals with the challenge of adapting a model trained from a data-rich source domain to perform well in a data-poor target domain.", "In general, this requires learning plausible mappings between domains.", "CycleGAN is a powerful framework that efficiently learns to map inputs from one domain to another using adversarial training and a cycle-consistency constraint.", "However, the conventional approach of enforcing cycle-consistency via reconstruction may be overly restrictive in cases where one or more domains have limited training data.", "In this paper, we propose an augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction.", "We explore digit classification in a low-resource setting in supervised, semi and unsupervised situation, as well as high resource unsupervised.", "In low-resource supervised setting, the results show that our approach improves absolute performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively, which outperforms unsupervised domain adaptation methods that require high-resource unlabeled target domain.  ", "Moreover, using only few unsupervised target data, our approach can still outperforms many high-resource unsupervised models.", "Our model also outperforms on USPS to MNIST and synthetic digit to SVHN for high resource unsupervised adaptation.", "In speech domains, we similarly adopt a speech recognition model from each domain as the task specific model.", "Our approach improves absolute performance of speech recognition by 2% for female speakers in the TIMIT dataset, where the majority of training samples are from male voices."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "B1G9doA9F7", "target": ["Un nuevo aprendizaje adversarial c\u00edclico aumentado con un modelo de tarea auxiliar que mejora el rendimiento de la adaptaci\u00f3n del dominio en situaciones supervisadas y no supervisadas de bajos recursos ", "Propone una extensi\u00f3n de los m\u00e9todos de adaptaci\u00f3n adversativa consistente en ciclos para abordar la adaptaci\u00f3n del dominio cuando se dispone de datos objetivo supervisados limitados.", "Este trabajo introduce un enfoque de adaptaci\u00f3n de dominio basado en la idea de GAN c\u00edclico y propone dos algoritmos diferentes."]}
{"source": ["Nodes residing in different parts of a graph can have similar structural roles within their local network topology.", "The identification of such roles provides key insight into the organization of networks and can also be used to inform machine learning on graphs.", "However, learning structural representations of nodes is a challenging unsupervised-learning task, which typically involves manually specifying and tailoring topological features for each node.", "Here we develop GraphWave, a method that represents each node\u2019s local network neighborhood via a low-dimensional embedding by leveraging spectral graph wavelet diffusion patterns.", "We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network.", "Our method scales linearly with the number of edges and does not require any hand-tailoring of topological features.", "We evaluate performance on both synthetic and real-world datasets, obtaining improvements of up to 71% over state-of-the-art baselines."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJR2ylbRb", "target": ["Desarrollamos un m\u00e9todo de aprendizaje de firmas estructurales en redes basado en la difusi\u00f3n de ond\u00edculas de grafos espectrales.", "Utilizaci\u00f3n de patrones de difusi\u00f3n de ond\u00edculas de gr\u00e1ficos espectrales de un nodo local para incrustar el nodo en un espacio de baja dimensi\u00f3n", "El documento deriva una forma de comparar nodos en el gr\u00e1fico basada en el an\u00e1lisis wavelet del laplaciano del gr\u00e1fico. "]}
{"source": ["Driving simulators play an important role in vehicle research.", "However, existing virtual reality simulators do not give users a true sense of presence.", "UniNet is our driving simulator, designed to allow users to interact with and visualize simulated traffic in mixed reality.", "It is powered by SUMO and Unity.", "UniNet's modular architecture allows us to investigate interdisciplinary research topics such as vehicular ad-hoc networks, human-computer interaction, and traffic management.", "We accomplish this by giving users the ability to observe and interact with simulated traffic in a high fidelity driving simulator.", "We present a user study that subjectively measures user's sense of presence in UniNet.", "Our findings suggest that our novel mixed reality system does increase this sensation."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "4ZO8BVlix-", "target": ["Un simulador de conducci\u00f3n de realidad mixta que utiliza c\u00e1maras estereosc\u00f3picas y RV de paso evaluado en un estudio de usuarios con 24 participantes.", "Propone un complicado sistema de simulaci\u00f3n de conducci\u00f3n.", "Este trabajo presenta un montaje de simulador de conducci\u00f3n de realidad mixta para mejorar la sensaci\u00f3n de presencia", "Propone un simulador de conducci\u00f3n de realidad mixta que incorpora la generaci\u00f3n de tr\u00e1fico y reclama una mayor \"presencia\" gracias a un sistema de RM."]}
{"source": ["We consider the problem of improving kernel approximation via feature maps.", "These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets.", "We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods.", "Our approach allows to use information about the integrand to enhance approximation and facilitates fast computations.", "We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis."], "source_labels": [1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "H1U_af-0-", "target": ["Reglas de cuadratura para la aproximaci\u00f3n del n\u00facleo.", "El art\u00edculo propone mejorar la aproximaci\u00f3n del n\u00facleo de las caracter\u00edsticas aleatorias mediante el uso de reglas de cuadratura como las reglas estoc\u00e1sticas esf\u00e9ricas-radiales.", "Los autores proponen una versi\u00f3n novedosa del enfoque del mapa de caracter\u00edsticas aleatorias para resolver de forma aproximada los problemas del n\u00facleo a gran escala.", "Este trabajo muestra que las t\u00e9cnicas debidas a Genz & Monahan (1998) pueden ser utilizadas para lograr un bajo error de aproximaci\u00f3n del n\u00facleo bajo el marco de la caracter\u00edstica aleatoria de Fourier, una nueva forma de aplicar las reglas de cuadratura para mejorar la aproximaci\u00f3n del n\u00facleo."]}
{"source": ["Human world knowledge is both structured and flexible.", "When people see an object, they represent it not as a pixel array but as a meaningful arrangement of semantic parts.", "Moreover, when people refer to an object, they provide descriptions that are not merely true but also relevant in the current context.", "Here, we combine these two observations in order to learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects.", "To do this, we employed an interactive communication task with human participants to construct a large dataset containing natural utterances referring to 3D objects from ShapeNet in a wide variety of contexts.", "Using this dataset, we developed neural listener and speaker models with strong capacity for generalization.", "By performing targeted lesions of visual and linguistic input, we discovered that the neural listener depends heavily on part-related words and associates these words correctly with the corresponding geometric properties of objects, suggesting that it has learned task-relevant structure linking the two input modalities.", "We further show that a neural speaker that is `listener-aware' --- that plans its utterances according to how an imagined listener would interpret its words in context --- produces more discriminative referring expressions than an `listener-unaware' speaker, as measured by human performance in identifying the correct object."], "source_labels": [0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkgZ3oR9FX", "target": ["C\u00f3mo construir altavoces/escuchas neuronales que aprendan las caracter\u00edsticas finas de los objetos 3D, a partir del lenguaje referencial.", "Los autores presentan un estudio sobre el aprendizaje de la referencia a objetos 3D, recopilando un conjunto de datos de expresiones referenciales y entrenando varios modelos mediante la experimentaci\u00f3n de una serie de opciones arquitect\u00f3nicas"]}
{"source": ["Object-based factorizations provide a useful level of abstraction for interacting with the world.", "Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice.", "We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties.", "Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels.", "For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics.", "After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "HJx9EhC9tQ", "target": ["Presentamos un marco de trabajo para el aprendizaje de representaciones centradas en el objeto adecuadas para la planificaci\u00f3n en tareas que requieren una comprensi\u00f3n de la f\u00edsica.", "El art\u00edculo presenta una plataforma para predecir im\u00e1genes de objetos que interact\u00faan entre s\u00ed bajo el efecto de las fuerzas gravitatorias.", "El art\u00edculo presenta un m\u00e9todo que aprende a reproducir \"torres de bloques\" a partir de una imagen dada.", "Propone un m\u00e9todo que aprende a razonar sobre la interacci\u00f3n f\u00edsica de diferentes objetos sin supervisar sus propiedades."]}
{"source": ["We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.", "Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.", "For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.", "Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.", "We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting."], "source_labels": [0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "BJk7Gf-CZ", "target": ["Proporcionamos condiciones necesarias y suficientes eficientemente comprobables para la optimizaci\u00f3n global en redes neuronales lineales profundas, con algunas extensiones iniciales a entornos no lineales.", "El art\u00edculo da condiciones para la optimalidad global de la funci\u00f3n de p\u00e9rdida de las redes neuronales lineales profundas", "El art\u00edculo ofrece resultados te\u00f3ricos sobre la existencia de m\u00ednimos locales en la funci\u00f3n objetivo de las redes neuronales profundas.", "Estudia algunas propiedades te\u00f3ricas de las redes lineales profundas."]}
{"source": ["Recurrent auto-encoder model can summarise sequential data through an encoder structure into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure.", "The summarised information can be used to represent time series features.", "In this paper, we propose relaxing the dimensionality of the decoder output so that it performs partial reconstruction.", "The fixed-length vector can therefore represent features only in the selected dimensions.", "In addition, we propose using rolling fixed window approach to generate samples.", "The change of time series features over time can be summarised as a smooth trajectory path.", "The fixed-length vectors are further analysed through additional visualisation and unsupervised clustering techniques. \n\n", "This proposed method can be applied in large-scale industrial processes for sensors signal analysis purpose where clusters of the vector representations can be used to reflect the operating states of selected aspects of the industrial system."], "source_labels": [0, 1, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1cLblgCZ", "target": ["Utilizaci\u00f3n de un modelo de autoencoder recurrente para extraer caracter\u00edsticas de series temporales multidimensionales", "Este escrito describe una aplicaci\u00f3n del autoencoder recurrente para analizar series temporales multidimensionales", "El art\u00edculo describe un modelo de autocodificador secuencia a secuencia que se utiliza para aprender representaciones de la secuencia, mostrando que para su aplicaci\u00f3n se obtiene un mejor rendimiento cuando la red s\u00f3lo se entrena para reconstruir un subconjunto de las mediciones de datos. ", "Propone una estrategia inspirada en el modelo de autocodificador recurrente para poder realizar la agrupaci\u00f3n de los datos de las series temporales multidimensionales bas\u00e1ndose en los vectores de contexto."]}
{"source": ["We view molecule optimization as a graph-to-graph translation problem.", "The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules.", "Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.", "A key challenge is therefore to model diverse translation outputs.", "Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules.", "Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process.", "We evaluate our model on multiple molecule optimization tasks and show that our model outperforms previous state-of-the-art baselines by a significant margin. \n"], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1xJAsA5F7", "target": ["Introducimos un marco de codificador-decodificador de grafos para aprender diversas traducciones de grafos.", "Propone un modelo de traducci\u00f3n de gr\u00e1fico a gr\u00e1fico para la optimizaci\u00f3n de mol\u00e9culas inspirado en el an\u00e1lisis de pares moleculares emparejados.", "Extensi\u00f3n de JT-VAE al escenario de traducci\u00f3n de gr\u00e1fico a gr\u00e1fico a\u00f1adiendo la variable latente para capturar la multimodalidad y una regularizaci\u00f3n adversarial en el espacio latente", "Propone un sistema bastante complejo, que implica muchas opciones y componentes diferentes, para obtener nubes qu\u00edmicas con propiedades mejoradas a partir de un corpus dado."]}
{"source": ["Partial differential equations (PDEs) are widely used across the physical and computational sciences.", "Decades of research and engineering went into designing fast iterative solution methods.", "Existing solvers are general purpose, but may be sub-optimal for specific classes of problems.", "In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain.", "We achieve this goal by learning to modify the updates of an existing solver using a deep neural network.", "Crucially, our approach is proven to preserve strong correctness and convergence guarantees.", "After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rklaWn0qK7", "target": ["Aprendemos un solucionador neuronal r\u00e1pido para las EDP que tiene garant\u00edas de convergencia.", "Desarrolla un m\u00e9todo para acelerar el m\u00e9todo de las diferencias finitas en la resoluci\u00f3n de las EDP y propone un marco revisado para la iteraci\u00f3n del punto fijo despu\u00e9s de la discretizaci\u00f3n.", "Los autores proponen un m\u00e9todo lineal para acelerar los solucionadores de EDP."]}
{"source": ["Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space.", "We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions.", "We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs.", "Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator.", "With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes.", "Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets."], "source_labels": [0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rkxacs0qY7", "target": ["Realizamos una inferencia variacional funcional sobre los procesos estoc\u00e1sticos definidos por las redes neuronales bayesianas.", "Ajuste de aproximaciones de Redes Neuronales Bayesianas variacionales en forma funcional y considerando la adecuaci\u00f3n a un proceso estoc\u00e1stico a priori de forma impl\u00edcita a trav\u00e9s de muestras.", "Presenta un novedoso objetivo ELBO para el entrenamiento de BNNs que permite codificar en el modelo unas priores m\u00e1s significativas que las caracter\u00edsticas de peso menos informativas de la literatura.", "Presenta un nuevo algoritmo de inferencia variacional para modelos bayesianos de redes neuronales en los que la prioridad se especifica funcionalmente en lugar de a trav\u00e9s de una prioridad sobre los pesos. "]}
{"source": ["Words are not created equal.", "In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal.", "In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry.", "This connection allows us to introduce a novel principled hypernymy score for word embeddings.", "Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds.", "We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry.", "Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection.", "In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "Ske5r3AqK7", "target": ["Incrustamos las palabras en el espacio hiperb\u00f3lico y establecemos la conexi\u00f3n con las incrustaciones de palabras gaussianas.", "Este trabajo adapta la incrustaci\u00f3n de palabras Glove a un espacio hiperb\u00f3lico dado por el modelo de medio plano de Poincare", "Este trabajo propone un enfoque para implementar un modelo de incrustaci\u00f3n de palabras hiperb\u00f3lico basado en GLOVE, que se optimiza a trav\u00e9s de los m\u00e9todos de optimizaci\u00f3n de Riemann."]}
{"source": ["Answering questions about a text frequently requires aggregating information from multiple places in that text.", "End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so.", "We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings.", "In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism.", "However, with this supervision, it can perform well.", "On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities.", "A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision.", "We hypothesize that many \"multi-hop\" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 1], "rouge_scores": [], "paper_id": "B1lf43A5Y7", "target": ["Las redes de memoria no aprenden el razonamiento multisalto a menos que las supervisemos.", "Afirma que el razonamiento multisaltos no es f\u00e1cil de aprender directamente y requiere supervisi\u00f3n directa, y que hacerlo bien en WikiHop no significa necesariamente que el modelo est\u00e9 aprendiendo realmente a saltar.", "El art\u00edculo propone investigar el conocido problema del aprendizaje de redes de memoria y, m\u00e1s concretamente, la dificultad de la supervisi\u00f3n del aprendizaje de la atenci\u00f3n con tales modelos.", "En este trabajo se argumenta que la red de memoria no aprende un razonamiento multisalto razonable."]}
{"source": ["Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood.", "We compute deep convolutional network generators by inverting a fixed embedding operator.", "Therefore, they do not require to be optimized with a discriminator or an encoder.", "The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images.", "This embedding is computed with a wavelet Scattering transform.", "Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder."], "source_labels": [0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "r1NYjfbR-", "target": ["Introducimos redes generativas que no requieren ser aprendidas con un discriminador o un codificador; se obtienen invirtiendo un operador especial de incrustaci\u00f3n definido por una transformada wavelet de dispersi\u00f3n.", "Introduce las transformadas de dispersi\u00f3n como modelos generativos de im\u00e1genes en el contexto de las Redes Generativas Adversariales y sugiere por qu\u00e9 podr\u00edan verse como transformadas de gaussianizaci\u00f3n con p\u00e9rdida de informaci\u00f3n e invertibilidad controladas. ", "El art\u00edculo propone un modelo generativo para im\u00e1genes que no requiere aprender un discriminador (como en los GANs) o un embedding aprendido."]}
{"source": ["Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token.", "Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance.", "Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input.", "We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference.", "The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.\n", "A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that \n", "Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text."], "source_labels": [0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1xf9jAqFQ", "target": ["Proponemos un nuevo modelo de lectura neuronal r\u00e1pida que utiliza la estructura de puntuaci\u00f3n inherente a un texto para definir el comportamiento de salto y omisi\u00f3n efectivo.", "El art\u00edculo propone un modelo Structural-Jump-LSTM para acelerar la lectura mec\u00e1nica con dos agentes en lugar de uno", "Propone un novedoso modelo de lectura r\u00e1pida neuronal en el que el nuevo lector tiene la capacidad de saltarse una palabra o secuencia de palabras.", "El art\u00edculo propone un m\u00e9todo de lectura r\u00e1pida que utiliza acciones de salto y salto, demostrando que el m\u00e9todo propuesto es tan preciso como el LSTM pero utiliza mucho menos c\u00e1lculo."]}
{"source": ["One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions.", "In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP).", "In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning.", "Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN).", "Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components.", "The imagination core generates predicted trajectories, i.e., imagined items that users may purchase.", "The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards.", "To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms.", "Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines.", "Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning."], "source_labels": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SkfTIj0cKX", "target": ["Proponemos la arquitectura IRN para aumentar la recompensa de compra dispersa y retrasada para la recomendaci\u00f3n basada en la sesi\u00f3n.", "El art\u00edculo propone mejorar el rendimiento de los sistemas de recomendaci\u00f3n a trav\u00e9s del aprendizaje por refuerzo mediante el uso de una red de reconstrucci\u00f3n de la imaginaci\u00f3n.", "El art\u00edculo presenta un enfoque de recomendaci\u00f3n basado en la sesi\u00f3n, centr\u00e1ndose en las compras del usuario en lugar de los clics. "]}
{"source": ["The question why deep learning algorithms generalize so well has attracted increasing\n", "research interest.", "However, most of the well-established approaches,\n", "such as hypothesis capacity, stability or sparseness, have not provided complete\n", "explanations (Zhang et al., 2016; Kawaguchi et al., 2017).", "In this work, we focus\n", "on the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\n", "will not change much due to perturbations of its training examples, then it\n", "will also generalize well.", "As most deep learning algorithms are stochastic (e.g.,\n", "Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\n", "arguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\n", "robustness \u2013 that concerns the robustness of a population of hypotheses.", "Through\n", "the lens of ensemble robustness, we reveal that a stochastic learning algorithm can\n", "generalize well as long as its sensitiveness to adversarial perturbations is bounded\n", "in average over training examples.", "Moreover, an algorithm may be sensitive to\n", "some adversarial examples (Goodfellow et al., 2015) but still generalize well.", "To\n", "support our claims, we provide extensive simulations for different deep learning\n", "algorithms and different network architectures exhibiting a strong correlation between\n", "ensemble robustness and the ability to generalize."], "source_labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "r1YUtYx0-", "target": ["Explicaci\u00f3n de la generalizaci\u00f3n de los algoritmos de aprendizaje profundo estoc\u00e1stico, te\u00f3rica y emp\u00edricamente, a trav\u00e9s de la robustez del conjunto", "Este trabajo presenta una adaptaci\u00f3n de la robustez algor\u00edtmica de Xu&Mannor'12 y presenta l\u00edmites de aprendizaje y un experimento que muestra la correlaci\u00f3n entre la robustez emp\u00edrica del conjunto y el error de generalizaci\u00f3n. ", "Propone un estudio de la capacidad de generalizaci\u00f3n de los algoritmos de aprendizaje profundo utilizando una extensi\u00f3n de la noci\u00f3n de estabilidad llamada robustez de conjunto y da l\u00edmites al error de generalizaci\u00f3n de un algoritmo aleatorio en t\u00e9rminos del par\u00e1metro de estabilidad y proporciona un estudio emp\u00edrico que intenta conectar la teor\u00eda con la pr\u00e1ctica.", "El trabajo estudi\u00f3 la capacidad de generalizaci\u00f3n de los algoritmos de aprendizaje desde el punto de vista de la robustez en un contexto de aprendizaje profundo"]}
{"source": ["Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet.  ", "However, such models require many thousands of gradient-based weight updates and unique image examples for training.", "Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks.  ", "In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation.", "Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset.  ", "Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision.", "Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "r1wEFyWCW", "target": ["Aprendizaje de pocos disparos PixelCNN", "El art\u00edculo propone utilizar la estimaci\u00f3n de la densidad cuando la disponibilidad de datos de entrenamiento es baja mediante un modelo de meta-aprendizaje.", "Este trabajo considera el problema de la estimaci\u00f3n de la densidad de uno/varios disparos, utilizando t\u00e9cnicas de metalearning que han sido aplicadas al aprendizaje supervisado de uno/varios disparos", "El art\u00edculo se centra en el aprendizaje de pocos disparos con estimaci\u00f3n de densidad autorregresiva y mejora PixelCNN con t\u00e9cnicas de atenci\u00f3n neuronal y metaaprendizaje."]}
{"source": ["Neural networks exhibit good generalization behavior in the\n", "over-parameterized regime, where the number of network parameters\n", "exceeds the number of observations.", "Nonetheless,\n", "current generalization bounds for neural networks fail to explain this\n", "phenomenon.", "In an attempt to bridge this gap, we study the problem of\n", "learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function.", "In the case where the network has Leaky\n", "ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks.\n", "Specifically, we prove convergence rates of SGD to a global\n", "minimum and provide generalization guarantees for this global minimum\n", "that are independent of the network size. \n", "Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model.", "This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "rJ33wwxRb", "target": ["Demostramos que SGD aprende redes neuronales de dos capas sobreparametrizadas con activaciones Leaky ReLU que generalizan de forma demostrable en datos linealmente separables.", "El art\u00edculo estudia los modelos sobreparametrizados siendo capaces de aprender soluciones bien generalizadas utilizando una red de 1 capa oculta con capa de salida fija.", "Este trabajo muestra que en datos linealmente separados, el SGD en una red sobreparametrizada puede a\u00fan inclinar un clasificador que se generaliza de forma demostrable."]}
{"source": ["A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed.", "We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\\it decision states}.", "These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions.", "We propose to learn about decision states from prior experience.", "By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck.", "We find that this simple mechanism effectively identifies decision states, even in partially observed settings.", "In effect, the model learns the sensory cues that correlate with potential subgoals.", "In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJg8yhAqKm", "target": ["Entrenar a los agentes con los cuellos de botella de la informaci\u00f3n de la pol\u00edtica de objetivos promueve la transferencia y produce un potente bono de exploraci\u00f3n", "Propone regularizar las p\u00e9rdidas est\u00e1ndar de RL con la informaci\u00f3n mutua condicional negativa para la b\u00fasqueda de pol\u00edticas en un entorno de RL multiobjetivo.", "Este trabajo propone el concepto de estado de decisi\u00f3n y propone una regularizaci\u00f3n de divergencia KL para aprender la estructura de las tareas y utilizar esta informaci\u00f3n para animar a la pol\u00edtica a visitar los estados de decisi\u00f3n.", "El art\u00edculo propone un m\u00e9todo de regularizaci\u00f3n de las pol\u00edticas condicionadas por objetivos con un t\u00e9rmino de informaci\u00f3n mutua. "]}
{"source": ["Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead.", "This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables).", "We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search.", "We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients.", "This allows us to estimate a descent direction which can then be passed to a first-order optimizer.", "We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems.", "Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient).", "We provide a demo of Guided ES at: redacted URL"], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "B1xFxh0cKX", "target": ["Proponemos un m\u00e9todo de optimizaci\u00f3n para cuando s\u00f3lo se dispone de gradientes sesgados: definimos un nuevo estimador de gradiente para este escenario, derivamos el sesgo y la varianza de este estimador y lo aplicamos a problemas de ejemplo.", "Los autores proponen un enfoque que combina la b\u00fasqueda aleatoria con la informaci\u00f3n del gradiente sustituto y ofrecen una discusi\u00f3n sobre el equilibrio entre la varianza y el sesgo, as\u00ed como una discusi\u00f3n sobre la optimizaci\u00f3n de los hiperpar\u00e1metros.", " El art\u00edculo propone un m\u00e9todo para mejorar la b\u00fasqueda aleatoria mediante la construcci\u00f3n de un subespacio de los k gradientes sustitutos anteriores.", "Este trabajo intenta acelerar la evoluci\u00f3n del tipo OpenAI introduciendo una distribuci\u00f3n no isotr\u00f3fica con una matriz de covarianza de la forma I + UU^t e informaci\u00f3n externa como un gradiente sustituto para determinar U"]}
{"source": ["Point clouds are an important type of geometric data and have widespread use in computer graphics and vision.", "However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space.", "Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation.", "This paper studies the unsupervised problem of a generative model exploiting graph convolution.", "We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator.", "The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry.", "We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively."], "source_labels": [0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "SJeXSo09FQ", "target": ["Un GAN que utiliza operaciones de convoluci\u00f3n de grafos con grafos calculados din\u00e1micamente a partir de caracter\u00edsticas ocultas", "El trabajo propone una versi\u00f3n de GANs espec\u00edficamente dise\u00f1ada para la generaci\u00f3n de nubes de puntos, siendo la aportaci\u00f3n principal del trabajo la operaci\u00f3n de upsampling.", "Este trabajo propone GANs grafo-convolucionales para nubes de puntos 3D irregulares que aprenden el dominio y las caracter\u00edsticas al mismo tiempo."]}
{"source": ["Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples.", "However, mislabeled examples are to hard avoid in extremely large datasets.", "We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics.", "We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses.", "This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead.", "Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "HyGDdsCcFQ", "target": ["Introducimos un algoritmo r\u00e1pido y f\u00e1cil de implementar que es robusto al ruido del conjunto de datos.", "El documento pretende eliminar los ejemplos potenciales con ruido de etiqueta descartando los que tienen grandes p\u00e9rdidas en el procedimiento de entrenamiento."]}
{"source": ["Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency.", "Concurrently, it has been shown that neural networks may be overly sensitive to ``attacks\" -- tiny adversarial changes in the input -- which may be detrimental to their use in safety-critical domains.", "Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks.\n", "The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks.", "In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization.", "We propose a Mixed Integer Linear Programming (MILP) formulation of the problem.", "While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow.", "To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems.", "Experimentally, we evaluate both proposed methods against the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to PGD, while scaling beyond the limits of the MILP."], "source_labels": [0, 0, 0, 0, 1, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "S1lTEh09FQ", "target": ["Los ataques basados en el gradiente a las redes neuronales binarizadas no son eficaces debido a la no diferenciabilidad de dichas redes; nuestro algoritmo IPROP resuelve este problema utilizando la optimizaci\u00f3n de enteros", "Propone un nuevo algoritmo de estilo de propagaci\u00f3n de objetivos para generar fuertes ataques adversarios en redes neuronales binarizadas.", "Este trabajo propone un nuevo algoritmo de ataque basado en MILP sobre redes neuronales binarias.", "Este trabajo presenta un algoritmo para encontrar ataques adversarios a redes neuronales binarias que encuentra iterativamente las representaciones deseadas capa por capa desde la parte superior hasta la entrada y es m\u00e1s eficiente que resolver el solucionador de programaci\u00f3n lineal entera mixta (MILP) completo."]}
{"source": ["Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling.", "We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token.", "This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token.", "With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.", "We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets.", "In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.", "These results constitute a new state-of-the-art in their respective settings."], "source_labels": [0, 1, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "SklckhR5Ym", "target": ["La decodificaci\u00f3n del \u00faltimo token del contexto utilizando la distribuci\u00f3n del siguiente token predicho act\u00faa como un regularizador y mejora el modelado del lenguaje.", "Los autores introducen la idea de decodificaci\u00f3n pasada con el fin de regularizar para mejorar la perplejidad en Penn Treebank", "Propone un t\u00e9rmino de p\u00e9rdida adicional para usar cuando se entrena un LM LSTM y muestra que a\u00f1adiendo este t\u00e9rmino de p\u00e9rdida pueden alcanzar la perplejidad SOTA en una serie de puntos de referencia de LM.", "Sugiere una nueva t\u00e9cnica de regularizaci\u00f3n que puede a\u00f1adirse a las utilizadas en AWD-LSTM de Merity et al. (2017) con poca sobrecarga."]}
{"source": ["The assumption that data samples are independently identically distributed is the backbone of many learning algorithms.", "Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances.", "Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically.", "Specifically, we assume that the instances are sampled from a Markov chain.", "Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations.", "One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix.", "This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances.", "To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training.  ", "Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task."], "source_labels": [0, 0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "rJ695PxRW", "target": ["Proponer la observaci\u00f3n de \u00f3rdenes impl\u00edcitas en conjuntos de datos desde el punto de vista de un modelo generativo.", "Los autores abordan el problema de la ordenaci\u00f3n impl\u00edcita en un conjunto de datos y el reto de recuperarla y proponen aprender un modelo sin m\u00e9trica de distancia que asume una cadena de Markov como mecanismo generador de los datos ", "El art\u00edculo propone las redes de Markov generativas, un enfoque basado en el aprendizaje profundo para modelar secuencias y descubrir el orden en los conjuntos de datos.", "Propone aprender el orden de una muestra de datos desordenados mediante el aprendizaje de una cadena de Markov."]}
{"source": ["We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples.", "The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it.", "To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs.", "We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline."], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "B1KJJf-R-", "target": ["S\u00edntesis de programas a partir de la descripci\u00f3n en lenguaje natural y de ejemplos de entrada/salida mediante la b\u00fasqueda en \u00e1rbol sobre el modelo Seq2Tree", "Presenta un modelo seq2Tree para traducir el enunciado de un problema en lenguaje natural al correspondiente programa funcional en DSL, que ha mostrado una mejora sobre el enfoque de base seq2seq.", "Este art\u00edculo aborda el problema de hacer s\u00edntesis de programas cuando se da una descripci\u00f3n del problema y un peque\u00f1o n\u00famero de ejemplos de entrada-salida.", "El art\u00edculo introduce una t\u00e9cnica de s\u00edntesis de programas que implica una gram\u00e1tica restringida de problemas que se busca mediante una red atencional de codificador-decodificador."]}
{"source": ["Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically  neural networks.", "The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable).", "In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions.", "This is a very mild condition satisfied even by neural networks with a single neuron.", "Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics.", "When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set.", "When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training.", "Our analysis sheds lights on understanding the practical performance of GANs."], "source_labels": [0, 0, 0, 0, 0, 1, 0, 0], "rouge_scores": [], "paper_id": "Hk9Xc_lR-", "target": ["Este trabajo estudia las propiedades de discriminaci\u00f3n y generalizaci\u00f3n de las GANs cuando el conjunto discriminador es una clase de funci\u00f3n restringida como las redes neuronales.", "Equilibra las capacidades de las clases de generadores y discriminadores en los GANs garantizando que las MIPs inducidas son m\u00e9tricas y no pseudom\u00e9tricas", "Este trabajo proporciona un an\u00e1lisis matem\u00e1tico del papel del tama\u00f1o del conjunto adversario/discriminador en los GANs"]}
{"source": ["Normalization layers are a staple in state-of-the-art deep neural network architectures.", "They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic.", "In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization.", "Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization.", "We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers.", "Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation."], "source_labels": [0, 0, 0, 0, 1, 0], "rouge_scores": [], "paper_id": "H1gsz30cKX", "target": ["Todo lo que se necesita para entrenar redes residuales profundas es una buena inicializaci\u00f3n; las capas de normalizaci\u00f3n no son necesarias.", "Se presenta un m\u00e9todo para la inicializaci\u00f3n y normalizaci\u00f3n de redes residuales profundas. Se basa en observaciones de la explosi\u00f3n hacia delante y hacia atr\u00e1s en dichas redes. El rendimiento del m\u00e9todo est\u00e1 a la altura de los mejores resultados obtenidos por otras redes con una normalizaci\u00f3n m\u00e1s expl\u00edcita.", "Los autores proponen una forma novedosa de inicializar las redes residuales, motivada por la necesidad de evitar la explosi\u00f3n/desvanecimiento de los gradientes.", "Propone un nuevo m\u00e9todo de inicializaci\u00f3n utilizado para entrenar RedNets muy profundas sin utilizar batch-norm."]}
{"source": ["Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult.", "In a such situation, learning a metric of a sequence from data is one possible solution.", "The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning.", "In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training.", "The partial reward function is a reward function for a partial sequence of a certain length.", "SeqGAN employs a reward function for completed sequence only.", "By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole.", "In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence.", "Expert-based reward function training is not a kind of GAN frameworks.", "This makes the optimization of the generator easier.", "We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE.", "Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement."], "source_labels": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "rouge_scores": [], "paper_id": "r1kP7vlRb", "target": ["Este trabajo pretende aprender una m\u00e9trica mejor para el aprendizaje no supervisado, como la generaci\u00f3n de textos, y muestra una mejora significativa sobre SeqGAN.", "Describe un enfoque para generar secuencias temporales mediante el aprendizaje de valores estado-acci\u00f3n, donde el estado es la secuencia generada hasta el momento, y la acci\u00f3n es la elecci\u00f3n del siguiente valor. ", "Este trabajo considera el problema de mejorar la generaci\u00f3n de secuencias mediante el aprendizaje de mejores m\u00e9tricas, espec\u00edficamente el problema del sesgo de exposici\u00f3n"]}
{"source": ["One of the most successful techniques in generative models has been decomposing a complicated generation task into a series of simpler generation tasks.  ", "For example, generating an image at a low resolution and then learning to refine that into a high resolution image often improves results substantially.  ", "Here we explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables, and then map from these latent variables to the observed space.  ", "We show that this allows us to achieve decoupled training of complicated generative models and present both theoretical and experimental results supporting the benefit of such an approach.  "], "source_labels": [1, 0, 0, 0], "rouge_scores": [], "paper_id": "rJTGkKxAZ", "target": ["Descomponer la tarea de aprendizaje de un modelo generativo en el aprendizaje de factores latentes desentra\u00f1ados para subconjuntos de los datos y luego el aprendizaje de la uni\u00f3n sobre esos factores latentes.  ", "Factores localmente desentra\u00f1ados para el modelo generativo jer\u00e1rquico de variables latentes, que puede considerarse como una variante jer\u00e1rquica de la inferencia aprendida adversarialmente", "El art\u00edculo investiga el potencial de los modelos de variables latentes jer\u00e1rquicas para generar im\u00e1genes y secuencias de im\u00e1genes y propone entrenar varios modelos ALI apilados unos sobre otros para crear una representaci\u00f3n jer\u00e1rquica de los datos.", "El trabajo tiene como objetivo aprender las jerarqu\u00edas para el entrenamiento de GAN en un programa de optimizaci\u00f3n jer\u00e1rquica directamente en lugar de ser dise\u00f1ado por un humano"]}
{"source": ["Visual grounding of language is an active research field aiming at enriching text-based representations with visual information.", "In this paper, we propose a new way to leverage visual knowledge for sentence representations.", "Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space.", "We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information.", "We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks."], "source_labels": [0, 1, 0, 0, 0], "rouge_scores": [], "paper_id": "BJe8niAqKX", "target": ["Proponemos un modelo conjunto para incorporar el conocimiento visual en las representaciones de frases", "El art\u00edculo propone un m\u00e9todo para utilizar v\u00eddeos emparejados con subt\u00edtulos para mejorar la incrustaci\u00f3n de frases", "Esta presentaci\u00f3n propone un modelo para el aprendizaje de representaciones de oraciones que se basan en datos de v\u00eddeo asociados.", "Propone un m\u00e9todo para mejorar las incrustaciones de frases basadas en el texto mediante un marco multimodal conjunto."]}
