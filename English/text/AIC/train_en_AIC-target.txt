We provide necessary and sufficient analytical forms for the critical points of the square loss functions for various neural networks, and exploit the analytical forms to characterize the landscape properties for the loss functions of these neural networks.
Biologically plausible learning algorithms, particularly sign-symmetry, work well on ImageNet
We introduce the 2-simplicial Transformer and show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning.
Accurate forecasting over very long time horizons using tensor-train RNNs
We propose a variational message-passing algorithm for models that contain both the deep model and probabilistic graphical model.
A simple modification to low-rank factorization that improves performances (in both image and language tasks) while still being compact.
We propose a simple, general, and space-efficient data format to accelerate deep learning training by allowing sample fidelity to be dynamically selected at training time
ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE
Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.
Equip MMD GANs with a new random-forest kernel.
A method for more accurate critic estimates in reinforcement learning.
We introduce a systematic framework for quantifying the robustness of classifiers to naturally occurring perturbations of images found in videos.
Deep learning for structured tabular data machine learning using pre-trained CNN model from ImageNet.
Decoding pixels can still work for representation learning on images
fast, truly scalable full-matrix AdaGrad/Adam, with theory for adaptive stochastic non-convex optimization
In this paper, we propose to learn a dialogue system that independently parameterizes different dialogue skills, and learns to select and combine each of them through Attention over Parameters (AoP). 
We propose to distill a large dataset into a small set of synthetic data that can train networks close to original performance. 
We propose a primal-dual subgradient method for training GANs and this method effectively alleviates mode collapse.
We find that irrationality from an expert demonstrator can help a learner infer their preferences. 
We compare robustness of models from 4 popular NLP tasks: Q&A, NLI, NER and Sentiment Analysis by testing their performance on perturbed inputs.
A novel cluster-based algorithm of curriculum learning is proposed to solve the robust training of generative models.
We proposed a novel distributed backdoor attack on federated learning and show that it is not only more effective compared with standard centralized attacks, but also harder to be defended by existing robust FL methods
Neural net for graph-based semi-supervised learning; revisits the classics and propagates *labels* rather than feature representations
Neural Architecture Search for a series of Natural Language Understanding tasks. Design the search space for NLU tasks. And Apply differentiable architecture search to discover new models
In this paper we introduce EvalNE, a Python toolbox for automating the evaluation of network embedding methods on link prediction and ensuring the reproducibility of results.
Recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs.
Jumpout applies three simple yet effective modifications to dropout, based on novel understandings about the generalization performance of DNN with ReLU in local regions.
We study the natural emergence of sparsity in the activations and gradients for some layers of a dense LSTM language model, over the course of training.
Conventional memory networks generate many redundant latent vectors resulting in overfitting and the need for larger memories. We introduce memory dropout as an automatic technique that encourages diversity in the latent space.
We derive Nesterov's method arises as a straightforward discretization of an ODE different from the one in Su-Boyd-Candes and prove acceleration the stochastic case
We propose learning to transfer learn (L2TL) to improve transfer learning on a target dataset by judicious extraction of information from a source dataset.
In Deep RL, order-invariant functions can be used in conjunction with standard memory modules to improve gradient decay and resilience to noise.
This paper introduces an algorithm to handle optimization problem with multiple constraints under vision of manifold.
A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.
Our method incorporates WGAN to achieve occupancy measure matching for transition learning.
Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.
We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.
We propose DVD-GAN, a large video generative model that is state of the art on several tasks and produces highly complex videos when trained on large real world datasets.
We propose a new recurrent memory architecture that can track common sense state changes of entities by simulating the causal effects of actions.
We investigate the space efficiency of memory-augmented neural nets when learning set membership.
We construct a Kronecker factored Laplace approximation for neural networks that leads to an efficient matrix normal distribution over the weights.
Graph regularization forces spectral embedding to focus on the largest clusters, making the representation less sensitive to noise. 
We show that exposure bias could be much less serious than it is currently assumed to be for MLE LM training.
A controlled study of the role of environments with respect to properties in emergent communication protocols.
Grid-based document representation with contextualized embedding vectors for documents with 2D layouts
Deep RL policies can be attacked by other agents taking actions so as to create natural observations that are adversarial.
We present a novel iterative algorithm based on generalized low rank models for computing and interpreting word embedding models.
We learn a conditional autoregressive flow to propose perturbations that don't induce simulator failure, improving inference performance.
We improve answering of questions that require multi-hop reasoning extracting an intermediate chain of sentences.
We develop a new method for normalization constant (Bayesian evidence) estimation using Optimal Bridge Sampling and a novel Normalizing Flow, which is shown to outperform existing methods in terms of accuracy and computational time.
We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.
Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.
We identify downsampling as a mechansim for memorization in convolutional autoencoders.
We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.
Generalization is strongly correlated with the Bayesian evidence, and gradient noise drives SGD towards minima whose evidence is large.
adversarial nets, attention mechanism, positron images, data scarcity
 Inspired by neuroscience research, solve three key weakness of the widely-cited recurrent attention model by simply adding two terms on the objective function.
This paper introduces a clustering-based active learning algorithm on graphs.
We propose the InfoCNF, an efficient conditional CNF that employs gating networks to learn the error tolerances of the ODE solvers  
An unsupervised learning method that uses meta-learning to enable efficient learning of downstream image classification tasks, outperforming state-of-the-art methods.
Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.
A novel method of inductive transfer learning that employs adversarial learning and multi-task learning to address the discrepancy in input and output space
A novel, high-performing architecture for end-to-end named entity recognition and relation extraction that is fast to train.
 Variational Bayes scheme for Recurrent Neural Networks
case study on optimal deep learning model for UAVs
We show the first successful use of Transformer in generating music that exhibits long-term structure. 
We propose a new Monte Carlo Tree Search / rollout algorithm that relies on width-based search to construct a lookahead.
We propose a novel deep neural network layer for normalising within-class covariance of an internal representation in a neural network that results in significantly improving the generalisation of the learned representations.
The addition of a diversity criterion inspired from DPP in the GAN objective avoids mode collapse and leads to better generations. 
We suggest a generalization bound that could partly explain the improvement in generalization with over-parametrization.
We introduce three generic point cloud processing blocks that improve both accuracy and memory consumption of multiple state-of-the-art networks, thus allowing to design deeper and more accurate networks.
Methods to learn contextual acoustic word embeddings from an end-to-end speech recognition model that perform competitively with text-based word embeddings.
This paper propose a mask method which solves the previous blurred results of unsupervised monocular depth estimation caused by occlusion
We introduce a novel way to represent graphs as multi-channel image-like structures that allows them to be handled by vanilla 2D CNNs.
We propose a measure of long-term memory and prove that deep recurrent networks are much better fit to model long-term temporal dependencies than shallow ones.
We  compare perceptual, neural, and modeled representations of animal communication using machine learning, behavior, and physiology. 
The Information Bottleneck Principle applied to ResNets, using PixelCNN++ models to decode mutual information and conditionally generate images for information illustration
Adaptation of an RL agent in a target environment with unknown dynamics is fast and safe when we transfer prior experience in a variety of environments and then select risk-averse actions during adaptation.
We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them.
We introduce a Gaussian Process Prior over weights in a neural network and explore its ability to model input-dependent weights with benefits to various tasks, including uncertainty estimation and generalization in the low-sample setting.
We perform an in-depth investigation of the suitability of self-attention models for character-level neural machine translation.
we present the state-of-the-art results of using neural networks to diagnose chest x-rays
We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.
Internal-consistency constraints improve agents ability to develop emergent protocols that generalize across communicative roles.
We introduce a new analysis technique that discovers interpretable compositional structure in notoriously hard-to-interpret recurrent neural networks.
We reproduced neural representations found in biological visual systems by simulating their neural resource constraints in a deep convolutional model.
a theory connecting Hessian of the solution and the generalization power of the model
We introduced entropy maximization to GANs, leading to a reinterpretation of the critic as an energy function.
We present a long time-scale musical audio style transfer algorithm which synthesizes audio in the time-domain, but uses Time-Frequency representations of audio.
We propose a repeated reference benchmark task and a regularized continual learning approach for adaptive communication with humans in unfamiliar domains
Sort in encoder and undo sorting in decoder to avoid responsibility problem in set auto-encoders
We present a hierarchical learning framework for navigation within an embodied learning setting
Attribution can sometimes be misleading
Efficient Transformer with locality-sensitive hashing and reversible layers
We show language understanding via reading is promising way to learn policies that generalise to new environments.
We propose a hypothesis for why gradient descent generalizes based on how per-example gradients interact with each other.
Novel architecture for stereoscopic view synthesis at arbitrary camera shifts utilizing adaptive t-shaped kernels with adaptive dilations.
This paper proposes fundamental theory and optimal algorithms for DNN training, which reduce up to 80% of training memory for popular DNNs.
This paper proves the universal  approximability of quantized ReLU neural networks and puts forward the complexity bound given arbitrary error.
A general framework of value-based reinforcement learning for continuous control
Generative Adversarial Network Training is a Continual Learning Problem.
A unified frame for both few-shot learning and zero-shot learning based on network reparameterization
GraphQA is a graph-based method for protein Quality Assessment that improves the state-of-the-art for both hand-engineered and representation-learning approaches
We address the active learning in batch setting with noisy oracles and use model uncertainty to encode the decision quality of active learning algorithm during acquisition.
Stochastic style transfer with adjustable features. 
We propose AGILE, a framework for training agents to perform instructions from examples of respective goal-states.
In Hierarchical RL, we introduce the notion of a 'soft', i.e. adaptable, option and show that this helps learning in multitask settings.
Learning a controllable generative model by performing latent representation disentanglement learning.
Enhance the language model for supervised learning task 
dynamically generate filters conditioned on the input image for CNNs in each forward pass 
We propose a new anytime neural network which allows partial evaluation by subnetworks with different widths as well as depths.
We propose a new model for making generalizable and diverse retrosynthetic reaction predictions.
Disentanglement-PyTorch is a library for variational representation learning
We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.
We adapt a family of combinatorial games with tunable difficulty and an optimal policy expressible as linear network, developing it as a rich environment for reinforcement learning, showing contrasts in performance with supervised learning, and analyzing multiagent learning and generalization. 
An add-on method for deep learning to detect outliers during prediction-time
A fully connected architecture is used to produce word embeddings from character representations, outperforms traditional embeddings and provides insight into sparsity and dropout.
We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting
An empirical investigation of GAN-based alignment of word vector spaces, focusing on cases, where linear transformations provably exist, but training is unstable.
Our aim in this paper is to propose a new approach for tackling the problem of transfer learning from labeled to unlabeled software projects in the context of SVD in order to resolve the mode collapsing problem faced in previous approaches.
A faster method for generating node embeddings that employs a number of permutations over a node's immediate neighborhood as context to generate its representation.
We show how to initialize recurrent architectures with the closed-form solution of a linear autoencoder for sequences. We show the advantages of this approach compared to orthogonal RNNs.
We provide insightful understanding of sequence-labeling NER and propose to use two types of cross structures, both of which bring theoretical and empirical improvements.
We present a theoretically proven generative model of knowledge graph embedding. 
We study empirically how hard it is to recover missing parts of trained models
This paper proposes variational domain adaptation, a uniﬁed, scalable, simple framework for learning multiple distributions through variational inference
We propose a novel combination of adversarial training and provable defenses which produces a model with state-of-the-art accuracy and certified robustness on CIFAR-10. 
Programs have structure that can be represented as graphs, and graph neural networks can learn to find bugs on such graphs
We introduce and analyze several criteria for detecting overfitting.
We develop a point-based value iteration solver for POMDPs with active perception and planning tasks.
We introduce an underparameterized, nonconvolutional, and simple deep neural network that can, without training, effectively represent natural images and solve image processing tasks like compression and denoising competitively.
This paper 1) characterizes functions representable by ReLU DNNs, 2) formally studies the benefit of depth in such architectures,  3) gives an algorithm to implement empirical risk minimization to global optimality for two layer ReLU nets.
Benchmarks for biologically plausible learning algorithms on complex datasets and architectures
We have proposed using the recent GrOWL regularizer for simultaneous parameter sparsity and tying in DNN learning. 
An analysis of the learning and optimization structures of architecture search in neural networks and beyond.
We do lossless compression of large image datasets using a VAE, beat existing compression algorithms.
Bayesian optimization based online hyperparameter optimization.
In this paper, we propose the Latent Question Reformulation Network (LQR-net), a multi-hop and parallel attentive network designed for question-answering tasks that require reasoning capabilities.
Explaining Multivariate Time Series Models by finding important observations in time using Counterfactuals
We use self-supervision on both domain to align them for unsupervised domain adaptation.
The minimum of a set of exponentially distributed hashes has a very useful collision probability that generalizes the Jaccard Index to probability distributions.
A graph neural network model with parameters generated from natural languages, which can perform multi-hop reasoning. 
We present Meta-Critic, an auxiliary critic module for off-policy actor-critic methods that can be meta-learned online during single task learning.
We obtain non-vacuous generalization bounds on ImageNet-scale deep neural networks by combining an original PAC-Bayes bound and an off-the-shelf neural network compression method.
We propose an alternative measure for determining effectiveness of adversarial attacks in NLP models according to a distance measure-based method like incremental L2-gain in control theory.
We propose the Warped Residual Network using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. 
We propose a suite of metrics that capture desired properties of explainability algorithms and use it to objectively compare and evaluate such methods
A recent out-of-distribution detection method helps to measure the confidence of RNN predictions for some NLP tasks
depth-2-vs-3 separation for sigmoidal neural networks over general distributions
We propose a scalable method to approximate the eigenvectors of the Laplacian in the reinforcement learning context and we show that the learned representations can improve the performance of an RL agent.
Simulation to real images translation and video generation
We propose PocketFlow, an automated framework for model compression and acceleration, to facilitate deep learning models' deployment on mobile devices.
How to learn GANs from noisy, distorted, partial observations
See the abstract.  (For the revision, the paper is identical, except for a 59 page Supplementary Material, which can serve as a stand-along technical report version of the paper.)
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting.
We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces.
Using asynchronous gradient updates to accelerate dynamic neural network training
We study reward design problem in cooperative MARL based on packet routing environments. The experimental results remind us to be careful to design the rewards, as they are really important to guide the agent behavior.
Neumann networks are an end-to-end, sample-efficient learning approach to solving linear inverse problems in imaging that are compatible with the MSE optimal approach and admit an extension to patch-based learning.
GLMP: Global memory encoder (context RNN, global pointer) and local memory decoder (sketch RNN, local pointer) that share external knowledge (MemNN) are proposed to strengthen response generation in task-oriented dialogue.
We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.
We systematically analyze the convergence behaviour of popular gradient algorithms for solving bilinear games, with both simultaneous and alternating updates.
We use VAEs to learn a shared latent space embedding between image features and attributes and thereby achieve state-of-the-art results in generalized zero-shot learning.
Spatial information at last layers is not necessary for a good classification accuracy.
We use Siamese Networks to guide and disentangle the generation process in GANs without labeled data.
We present Predicted Variables, an approach to making machine learning a first class citizen in programming languages.
We show ways to train a hierarchical video prediction model without needing pose labels.
In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.
We employ linear homomorphic compression schemes to represent the sufficient statistics of a conditional random field model of coreference and this allows us to scale inference and improve speed by an order of magnitude.
We give a theoretical analysis of the measurement and optimization of mutual information.
By breaking the layer hierarchy, we propose a 3-step approach to the construction of neuron-hierarchy networks that outperform NAS, SMASH and hierarchical representation with fewer parameters and shorter searching time.
We propose an algorithm that automatically adjusts parameters of a simulation engine to generate training data for a neural network such that validation accuracy is maximized.
A model-agnostic regularization scheme for neural network-based conditional density estimation.
A patch-based bottleneck formulation in a VAE framework that learns unsupervised representations better suited for visual recognition.
To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.
A paper suggesting a method to transform the style of images using deep neural networks.
We improve existing dialogue systems for responding to people sharing personal stories, incorporating emotion prediction representations and also release a new benchmark and dataset of empathetic dialogues.
A new recurrent neural network architecture for detecting pairwise Granger causality between nonlinearly interacting time series. 
A control variate based stochastic training algorithm for graph convolutional networks that the receptive field can be only two neighbors per node.
Monte Carlo methods for quantizing pre-trained models without any additional training.
Information theoretical approach for unsupervised learning of unsupervised learning of a hybrid of discrete and continuous representations, 
It's important to consider optimization in function space, not just parameter space. We introduce a learning rule that reduces distance traveled in function space, just like SGD limits distance traveled in parameter space.
Convergence theory for biased (but consistent) gradient estimators in stochastic optimization and application to graph convolutional networks
We use snapshots from the training process to improve any uncertainty estimation method of a DNN classifier.
A new face image dataset for balanced race, gender, and age which can be used for bias measurement and mitigation
We attempt to model the drawing process of fonts by building sequential generative models of vector graphics (SVGs), a highly structured representation of font characters.
We develop 'dynamic neural relational inference', a variational autoencoder model that can explicitly and interpretably represent the hidden dynamic relations between neurons.
To the best of our knowledge, DeePa is the first deep learning framework that controls and optimizes the parallelism of CNNs in all parallelizable dimensions at the granularity of each layer.
We combine kernel method with connectionist models and show that the resulting deep architectures can be trained layer-wise and have more transparent learning dynamics. 
We propose a method to stochastically optimize second-order penalties and show how this may apply to training fairness-aware classifiers.
We show that deep learning network derivatives have a low-rank structure, and this structure allows us to use second-order derivative information to calculate learning rates adaptively and in a computationally feasible manner.
A unified min-max optimization framework for adversarial attack and defense
dimensionality reduction for cases where examples can be represented as soft probability distributions
We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of goal space representations, and evaluate how various implementations enable the discovery of a diversity of policies.
We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.
Compressing the word embeddings over 94% without hurting the performance.
OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work
A method for learning a transformation between one pair of source/target datasets and applying it a separate source dataset for which there is no target dataset
Combining Imitation Learning and Reinforcement Learning to learn to outperform the expert
An unsupervised learning approach for separating two structured signals from their superposition
We explore the relationship between Normalising Flows and Variational- and Denoising Autoencoders, and propose a novel model that generalises them.
We use reinforcement learning for query reformulation on two tasks and surprisingly find that when training multiple agents diversity of the reformulations is more important than specialisation.
A general framework for incorporating long-term safety constraints in policy-based reinforcement learning
Evaluating generative networks through their data augmentation capacity on discrimative models.
New: application of seq2seq modelling to automating sciene journalism; highly abstractive dataset; transfer learning tricks; automatic evaluation measure.
We present an approach to redesign the environment such that uninterpretable agent behaviors are minimized or eliminated.
We propose a new class of inference models that iteratively encode gradients to estimate approximate posterior distributions.
We present a learning rule for feedback weights in a spiking neural network that addresses the weight transport problem.
We combine variational inference and manifold learning (specifically VAEs and diffusion maps) to build a generative model based on a diffusion random walk on a data manifold; we generate samples by drawing from the walk's stationary distribution.
We develop a simple and general approach for avoiding interference between gradients from different tasks, which improves the performance of multi-task learning in both the supervised and reinforcement learning domains.
Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm
Generalized BERT for continuous and cross-modal inputs; state-of-the-art self-supervised video representations.
A generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode hard data structure assumptions. Applied to CLEVR VQA and expression evaluation.
We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.
We apply gradient based meta-learning to the graph domain and introduce a new graph specific transfer function to further bootstrap the process.
We propose a new method to incrementally train a mixture generative model to approximate the information projection of the real data distribution.
Recover videos from compressive measurements by learning a low-dimensional (low-rank) representation directly from measurements while training a deep generator. 
We study a multi-layer generalization of the magnitude-based pruning.
We introduce hypervolume maximization for training GANs with multiple discriminators, showing performance improvements in terms of sample quality and diversity. 
A new state-of-the-art on Imagenet for mobile setting
A high performance robotics simulation and algorithm development framework.
A novel unsupervised domain adaptation paradigm - performing adaptation without accessing the source data ('source-free') and without any assumption about the source-target category-gap ('universal').
Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially
We propose Answer-containing Sentence Generation (ASGen), a novel pre-training method for generating synthetic data for machine reading comprehension.
Soft quantization approach to learn pure fixed-point representations of deep neural networks
We present a novel network architecture for learning compact and efficient deep neural networks
We study the adversarial machine learning attacks against the Multiple Object Tracking mechanisms for the first time. 
Coupling semi-supervised learning with self-supervised learning and explicitly modeling the self-supervised task conditioned on the semi-supervised one
A pointer network architecture for re-ranking items, learned from click-through logs.
Stochastic gradient method with momentum generalizes.
Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.
We show the possibility of pruning to find a small sub-network with significantly higher convergence rate than the full model.
We propose a variational inference based approach for encouraging the inference of disentangled latents. We also propose a new metric for quantifying disentanglement. 
Our proposed ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them.
 We demonstrate a gated recurrent asynchronous spiking neural network that corresponds to an LSTM unit.
Efficient video classification using frame-based conditional gating module for selecting most-dominant frames, followed by temporal modeling and classifier.
Differentiable dynamic programming over perturbed input weights with application to semi-supervised VAE
Without learning, it is impossible to explain a machine learning model's decisions.
Simple and effective graph neural network with mixture of random walk steps and attention
We present an unsupervised deep learning reconstruction for imaging inverse problems that combines neural networks with model-based constraints.
We introduce a diagnostic task which is a variation of few-shot learning and introduce a dataset for it.
We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.
A temporally consistent and modality flexible unsupervised video-to-video translation framework trained in a self-supervised manner.
Argumentation frameworks are used to represent causality of plans/models to be utilized for explanations.
We propose a generative neural network approach for temporally coherent point clouds.
We present a unifying view on black-box adversarial attacks as a gradient estimation problem, and then present a framework (based on bandits optimization) to integrate priors into gradient estimation, leading to significantly increased performance.
Improving label efficiency through multi-task learning on auditory data
The paper presents two techniques to incorporate high level structure in generating procedural text from a sequence of images.
We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.
Ideal methodology to inject noise to input data during CNN training
We propose to explicitly model deep feature distributions of source and target data as Gaussian mixture distributions for Unsupervised Domain Adaptation (UDA) and achieve superior results in multiple UDA tasks than state-of-the-art methods.
We learn a task-agnostic world graph abstraction of the environment and show how using it for structured exploration can significantly accelerate downstream task-specific RL.
We represent a computer program using a set of simpler programs and use this representation to improve program synthesis techniques.
How can we build artificial agents that solve social dilemmas (situations where individuals face a temptation to increase their payoffs at a cost to total welfare)?
We propose a novel generalized transformation-based gradient model and propose a polynomial-based gradient estimator based upon the model.
semi-supervised and transfer learning on packet flow classification, via a system of cooperative or adversarial neural blocks
Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.
We propose a method for computing adversarially robust representations in an entirely unsupervised way.
We are proposing a new score-based approach to structure/causal learning leveraging neural networks and a recent continuous constrained formulation to this problem
Paper analyzes the problem of designing adversarial attacks against multiple classifiers, introducing algorithms that are optimal for linear classifiers and which provide state-of-the-art results for deep learning.
We present general closed loop analysis for Markov potential games and show that deep reinforcement learning can be used for learning approximate closed-loop Nash equilibrium.
We scale up lossless compression with latent variables, beating existing approaches on full-size ImageNet images.
We hypothesize that the vulnerability of image models to small adversarial perturbation is a naturally occurring result of the high dimensional geometry of the data manifold. We explore and theoretically prove this hypothesis for a simple synthetic dataset.
We introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide fast task inference through the successor features framework.
A domain adaptation method for structured output via learning patch-level discriminative feature representations
A novel adversarial attack that can directly attack real-world black-box machine learning models without transfer.
User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.
Training convnets with mixed image size can improve results across multiple sizes at evaluation
The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.
We propose to structure the generator of a GAN to consider objects and their relations explicitly, and generate images by means of composition
We manage to emerge communication with selfish agents, contrary to the current view in ML
we propose a new framework for data-dependent DNN regularization that can prevent DNNs from overfitting random data or random labels.
Multi-task learning improves word-and-character-level speech recognition by interpolating the preference biases of its components: frequency- and word length-preference.
We learn a neural network that uniformizes the input distribution, which leads to competitive indexing performance in high-dimensional space
This paper provides a rigorous study of the variance reduced TD learning and characterizes its advantage over vanilla TD learning
A Multiflow Network is a dynamic architecture for domain adaptation that learns potentially different computational graphs per domain, so as to map them to a common representation where inference can be performed in a domain-agnostic fashion.
IMPACT helps RL agents train faster by decreasing training wall-clock time and increasing sample efficiency simultaneously.
This paper introduces a coloring scheme for node disambiguation in graph neural networks based on separability, proven to be a universal MPNN extension.
Representing melodies as images with semantic units aligned we can generate them using a DCGAN without any recurrent components.
We introduce and analyze the phenomenon of "hallucinations" in NMT, or spurious translations unrelated to source text, and propose methods to reduce its frequency.
An evaluation framework based on a real-world neural network for post-hoc explanatory methods
We present a neuroscience-inspired method based on neural networks for latent space search
By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.
A cloze test dataset designed by teachers to assess language proficiency
Artificial neural networks trained with gradient descent are capable of recapitulating both realistic neural activity and the anatomical organization of a biological circuit.
Prune and ReLU in Winograd domain for efficient convolutional neural network
A new algorithm to train deep neural networks. Tested on optimization functions and MNIST.
We introduce flipout, an efficient method for decorrelating the gradients computed by stochastic neural net weights within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example.
A new model Latently Invertible Autoencoder is proposed to solve the problem of variational inference in VAE using the invertible network and two-stage adversarial training.
We introduce the PHP model for hierarchical representation of neural programs, and an algorithm for learning PHPs from a mixture of strong and weak supervision.
We propose a method of transferring knowledge between related RL tasks using visual mappings, and demonstrate its effectiveness on visual variants of the Atari Breakout game and different levels of Road Fighter, a Nintendo car driving game.
Adaptive gradient methods, when done right, do not incur a generalization penalty. 
We introduce a model which generalizes quickly from few observations by storing surprising information and attending over the most relevant data at each time point.
We develop an end-to-end trainable approach for skimming, rereading and early stopping applicable to classification tasks. 
We view exploration in RL as a problem of matching a marginal distribution over states.
We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.
Proposing a novel object localization(detection) approach based on interpreting the deep CNN using internal representation and network's thoughts
Trellis networks are a new sequence modeling architecture that bridges recurrent and convolutional models and sets a new state of the art on word- and character-level language modeling.
High object-detection accuracy can be obtained by training domain specific compact models and the training can be very short.
We compare deep model-based and model-free RL algorithms by studying the approximability of $Q$-functions, policies, and dynamics by neural networks. 
We introduce a novel approach to common-sense physical reasoning that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion
A very strong bias towards simple outpouts is observed in many simple input-ouput maps. The parameter-function map of deep networks is found to be biased in the same way.
In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning: probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals.
Novel architecture of memory based attention mechanism for multi-agent communication.
This work enforces Hamiltonian dynamics with control to learn system models from embedded position and velocity data, and exploits this physically-consistent dynamics to synthesize model-based control via energy shaping.
We investigate the internal reasons of our observations, the diminishing effects of the well-known hyperparameter optimization methods on federated learning from decentralized non-IID data.
A novel adversarial imitation attack to fool machine learning models.
Large batch size training using adversarial training and second order information
first deep neural network for modeling Egocentric Spatial Memory inspired by neurophysiological discoveries of navigation cells in mammalian brain
We prove generalization of DNNs by adding a Lipschitz regularization term to the training loss. We resolve a question posed in Zhang et al. (2016).
We train wide residual networks that can be immediately deployed using only a single bit for each convolutional weight, with signficantly better accuracy than past methods.
Immersive Visualization of the Classical Non-Euclidean Spaces using Real-Time Ray Tracing.
We propose the set autoencoder, a model for unsupervised representation learning for sets of elements.
We propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt.
We present evidence that LMs do capture common sense with state-of-the-art results on both Winograd Schema Challenge and Commonsense Knowledge Mining.
An online algorithm for cost-aware feature acquisition and prediction
We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.
Training DNNs to interface w\ black box functions w\o intermediate labels by using an estimator sub-network that can be replaced with the black box after training
We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.
A robust domain adaptation by employing a task specific loss in cyclic adversarial learning
Policy optimization by using past good rollouts from the agent; learning shaped rewards via divergence minimization; SVPG with JS-kernel for population-based exploration.
We study the functioning of autoencoders in a simple setting and advise new strategies for their regularisation in order to obtain bettre generalisation with latent interpolation in mind for image sythesis. 
We present a simple idea that allows to record a speaker in a given language and synthesize their voice in other languages that they may not even know.
In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.
A system for rewriting text conditioned on multiple controllable attributes
We develop a new optimization approach for vanilla ReLU-based RNN that enables long short-term memory and identification of arbitrary nonlinear dynamical systems with widely differing time scales.
Propose an improved framework for WGANs and demonstrate its better performance in theory and practice.
Operations in the GAN latent space can induce a distribution mismatch compared to the training distribution, and we address this using optimal transport to match the distributions. 
A new way of learning semantic program embedding
We propose a extension of the batch normalization, show a first-of-its-kind convergence analysis for this extension and show in numerical experiments that it has better performance than the original batch normalizatin.
We propose a framework for modifying the latent space operations such that the distribution mismatch between the resulting outputs and the prior distribution the generative model was trained on is fully eliminated.
This paper provides a multi -stream end to end approach to learn unified embeddings for query-response pairs in dialogue systems by leveraging contextual, syntactic, semantic and external information together.
Techniques for combining generalized policies with search algorithms to exploit the strengths and overcome the weaknesses of each when solving probabilistic planning problems
We obtain state-of-the-art on robustness to data shifts, and we maintain calibration under data shift even though even when accuracy drops
We automatically extract fingering information from videos of piano performances, to be used in automatic fingering prediction models.
SOTA on unsupervised domain adaptation by leveraging the cluster assumption.
Graph generative models based on generalization of message passing to continuous time using ordinary differential equations 
We show that several claims of the information bottleneck theory of deep learning are not true in the general case.
Neural nets have large gradients by design; that makes them adversarially vulnerable.
We introduce amortized proximal optimization (APO), a method to adapt a variety of optimization hyperparameters online during training, including learning rates, damping coefficients, and gradient variance exponents.
Without requiring any constraints or post-processing, we show that the salient dimensions of word vectors can be interpreted as semantic features. 
strategy to repair damaged neural networks
Automatic question generation from paragraph using hierarchical models
Querying a black-box neural network reveals a lot of information about it; we propose novel "metamodels" for effectively extracting information from a black box.
Using a supervised latent variable modeling framework to determine reward in inverse reinforcement learning task
This paper combines Monte Carlo tree search with 2-opt local search in a variable neighborhood mode to solve the TSP effectively.
Applying program synthesis to the tasks of image completion and generation within a deep learning framework
This paper presents a computational model for efficient human postural control adaptation based on hierarchical acquisition functions with well-known features. 
We study the problem of continuous control agents in deep RL with adversarial attacks and proposed a two-step algorithm based on learned model dynamics. 
We study the sparsity-inducing bias of deep models, caused by their learning dynamics.
Parametric adversarial divergences implicitly define more meaningful task losses for generative modeling, we make parallels with structured prediction to study the properties of these divergences and their ability to encode the task of interest.
We provide a rigorous comparison of different Graph Neural Networks for graph classification.
Automatic Learning of data augmentation using a GAN based architecture to improve an image classifier
New Class of Autoencoders with pseudo invertible architecture
We exploit an inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework applicable to many topologies.
Comparison of siamese neural networks, GANs, and VAT for few shot learning. 
We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve SotA on SQuAD and adversarial SQuAD.
Adversarially trained hierarchical generative model with robust and semantically learned latent representation.
We observe that numerical PDE solvers can be regarded as Markov Desicion Processes, and propose to use Reinforcement Learning to solve 1D scalar Conservation Laws
We introduce a neural rendering architecture that helps VAEs learn disentangled latent representations.
Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.
We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search.
Improving the performance of an RL agent in the continuous action and state space domain by using prioritised experience replay and parameter noise.
show multi-channel attention weight contains semantic feature to solve natural language inference task.
We approximate Determinantal Point Processes with neural nets; we justify our model theoretically and empirically.
This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature bundle adjustment (BA)
We show that adding a constraint to TD updates stabilizes learning and allows Deep Q-learning without a target network
We propose DuoRC, a novel dataset for Reading Comprehension (RC) containing 186,089 human-generated QA pairs created from a collection of 7680 pairs of parallel movie plots and introduce a RC task of reading one version of the plot and answering questions created from the other version; thus by design, requiring complex reasoning and deeper language understanding to overcome the poor lexical overlap between the plot and the question.
A model-based planning component improves RL-based semantic parsing on WikiTableQuestions.
A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.
We demonstrate that pruning methods which introduce greater instability into the loss also confer improved generalization, and explore the mechanisms underlying this effect.
Less biologically implausible deep neural networks trained without weight transport can be harder to fool.
A generative model for reaction prediction that learns the mechanistic electron steps of a reaction directly from raw reaction data.
Incorporating the ability to say I-don't-know can improve the fairness of a classifier without sacrificing too much accuracy, and this improvement magnifies when the classifier has insight into downstream decision-making.
An approach to perform HTN planning using external procedures to evaluate predicates at runtime (semantic attachments).
Max-pooled word vectors with fuzzy Jaccard set similarity are an extremely competitive baseline for semantic similarity; we propose a simple dynamic variant that performs even better.
The overall goal of this work is to enable sample-efficient imitation from expert demonstrations, both with and without the provision of expert action labels, through the use of f-divergences.
With a cognitive brain-machine interface, we show a direct link between attentional effects on perceptual accuracy and neural gain in EEG-SSVEP power, in the human brain.
A general and easy-to-use framework that improves the adversarial robustness of deep classification models through embedding regularization.
We propose a matrix-completion based task clustering algorithm for deep multi-task and few-shot learning in the settings with large numbers of diverse tasks.
This approach overcomes scalability issues and implies novel mathematical connections among quantum many-body physics, quantum information theory, and machine learning.
We train a combination of neural networks to predict optimal trajectories for complex physical systems.
We provide a PAC-Bayes based generalization guarantee for uncompressed, deterministic deep networks by generalizing noise-resilience of the network on the training data to the test data.
We prove that for a large class of functions f there exists an interval certified robust network approximating f up to arbitrary precision.
This is a work aiming for boosting all the existing pruning and mimic method.
We introduce an extra data-dependent Gaussian prior objective to augment the current MLE training, which is designed to capture the prior knowledge in the ground-truth data.
We propose an interactive approach for classifying natural language queries by asking users for additional information using information gain and a reinforcement learning policy controller.
Convolutional autoencoders generalized to mesh surfaces for encoding and reconstructing extreme 3D facial expressions.
Jiffy is a convolutional approach to learning a distance metric  for multivariate time series that outperforms existing methods in terms of nearest-neighbor classification accuracy.
Extendable Modular Architecture is proposed for developing of variety of Agent Behaviors in DQN.
We isolate one factor of RL generalization by analyzing the case when the agent only overfits to the observations. We show that architectural implicit regularizations occur in this regime.
In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model.
We proposed a comprehensive approach for unsupervised embedding learning on the basis of AND algorithm.
We propose a weakly supervised learning method for the classification and localization of cancers in extremely high resolution histopathology whole slide images using only image-wide labels.
 We propose a new method for using ontology information to improve performance on massively multi-label prediction/classification problems.
We apply a greedy assignment on the projected samples instead of sorting to approximate Wasserstein distance
This paper studies the interactions between the fast-learning and slow-prediction models and demonstrate how such interactions can improve machine capability to solve the joint lifelong and few-shot learning problems.
The first principled weight initialization method for hypernetworks
A novel Bayesian deep learning framework that captures and relates hierarchical semantic and visual concepts, performing well on a variety of image and text modeling and generation tasks.
This paper introduces partial grounding to tackle the problem that arises when the full grounding process, i.e., the translation of a PDDL input task into a ground representation like STRIPS, is infeasible due to memory or time constraints.
Introducing the response charactrization method for interpreting cell dynamics in learned long short-term memory (LSTM) networks. 
To our knowledge, this is the first study to show how neural representations of space, including grid-like cells and border cells as observed in the brain, could emerge from training a recurrent neural network to perform navigation tasks.
We match the performance of spectrogram based model with a model trained end-to-end in the waveform domain
We provide a scalable solution to multi-agent evaluation with linear rate complexity in both time and memory in terms of number of agents
We propose a novel semi-supervised learning approach with SOTA performance on combating learning with noisy labels.
We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks
Effective model poisoning attacks on federated learning able to cause high-confidence targeted misclassification of desired inputs
 In this paper, we hypothesize that superficially perturbed data points shouldn’t merely map to the same class---they should map to the same representation.
harmonic acoustic model
Address the trade-off caused by the dependency of classes on domains by improving domain adversarial nets
We propose FVD: a new metric for generative models of video based on FID. A large-scale human study confirms that FVD correlates well with qualitative human judgment of generated videos.
A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.
Language modeling for lifelong language learning.
We use ideas from quantum computing to proposed word embeddings that utilize much fewer trainable parameters.
We train neural network agents to develop a language with compositional properties from raw pixel input.
We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.
Representations from language models consistently perform better than translation encoders on syntactic auxiliary prediction tasks.
We propose surrogate based Constrained Langevin sampling with application in nano-porous material configuration design.
Improve hierarchical embedding models using kernel smoothing
Augmented bootstrapping approach combining information from a reference set with iterative refinements of soft labels to improve Name Entity Recognition from biomedical literature.
We extend quantum SVMs to semi-supervised setting, to deal with the likely problem of many missing class labels in huge datasets.
This paper bridges deep network architectures with numerical (stochastic) differential equations. This new perspective enables new designs of more effective deep neural networks.
CNN and LSTM to generate markup-like code describing graphical user interface images.
We show that hyperbolic embeddings are useful for high-level computer vision tasks, especially for few-shot classification.
We present a method to learn interpretable representations on time series using ideas from variational autoencoders, self-organizing maps and probabilistic models.
Convolutional architecture for learning data-dependent weights for autoregressive forecasting of time series.
We present a novel interpretation of MixUp as belonging to a class highly analogous to adversarial training, and on this basis we introduce a simple generalization which outperforms MixUp
Handling Uncertainty in Visual Perception for Plan Recognition
Differentiable multi-hop access to a textual knowledge base of indexed contextual representations
Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.
We present a mixed media assembly tutorial authoring system that streamlines creation of videos, images, text and dynamic instructions in situ.
We demostarte that using clinical notes in conjuntion with ICU instruments data improves the perfomance on ICU management benchmark tasks
We propose an event-based policy gradient  to train the leader and an action abstraction policy gradient to train the followers in leader-follower Markov game.
We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.
We propose a SVD based method to explore the local dimension of activation manifold in deep neural networks.
Inference in large Transformers is expensive due to the self-attention in multiple layers. We show a simple decomposition technique can yield a faster, low memory-footprint model that is just as accurate of the original models.
A Deep Learning adaptation of Randomized Least Squares Value Iteration
Parameters of a trained neural network can be permuted to produce a completely separate model for a different task, enabling the embedding of Trojan horse networks inside another network.
We introduce an alternative GAN design based on random routes in generator, which can serve as a tool for generative models interpretability.
We present a theoretical and experimental framework for defining, understanding, and achieving generalization, and as a result robustness, in deep learning by drawing on algorithmic information theory and coding theory.
We cast causal structure discovery as a Bayesian model selection in a way that allows us to discriminate between Markov equivalent graphs to identify the unique causal graph.
Lower bound for compressed sensing w/ generative models that matches known upper bounds
This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.
We design incremental sequence-to-action parsers for text-to-SQL task and achieve SOTA results. We further improve by using non-deterministic oracles to allow multiple correct action sequences. 
An approach that speeds up neural architecture search by 10x, whilst using 100x less computing resource.
We propose a new DNN architecture for deep learning on tabular data
A framework that conducts online refinement of pseudo labels with a novel soft softmax-triplet loss for unsupervised domain adaptation on person re-identification.
We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.
This work presents a method of generating and using ensembles effectively to identify noisy examples in the presence of annotation noise. 
A data-driven learning algorithm based on unrolling the Alternating Minimization optimization for sparse graph recovery.
We proposed a double neural framework to solve large-scale imperfect information game. 
We present the first verification that a neural network for perception tasks produces a correct output within a specified tolerance for every input of interest. 
We study rate distortion approximations for evaluating deep generative models, and show that rate distortion curves provide more insights about the model than the log-likelihood alone while requiring roughly the same computational cost.
A model-based meta-RL algorithm that enables a real robot to adapt online in dynamic environments
The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.
We analyze the expressive power of the connections used in DenseNets via tensor decompositions.
We use implicit human feedback (via error-potentials, EEG) to accelerate and optimize the training of a DRL algorithm, in a practical manner.
TCN for multimodal semi-supervised learning + ablation study of its mechanisms + interpretations of latent representations
Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. 
Defending Against Physically Realizable Attacks on Image Classification
Hebbian plastic weights can behave as a compressed episodic memory storage in neural networks and with the combination of task-specific synaptic consolidation can improve the ability to alleviate catastrophic forgetting in continual learning.
This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.
We introduce the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants and general SMT formulas.
Our finding shed lights in preventing cancer progression
We extend autoregressive flows and RealNVP to discrete data.
A noise robust deep learning architecture.
We learn neural embeddings of graphs in hyperbolic instead of Euclidean space
Ideas for future ICKEPS
We generate Wikipedia articles abstractively conditioned on source document text.
An algorithm for unifying SGD and Adam and empirical study of its performance
Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information
Traditional image processing algorithms are combined with Convolutional Neural Networks，a new neural network.
We proposed a specific back-propagation method via proper spectral sub-gradient to integrate determinantal point process to deep learning framework.
We invent a novel cluster-to-cluster framework for NMT training, which can better understand the both source and target language diversity.
An efficient differentiable ILP model that learns first-order logic rules that can explain the data.
We introduce a new method for synthesizing adversarial examples robust in the physical world and use it to fabricate the first 3D adversarial objects.
Learn how to permute a set, then encode permuted set with RNN to obtain a set representation.
Use deep reinforcement learning to design the physical attributes of a robot jointly with a control policy.
We propose an approach that endows a single model with the ability to represent both extremes: joint training and independent training, which leads to effective multi-task learning.
We propose a regularization term that, when added to the reinforcement learning objective, allows the policy to maximize the reward and simultaneously learn to be invariant to the irrelevant changes within the input..
This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.
By combining ideas from traditional algorithms design and reinforcement learning, we introduce a novel framework for learning algorithms that solve online combinatorial optimization problems.
A functional approach reveals that flat initialization, preserved by gradient descent, leads to generalization ability.
Network depth increases outlier eigenvalues in the Hessian. Residual connections mitigate this.
An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.
A novel approach to curriculum learning by incrementally learning labels and adaptively smoothing labels for mis-classified samples which boost average performance and decreases standard deviation.
We propose a method to deal with rare words by computing their embedding from definitions.
This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.
Show that age confounds cognitive impairment detection + solve with fair representation learning + propose metrics and models.
We introduce NetScore, new metric designed to provide a quantitative assessment of the balance between accuracy, computational complexity, and network architecture complexity of a deep neural network.
ordered Top-k adversarial attacks
Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.
The choice of the hub (target) language affects the quality of cross-lingual embeddings, which shouldn't be evaluated only  on English-centric dictionaries.
Typical GAN training doesn't optimize Jensen-Shannon, but something like a reverse KL divergence.
We show that by drawing multiple samples (predictions) per input (datapoint), we can learn with less data as we freely obtain a REINFORCE baseline.
We efficiently solve multi-task problems with an automatic curriculum generation algorithm based on a generative model that tracks the learning agent's performance.
This paper identifies classes of problems for which adversarial examples are inescapable, and derives fundamental bounds on the susceptibility of any classifier to adversarial examples. 
Lowering precision (to 4-bits, 2-bits and even binary) and widening the filter banks gives as accurate network as those obtained with FP32 weights and activations.
We embed a CRF in a VAE of tokens and NER tags for semi-supervised learning and show improvements in low-resource settings.
A principled framework for model quantization using the proximal gradient method.
A new generative modeling technique based on asymmetrical adversarial training, and its applications to adversarial example detection and robust classification
Meta-learning curiosity algorithms by searching through a rich space of programs yields novel mechanisms that generalize across very different reinforcement-learning domains.
This paper proposes a simple procedure for evaluating compositional structure in learned representations, and uses the procedure to explore the role of compositionality in four learning problems.
A DL model for RNA secondary structure prediction, which uses an unrolled algorithm in the architecture to enforce constraints.
We present eligibility propagation an alternative to BPTT that is compatible with experimental data on synaptic plasticity and competes with BPTT on machine learning benchmarks.
Extracting a finite state machine from a recurrent neural network via quantization for the purpose of interpretability with experiments on Atari.
We investigate the theoretical and practical evidence of on-policy reinforcement learning improvement by reusing the data from several consecutive policies.
We use non-Euclidean Fourier Transformation of shapes defined by a simplicial complex for deep learning, achieving significantly better results than point-based sampling techiques used in current 3D learning literature.
We apply reinforcement learning to score-based causal discovery and achieve promising results on both synthetic and real datasets
We proposed a universal method which can be used in the data preprocessing stage to generate the more meaningful topic that better represents the given document
We propose a novel multi-task learning framework which extracts multi-view dependency relationship automatically and use it to guide the knowledge transfer among different tasks.
Testing of global translational invariance in Convolutional and Capsule Networks
We develop an analytical method to study Bayesian inference of finite-width neural networks and find that the renormalization-group flow picture naturally emerges.
theoretically understand the regularization effect of distillation. We show that early stopping is essential in this process. From this perspective, we developed a distillation method for learning with corrupted Label with theoretical guarantees.
a novel approach for online lifelong learning using output kernels.
We model a house-construction scenario in Minecraft in classical and HTN planning and compare the advantages and disadvantages of both kinds of models.
We present a framework for evaluating adversarial examples in natural language processing and demonstrate that generated adversarial examples are often not semantics-preserving, syntactically correct, or non-suspicious.
Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.
The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.
We tackle goal-conditioned tasks by combining Hindsight Experience Replay and Imitation Learning algorithms, showing faster convergence than the first and higher final performance than the second.
We derive a new PAC-Bayesian Bound for unbounded loss functions (e.g. Negative Log-Likelihood). 
We propose a simple self-supervised data augmentation technique which improves performance of fully-supervised scenarios including few-shot learning and imbalanced classification.
A two branch LSTM based network architecture learns the representation and dynamics of 3D meshes of numerical crash simulations.
Feature vectors from SoundNet can predict brain activity of subjects watching a movie in auditory and language related brain regions.
We propose a generative autoencoder that can learn expressive posterior and conditional likelihood distributions using implicit distributions, and train the model using a new formulation of the ELBO.
Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.
Spiking recurrent neural networks performing a working memory task utilize long heterogeneous timescales, strikingly similar to those observed in prefrontal cortex.
 In this paper, we address the problem of Low-shot network-expansion learning
We introduce a model of human question asking that combines neural networks and symbolic programs, which can learn to generate good questions with or without supervised examples.
A visual understanding mechanism for special environment
Better adversarial training by learning to map back to the data manifold with autoencoders in the hidden states.  
We show that minimizing the cross-entropy loss by using a gradient method could lead to a very poor margin if the features of the dataset lie on a low-dimensional subspace.
A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.
Surprising negative results on Model Based + Model deep RL
Inspired by Information Bottleneck theory,  we propose a new architecture of GAN for a disentangled representation learning
Explain bias situation with MMD GANs; MMD GANs work with smaller critic networks than WGAN-GPs; new GAN evaluation metric.
A semi-supervised multi-modal classification framework, TCN, that outperforms various benchmarks.
An iterative neural method for extracting signals that are only observed mixed with other signals
Physiological signal embeddings for prediction performance and hospital transference with a general Shapley value interpretability method for stacked models.
We present a provable algorithm for exactly recovering both factors of the dictionary learning model. 
Data Augmented Relation Extraction with GPT-2
An approximate inference algorithm for deep learning
We propose a novel manifold regularization strategy based on adversarial training, which can significantly improve the performance of semi-supervised learning.
A novel method for supervised learning through subdivisioning the input space along with function approximation.
Adversarial attacks on unsupervised node embeddings based on eigenvalue perturbation theory.
We present a novel algorithm for hierarchical subtask discovery which leverages the multitask linear Markov decision process framework.
We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. 
We introduce a novel method to train Seq2Seq models with language models that converge faster, generalize better and can almost completely transfer to a new domain using less than 10% of labeled data.
We introduce the online meta learning problem setting to better capture the spirit and practice of continual lifelong learning.
Attention weights don't fully expose what BERT knows about syntax.
We propose a novel face super resolution method that explicitly incorporates 3D facial priors which grasp the sharp facial structures.
Self-training with different views of the input gives excellent results for semi-supervised image recognition, sequence tagging, and dependency parsing.
A non-reversible way of making accept/reject decisions can be beneficial
We provide a new framework for MAML in the ES/blackbox setting, and show that it allows deterministic and linear policies, better exploration, and non-differentiable adaptation operators.
We present a software framework for transforming distributions and demonstrate its flexibility on relaxing mean-field assumptions in variational inference with the use of coupling flows to replicate structure from the target generative model.
Adversarial Domain adaptation and Multi-domain learning: a new loss to handle multi- and single-domain classes in the semi-supervised setting.
A learning network which generalizes the MLP framework to perform distribution-to-distribution regression
Proposed methods for time-dependent event representation and regularization for sequence prediction; Evaluated these methods on five datasets that involve a range of sequence prediction tasks.
We develop a method for stable offline reinforcement learning from logged data. The key is to regularize the RL policy towards a learned "advantage weighted" model of the data.
This paper presents a deep learning model that combines self-organizing maps and convolutional neural networks for representation learning of multi-omics data
Sparsification as fine-tuning of language models
Method for addressing covariate shift in imitation learning using ensemble uncertainty
We use supervised finetuning of feature vectors to improve transfer from simulation to the real world
We introduce the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended to policy gradient methods.
We propose a new video understanding benchmark, with tasks that by-design require temporal reasoning to be solved, unlike most existing video datasets.
We propose an efficient and robust asynchronous federated learning algorithm on the existence of stragglers
Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.
New methodology for variational marginal inference of permutations based on Sinkhorn algorithm, applied to probabilistic identification of neurons
We propose the first attack-independent robustness metric, a.k.a CLEVER, that can be applied to any neural network classifier.
This paper proposes a spontaneous and self-organizing communication (SSoC) learning scheme for multi-agent RL tasks.
On using BERT as an encoder for sequential prediction of labels in multi-label text classification task
DNN and Encoder enhanced FM with bilinear attention and max-pooling for CTR
Dropout based Bayesian inference is extended to deal with multi-modality and is evaluated on scene anticipation tasks.
We introduce a new type of conditional GAN, which aims to leverage structure in the target space of the generator. We augment the generator with a new, unsupervised pathway to learn the target structure. 
In this paper, we propose a novel regularized adversarial training framework ATLPA,namely Adversarial Tolerant Logit Pairing with Attention.
We address the problem of generalization of reinforcement learning to unseen action spaces.
Learn in temporal point processes by modeling the conditional density, not the conditional intensity.
A deep model for topic modelling
Novel adaptive instance normalization based GAN framework for non parallel many-to-many and zero-shot VC. 
This work propose Sparse Transformer to improve the concentration of attention on the global context through an explicit selection of the most relevant segments for sequence to sequence learning. 
Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.
Redistributing and growing weights according to the momentum magnitude enables the training of sparse networks from random initializations that can reach dense performance levels with 5% to 50% weights while accelerating training by up to 5.6x.
The loss surface of neural networks is a disjoint union of regions where every local minimum is a global minimum of the corresponding region.
We distill language models representations for syntax by unsupervised metric learning
We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals.
We propose a novel architecture that traverses an image pyramid in a top-down fashion, while it visits only the most informative regions along the way.
We use a hypernetwork to predict optimal weights given hyperparameters, and jointly train everything together.
We propose a new metric for evaluating conditional GANs that captures image quality, conditional consistency, and intra-conditioning diversity in a single measure.
We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.
We propose Message Passing Encoder-Decode networks for a fast and accurate way of modelling label dependencies for multi-label classification.
We propose a method of doing few-shot regression by learning a set of basis functions to represent the function distribution.
We propose a model-agnostic way to leverage BERT for text generation and achieve improvements over Transformer on 2 tasks over 4 datasets.
CNN-F extends CNN with a feedback generative network for robust vision.
It is shown that ResNet-type CNNs are a universal approximator and its expression ability is not worse than fully connected neural networks (FNNs) with a \textit{block-sparse} structure even if the size of each layer in the CNN is fixed.
A novel few shot learning method to generate query-specific classification weights via information maximization.
A neural method for conversational question answering with attention mechanism and a novel usage of BERT as contextual embedder
alternative to gradient penalty
automatic search for multi-task architectures that reduce per-task feature use
We propose nearest neighbor overlap, a procedure which quantifies similarity between embedders in a task-agnostic manner, and use it to compare 21 sentence embedders.
We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.
A new method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a life-long learning property.
We learn feature maps invariant to translation, and equivariant to rotation and scale.
A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from approximate inference and curvature estimation.
Automate machine learning system with efficient search algorithm and innovative structure to provide better model baselines.
We propose principled batch Bayesian experimental design strategies and a method for uncertainty quantification of the posterior summaries in a Gaussian process surrogate-based approximate Bayesian computation framework.
We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.
We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.
We analysis and solve the non-convergence issue of Adam.
In this paper we propose generative method for multisource domain adaptation based on decomposition of content, style and domain factors.
Using annealed importance sampling on the co-generation problem. 
We develop a new likelihood-free parameter estimation method that is equivalent to maximum likelihood under some conditions
Our method infers constraints on task execution by leveraging the principle of maximum entropy to quantify how demonstrations differ from expected, un-constrained behavior.
System to learn robotic tasks in the real world with reinforcement learning without instrumentation
We use a Variational Autoencoder to separate style and content, and achieve voice conversion by modifying style embedding and decoding. We investigate using a multi-language speech corpus and investigate its effects.
The paper proposes a method for forcing CNNs to leverage spatial attention in learning more object-centric representations that perform better in various respects.
Recurrent neural networks for Cybersecurity use-cases
Input Structuring along Chaos for Stability
We address training GANs with discrete data by formulating a policy gradient that generalizes across f-divergences
Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.
We propose an active multitask learning algorithm that achieves knowledge transfer between tasks.
We learn an efficient lossy image codec that can be optimized to facilitate reliable photo manipulation detection at fractional cost in payload/quality and even at low bitrates.
This paper proposes an effective generic sequence model which leverages the strengths of both RNNs and Multi-head attention.
A structured latent-variable approach that adds discrete control states within a standard autoregressive neural paradigm to provide arbitrary grounding of internal model decisions, without sacrificing any representational power of neural models.
Estimation of training data distribution from trained classifier using GAN.
We establish that the scaling laws derived in (Bora et al., 2017) are optimal or near-optimal in the absence of further assumptions.
meta-learn a learning algorithm capable of causal reasoning
Corpus based Algorithm is developed generate Amharic Sentiment lexicon relying on corpus
We augment the Q-value estimates with a count-based bonus that ensures optimism during action selection and bootstrapping, even if the Q-value estimates are pessimistic.
We propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework.
Distribution matching through divergence minimization provides a common ground for comparing adversarial Maximum-Entropy Inverse Reinforcement Learning methods to Behaviour Cloning.
We propose a generic framework that allows for exploiting the low-rank structure in both planning and deep reinforcement learning.
A benchmark to evaluate neural embeddings of identifiers in source code.
Rearranging the terms in maximum mean discrepancy yields a much better loss function for the discriminator of generative adversarial nets
This paper finds algorithms that directly use lossless compressed representations of deep feedforward networks, to perform inference without full decompression.
We cast GANs in the variational inequality framework and import techniques from this literature to optimize GANs better; we give algorithmic extensions and empirically test their performance for training GANs.
Addressing task heterogeneity problem in meta-learning by introducing meta-knowledge graph
 A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.
An automatic method for converting music between instruments and styles
We propose several new attacks and a methodology to measure robustness against unforeseen adversarial attacks.
Deep-Net: Deep Neural Network for Cyber Security Use Cases
We learn a space of motor primitives from unannotated robot demonstrations, and show these primitives are semantically meaningful and can be composed for new robot tasks.
We demonstrate the feasibility of a weakly supervised time series classification approach for wearable sensor data. 
We propose a local-to-global alignment framework to learn semantic correspondences from noisy data-text pairs with weak supervision
Learning how to reach goals from scratch by using imitation learning with data relabeling
In the paper, we proposed an ensemble method called InterBoost for training neural networks for small-sample classification. The method has better generalization performance than other ensemble methods, and reduces variances significantly.
We detect statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights.
We benchmark the neural linear model on the UCI and UCI "gap" datasets.
We reproduced AlphaZero on Google Cloud Platform
We establish global convergence to optimality for IPM-based GANs where the generator is an overparametrized neural network. 
We develop efficient multi-scale approximate attributed network embedding procedures with provable properties.
 A detailed empirical study in few-shot classification that revealing challenges in standard evaluation setting and showing a new direction.
We present a Bayesian inference model to infer contrastive explanations (as LTL specifications) describing how two sets of plan traces differ.
Tropical geometry can be leveraged to represent the decision boundaries of neural networks and bring to light interesting insights.
A novel Gram-Gauss-Newton method to train neural networks, inspired by neural tangent kernel and Gauss-Newton method, with fast convergence speed both theoretically and experimentally.
We investigate the implicit syntactic knowledge of sentence embeddings using a new analysis set of grammatically annotated sentences with acceptability judgments.
We propose an extension of multi-output learning to a continuum of tasks using operator-valued kernels.
We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.
We propose a data augmentation approach for meta-learning and prove that it is valid.
A general framework for distilling Bayesian posterior expectations for deep neural networks.
In this paper, we introduce a discrete hierarchy of categorical latent variables that we train using the Concrete/Gumbel-Softmax relaxation and we derive an upper bound for the absolute difference between the unbiased and the biased objective.
We propose a new class of optimizers for accelerated non-convex optimization via a nonlinear gradient transformation. 
Solve tasks involving vision-guided humanoid locomotion, reusing locomotion behavior from motion capture data.
We propose Gated Linear Unit networks — a model that performs similarly to ReLU networks on real data while being much easier to analyze theoretically.
We propose an architecture search method to identify a distribution of architectures and use it to construct a Bayesian ensemble for outlier detection.
A novel approach that detects outliers from image data,  while preserving the classification accuracy of image classification
This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources.
We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space in an explainable, robust, and geometrically coherent way.
We introduce a new gradient detach based complementary objective training strategy for domain adaptive object detection.
We propose a novel approach for connecting task-specific networks in a multi-task learning setting based on recent residual network advances.
How to use cross-entropy loss for zero shot learning with soft labeling on unseen classes : a simple and effective solution that achieves state-of-the-art performance on five ZSL benchmark datasets.
Progressively growing the available action space is a great curriculum for learning agents
A game-theoretic solution to adversarial attacks and defenses.
A novel method to create dense descriptors of time (Time Embeddings) to make simple models understand temporal structures
We propose a novel graph neural network architecture based on the non-backtracking matrix defined over the edge adjacencies and demonstrate its effectiveness in community detection tasks on graphs.
Residual connections really perform iterative inference
We improve the reconstruction time and quality on an experimental mask-based lensless imager using an end-to-end learning approach which incorporates knowledge of the imaging model.
We introduce a new representation learning model, namely “Sample-Ensemble Genetic Evolutionary Network” (SEGEN), which can serve as an alternative approach to deep learning models.
We propose to use meta-learning for more efficient language learning, via a kind of 'domain randomization'. 
MARTHE: a new method to fit task-specific learning rate schedules from the perspective of hyperparameter optimization
Interactively generating image from incrementally growing scene graphs in multiple steps using GANs while preserving the contents of image generated in previous steps
We study low- and very-low-signal-to-noise classification scenarios, where objects that correlate with class label occupy tiny proportion of the entire image (e.g. medical or hyperspectral imaging).
A self-attention layer can perform convolution and often learns to do so in practice.
Learning-based algorithms can improve upon the performance of classical algorithms for the low-rank approximation problem while retaining the worst-case guarantee.
Proposed architecture to solve morphological agreement task
This paper proposes the use of spectral element methods for fast and accurate training of Neural Ordinary Differential Equations for system identification.
A new intrinsic reward signal based on successor features and a novel way to combine extrinsic and intrinsic reward.
We propose to use a separate exploration policy to collect the pre-adaptation trajectories in MAML. We also show that using a self-supervised objective in the inner loop leads to more stable training and much better performance.
Generalizing backward propagation, using formal methods from supersymmetry.
Regularizing the optimization trajectory with the Fisher information of old tasks reduces catastrophic forgetting greatly
We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.
We show how autoregressive flows can be used to improve sequential latent variable models.
Adversarial examples can fool YouTube's copyright detection system
We propose a continual version of Equilibrium Propagation, where neuron and synapse dynamics occur simultaneously throughout the second phase, with theoretical guarantees and numerical simulations.
We propose a new Meta Module Network to resolve some of the restrictions of previous Neural Module Network to achieve strong performance on realistic visual reasoning dataset.
We propose a new attack for taking full control of neural policies in realistic settings.
It can generate effective hash codes for efficient cold-start recommendation and meanwhile provide a feasible marketing strategy.
We propose a neural framework that can learn to solve the Circuit Satisfiability problem from (unlabeled) circuit instances.
A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.
We introduce a "Resource by Collaborative Construction" scheme to create KB, structured Wikipedia 
A state-of-the-art model based on global reasoning for image super-resolution
Propose an assessment framework to analyze and learn graph convolutional filter
A new pooling layer for GNNs that learns how to pool nodes, according to their features, the graph connectivity, and the dowstream task objective.
 We propose TuckER, a relatively simple but powerful linear model for link prediction in knowledge graphs, based on Tucker decomposition of the binary tensor representation of knowledge graph triples. 
An algorithm to reduce the amount of memory required for training deep networks, based on an approximation strategy.
Data stream algorithms can be improved using deep learning, while retaining performance guarantees.
We propose Neural Hyperlink Predictor (NHP). NHP adapts graph convolutional networks for link prediction in hypergraphs
A method which learns separate representations for the meaning and the form of a sentence
We explored the visualization designs that can support chronic patients to present and review their health data with healthcare providers during clinical visits.
We propose a stochastic differentiable forward dynamics predictor that is able to sample multiple physically plausible trajectories under the same initial input state and show that it can be used to train model-free policies more efficiently.
Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.
A model to control the generation of images with GAN and beta-VAE with regard to scale and position of the objects
We propose a differentiable family of "kaleidoscope matrices," prove that all structured matrices can be represented in this form, and use them to replace hand-crafted linear maps in deep learning models.
Learning Label Representation for Deep Networks
We propose Choco-SGD---decentralized SGD with compressed communication---for non-convex objectives and show its strong performance in various deep learning applications (on-device learning, datacenter case).
We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.
We experimentally show that transfer learning makes sparse features in the network and thereby produces a more compressible network. 
We extend the classical label propation methods to jointly model graph and feature information from a graph filtering perspective, and show connections to the graph convlutional networks.
We provide a software package that drastically simplifies, automates, and improves the evaluation of deep learning optimizers.
extract contextual embeddings from off-the-shelf supervised model. Helps downstream NLP models in low-resource settings
Practical adaptive algorithms for gradient-based meta-learning with provable guarantees.
We aim to exploit the diversity of linguistic structures to build sentence representations.
Unsupervised analysis of data recorded from the peripheral nervous system denoises and categorises signals.
We enhance existing transformation-based defenses by using a distribution classifier on the distribution of softmax obtained from transformed images.
We propose an approach to learn decentralized policies in multi-agent settings using attention-based critics and demonstrate promising results in environments with complex interactions.
We apply RNN to solve the biological problem of chromatin folding patterns prediction from epigenetic marks and demonstrate for the first time that utilization of memory of sequential states on DNA molecule is significant for the best performance.
We propose an efficient, provable and data independent method for network compression via neural pruning using coresets of neurons -- a novel construction proposed in this paper.
Memory Augmented Network to plan in partially observable environments. 
A procedure for distilling contextual models into static embeddings; we apply our method to 9 popular models and demonstrate clear gains in representation quality wrt Word2Vec/GloVe and improved analysis potential by thoroughly studying social bias.
Metalearning unsupervised update rules for neural networks improves performance and potentially demonstrates how neurons in the brain learn without access to global labels.
We show that removing constant terms from CNN architectures provides interpretability of the denoising method via linear-algebra techniques and also boosts generalization performance across noise levels.
We provide for the first time a rigorous proof that orthogonal initialization speeds up convergence relative to Gaussian initialization, for deep linear networks.
A first differentially private estimate of the survival function
CAML is an instance of MAML with conditional class dependencies.
We study the problem of multiset prediction and propose a novel multiset loss function, providing analysis and empirical evidence that demonstrates its effectiveness.
This paper presents a theoretical framework that models data distribution explicitly for deep and locally connected ReLU network
We introduce a biologically-inspired modular evolutionary algorithm in which deep RL agents learn to cooperate in a difficult multi-agent social game, which could help to explain the evolution of altruism.
We introduce a type of neural network that is structurally resistant to adversarial attacks, even when trained on unaugmented training sets.  The resistance is due to the stability of network units wrt input perturbations.
We show that adversarial robustness might come at the cost of standard classification performance, but also yields unexpected benefits.
Training on convex combinations between random training examples and their labels improves generalization in deep neural networks
We present a novel approach to spike sorting using the Neural Clustering Process (NCP), a recently introduced neural architecture that performs scalable amortized approximate Bayesian inference for efficient probabilistic clustering.
Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.
Implementing and evaluating episodic memory for RL.
We present a method of adapting hyperparameters of probabilistic models using optimal transport with applications in robotics
An algorithm to learn a predictive state representation with general value functions and off-policy learning is applied to the problem of vision-based steering in autonomous driving.
We address the end-to-end learning of energy-based representations for signal and image observation dataset with irregular sampling patterns.
A novel and theoretically grounded meta-reinforcement learning algorithm
Side-tuning adapts a pre-trained network by training a lightweight "side" network that is fused with the (unchanged) pre-trained network using a simple additive process.
Train GANs with differential privacy to generate artificial privacy-preserving datasets.
This paper presents methods to disentangle and interpret contextual effects that are encoded in a deep neural network.
Proposing a novel method based on the guided attention to enforce the sparisty in deep neural networks.
We provably recover the lowest layer in a deep neural network assuming that the lowest layer uses a "high threshold" activation and the above network is a "well-behaved" polynomial.
We introduce FedProx, a framework to tackle statistical heterogeneity in federated settings with convergence guarantees and improved robustness and stability.
We enable both the cultural evolution of language and the genetic evolution of agents in a referential game, using a new Language Transmission Engine.
We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  
We develop a whole-connectome and body simulator for C. elegans and demonstrate joint state-space and parameter inference in the simulator.
Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.
Gen-RKM: a novel framework for generative models using Restricted Kernel Machines with multi-view generation and uncorrelated feature learning.
We compare many tasks and task combinations for pretraining sentence-level BiLSTMs for NLP tasks. Language modeling is the best single pretraining task, but simple baselines also do well.
An insight into the reason of adversarial vulnerability, an effective defense method against adversarial attacks.
Apply Monte Carlo Tree Search to episode generation in Alpha Zero
For graph neural networks, the aggregation on a graph can benefit from a continuous space underlying the graph.
We use a simple search algorithm involving an RNN and priority queue to find solutions to coding tasks.
We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcoming of previous spectral graph CNN methods that depend on graph Fourier transform.
We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.
Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark
Neural Probabilistic Motor Primitives compress motion capture tracking policies into one flexible model capable of one-shot imitation and reuse as a low-level controller.
We present an automated adaptive data augmentation that works for multiple different tasks. 
We propose a new regularization technique based on the knowledge distillation.
Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. 
Looking for object detectors using many different selectivity measures; CNNs are slightly selective , but not enough to be termed object detectors.
A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs
A learnable clustering objective to facilitate transfer learning across domains and tasks
Joint method for learning cross-lingual embeddings with state-of-art performance for cross-lingual tasks and mono-lingual quality
Generative model of temporal data, that builds online belief state, operates in latent space, does jumpy predictions and rollouts of states.
Presents an information theoretic training objective for co-training and demonstrates its power in unsupervised learning of phonetics.
Our models generate singing voices without lyrics and scores. They take accompaniment as input and output singing voices.
We increase the efficiency of neural network dependency parsers with teacher-student distillation.
Adversarially Regularized Autoencoders learn smooth representations of discrete structures allowing for interesting results in text generation, such as unaligned style transfer, semi-supervised learning, and latent space interpolation and arithmetic.
We present a method to estimate collections of regression models in which each model is personalized to a single sample.
A recurrent neural network cell with extended-long short-term memory and a multi-task RNN model for sequence-in-sequence-out problems
We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.
A primal dual graph neural network model for semi-supervised learning
We describe two end-to-end autoencoding parsers for semi-supervised graph-based dependency parsing.
An improved Fast Weight network which shows better results on a general toy task.
Introduction of a new optimization method and its application to deep learning.
Introducing a new class of quantum neural networks for learning graph-based representations on quantum computers.
"In this paper, we tested communication strategies' influence on users mental models of a data breach."
A goal recognition approach based on operator counting heuristics used to account for noise in the dataset.
distilling single-task models into a multi-task model improves natural language understanding performance
Evaluating pixel-level out-of-distribution detection methods on two new real world datasets using PSPNet and DeeplabV3+.
We present a new adversarial method for adapting neural representations based on a critic that detects non-discriminative features.
We propose a statistical framework and a theoretically consistent procedure for saliency estimation.
We explored how a novel method of compositional set embeddings can both perceive and represent not just a single class but an entire set of classes that is associated with the input data.
We introduce a dataset, models, and training + evaluation protocols for a collaborative drawing task that allows studying goal-driven and perceptually + actionably grounded language generation and understanding. 
We propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s) by incorporating a loss function that encourages a vanished correlation between the bias and learned features.
A modular approach consisting of a sentence selector module followed by the QA model can be made more robust to adversarial attacks in comparison to a QA model trained on full context.
Multi-relational graph embedding with Riemannian manifolds and TransE-like loss function. 
We propose a meta learning algorithm for continual learning which can effectively prevent catastrophic forgetting problem and support backward transfer learning.
Analysis of deep convolutional networks in terms of associated arrangement of hyperplanes
We proposed a Bayesian meta sampling method for adapting the model uncertainty in meta learning
Context-adaptive entropy model for use in end-to-end optimized image compression, which significantly improves compression performance
A method for learning better representations, that acts as a regularizer and despite its no significant additional computation cost , achieves improvements over strong baselines on Supervised and Semi-supervised Learning tasks.
Packing region of Interest (ROI) such as cancerous regions identified in 3D Volume Data, Packing spheres inside the ROI, rotating the ROI , measures of difference in sphere packing before and after the rotation.
Filter level sparsity emerges implicitly in CNNs trained with adaptive gradient descent approaches due to various phenomena, and the extent of sparsity can be inadvertently affected by different seemingly unrelated hyperparameters.
Drawing parallels with human learning, we propose a unified framework to exhibit many lifelong learning abilities in neural networks by utilizing a small number of weight consolidation parameters.
We show that robust GAN priors work better than GAN priors for limited angle CT reconstruction which is a highly under-determined inverse problem.
A new form of attention that works well for the distant supervision setting, and a multitask learning approach to add sentence-level annotations. 
Analysis of attention mechanism across diverse NLP tasks.
Representing programs as graphs including semantics helps when generating programs
A new method for inferring a model of, estimating the entropy rate of, and predicting continuous-time, discrete-event processes.
A unified  model  to improve model robustness against multiple tasks
We proposed a progressive learning method to improve learning and disentangling latent representations at different levels of abstraction.
We apply the copula transformation to the Deep Information Bottleneck which leads to restored invariance properties and a disentangled latent space with superior predictive capabilities.
Benchmark and method to measure compositional generalization by maximizing divergence of compound frequency at small divergence of atom frequency.
Unsupervised networks learn from bottom up; machines and infants acquire visual classes in different orders
For classification problems with k classes, we show that the gradient tends to live in a tiny, slowly-evolving subspace spanned by the eigenvectors corresponding to the k-largest eigenvalues of the Hessian.
Graph-based recurrent retriever that learns to retrieve reasoning paths over Wikipedia Graph outperforms the most recent state of the art on HotpotQA by more than 14 points.
We introduced a shallow featrue extraction network with a large receptive field for stereo matching tasks, which uses a simple structure to get better performance.
The paper proposes a new output layer for deep networks that permits the use of logged contextual bandit feedback for training. 
Protein Family Classification using Deep Learning
This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.
diagnosed all the problem of STOA VAEs theoretically and qualitatively
We propose a new sparse and structured attention mechanism, TVmax, which promotes sparsity and encourages the weight of related adjacent locations to be the same.
Learned phoneme embeddings of multilingual neural speech synthesis network could represent relations of phoneme pronunciation between the languages.
GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.
Sparse MobileNets are faster than Dense ones with the appropriate kernels.
 We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.
As safety is becoming a critical notion in machine learning we believe that this work can act as a foundation for a number of research directions such as safety-aware learning algorithms.
We introduce two approaches for efficient and scalable inference in stochastic simulators for which the density cannot be evaluated directly due to, for example, rejection sampling loops.
Even if there is no tradeoff in the infinite data limit, adversarial training can have worse standard accuracy even in a convex problem.
We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.
Graph-based Deep Q Network for Web Navigation 
We construct a theoretical framework for weakly supervised disentanglement and conducted lots of experiments to back up the theory.
We propose an expansion-based approach for task-free continual learning for the first time. Our model consists of a set of neural network experts and expands the number of experts under the Bayesian nonparametric principle.
Amortizing Nesterov's momentum for more robust, lightweight and fast deep learning training.
We propose new model that can disentangle multiple dynamic factors in sequential data
We verify deterministic and probabilistic properties of neural networks using non-convex relaxations over visible transformations specified by generative models
An efficient multi-view video summarization scheme advanced to activity recognition in IoT environments.
Rectification in deep neural networks naturally leads them to favor an invariant representation.
The Wave-U-Net architecture, recently introduced by Stoller et al for music source separation, is highly effective for speech enhancement, beating the state of the art.
We introduce a novel framework for learning from demonstration that uses continuous human feedback; we evaluate this framework on continuous control for autonomous vehicles.
scale and enhance VQ-VAE with powerful priors to generate near realistic images.
A Graph Neural Network Assisted Monte Carlo Tree Search Approach to Traveling Salesman Problem
Directional message passing incorporates spatial directional information to improve graph neural networks.
We design a simple and efficient model-free off-policy method for image-based reinforcement learning that matches the state-of-the-art model-based methods in sample efficiency
We present a novel simple operator, chopout, with which neural networks are trained, even in a single training process, so as to truncated sub-networks perform as well as possible.
Multi modal Guassian distribution of latent space in GAN models improves performance and allows to trade-off quality vs. diversity
SlowMo improves the optimization and generalization performance of communication-efficient decentralized algorithms without sacrificing speed.
Plan the syntactic structural of translation using codes
 We identify memorization as the inductive bias of interpolation in overparameterized fully connected and convolutional auto-encoders. 
We provably recover the span of a deep multi-layered neural network with latent structure and empirically apply efficient span recovery algorithms to attack networks by obfuscating inputs.
We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.
we propose convolutional tensor-train LSTM,  which learns higher-order Convolutional LSTM efficiently using convolutional tensor-train decomposition. 
The proposed method is an end-to-end neural SVM, which is optimized for few-shot learning.
A novel 4D CNN structure for video-level representation learning, surpassing  recent 3D CNNs.
We study the problem of learning and optimizing through physical simulations via differentiable programming, using our proposed DiffSim programming language and compiler.
We present a method for interpreting black-box models by using instance-wise backward selection to identify minimal subsets of features that alone suffice to justify a particular decision made by the model.
We propose a method that extracts the uncertainties of features in each layer of DNNs and combines them for detecting OOD samples when solving classification tasks.
We leverage deterministic autoencoders as generative models by proposing mixing functions which combine hidden states from pairs of images. These mixes are made to look realistic through an adversarial framework.
We introduce an augmented robust feature space for streaming wifi data that is capable of tackling concept drift for indoor localization
we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node.
Capsule networks with learned pose matrices and EM routing improves state of the art classification on smallNORB, improves generalizability to new view points, and white box adversarial robustness.  
We propose a meta-learning framework that learns a transferable policy from only weak supervision to solve synthesis tasks with different logical specifications and grammars.
Make the transformer streamable with monotonic attention.
Learning optimal mapping with deepNN between distributions along with theoretical guarantees.
A novel approach to building an unsupervised document (sentence) embeddings from pre-trainedword embeddings
We introduce a statistical approach to assessing neural network robustness that provides an informative notion of how robust a network is, rather than just the conventional binary assertion of whether or not of property is violated.
Enhancing the robustness of pretrained transformer models against the lexical overlap bias by extending the input sentences of the training data with their corresponding predicate-argument structures 
Neural network regression should use Dirichlet output distribution when targets are probabilities in order to quantify uncertainty of predictions.
Learning to Search Efficient DenseNet with Layer-wise Pruning
We train predictive models on proprioceptive information and show they represent properties of external objects.
A GAN based method to learn important topological features of an arbitrary input graph.
Predicting auction price of vehicle license plates in Hong Kong with deep recurrent neural network, based on the characters on the plates.
We present a new deep latent model of natural images that can be trained from unlabeled datasets and can be utilized to solve various image restoration tasks.
Automatically score essays on sparse data by comparing new essays with known samples with Referee Network. 
We combine the matching network framework for few shot learning into a large scale multi-label model for genomic sequence classification.
Applying the softmax function in training leads to indirect and unexpected supervision on features. We propose a new training objective to explicitly induce dense feature regions for locally sufficient samples to benefit adversarial robustness.
GAN representations are examined in detail, and sets of representation units are found that control the generation of semantic concepts in the output.
Pixel-wise nearest neighbors used for generating multiple images from incomplete priors such as a low-res images, surface normals, edges etc.
We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.
We decompose the gap between the marginal log-likelihood and the evidence lower bound and study the effect of the approximate posterior on the true posterior distribution in VAEs.
Using ensembles and pseudo labels for unsupervised clustering 
Efficient dictionary learning by L1 minimization via a novel analysis of the non-convex non-smooth geometry.
We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.
Our neural network codec (which is based on transform coding and clustering) enables a low complexity and high efficient transparent compression of neural networks.
We accelerate secure DNN inference in trusted execution environments (by a factor 4x-20x) by selectively outsourcing the computation of linear layers to a faster yet untrusted co-processor.
This paper proposes an effective method to compress neural networks based on recent results in information theory.
A general framework for creating covariant graph neural networks
In this paper, we propose a three-dimensional regularization-based pruning method to accelerate the 3D-CNN.
A practical proposal for more ethical and responsive NLP technology, operationalizing transparency of test and training data
Discover the structure of functional causal models with generative neural networks
In network pruning, fine-tuning a pruned model only gives comparable or worse performance than training it from scratch. This advocate a rethinking of existing pruning algorithms.
This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.
We propose Tendency RL to efficiently solve goal-oriented tasks with large state space using automated curriculum learning and discriminative shaping reward, which has the potential to tackle robot manipulation tasks with perception.
We present scene programs, a structured scene representation that captures both low-level object appearance and high-level regularity in the scene.
Compression of neural networks which improves the state-of-the-art low rank approximation techniques and is complementary to most of other compression techniques. 
We present an attribution technique leveraging sparsity inducing norms to achieve interpretability.
We use sparsity to improve the computational complexity of variance reduction methods.
We propose a novel VAE-based framework learning from partially-observed data for imputation and generation. 
Insights on the domain adaptation challenge, when predicting user intent in enterprise email.
We propose HiPPO, a stable Hierarchical Reinforcement Learning algorithm that can train several levels of the hierarchy simultaneously, giving good performance both in skill discovery and adaptation.
We take a step towards measuring learning task difficulty and demonstrate that in practice performance strongly depends on the match of the representation of the information and the model interpreting it.
We combine Multi-output Gaussian processes with deep recurrent Q-networks to learn optimal treatments for sepsis and show improved performance over standard deep reinforcement learning methods,
We develop a new deep generative model for semi-supervised learning and propose a new Max-Min cross-entropy for training CNNs.
We propose a simple randomization technique for improving generalization in deep reinforcement learning across tasks with various unseen visual patterns.
Describes a study investigating interference, transfer, and retention of multiple mappings with the same set of chorded buttons
Formal verification of a specification on a model's prediction undersensitivity using Interval Bound Propagation
We propose a novel 8-bit format that eliminates the need for loss scaling, stochastic rounding, and other low precision techniques
Novel algorithm for Incremental learning of VAE with fixed architecture
MAML is great, but it has many problems, we solve many of those problems and as a result we learn most hyper parameters end to end, speed-up training and inference and set a new SOTA in few-shot learning
Detecting overlapping communities in graphs using graph neural networks
We empirically disprove a fundamental hypothesis of the widely-adopted weight sharing strategy in neural architecture search and explain why the state-of-the-arts NAS algorithms performs similarly to random search.
In this paper we use the sliced-Wasserstein distance to shape the latent distribution of an auto-encoder into any samplable prior distribution. 
We introduce a class of generative models that reliably learn Hamiltonian dynamics from high-dimensional observations. The learnt Hamiltonian can be applied to sequence modeling or as a normalising flow.
In this short paper, we briefly introduce the advantages of using AI planning in Cloud Migration, a preliminary prototype, as well as the chal- lenges the requires attention from the planning and schedul- ing society.
A general upper bound on the target domain's risk that reflects the role of embedding-complexity.
In order to forecast multivariate stationary time-series we learn embeddings containing contextual features within a RNN; we apply the framework on public transportation data
We investigate a framework for discovery: curating a large collection of predictions, which are used to construct the agent’s representation in partially observable domains.
A global geolocation inferencing strategy with novel meshing strategy and demonstrating incorporating additional information can be used to improve the overall performance of a geolocation inference model.
Technique for learning deep generative models with shared latent variables, applied to Omniglot with a PixelCNN decoder.
In this paper we present a task-agnostic reading architecture for the dynamic integration of explicit background knowledge in neural NLU models. 
Dynamic precision technique to train deep neural networks
We introduce the ISRLU activation function which is continuously differentiable and faster than ELU. The related ISRU replaces tanh & sigmoid.
We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.
We extend the information bottleneck method to the unsupervised multiview setting and show state of the art results on standard datasets
We describe a biologically plausible learning algorithm for fixed point recurrent networks without tied weights
We train a generative 3D model of shapes from natural images in an fully unsupervised way.
We show that a relatively simple black-box adversarial attack scheme using Bayesian optimization and dimension upsampling  is preferable to existing methods when the number of available queries is very low.
We revisit the simple idea of pruning connections of DNNs through $\ell_1$ regularization achieving state-of-the-art results on multiple datasets with theoretic guarantees.
A non-parametric method to measure the error moments of regressors without ground truth can be used with biased regressors
Cnvolutional neural networks characterization for backdoored classifier detection and understanding.
We can learn high-dimensional constraints from demonstrations by sampling unsafe trajectories and leveraging a known constraint parameterization.
Parametric Manifold Learning with Neural Networks in a Geometric Framework 
We propose a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL
We propose a new combination of evolution strategy and deep reinforcement learning which takes the best of both worlds
An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling
Understand how class labels help GAN training. Propose a new evaluation metric for generative models. 
Fast iterative algorithm to balance the energy of a network while staying in the same functional equivalence class
We find environment settings in which SOTA agents trained on navigation tasks display extreme failures suggesting failures in generalization.
Robust Bayesian Estimation via Maximum Mean Discrepancy
We create an unbiased estimator for the log probability of latent variable models, extending such models to a larger scope of applications.
Smaller batch sizes can outperform very large batches on the test set under constant step budgets and with properly tuned learning rate schedules.
Better Deep Reinforcement Learning algorithm to approximate Counterfactual Regret Minimization
Generates never seen data during training from a desired condition 
We analyze gradient descent for deep linear neural networks, providing a guarantee of convergence to global optimum at a linear rate.
A layer modelling local random connectomes in the cortex within deep networks capable of learning general non-parametric invariances from the data itself.
We introduce an effective, general framework for incorporating conditioning information into inference-based generative models.
This paper describes three techniques to allow a non-backtracking,  computationally limited scheduler to consider a small number of alternative activities based on resource availability.
We use simple and biologically motivated modifications of standard learning techniques to achieve state of the art performance on catastrophic forgetting benchmarks.
Using deep learning techniques on singing voice related tasks.
Language generation using seq2seq models which produce word embeddings instead of a softmax based distribution over the vocabulary at each step enabling much faster training while maintaining generation quality
Closed form results for deep learning in the layer decoupling limit applicable to Residual Networks
 In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.
Answering a wide class of logical queries over knowledge graphs with box embeddings in vector space
We address fully parallel hyperparameter optimization with Determinantal Point Processes. 
A multi-level spectral approach to improving the quality and scalability of unsupervised graph embedding.
A new generative model for discrete structured data. The proposed stochastic lazy attribute converts the offline semantic check into online guidance for stochastic decoding, which effectively addresses the constraints in syntax and semantics, and also achieves superior performance
the proposed models with external knowledge further improve the state of the art on the SNLI dataset.
Approach for improving prediction accuracy by learning deep features over neighboring scene images in satellite scene image analysis.
We propose physics-aware difference graph networks designed to effectively learn spatial differences to modeling sparsely-observed dynamics.
Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks
The paper describes a new algorithm by which sound correspondence patterns for multiple languages can be inferred.
A biologically plausible learning rule for training recurrent neural networks
A new method for unsupervised representation learning on graphs, relying on maximizing mutual information between local and global representations in a graph. State-of-the-art results, competitive with supervised learning.
A backward model of previous (state, action) given the next state, i.e. P(s_t, a_t | s_{t+1}), can be used to simulate additional trajectories terminating at states of interest! Improves RL learning efficiency.
This work is about tensor-based method for preposition representation training.
We use continuous time dynamics to define a generative model with exact likelihoods and efficient sampling that is parameterized by unrestricted neural networks.
A new non-adversarial feature matching-based approach to train generative models that achieves state-of-the-art results.
A novel architecture for few-shot classification capable of dealing with uncertainty.
We show that CNNs and ResNets with appropriate priors on the parameters are Gaussian processes in the limit of infinitely many convolutional filters.
We present an end-to-end design methodology for efficient deep learning deployment. 
Accelerate distributed optimization by exploiting stragglers.
Enabled by a novel differentiable renderer, we propose a new metric that has real-world implications for evaluating adversarial machine learning algorithms, resolving the lack of realism of the existing metric based on pixel norms.
We show that the Transformer architecture and the Neural GPU are Turing complete.
We use VAE to capture the shape feature for automatic segmentation evaluation
We investigate the eigenvalues of the linear layers in deep networks and show that the distributions develop heavy-tail behavior during training.
We propose a new type of end-to-end trainable attention module, which applies global weight balances among layers by utilizing co-propagating RNN with CNN.
Features auto-generated by the bio-mimetic MothNet model significantly improve the test accuracy of standard ML methods on vectorized MNIST. The MothNet-generated features also outperform standard feature generators.
By focusing more on the final predictions in anytime predictors (such as the very recent Multi-Scale-DenseNets), we make small anytime models to outperform large ones that don't have such focus. 
We propose a memory-efficient learning procedure that exploits the reversibility of the network’s layers to enable data-driven design for large-scale computational imaging.
Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.
A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. 
Deep learning on structured tabular data using two-dimensional word embedding with fine-tuned ImageNet pre-trained CNN model.
connections between predictive coding and VAEs + new frontiers
Novel variants of optimization methods that combine the benefits of both adaptive and non-adaptive methods.
In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.
An intuitive empirical and visual exploration of the generalization properties of deep neural networks.
We use convolution to make neural networks behave more like symbolic systems.
Analytical Formulation of Equatorial Standing Wave Phenomena: Application to QBO and ENSO
We propose to train an Invertible Neural Network for each class to perform class-by-class Continual Learning.
A novel ensemble of retrieval-based and generation-based for open-domain conversation systems.
Understand transferability from the perspectives of improved generalization, optimization and the feasibility of transferability.
We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.
We propose a top-down modulation network for multi-task learning applications with several advantages over current schemes.    
Novel time series data clustring algorithm based on dynamical system features.
A method for encouraging axiomatic feature attributions of a deep model to match human intuition.
We propose high-performance LSTMs with binary/ternary weights, that can greatly reduce implementation complexity
We address the problem of unsupervised few-shot object recognition, where all training images are unlabeled and do not share classes with test images.
Query-based black-box attacks on deep neural networks with adversarial success rates matching white-box attacks
We convert subgraphs into structured images and classify them using 1. deep learning and 2. transfer learning (Caffe) and achieve stunning results.
Self-ensembling based algorithm for visual domain adaptation, state of the art results, won VisDA-2017 image classification domain adaptation challenge.
A VAE-variant which can create diverse images corresponding to novel concrete or abstract "concepts" described using attribute vectors.
We propose a model-based method called "Search with Amortized Value Estimates" (SAVE) which leverages both real and planned experience by combining Q-learning with Monte-Carlo Tree Search, achieving strong performance with very small search budgets.
A short proof of the equivalence of soft Q-learning and policy gradients.
We propose a policy transfer algorithm that can overcome large and challenging discrepancies in the system dynamics such as latency, actuator modeling error, etc.
Learn how to quantize speech signal and apply algorithms requiring discrete inputs to audio data such as BERT.
We develop Hierarchical Agent with Self-play (HASP), a learning approach for obtaining hierarchically structured policies that can achieve high performance than conventional self-play on competitive real-time strategic games.
Variable capacity input word embeddings and SOTA on WikiText-103, Billion Word benchmarks.
a deep multivariate mixture of Gaussians model for bounding box regression under occlusion
We replace the Lp ball constraint with the Voronoi cells of the training data to produce more robust models. 
We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.
we propose to use Wasserstein barycenters for semantic model ensembling
Label-efficient audio classification via multi-task learning and self-supervision
Propose first methods for exactly optimizing the softmax distribution using stochastic gradient with runtime independent on the number of classes or datapoints.
Use Monte carlo Tree Search and Homoglyphs to generate indistinguishable adversarial samples on text data
Learn to convert a hand drawn sketch into a high-level program
This paper uses principles from the field of calibration in machine learning on the logits of a neural network to defend against adversarial attacks
We jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to learn high quality multilingual text embeddings that perform well in the low resource scenario.
We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.
A study of how different components in the NAS pipeline contribute to the final accuracy. Also, a benchmark of 8 methods on 5 datasets.
Finding correspondences between domains by performing matching/mapping iterations
a neural sparsity-enhanced topic model based on VAE
Keras for infinite neural networks.
We introduce NLProlog, a system that performs rule-based reasoning on natural language by leveraging pretrained sentence embeddings and fine-tuning with Evolution Strategies, and apply it to two multi-hop Question Answering tasks.
We propose Fidelity-weighted Learning, a semi-supervised teacher-student approach for training neural networks using weakly-labeled data.
We prove that gradient descent is robust to label corruption despite over-parameterization under a rich dataset model.
We present a new weight encoding scheme which enables high compression ratio and fast sparse-to-dense matrix conversion.
Improved a GAN based pixel inpainting network for compressed seismic image recovery andproposed a non-uniform sampling survey recommendatio, which can be easily applied to medical and other domains for compressive sensing technique.
We develop Simplified Action Decoder, a simple MARL algorithm that beats previous SOTA on Hanabi by a big margin across 2- to 5-player games.
End-to-end trainable Optical Character Recognition on printed documents; we achieve state-of-the-art results, beating Tesseract4 on benchmark datasets both in terms of accuracy and runtime, using a purely computer vision based approach.
NovoGrad -  an adaptive SGD method with layer-wise gradient normalization and decoupled weight decay. 
Better audio synthesis by combining interpretable DSP with end-to-end learning.
A novel approach to graph classification based on spectral graph convolutional networks and its extension to multigraphs with learnable relations and hierarchical structure. We show state-of-the art results on chemical, social and image datasets.
We show how to successfully perform backdoor attacks without changing training labels.
We find that deep networks which generalize poorly are more reliant on single directions than those that generalize well, and evaluate the impact of dropout and batch normalization, as well as class selectivity on single direction reliance.
Instead of learning the parameters of a graphical model from data, learn an inference network that can answer the same probabilistic queries.
We demonstrate how residual blocks can be viewed as Gauss-Newton steps; we propose a new residual block that exploits second order information.
We introduce a machine learning model that uses domain-independent features to estimate the criticality of the current state to cause a known undesirable state.
We develop a framework to find modular internal representations in generative models and manipulate then to generate counterfactual examples.
Hebbian plastic weights can behave as a compressed episodic memory storage in neural networks; improving their ability to alleviate catastrophic forgetting in continual learning.
We develop an approach to parcellate a hidden layer in DNN into functionally related groups, by applying spectral coclustering on the attribution scores of hidden neurons.
Deploy text classification and sentiment analysis applications for English and Chinese on a 300mW CNN accelerator chip for on-device application scenarios.
We develop VAEs where the encoder takes a model parameter vector as input, so we can do rapid inference for many models
Secret is a transfer method for RL based on the transfer of credit assignment.
Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. 
A gradient flow based dynamical system for invertible generative modeling
We introduce an efficient memory layer that can learn representation and coarsen input graphs simultaneously without relying on message passing.
A novel encoding scheme of using {-1, +1} to decompose QNNs into multi-branch binary networks, in which we used bitwise operations (xnor and bitcount) to achieve model compression, computational acceleration and resource saving. 
We propose a novel way to incorporate conditional image information into the discriminator of GANs using feature fusion that can be used for structured prediction tasks.
We propose an algorithm for learning useful skills without a reward function, and show how these skills can be used to solve downstream tasks.
The paper solves a lexical ambiguity problem caused from homonym in neural translation by BERT.
This paper focuses on the synthetic generation of human mobility data in urban areas using GANs. 
This paper proposes an effective coding scheme for neural networks that encodes a random set of weights from a variational distribution.
By analyzing an algorithms minimizing a non-convex loss, we show that all but a small fraction of noise can be removed from an image using a deep neural network based generative prior.
Large scale multi-task architecture solves ImageNet and translation together and shows transfer learning.
Continental-philosophy-inspired approach to learn with few data.
Learning to synthesize raw waveform audio with GANs
A novel domain adaptation method to align manifolds from source and target domains using label propagation for better accuracy.
This paper proposes a new method for neural network learning in online bandit settings by marginalizing over the last layer
We show in theory and in practice that combining multiple explanation methods for DNN benefits the explanation.
Learning with limited training data by exploiting "helpful" instances from a rich data source.  
We increase the amount of trace supervision possible to utilize when training fully differentiable neural machine architectures.
We propose Bayesian quantized networks, for which we learn a posterior distribution over their quantized parameters.
Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.
We show that in contras to popular wisdom, the exploding gradient problem has not been solved and that it limits the depth to which MLPs can be effectively trained. We show why gradients explode and how ResNet handles them.
We found adversarial training not only speeds up the GAN training but also increases the image quality
A method to binarize both weights and activations of a deep neural network that is efficient in computation and memory usage and performs better than the state-of-the-art.
We present Good-Enough Model Spaces (GEMS), a framework for learning an aggregate model over distributed nodes within a small number of communication rounds.
We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.
Preserving Differential Privacy in Adversarial Learning with Provable Robustness to Adversarial Examples
We automatically construct and explore a small abstract Markov Decision Process, enabling us to achieve state-of-the-art results on Montezuma's Revenge, Pitfall!, and Private Eye by a significant margin.
We show that KL-control from a pre-trained prior can allow RL models to learn from a static batch of collected data, without the ability to explore online in the environment.
Single episode policy transfer in a family of environments with related dynamics, via optimized probing for rapid inference of latent variables and immediate execution of a universal policy.
We propose a Graph Convolutional Network based encoder-decoder model with sequential attention for goal-oriented dialogue systems.
Node sequence embedding mechanism that captures both graph and text properties.
This paper aims to leverage good properties of robust visual features like SIFT to renovate CNN architectures towards better accuracy and robustness.
Theory predicts the phase transition between unlearnable and learnable values of beta for the Information Bottleneck objective
Generate corrupted training images that are imperceptible yet change CNN behavior on a target during any new training.
We give an algorithm for learning a two-layer neural network with symmetric input distribution. 
We show that training a student and teacher network iteratively, rather than jointly, can produce emergent, interpretable teaching strategies.
Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings.
We systematically examine why knowledge distillation is crucial to the training of non-autoregressive translation (NAT) models, and propose methods to further improve the distilled data to best match the capacity of an NAT model.
We succeed in stabilizing transformers for training in the RL setting and demonstrate a large improvement over LSTMs on DMLab-30, matching an external memory architecture.
Inspired by trial-to-trial variability in the brain that can result from multiple noise sources, we introduce variability through noise in the knowledge distillation framework and studied their effect on generalization and robustness.
We introduce a novel end-to-end approach for learning to cluster in the absence of labeled examples. We define a differentiable loss function equivalent to the expected normalized cuts.
We introduce several datasets for Cyrillic OCR and a method for its recognition
We propose an unsupervised way to learn multiple embeddings for sentences and phrases 
Sample efficient algorithms to adapt a text-to-speech model to a new voice style with the state-of-the-art performance.
This paper introduces a probabilistic framework for k-shot image classification that achieves state-of-the-art results
Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.
Our combination of multi-task learning and self-attention, training the model to attend to parents in a syntactic parse tree, achieves state-of-the-art CoNLL-2005 and CoNLL-2012 SRL results for models using predicted predicates.
We propose a new module that improves any ResNet-like architectures by enforcing "channel selective" behavior to convolutional layers
A theory and algorithmic framework for prediction under distributional shift, including causal effect estimation and domain adaptation
We study deep ensembles through the lens of loss landscape and the space of predictions, demonstrating that the decorrelation power of random initializations is unmatched by subspace sampling that only explores a single mode.
Can we trust our deep learning models? A framework to measure and improve a deep learning model's trust during training.
A model-based RL approach which uses a differentiable uncertainty penalty to learn driving policies from purely observational data.
Tailoring predictions from sequence models (such as LDSs and RNNs) via an explicit latent code.
We propose a novel adversarial training with domain adaptation method that significantly improves the generalization ability on adversarial examples from different attacks.
This paper proposes a novel lightweight Transformer for character-level language modeling, utilizing group-wise operations.
A stable domain-adversarial training approach for robust and comprehensive domain adaptation
We use machine learning to generate synonyms for large shopping taxonomies.
MOHART uses a self-attention mechanism to perform relational reasoning in multi-object tracking.
We investigate and propose solutions for two challenges in reinforcement learning: (a) efficient actor-critic learning with experience replay (b) stability of very off-policy learning.
We propose a gradient estimator selection algorithm with the aim on improving optimization efficiency.
We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design, with theoretical guarantees.
This work proves the non-acceleration of Nesterov SGD with any hyper-parameters, and proposes new algorithm which provably accelerates SGD in the over-parameterized setting.
A feedforward layer to incorporate structured smoothness into a deep learning model
Don't deform your convolutions -- deform your kernels.
We present a method for learning protein sequence embedding models using structural information in the form of global structural similarity between proteins and within protein residue-residue contacts.
Considering neural network optimization process as a model selection problem, we introduce a biological plausible normalization method that extracts statistical regularity under MDL principle to tackle imbalanced and limited data issue.
"Generative modeling with no need for adversarial training"
We propose an imitation learning method to learn from diverse-quality demonstrations collected by demonstrators with different level of expertise.
Describing a semantic heuristics which builds upon an OWL-S service description and uses word and sentence distance measures to evaluate the usefulness of services for a given goal. 
Topology-Based Graph Convolutional Network (GCN)
Artificial neural networks evolved the same structures present in the olfactory systems of flies and mice after being trained to classify odors
Smooth regularization over sample graph for unpaired image-to-image translation results in significantly improved consistency
A convolution neural network for multi-view stereo matching whose design is inspired by best practices of traditional geometry-based approaches
We augment model-free policy learning with a sequence-level surrogate reward functions and count-based visitation bonus and demonstrate effectiveness in the large batch, low-round regime seen in designing DNA and protein sequences.
We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.
A new RL algorithm called Interior Policy Differentiation is proposed to learn a collection of diverse policies for a given primal task.
An empirical evaluation on generative adversarial networks
Prediction of numerical attribute values associated with entities in knowledge bases.
We propose procedures for evaluating and strengthening contextual embedding alignment and show that they both improve multilingual BERT's zero-shot XNLI transfer and provide useful insights into the model.
We use neural networks trained for image denoising as plug-and-play priors in energy minimization algorithms for image reconstruction problems with provable convergence.
We propose a novel method to calibrate knowledge graph embedding models without the need of negative examples.
A novel  Adaptive Curriculum Learning loss for deep face recognition
Adversarial training with single-step methods overfits, and remains vulnerable to simple black-box and white-box attacks. We show that including adversarial examples from multiple sources helps defend against black-box attacks.
This paper proposes a new approach to incorporating desired invariance to representations learning, based on the observations that the current state-of-the-art AFL has practical issues.
The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.
We propose CR-NAS to reallocate engaged  computation resources in different resolution and spatial position.
A training method that can make deep learning algorithms work better on neuromorphic computing chips with uncertainty
A CNN architecture that can effective rejects the unknowns in test objects
We perform amortized variational inference on a latent Gaussian process model to achieve superior imputation performance on multivariate time series with missing data.
We perform massive experimental studies characterizing the relationships between Jacobian norms, linear regions, and generalization.
Parameter space noise allows reinforcement learning algorithms to explore by perturbing parameters instead of actions, often leading to significantly improved exploration performance.
We present novel distillation techniques that enable training student models with different vocabularies and compress BERT by 60x with minor performance drop.
End-to-end learning of invariant representations with variables across examples such as if someone went somewhere then they are there.
We solve ill-posed inverse problems with scarce ground truth examples by estimating an ensemble of random projections of the model instead of the model itself.
A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.
We propose four new ways of collecting NLI data. Some help slightly as pretraining data, all help reduce annotation artifacts.
Stochastic variational video prediction in real-world settings.
Modeling complex multi-agent interactions under multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies. 
A system for cross-lingual (English-Russian) plagiarism detection
We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.
Perturbations can be used to learn feedback weights on large fully connected and convolutional networks.
We suggest the sufficient number of bits for representing weights of DNNs and the optimum bits are conservative when solving real problems.
Refining segmentation proposals by performing iterative inference with conditional denoising autoencoders.
We propose a self-attention based GAN architecture for unconditional text generation and improve on previous adversarial code-based results.
We propose a novel watermark encoder-decoder neural networks. They perform a cooperative game to define their own watermarking scheme. People do not need to design watermarking methods any more.
We derive a low-variance, unbiased gradient estimator for expectations over discrete random variables based on sampling without replacement
We propose a method that enables CNN folding to create recurrent connections
Gradient clipping doesn't endow robustness to label noise, but a simple loss-based variant does.
show experimental evidences about the weak correlation between flows' likelihoods and image semantics.
We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.
We prove the first-ever convergence proof of an asynchronous accelerated algorithm that attains a speedup.
We embed SG-MCMC samplers inside a variational approximation
We argue theoretically that by simply assuming the weights of a ReLU network to be Gaussian distributed (without even a Bayesian formalism) could fix this issue; for a more calibrated uncertainty, a simple Bayesian method could already be sufficient.
We use representations trained without any parallel data for creating word alignments.
A new approach for learning with noisy rewards in reinforcement learning
This paper focuses upon a traditionally overlooked mechanism -- an architecture with explicitly designed private and shared hidden units designed to mitigate the detrimental influence of the auxiliary unsupervised loss over the main supervised task.
A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning
New method for assessing the quaility of similarity evaluators and showing potential of Transformer-based language models in replacing BLEU and ROUGE.
Meta-learning methods used for vision, directly applied to NLP, perform worse than nearest neighbors on new classes; we can do better with distributional signatures.
A theoretical analysis of a new class of RNNs, trained on neuroscience tasks, allows us to identify the role of dynamical dimensionality and cell classes in neural computations.
We propose the fusion discriminator, a novel architecture for incorporating conditional information into the discriminator of GANs for structured prediction tasks.
We define, explore, and begin to address the objective mismatch issue in model-based reinforcement learning.
We give a detailed explanation of the trajectories in the information plane and investigate its usage for neural network design (pruning)
Employing quantum entanglement measures for quantifying correlations in deep learning, and using the connection to fit the deep network's architecture to correlations in the data.
We use a reinforcement learning over molecular graphs to generate rationales for interpretable molecular property prediction.
We unify graph convolutional networks as co-training and unitized matrix factorization.
A flow-based autoregressive model for molecular graph generation. Reaching state-of-the-art results on molecule generation and properties optimization.
We propose a new framework for preconditioner learning, derive new forms of preconditioners and learning methods, and reveal the relationship to methods like RMSProp, Adam, Adagrad, ESGD, KFAC, batch normalization, etc.
Simple text augmentation techniques can significantly boost performance on text classification tasks, especially for small datasets.
We propose a model that is able to perform physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available.
ASAL is a pool based active learning method that generates high entropy samples and retrieves matching samples from the pool in sub-linear time.
We point out important problems with the common practice of using the best single model performance for comparing deep learning architectures, and we propose a method that corrects these flaws.
We study the effect of the embedding complexity in learning domain-invariant representations and develop a strategy that mitigates sensitivity to it.
We propose a new  architecture termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER) and achieve new state-of-the-art performances on CoNLL and Twitter NER.
Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field
We propose a confidence based Graph Convolutional Network for Semi-Supervised Learning.
We propose ImageNet-C to measure classifier corruption robustness and ImageNet-P to measure perturbation robustness
Obfuscate code using seq2seq networks, and execute using the obfuscated code and key pair
GAN-based method for joint image and per-pixel annotation synthesis
We introduce a new task and dataset on cross-lingual vision-language navigation, and propose a general cross-lingual VLN framework for the task.
Unsupervised classification via deep generative modeling with controllable feature learning evaluated in a difficult real world task
Models Representation Learning over dynamic graphs as latent hidden process bridging two observed processes of Topological Evolution of and Interactions on dynamic graphs.
We introduce a class of n-player games suited to gradient-based methods.
We study the class of formal languages acceptable by real-time counter automata, a model of computation related to some types of recurrent neural networks.
A summarization model combining a new intra-attention and reinforcement learning method to increase summary ROUGE scores and quality for long sequences.
We study whether and how adaptive data augmentation and knowledge distillation can be leveraged simultaneously in a synergistic manner for better training student networks.
We introduce a simple procedure to repurpose pre-trained transformer-based language models to perform abstractive summarization well.
We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.
Formulating sparse signal recovery as a sequential decision making problem, we develop a method based on RL and MCTS that learns a policy to discover the support of the sparse signal. 
Learning embedding for control with high-dimensional observations
Initial findings in the intersection of network neuroscience and deep learning. C. Elegans and a mouse visual cortex learn to recognize handwritten digits.
An unsupervised structure learning method for Parsimonious Deep Feed-forward Networks.
Using GANs as priors for efficient Bayesian inference of complex fields.
Omniglot and miniImageNet are too simple for few-shot learning because we can solve them without using labels during meta-evaluation, as demonstrated with a method called centroid networks
Identify decision states (where agent can take actions that matter) without reward supervision, use it for transfer.
An approach to combine variational inference and Bayesian optimisation to solve complicated inverse problems
We show that under some assumptions on vehicle dynamics and environment uncertainty it is possible to automatically synthesize motion primitives that do not accumulate error over time.
Existing Deep Convolutional Networks in image classification tasks are sensitive to Gabor noise patterns, i.e. small structured changes to the input cause large changes to the output.
Probing robustness and redundancy in deep neural networks reveals capacity-constraining features which help to explain non-overfitting.
A scalable algorithm to establish robust derivatives of deep networks w.r.t. the inputs.
In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.
In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation.
Information about whether a neural network's output will be correct or incorrect is somewhat present in the outputs of the network's intermediate layers.
Unsupervised self-imitation algorithm capable of inference from a single expert demonstration.
We develop a framework for generating human-understandable explanations for why infeasibility is occurring in over-constrained instances of a class of resource-constrained scheduling problems.
We show that gradients are unable to capture shifts in saliency due to adversarial perturbations and present an alternative adversarial defense using learnt saliency models that is effective against both black-box and white-box attacks.
Inadequacy of Disentanglement Metrics
We propose Mind-aware Multi-agent Management Reinforcement Learning (M^3RL) for training a manager to motivate self-interested workers to achieve optimal collaboration by assigning suitable contracts to them.
A method for persistent latent states in ResBlocks demonstrated for super-resolution of alised image sequences.
We propose Diversely Stale Parameters to break lockings of the backpropoagation algorithm and train a CNN in parallel.
An auxiliary prediction task can speed up learning in language emergence setups.
TEB Module for IPC
Fully parallelizable and adversarial-noise resistant metric learning algorithm with theoretical guarantees.
We develop engaging image captioning models conditioned on personality that are also state of the art on regular captioning tasks.
We propose a differentially private Laplacian smoothing stochastic gradient descent to train machine learning models with better utility and maintain differential privacy guarantees.
We provide statistical and computational analysis of one-bit compressed sensing problem with a generative prior. 
A principled approach for structure learning of deep neural networks with a new interpretation for depth and inter-layer connectivity. 
We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.
This work aims to provide quantitative answers to the relative importance of concepts of interest via concept activation vectors (CAV). In particular, this framework enables non-machine learning experts to express concepts of interest and test hypotheses using examples (e.g., a set of pictures that illustrate  the concept). We show that CAV can be learned given a relatively small set of examples. Hypothesis testing with CAV can answer whether a particular concept (e.g., gender) is more important in predicting a given class (e.g., doctor) than other sets of concepts. Interpreting networks with CAV does not require any retraining or modification of the network. 
The Conditional Entropy Bottleneck is an information-theoretic objective function for learning optimal representations.
We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.
Combining auxiliary and adversarial training to interrogate and help physical understanding.
Flow based models, but non-invertible, to also learn discrete variables
We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.
Compositional attribute-based planning that generalizes to long test tasks, despite being trained on short & simple tasks.
We identify and formalize the memorization problem in meta-learning and solve this problem with novel meta-regularization method, which greatly expand the domain that meta-learning can be  applicable to and effective on.
We propose a novel framework for meta-learning a gradient-based update rule that scales to beyond few-shot learning and is applicable to any form of learning, including continual learning.
Defense-GAN uses a Generative Adversarial Network to defend against white-box and black-box attacks in classification models.
We develop efficient methods to train neural embedding models with a dot-product structure, by reformulating the objective function in terms of generalized Gram matrices, and maintaining estimates of those matrices.
We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.
A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.
We address sample inefficiency and reward bias in adversarial imitation learning algorithms such as GAIL and AIRL.
A variant of capsule networks that can be used for pairwise learning tasks. Results shows that Siamese Capsule Networks work well in the few shot learning setting.
This paper introduces neuromodulation in artificial neural networks.
We propose a dynamic convolution method to significantly accelerate inference time of CNNs while maintaining the accuracy.
In this paper we introduce a training method, called look-up table quantization (LUT-Q), which learns a dictionary and assigns each weight to one of the dictionary's values
We look at negative transfer from a domain adaptation point of view to derive an adversarial learning algorithm.
We train Quantum Boltzmann Machines using a replica stacking method and a quantum annealer to perform a reinforcement learning task.
We proposed a Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and a Scale-Invariant attack Method (SIM) that can boost the transferability of adversarial examples for image classification.
We introduce a stochastic training method for training Binary Neural Network with both binary weights and activations.
We analyze the impact of the latent space of fully trained generators by pseudo inverting them.
We propose and end-to-end model-building process that is universally applicable to a wide variety of authorship verification corpora and outperforms state-of-the-art with little to no modification or fine-tuning.
We consider tackling a single-agent RL problem by distributing it to $n$ learners.
A hybrid model utilizing both raw-audio and spectrogram information for speech enhancement tasks.
This paper compares statistical tests for RL comparisons (false positive, statistical power), checks robustness to assumptions using simulated distributions and empirical distributions (SAC, TD3), provides guidelines for RL students and researchers.
We give a theoretical analysis of the Information Bottleneck objective to understand and predict observed phase transitions in the prediction vs. compression tradeoff.
Proposing locally adaptive activation functions in deep and physics-informed neural networks for faster convergence
Analysis of Bayesian Hyperparameter Inference in Gaussian Process Regression 
We train variational models with quantized networks for computational determinism. This enables using them for cross-platform data compression.
A neural simulation of Universal Turing Machine
Decaying the learning rate and increasing the batch size during training are equivalent.
We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.
A NAS benchmark applicable to almost any NAS algorithms.
We propose a zero-centered gradient penalty for improving generalization and stability of GANs
A sober view on the current state of GANs from a practical perspective
We propose a model-agnostic approach to explain the behaviour of black-box deep RL agents, trained to play Atari and board games, by highlighting relevant features of an input state.
We investigate the deep representation of untrained, random weight CNN-DCN architectures, and show their image reconstruction quality and possible applications.
This paper introduces a new dynamic feature representation approach to provide a more efficient way to do inference on deep neural networks.
When initialized properly, neural networks can learn the simple class of symmetric functions; when initialized randomly, they fail.  
We propose a method for efficient Multi-Objective Neural Architecture Search based on Lamarckian inheritance and evolutionary algorithms.
Unified neural model of topic and language modeling to introduce language structure  in topic models for contextualized topic vectors 
We propose a new way to compress neural networks using probabilistic data structures.
We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.
An empirical study that provides a novel perspective on few-shot learning, in which a fine-tuning method shows comparable accuracy to more complex state-of-the-art methods in several classification tasks.
We propose an approach to generate realistic and high-fidelity stock market data based on generative adversarial networks.
We present a sign-based, rather than magnitude-based, gradient estimation approach that shifts gradient estimation from continuous to binary black-box optimization.
We analyze the gradient propagation in deep RNNs and from our analysis, we propose a new multi-layer deep RNN.
We mathematically analyze the effect of batch normalization on a simple model and obtain key new insights that applies to general supervised learning.
For complex constraints in which it is not easy to estimate the gradient, we use the discounted penalty as a guiding signal. We prove that under certain assumptions it converges to a feasible solution.
The paper investigates target acquisition for handheld virtual panels in VR and shows that target width, distance, direction of approach with respect to gravity, and angle of approach, all impact user performance.
iSparse eliminates irrelevant or insignificant network edges with minimal impact on network performance by determining edge importance w.r.t. the final network output. 
We learn entity representations that can reconstruct Wikipedia categories with just a few exemplars.
Using the q-deformed logarithm, we derive tighter bounds than IWAE, to train variational autoencoders.
Bregman's dilemma is shown in deep learning that improvement of margins of over-parameterized models may result in overfitting, and dynamics of normalized margin distributions are proposed to predict generalization error and identify such a dilemma. 
We proposed an end-to-end dialogue system with a novel multi-level dialogue state tracker and achieved consistent performance on MultiWOZ2.1 in state tracking, task completion, and response generation performance.
We present a scalable approximation to a wide range of EBM objectives, and applications in implicit VAEs and WAEs
This paper develops a principled method for continual learning in deep models.
A deep RL algorithm for solving POMDPs by auto-encoding the underlying states using a variational recurrent model
This paper formalises the problem of online algorithm selection in the context of Reinforcement Learning.
We propose a new latent variable model to learn latent embeddings for some high-dimensional data. 
We proposal strategies for adversarial defense based on data dependent activation function, total variation minimization, and training data augmentation
This work presents a scalable solution to continuous visual speech recognition.
We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.
We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.
We control the topic and sentiment of text generation (almost) without any training. 
Relaxing the constraint of shared hierarchies enables more effective deep multitask learning.
We propose a framework to learn confidence-calibrated networks by designing a novel loss function that incorporates predictive uncertainty estimated through stochastic inferences.
Learning particle dynamics with dynamic interaction graphs for simulating and control rigid bodies, deformable objects, and fluids. 
We introduce theory to explain the failure of GANs on complex datasets and propose a solution to fix it.
Data augmentation and adversarial training are very effective for disentangling correlated speaker and noise, enabling independent control of each attribute for text-to-speech synthesis.
LSTMs learn long-range dependencies compositionally by building them from shorter constituents over the course of training.
We train neural networks by locally linearizing them and using a linear SVM solver (Frank-Wolfe) at each iteration.
We present an improved version of Universal Successor Features based DRL method which can improve the transfer learning of agents.
A method for learning image representations that are good for both disentangling factors of variation and obtaining faithful reconstructions.
We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.
The goal of this paper is to get the effect of multiple teacher networks by exploiting stochastic blocks and skip connections.
We built an Android keyboard with both lexical (word-based) and semantic (meaning-based) emoji suggestion capabilities and compared their effects in two different chat studies. 
We propose a self-adversarial learning (SAL) paradigm which improves the generator in a self-play fashion for improving GANs' performance in text generation.
In this study, we introduce a novel method that relies on SVD to discover the number of latent dimensions.
A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  
An end-to-end trainable model compression method optimizing accuracy jointly with the expected model size.
We suggest a smart batch selection technique called Ada-Boundary.
We present ontology-based neural network architectures for sound event classification.
Our work shows positional information has been implicitly encoded in a network. This information is important for detecting position-dependent features, e.g. semantic and saliency.
General-to-detailed neural network(GDNN)  with Multi-Task Learning by incorporating cross-domain sketch(CDS) for semantic parsing
We show that the learnability of different neural architectures can be characterized directly by computable measures of data complexity.
Tackling inverse design via genetic algorithms augmented with deep neural networks. 
We investigate hidden state activations of Transformer Models in Question Answering Tasks.
combine reinforcement learning and imitation learning to solve complex robot manipulation tasks from pixels
We introduced BatchEnsemble, an efficient method for ensembling and lifelong learning which can be used to improve the accuracy and uncertainty of any neural network like typical ensemble methods.
Reward Estimation from Game Videos
We propose measurement of phrase importance and algorithms for hierarchical explanation of neural sequence model predictions
Higher momentum parameter $\beta$ helps for escaping saddle points faster
We describe how to improve an image generative model according to a slow- or difficult-to-evaluate objective, such as human feedback, which could have many applications, like making more aesthetic images.
Our proposed algorithm does not use all of the unlabeled data for the training, and it rather uses them selectively.
A new approach to conditional generation by constraining the latent space of an unconditional generative model.
We quantify the energy cost in terms of money (cloud credits) and carbon footprint of training recently successful neural network models for NLP. Costs are high.
The Pruning VAE is proposed to search for disentangled variables with intrinsic dimension.
Byte-level recurrent language models learn high-quality domain specific representations of text.
Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.
Scalable and accurate deep multi label learning with millions of labels.
GANs are shown to provide us a new effective robust mean estimate against agnostic contaminations with both statistical optimality and practical tractability.
We present State Space LSTM models, a combination of state space models and LSTMs, and propose an inference algorithm based on sequential Monte Carlo. 
We extend the wake-sleep algorithm and use it to learn to learn structured models from few examples, 
Proteins, amino-acid sequences, machine learning, deep learning, recurrent neural network(RNN), long short term memory(LSTM), gated recurrent unit(GRU), deep neural networks
Spoken Term Detection, using structured prediction and deep networks, implementing a new loss function that both maximizes AUC and ranks according to a predefined threshold.
We propose a novel optimization objective that encourages fairness in heterogeneous federated networks, and develop a scalable method to solve it.
We propose a novel autoencoding model with augmented adversarial reconstruction loss. We intoduce new metric for content-based assessment of reconstructions. 
a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables
We extended single-trial space-by-time tensor decomposition based on non-negative matrix factorization to efficiently discount pre-stimulus baseline activity that improves decoding performance on data with non-negligible baselines.
GAN-based extreme image compression method using less than half the bits of the SOTA engineered codec while preserving visual quality
Learning to rank using the Transformer architecture.
The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.
We use MCTS to further optimize a bootstrapped policy for continuous action spaces under a policy iteration setting.
why previous VAEs on text cannot learn controllable latent representation as on images, as well as a fix to enable the first success towards controlled text generation without supervision
A new hierarchical cortical model for encoding spatiotemporal memory and video prediction
Proposing a new counterfactual-based methodology to evaluate the hypotheses generated from saliency maps about deep RL agent behavior. 
Most neural networks approximate the same classification function, even across architectures, through all stages of learning.
Represent each entity based on its histogram of contexts and then Wasserstein is all you need!
Methodologies for recommender systems with side information based on trace-norm regularization
This work presents an exploration and imitation-learning-based agent capable of state-of-the-art performance in playing text-based computer games. 
In neural network pruning, zeroing pruned weights is important, sign of initialization is key, and masking can be thought of as training.
We proposed SesameBERT, a generalized fine-tuning method that enables the extraction of global information among all layers through Squeeze and Excitation and enriches local information by capturing neighboring contexts via Gaussian blurring.
Opponent shaping is a powerful approach to multi-agent learning but can prevent convergence; our SOS algorithm fixes this with strong guarantees in all differentiable games.
We show that question-answer matching is a particularly good pre-training task for question-similarity and release a dataset for medical question similarity
A study on the benefit of sharing representation in Multi-Task Reinforcement Learning.
Deep architectures for 3D point clouds that are equivariant to SO(3) rotations, as well as translations and permutations. 
A comparison and detailed analysis of various sentence embedding models through the real-world task of automatic summarization.
We propose Value Propagation, a novel end-to-end planner which can learn to solve 2D navigation tasks via Reinforcement Learning, and that generalizes to larger and dynamic environments.
Proposed a method to extract and leverage interpretations of feature interactions
We show that it is possible to recover the parameters of a 1-layer ReLU generative model from looking at samples generated by it
Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets
Finite Automata Can be Linearly decoded from Language-Recognizing RNNs using low coarseness abstraction functions and high accuracy decoders. 
We propose HURRICANE to address the challenge of hardware diversity in one-shot neural architecture search
Neural phrase-based machine translation with linear decoding time
We propose an AE-based GAN that alleviates mode collapse in GANs.
Reinforcement learning and Adaptive Sampling for Optimized Compilation of Deep Neural Networks.
We design a grammar that is learned in an adversarial setting and apply it to future prediction in video.
We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD
A novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning, and also class-specific learning within each task.
We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.
We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.
A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.
What can we learn about training neural networks if we treat each layer as a gradient boosting problem?
Anticipation improves convergence of deep reinforcement learning.
We find movement in function space is not proportional to movement in parameter space during optimization. We propose a new natural-gradient style optimizer to address this.
We introduce a general family of Lagrangians that allow exploring the IB curve in all scenarios. When these are used, and the IB curve is known, one can optimize directly for a performance/compression level directly.
We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.
We propose a semantic-aware neural abstractive summarization model and a novel automatic summarization evaluation scheme that measures how well a model identifies off-topic information from adversarial samples.
Print the input sentence and current response sentence onto an image and use fine-tuned ImageNet CNN model to predict the next response word.
We enable ordinary CNNs for few-shot learning by exploiting visual concepts which are interpretable visual cues learnt within CNNs.
Apply ordinary differential equation model on graph structured data
Proposed new task, datasets and baselines; 3D Conv CycleGAN preserves object properties across frames; batch structure in frame-level methods matters.
A new scalable, group-equivariant model for capsule networks that preserves compositionality under transformations, and is empirically more transformation-robust to older capsule network models.
The paper proposed a simple yet effective baseline for learning with noisy labels.
Raters prefer adequacy in human over machine translation when evaluating entire documents, but not when evaluating single sentences.
This paper extends the multi-agent generative adversarial imitation learning to extensive-form Markov games.
We revisit self-training as a semi-supervised learning method for neural sequence generation problem, and show that self-training can be quite successful with injected noise.
A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory.
We present a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of neural network models without modifications.
A technique for automatically labeling large unlabeled datasets so that they can train source models for transfer learning and its experimental evaluation. 
Our paper proposes an attention module which captures inter-channel relationships and offers large performance gains.
The applicability of inverse reinforcement learning is often hampered by the expense of collecting expert demonstrations; this paper seeks to broaden its applicability by incorporating prior task information through meta-learning.
A new neural architecture where top dense layers of standard convolutional architectures are replaced with an approximation of a kernel function by relying on the Nyström approximation.
There's non-consensual and pornographic images in the ImageNet dataset
This paper proposed a novel framework for graph similarity learning in inductive and unsupervised scenario.
We show how neural encoding models can be trained to capture both the signal and spiking variability of neural population data using GANs.
A weakly supervised learning based clustering framework performs comparable to that of fully supervised learning models by exploiting unique class count.
Deep Model-Based RL that works well.
We analyze and determine the precision requirements for training neural networks when all tensors, including back-propagated signals and weight accumulators, are quantized to fixed-point format.
We can identify prototypical and outlier examples in machine learning that are quantifiably very different, and make use of them to improve many aspects of neural networks.
We propose to enhance the Deep Scattering Network  in order to improve control and stability of any given machine learning pipeline by proposing a continuous wavelet thresholding scheme
Neural clustering without needing a number of clusters
Model Reconciliation is an established framework for plan explanations, but can be easily hijacked to produce lies.
We consider new variants of optimization algorithms for training deep nets.
We accelerate RNN inference by dynamically reducing redundant memory access using a mixture of accurate and approximate modules.
We initiate a push towards building ER systems to recognize thousands of types by providing a method to automatically construct suitable datasets based on the type hierarchy. 
This paper proposes a theory of classifying Method Invocations by different abstraction levels and conducting a statistical approach for code completion from method name to method invocation.
We focus on creating universal adversaries to fool object detectors and hide objects from the detectors. 
Wasserstein Autoencoder with hyperbolic latent space
Feature map compression method that converts quantized activations into binary vectors followed by nonlinear dimensionality reduction layers embedded into a DNN
Achieving strong adversarial robustness comparable to adversarial training without training on adversarial examples
We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.
Over-parametrization in width seems to help in deep reinforcement learning, just as it does in supervised learning.
Hierarchical generative model (hybrid of VAE and GAN) that learns a disentangled representation of data without compromising the generative quality.
We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.
We train accurate fully quantized networks using a loss function maximizing full precision model accuracy and minimizing the difference between the full precision and quantized networks.
In this paper we introduced an algorithm to learn the connectivity of deep multi-branch networks. The approach is evaluated on image categorization where it consistently yields accuracy gains over state-of-the-art models that use fixed connectivity.
We show that individual units in CNN representations learned in NLP tasks are selectively responsive to natural language concepts.
We propose a novel HRL framework, in which we formulate the temporal abstraction problem as learning a latent representation  of  action  sequence.
We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.
(Camera-ready version) A feature intertwiner module to leverage features from one accurate set to help the learning of another less reliable set.
A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. 
We propose Noisy-DR-L0-SSC (Noisy Dimension Reduction L0-Sparse Subspace Clustering) to efficiently partition noisy data in accordance to their underlying subspace structure.
A novel approach using mode connectivity in loss landscapes to mitigate adversarial effects, repair tampered models and evaluate adversarial robustness
A multi-generator GAN framework with an additional network to learn a prior over the input noise.
BigGANs do not capture the ImageNet data distributions and are only modestly successful for data augmentation.
We present Leaf, a modular benchmarking framework for learning in federated data, with applications to learning paradigms such as federated learning, meta-learning, and multi-task learning.
We introduce a new spatio-temporal embedding loss on videos that generates temporally consistent video instance segmentation, even with occlusions and missed detections, using appearance, geometry, and temporal context.
This paper proposes an advanced policy optimization method with hindsight experience for sparse reward reinforcement learning.
We explore how using background knowledge with query reformulation can help retrieve better supporting evidence when answering multiple-choice science questions.
We compress deep CNNs by reusing a single convolutional layer in an iterative manner, thereby reducing their parameter counts by a factor proportional to their depth, whilst leaving their accuracies largely unaffected
Spectral analysis for understanding how different representations can improve optimization and generalization.
We provide theoretical support to uncertainty estimates for deep learning obtained fitting random priors.
We propose an arbitrarily-conditioned data imputation framework built upon variational autoencoders and normalizing flows
We develop a privacy attack that can recover the sensitive input data of a deep net from its output
We present a novel method for Bilingual Text Generation producing parallel concurrent sentences in two languages.
We introduce online explanation to consider the cognitive requirement of the human for understanding the generated explanation by the agent.
We propose a reinforcement learning based variable swapping and recomputation algorithm to reduce the memory cost.
Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.
This paper proposes the use of a deep generative acoustic model for automatic speech recognition, combining naturally with other deep sequence-to-sequence modules using Bayes' rule.
We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. 
A DQN and DDPG hybrid algorithm is proposed to deal with the discrete-continuous hybrid action space.
A novel method of modelling Knowledge Graphs based on Distance Embeddings and Neural Networks
Improve Training Stability of Semi-supervised Generative Adversarial Networks with Collaborative Training
We show how to make predictions using deep networks, without training deep networks.
Graphon is a good search space for neural architecture search and empirically produces good networks.
Instance adaptive adversarial training for improving robustness-accuracy tradeoff
We propose a defensive distinction protection approach and demonstrate the strong distinguishability of adversarial examples.
We introduce the NLC, a metric that is cheap to compute in the networks randomly initialized state and is highly predictive of generalization, at least in fully-connected networks.
bridge the gap in soft computing
This paper introduces MarginAttack, a stronger and faster zero-confidence adversarial attack.
Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.
We investigate properties of the recently introduced Deep Image Prior (Ulyanov et al, 2017)
We propose a methodology of augmenting publicly available data for rumor studies based on samantic relatedness between limited labeled and unlabeled data.
Dynamic lightweight convolutions are competitive to self-attention on language tasks.
We show how the inclusion of an extra-gradient step in first-order GAN training methods can improve stability and lead to improved convergence results.
Batch normalization reduces robustness at test-time to common corruptions and adversarial examples.
We learn an optimization algorithm that generalizes to unseen tasks
We predict the generalization error and specify the model which attains it across model/data scales.
Unsupervised reinforcement learning method for learning a policy to robustly achieve perceptually specified goals.
Sequence model that dynamically adjusts the amount of computation for each input.
We characterize problematic global optima of the VAE objective and present a novel inference method to avoid such optima.
We propose a novel unsupervised word embedding which preserves the inclusion property in the context distribution and achieve state-of-the-art results on unsupervised hypernymy detection
A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance
This paper explores using wearable sensory augmenting technology to facilitate first-hand perspective-taking of what it is like to have cat-like whiskers.
We give some generalization error bounds of noisy gradient methods such as SGLD, Langevin dynamics, noisy momentum and so forth.
For sparse-reward reinforcement learning, the ensemble of multiple dynamics models is used to generate intrinsic reward designed as the minimum of the surprise.
We study representations of phonology in neural network models of spoken language with several variants of analytical techniques.
We solve the specific SR issue of low-quality JPG images by functional sub-models.
We propose an interpretable model for detecting user-chosen wakewords that learns from the user's examples.
We propose a model that learns to discover informative frames in a future video sequence and represent the video via its keyframes.
We learn deep representation by maximizing mutual information, leveraging structure in the objective, and are able to compute with fully supervised classifiers with comparable architectures
We first propose a fully-automated and target-directed atomic importance estimator based on the graph neural networks and a new concept of reverse self-attention.
Unsupervised spectral clustering using deep neural networks
We find that deep models are crucial for MAML to work and propose a method which enables effective meta-learning in smaller models.
Pooling is achieved using wavelets instead of traditional neighborhood approaches (max, average, etc).
We propose a novel dynamic ridesharing framework to form trips that optimizes both operational value for the service provider and user value to the passengers by factoring the users' social preferences into the decision-making process.
We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID).
We design and analyze a new zeroth-order stochastic optimization algorithm, ZO-signSGD, and demonstrate its connection and application to black-box adversarial attacks in robust deep learning
This paper proposes a Unified Recurrent Neural Network Architecture for  short-term multi-time-horizon solar forecasting and validates the forecast performance gains over the previously reported methods
The Skip-connection in ResNet and the batch-normalization improve the data separation ability and help to train a deep neural network.
We propose a generative model that not only produces data with desired features from the pre-defined latent space but also fully understands the features of the data to create characteristics that are not in the dataset.
A framework for studying emergent communication in a networked multi-agent reinforcement learning setup.
We extend deep sets to functional embeddings and Neural Processes to include translation equivariant members
A rotation-equivariant CNN model of V1 that outperforms previous models and suggest functional groupings of V1 neurons.
Rederive a wide class of inference procedures from an global information bottleneck objective.
Training agents with adaptive computation based on information bottleneck can promote generalization. 
We utilized a within-subjects study to evaluate four paced breathing visuals common in mobile apps to understand which is most effective in providing breathing exercise guidance.
A way to generate training corpora for neural code synthesis using a discriminator trained on unlabelled data
We demonstrate vulnerability to undersensitivity attacks in SQuAD2.0 and NewsQA neural reading comprehension models, where the model predicts the same answer with increased confidence to adversarially chosen questions, and compare defence strategies.
Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation
An entropy regularized policy optimization formalism subsumes a set of sequence prediction learning algorithms. A new interpolation algorithm with improved results on text generation and game imitation learning.
We propose a purely convolutional CNN model with attention mechanism to predict spatial-temporal origin-destination flows. 
We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and the standard generalization.
Why and how to constrain planning domain model verification with planning goals to avoid unreachable counterexamples (false positives verification outcomes).
StrokeNet is a novel architecture where the agent is trained to draw by strokes on a differentiable simulation of the environment, which could effectively exploit the power of back-propagation.
We demonstrate how machine learning is able to model experiments in quantum physics.
 a nonlinear unsupervised metric learning framework to boost the performance of clustering algorithms.
Facing complex, black-box models, encrypting the data is not as usable as approximating the model and using it to price a potential transaction.
Interactive stylus based sound incorporating writing system
We present an encoder-decoder framework for language style transfer, which allows for the use of non-parallel data and source data with various unknown language styles.
A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors 
This paper develops a framework for integrating user feedback under identity uncertainty in knowledge bases. 
In this paper we propose a novel generative model to craft systematic poisoning attacks with detectability constraints against machine learning classifiers, including deep networks. 
It's the quantum algorithm for Expectation Maximization. It's fast: the runtime depends only polylogarithmically on the number of elements in the dataset. 
A sparse optimization algorithm for deep CNN models.
A simple and effective alternative to adversarial imitation learning: initialize experience replay buffer with demonstrations, set their reward to +1, set reward for all other data to 0, run Q-learning or soft actor-critic to train.
We present a new deep architecture, VarPSOM, and its extension to time series data, VarTPSOM,  which achieve superior clustering performance compared to current deep clustering methods on static and temporal data.
We analyze what tasks are best learned together in one network, and which are best to learn separately. 
A deep semantic framework for textual search engine document retrieval
We propose a new algorithm based on the optimal transport to train a CNN in an SSL fashion.
Batch normalization reduces adversarial robustness, as well as general robustness in many cases, particularly to noise corruptions.
Learning HGNs, ND domains
a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks
Submitted in EMNLP
We introduce a new MuJoCo soccer environment for continuous multi-agent reinforcement learning research, and show that population-based training of independent reinforcement learners can learn cooperative behaviors
Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.
We propose a novel state space time series model with the capability to capture the structure of change points and anomaly points, so that it has a better forecasting performance when there exist change points and anomalies in the time series.
A multimodal transformer for multimodal sequential learning, with strong empirical results on multimodal language metrics such as multimodal sentiment analysis, emotion recognition and personality traits recognition. 
We utilize an adaptive coefficient on top of regular momentum inspired by geodesic optimization which significantly speeds up training in both convex and non-convex functions.
We use a transformer encoder to do translation by training it in the style of a masked translation model.
We present local ensembles, a method for detecting extrapolation in trained models, which approximates the variance of an ensemble using local-second order information.
Privacy can be thought about in the same way as other resources in planning
disentangled representation learning
We propose an adversarial training approach to the problem of clarification question generation which uses the answer to the question to model the reward. 
Using saturated cost partitioning to select patterns is preferable to all existing pattern selection algorithms.
Using branched attention with learned combination weights outperforms the baseline transformer for machine translation tasks.
Hybrid Vision-Driven Imitation Learning and Model-Based Reinforcement Learning for Planning, Forecasting, and Control
We highlight the problems with common metrics of in-domain uncertainty and perform a broad study of modern ensembling techniques.
Develop a general framework to establish certified robustness of ML models against various classes of adversarial perturbations
We propose a generative latent variable model for unsupervised scene decomposition that provides factorized object representation per foreground object while also decomposing background segments of complex morphology.
We propose an extension of conditional variational autoencoder that allows conditioning on an arbitrary subset of the features and sampling the remaining ones.
In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.
We propose Hallucinative Topological Memory (HTM), a visual planning algorithm that can perform zero-shot long horizon planning in new environments. 
We propose accelerating Batch Normalization (BN) through sampling less correlated data for reduction operations  with regular execution pattern, which achieves up to 2x and 20% speedup for BN itself and the overall training, respectively.
Communication efficient federated learning with layer-wise matching
Learning with limited training data by exploiting "helpful" instances from a rich data source.  
a new derivative-free optimization algorithms derived from Nesterov's accelerated gradient methods and Hamiltonian dynamics
Binarized Back-Propagation all you need for completely binarized training is to is to inflate the size of the network
How to effectively choose Initialization and Activation function for deep neural networks
We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score
This paper proposes a novel complex masking method for speech enhancement along with a loss function for efficient phase estimation.
Learning emergent behavior by minimizing Bayesian surprise with RL in natural environments with entropy.
Deep representations combined with gradient descent can approximate any learning algorithm.
We present an open-loop brain-machine interface whose performance is unconstrained to the traditionally used bag-of-words approach.
We present a full-network embedding of CNN which outperforms single layer embeddings for transfer learning tasks.
We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold
We evaluate new ML learning algorithms' biological plausibility in the abstract based on mathematical operations needed
We introduce a method to train models with provable robustness wrt all the $l_p$-norms for $p\geq 1$ simultaneously.
We propose a novel method to handle image degradations of different levels by learning a diffusion terminal time. Our model can generalize to unseen degradation level and different noise statistic.
We show that it is possible to fastly approximate Wasserstein distances computation by finding an appropriate embedding where Euclidean distance emulates the Wasserstein distance
We present a novel training scheme for efficiently obtaining order-aware sentence representations.
Recursive Parameterization of Recurrent Models improve performance 
A comparative study of generative models on Continual Learning scenarios.
first posing and solving the sample efficiency optimization problem in the non-parameterized policy space, and then solving a supervised regression problem to find a parameterized policy that is near the optimal non-parameterized policy.
A goal-driven approach to model four mouse visual areas (V1, LM, AL, RL) based on deep neural networks trained on static object recognition does not unveil a functional organization of visual cortex unlike in primates
a generalized transformation-based gradient model for variational inference
We introduce a novel larger-context language model to simultaneously captures syntax and semantics, making it capable of generating highly interpretable sentences and paragraphs
We train a natural media painting agent using environment model. Based on our painting agent, we present a novel approach to train a constrained painting agent that follows the command encoded in the observation.
We developed a search framework and consistency penalty to mitigate delusional bias.
Proposed System can prevent impersonators with facial disguises from completing a fraudulent transaction using a pre-trained DCNN.
theoretical analysis of nonlinear wide autoencoder
We introduce the first Hierarchical RL approach to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.
This paper studied the PUbN classification problem, where we incorporate biased negative (bN) data, i.e., negative data that is not fully representative of the true underlying negative distribution, into positive-unlabeled (PU) learning.
Reinforcment practices for machine translation performance gains might not come from better predictions.
We train a small, efficient CNN with the same performance as the OpenAI Transformer on text classification tasks
Ensemble method for reinforcement learning that weights Q-functions based on accumulated TD errors.
Bsuite is a collection of carefully-designed experiments that investigate the core capabilities of RL agents.
improved pretraining, and analysing encoder output and attention
A formulation for a black-box, reinforcement learning method to find the most-likely failure of a system acting in complex scenarios.
Use model free algorithms like DQN/TRPO to solve short horizon problems (model free) iteratively in a Policy/Value Iteration fashion.
We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.
Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization
using deep neural networks and clever algorithms to capture human mental visual concepts
We present a novel black-box targeted attack that is able to fool state of the art speech to text transcription.
Synthesizing human motions on interactive tasks using mocap data and hierarchical RL.
Analysis of convergence and mode collapse by studying GAN training process as regret minimization
We propose a novel framework to evaluate the interpretability of neural network.
A deep RL agent that learns hyperbolic (and other non-exponential) Q-values and a new multi-horizon auxiliary task.
This paper presents EEG based emotion detection of a person towards an image stimuli and its applicability on neuromarketing.
Fast variational approximations for approximating a user state and learning product embeddings
We present a provable and easily-computable evaluation function that estimates the performance of transferred representations from one learning task to another in task transfer learning.
We propose a reliable conditional adversarial learning scheme along with a simple, generic yet effective framework for UDA tasks.
From Distance to Kernel and Embedding via Random Features For Structured Inputs
A quantum inspired kernel for convolution network, exhibiting interference phenomena,  can be very useful (and compared it with real value  counterpart).
An MMO-inspired research game platform for studying emergent behaviors of large populations in a complex environment
This paper improves existing sample-based evaluation for GANs and contains some insightful experiments.
Residual Binary Neural Networks significantly improve the convergence rate and inference accuracy of the binary neural networks.
Unsupervised method to detect adversarial samples in autoencoder's activations and reconstruction error space
We propose to learn knowledgeable entity and relation representations from Bert for knowledge graph embeddings.
A scalable differentiable neural module that implements reasoning on symbolic KBs.
DCEM learns latent domains for optimization problems and helps bridge the gap between model-based and model-free RL --- we create a differentiable controller and fine-tune parts of it with PPO
We use dynamic rewards to train event extractors.
GANs benefit from scaling up.
joint error matters for unsupervised domain adaptation especially when the domain shift is huge
‘Knowledge Flow’ trains a deep net (student) by injecting information from multiple nets (teachers). The student is independent upon training and performs very well on learned tasks irrespective of the setting (reinforcement or supervised learning).
An efficient estimate to the Gaussian first moment of DNNs as a regularizer to training robust networks.
We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.
We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.
We demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video.
We provide theoretical and empirical analysis on the role of anisotropic noise introduced by stochastic gradient on escaping from minima.
Policy gradient through backpropagation through time using learned models and Q-functions. SOTA results in reinforcement learning benchmark environments.
Meta-learning on self-proposed task distributions to speed up reinforcement learning without human specified task distributions 
A novel and practically effective method to adapt pretrained neural networks to new tasks by retraining a minimal (e.g., less than 2%) number of parameters
A new theoretical explanation for the existence of adversarial examples
Elastic-InfoGAN is a modification of InfoGAN that learns, without any supervision, disentangled representations in class imbalanced data
a distributed latent-space based knowledge-sharing framework for deep multi-task learning
Game Changer is a system that provides both audio descriptions and tactile additions to make the state of the board game accessible to blind and visually impaired players.
Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.
Our method introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies from expert demonstrations.
RNNs implicitly implement tensor-product representations, a principled and interpretable method for representing symbolic structures in continuous space.
Learned data augmentation instills algorithm-favoring inductive biases that let RNNs learn list-processing algorithms from fewer examples.
We propose a special weakly-supervised multi-label learning problem along with a newly tailored algorithm that learns the underlying classifier by learning to assign pseudo-labels.
A Reinforcement Learning based conversational search assistant which provides contextual assistance in subjective search (like digital assets).
Convolutional Neural Networks behave as Compositional Nearest Neighbors!
Factorize LSTM states and zero-out/tie LSTM weight matrices according to real-world structural biases expressed by Datalog programs.
We propose a novel approach to solve data-driven model-based optimization problems in both passive and active settings that can scale to high-dimensional input spaces.
In this paper, we propose a new encoder-decoder model based on Tensor Product Representations for Natural- to Formal-language generation, called TP-N2F.
This paper presents a defogger, a model that learns to predict future hidden information from partial observations, applied to a StarCraft dataset.
We study generalization of neural networks in gradient-based meta- learning by analyzing various properties of the objective landscape.
We present a generative model that proves state-of-the-art results on gray-scale and natural images.
We propose a novel network called the Recurrent Identity Network (RIN) which allows a plain recurrent network to overcome the vanishing gradient problem while training very deep models without the use of gates.
The paper tackles fault-tolerance under random and adversarial stoppages.
We present a novel unified architecture that restores video frames from a single motion-blurred image in an end-to-end manner.
We propose a novel structured class-blind pruning technique to produce highly compressed neural networks.
We use Kronecker sum approximations for low-rank training to address challenges in training neural networks on edge devices that utilize emerging memory technologies.
Systematically examines how well we can explain the hidden features of a deep network in terms of logical rules.
Improved likelihood estimates in variational autoencoders using self-supervised feature learning
This paper shows that model-free policy gradient methods can converge to the global optimal solution for non-convex linearized control problems.
We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.
We introduce new pointwise convolution layers equipped with extremely fast conventional transforms in deep neural network.
We propose to use lattices to represent objects and prove a fundamental result on how to train networks that use them.
We try to design and train a classifier whose adversarial robustness is more resemblance to robustness of human.
We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.
It's a new AGI architecture for trans-sapient performance.This is a high-level overview of the Omega AGI architecture which is the basis of a data science automation system. Submitted to a workshop. 
We propose the Flow mechanism and an end-to-end architecture, FlowQA, that achieves SotA on two conversational QA datasets and a sequential instruction understanding task.
We present a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.
We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.
This paper introduces domain-independent compilations of user questions into constraints for contrastive explanations.
 We embed nodes in a graph as Gaussian distributions allowing us to capture uncertainty about their representation.
Logit regularization methods help explain and improve state of the art adversarial defenses
We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. 
We show that knowledge transfer techniques can improve the accuracy of low precision networks and set new state-of-the-art accuracy for ternary and 4-bits precision. 
The paper presents an improved training mechanism for obtaining binary networks with smaller accuracy drop that helps close the gap with it's full precision counterpart
Unifying framework to perform clustering using deep neural networks
HYPE is a reliable human evaluation metric for scoring generative models, starting with human face generation across 4 GANs.
We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.
We reward agents for having a causal influence on the actions of other agents, and show that this gives rise to better cooperation and more meaningful emergent communication protocols. 
We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.
We propose a new Q-value function that enables better learning of Gaussian policies.
We present KG-A2C, a reinforcement learning agent that builds a dynamic knowledge graph while exploring and generates natural language using a template-based action space - outperforming all current agents on a wide set of text-based games.
We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.
We propose CNN neuron ranking with two different methods and show their consistency in producing the result which allows to interpret what network deems important and compress the network by keeping the most relevant nodes.
A modular and hierarchical approach to learn policies for exploring 3D environments.
An unsupervised sim-to-real domain adaptation method for semantic segmentation using privileged information from a simulator with GAN-based image translation.
The first rigor diagnose of large-scale adversarial training on ImageNet
Attribute the bias terms of deep neural networks to input features by a backpropagation-type algorithm; Generate complementary and highly interpretable explanations of DNNs in addition to gradient-based attributions.
This paper presents a method to autonomously find multiple periodicities in a signal, using FFT and ACF and add three news steps (clustering/filtering/detrending)
A simple yet effective imitation learning scheme that incentivizes exploration of an environment without any extrinsic reward or human demonstration.
 a new framework using dual space for generating images corresponding to multiclass labels when the number of class is large
We present a new feed forward graph ConvNet based on generalizing the wavelet scattering transform of Mallat, and demonstrate its utility in graph classification and data exploration tasks.
We introduce a large-scale receipt dataset for post-OCR parsing tasks.
Accelerating CNN training on a Pipeline of Accelerators with Stale Weights
We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.
Improved training of current flow-based generative models (Glow and RealNVP) on density estimation benchmarks
Deep neural networks trained with data augmentation do not require any other explicit regularization (such as weight decay and dropout) and exhibit greater adaptaibility to changes in the architecture and the amount of training data.
This paper improves the quality of the recently proposed adversarial feature leaning (AFL) approach for incorporating explicit constrains to representations, by introducing the concept of the {\em vulnerableness} of the adversary. 
We extend a successful recurrent variational autoencoder for dynamic systems to model an instance of dynamic systems hierarchy in neuroscience using the ladder method.
We introduce an efficient quantization process that allows for performance acceleration on specialized integer-only neural network accelerator.
This paper proposes a new CNN model that combines energy cost with a dynamic routing strategy to enable adaptive energy-efficient inference.
we present LSH Softmax, a softmax approximation layer for sub-linear learning and inference with strong theoretical guarantees; we showcase both its applicability and efficiency by evaluating on a real-world task: language modeling.
RL can solve (stochastic) multi-robot/scheduling problems scalably and transferably using graph embedding
We present a new algorithm for learning by curriculum based on the notion of mastering rate that outperforms previous algorithms.
Inspiration from local dendritic processes of neocortical learning to make unsupervised learning great again.
Memory networks with faster inference
By taking inspiration from linguistics, specifically the Universal Grammar hypothesis, we learn language agnostic universal representations which we can utilize to do zero-shot learning across languages.
This paper shows that the Wasserstein distance objective enables the training of latent variable models with discrete latents in a case where the Variational Autoencoder objective fails to do so.
A graph neural network able to automatically learn and leverage a dynamic interactive graph structure
A new algorithm for training neural networks that compares favorably to popular adaptive methods.
Points out problems in loss function used in IRGAN, a recently proposed GAN framework for Information Retrieval. Further, a model motivated by co-training is proposed, which achieves better performance.
We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and bandwidth-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting.
Autoencoders for text with a new method for using discrete latent space.
Manifold-structured latent space for generative models
LoopGAN extends cycle length in CycleGAN to enable unaligned sequential transformation for more than two time steps.
We propose SWAP, a distributed algorithm for large-batch training of neural networks.
A new large batch training algorithm  based on Layer-wise Adaptive Rate Scaling (LARS); using LARS, we scaled AlexNet  and ResNet-50 to a batch of 16K.
Learning compositional Koopman operators for efficient system identification and model-based control.
We present a constant memory gradient computation procedure through solutions of stochastic differential equations (SDEs) and apply the method for learning latent SDE models.
Learning privacy-preserving transformations from data. A collaborative approach
We propose several intrinsic reward functions for encouraging coordinated exploration in multi-agent problems, and introduce an approach to dynamically selecting the best exploration method for a given task, online.
We propose to learn synthesizing few-shot classifiers and many-shot classifiers using one single objective function for GFSL.
We combine A* search with reinforcement learning to speed up machine learning code
Data-efficient deep reinforcement learning can be used to learning precise stacking policies.
policy parameterizations and unbiased policy entropy estimators for MDP with large multidimensional discrete action space
Decompose weights to use fewer FLOPs with SVD
Asymptotic convergence for stochastic subgradien method with momentum under general parallel asynchronous computation for general nonconvex nonsmooth optimization
We present a low-bias estimator for Boolean stochastic variable models with many stochastic layers.
We propose a novel method to manipulate given images using natural language descriptions.
Use GAN-based method to scalably solve optimal transport
Leveraging control as inference and Sequential Monte Carlo methods, we proposed a probabilistic planning algorithm.
A simple GAN modification that improves performance across many losses, architectures, regularization schemes, and datasets. 
How to estimate original probability vector for millions of classes from count-min sketch measurements -  a theoretical and practical setup.
You can fix the classifier in neural networks without losing accuracy
Learning to detect objects without image labels from 3 minutes of video
We introduce ReClor, a reading comprehension dataset requiring logical reasoning, and find that current state-of-the-art models struggle with real logical reasoning with poor performance near that of random guess.
Input only noise , glean the softmax outputs, steal the weights
We propose a new form of an autoencoding model which incorporates the best properties of variational autoencoders (VAE) and generative adversarial networks (GAN)
We use the connection between gradient-based meta-learning and hierarchical Bayes to learn a mixture of meta-learners that is appropriate for a heterogeneous and evolving task distribution.
We present a new routing method for Capsule networks, and it performs at-par with ResNet-18 on CIFAR-10/ CIFAR-100.
We introduce Doc2Dial, an end-to-end framework for generating conversational data grounded in business documents via crowdsourcing for train automated dialogue agents
We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation
Modern deep CNNs are not invariant to translations, scalings and other realistic image transformations, and this lack of invariance is related to the subsampling operation and the biases contained in image datasets.
Synthesize complex and extended human motions using an auto-conditioned LSTM network
We devise a mechanism called competition among pixels that allows (approximately) complete saliency methods to pass the sanity checks.
Image classification via iteratively querying for reference image from a candidate class with a RNN and use CNN to compare to the input image
we define the filter-level pruning problem for binary neural networks for the first time and propose method to solve it.
Reducing computation and memory complexity of RNN models by up to 100x using sparse low-rank compression modules, trained via knowledge distillation.
We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.
Comparison of complex- and real-valued multi-layer perceptron with respect to the number of real-valued parameters.
A spectral graph convolutional neural network with spectral zoom properties.
We present a real-time segmentation model automatically discovered by a multi-scale NAS framework, achieving 30% faster than state-of-the-art models.
We introduce R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions.
We investigate how a recurrent neural network successfully learns a task combining long-term memory and sequential recall.
We propose the idea of using the norm of the successor representation an exploration bonus in reinforcement learning. In hard exploration Atari games, our the deep RL algorithm matches the performance of recent pseudo-count-based methods.
Data-dependent factorization of dimensions in a multi-scale architecture based on contribution to the total log-likelihood
Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?
SGD and Adam under single spiked model for tensor PCA
This paper presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically.
Paper presents dynamic evaluation methodology for adaptive sequence modelling
We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model.
The Frechet Distance between train and test distribution correlates with the change in performance for functions that are not invariant to the shift.
A new, very simple dynamic system is introduced that generates pretty patterns; properties are proved and possibilities are explored
GMM-UNIT is an image-to-image translation model that maps an image to multiple domains in a stochastic fashion.
We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.
To understand the information stored in the latent space, we train a GAN-style decoder constrained to produce images that the VAE encoder will map to the same region of latent space.
Two novel GANs are constructed to generate high-quality 3D fMRI brain images and synthetic brain images greatly help to improve downstream classification tasks.
Zero-shot cross-lingual transfer by using multilingual neural machine translation 
We propose a differentiable architecture search algorithm for both convolutional and recurrent networks, achieving competitive performance with the state of the art using orders of magnitude less computation resources.
Current language generation systems either aim for high likelihood and devolve into generic repetition or miscalibrate their stochasticity—we provide evidence of both and propose a solution: Nucleus Sampling.
The first text adversarial defense method in word level, and the improved generic based attack method against synonyms substitution based attacks.
Towards Efficient Credit Assignment in Recurrent Networks without Backpropagation Through Time
We propose a novel adversarial learning framework for structured prediction, in which discriminative models can be used to refine structured prediction models at the inference stage. 
A new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder.
Learning preferences over plan traces using active learning.
We ground language commands in a high-dimensional visual environment by learning language-conditioned rewards using inverse reinforcement learning.
We advocate for random features as a theory of biological neural networks, focusing on sparsely connected networks
We propose the Prob2Vec method for problem embedding used in a personalized e-learning tool in addition to a data level classification method, called negative pre-training, for cases where the training data set is imbalanced.
We introduce CrescendoNet, a deep CNN architecture by stacking simple building blocks without residual connections.
We combine splines with neural networks to obtain a novel distribution over functions and use it to model intensity functions of point processes.
We develop a task-agnosticlly compressed BERT, which is 4.3x smaller and 4.0x faster than BERT-BASE while achieving competitive performance on GLUE and SQuAD.
We show that most variants of importance-weighted autoencoders can be derived in a more principled manner as special cases of adaptive importance-sampling approaches like the reweighted-wake sleep algorithm.
NUQSGD closes the gap between the theoretical guarantees of QSGD and the empirical performance of QSGDinf.
Neural networks can be trained to modify their own connectivity, improving their online learning performance on challenging tasks.
A simplex-based geometric method is proposed to cope with few-shot learning problems.
We show that a working memory input to a reservoir network makes a local reward-modulated Hebbian rule perform as well as recursive least-squares (aka FORCE)
We combine the computational advantages of temporal convolutional architectures with the expressiveness of stochastic latent variables.
A gradient-free method is proposed for non-convex optimization problem 
We propose a novel method that leverages the gradients from differentiable simulators to improve the performance of RL for robotics control
We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks.
Deep Innovation Protection allows evolving complex world models end-to-end for 3D tasks.
We propose MACER: a provable defense algorithm that trains robust models by maximizing the certified radius. It does not use adversarial training but performs better than all existing provable l2-defenses.
This paper proposes a novel actor-critic method that uses Hessians of a critic to update an actor.
scale generative classifiers  on complex datasets, and evaluate their effectiveness to reject illegal inputs including out-of-distribution samples and adversarial examples.
A theory for initialization and scaling of ReLU neural network layers
Outputs of modern NLP APIs on nonsensical text provide strong signals about model internals, allowing adversaries to steal the APIs.
We introduce SeaRNN, a novel algorithm for RNN training, inspired by the learning to search approach to structured prediction, in order to avoid the limitations of MLE training.
A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.
Explainable reinforcement learning model using novel combination of mixture of experts with non-differentiable decision tree experts.
We develop and analyze a new derivative free optimization algorithm with momentum and importance sampling with applications to continuous control.
We propose a new method for training deep hashing for image retrieval using only a relational distance metric between samples
We propose a novel approach to improve a given cross-surface mapping through local refinement with a new iterative method to deform the mesh in order to meet user constraints.
Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities
Representing the network architecture as a set of syntax trees and optimizing their structure leads to accurate and concise regression models. 
Empirical and theoretical study of the effects of staleness in non-synchronous execution on machine learning algorithms.
This work presents a scalable algorithm for non-linear offline system identification from partial observations.
General analysis of sign-based methods (e.g. signSGD) for non-convex optimization, built on intuitive bounds on success probabilities.
For off-policy learning with bandit feedbacks, we propose a new variance regularized counterfactual learning algorithm, which has both theoretical foundations and superior empirical performance.
We combine hard handcrafted constraints with a deep prior weak constraint to perform seismic imaging and reap information on the "posterior" distribution leveraging multiplicity in the data.
State of the art in complex text-to-SQL parsing by combining hard and soft relational reasoning in schema/question encoding.
We introduce a continual learning setup based on language modelling where no explicit task segmentation signal is given and propose a neural network model with growing long term memory to tackle it.
Proposed RNN-based algorithm to estimate predictive distribution in one- and multi-step forecasts in time series prediction problems
LSTMs can more effectively model the working memory if they are learned using reinforcement learning, much like the dopamine system that modulates the memory in the prefrontal cortex    
Reformulate deep networks nonlinearities from a vector quantization scope and bridge most known nonlinearities together.
We learn to conditionally generate protein sequences given structures with a model that captures sparse, long-range dependencies.
A framework that links deep network layers to stochastic optimization algorithms; can be used to improve model accuracy and inform network design.
A method for learning quantization configuration for low precision networks that achieves state of the art performance for quantized networks.
We propose several general debiasing strategies to address common biases seen in different datasets and obtain substantial improved out-of-domain performance in all settings.
We present a CNN inference-based reconstruction algorithm to address extremely few-view CT. 
We build knowledgeable conversational agents by conditioning on Wikipedia + a new supervised task.
Synthesis of GCN and LINUCB algorithms for online learning with missing feedbacks
A good tagger gives similar tags to a given paper and the papers it cites
We propose a quantization-based method which regularizes a CNN's learned representations to be automatically aligned with trainable concept matrix hence effectively filtering out adversarial perturbations.
The paper provides a full characterization of permutation invariant and equivariant linear layers for graph data.
Causally correct partial models do not have to generate the whole observation to remain causally correct in stochastic environments.
An efficient lifelong learning algorithm that provides a better trade-off between accuracy and time/ memory complexity compared to other algorithms. 
We demonstrated state-of-the-art training results using 8-bit floating point representation, across Resnet, GNMT, Transformer.
We propose instance cross entropy (ICE) which measures the difference between an estimated instance-level matching distribution and its ground-truth one. 
incorporating, in the model, latent variables that encode future content improves the long-term prediction accuracy, which is critical for better planning in model-based RL.
Integrative Tensor-based Anomaly Detection(ITAD) framework for a satellite system.
Limiting state information for the default policy can improvement performance, in a KL-regularized RL framework where both agent and default policy are optimized together
We compute saliency by using a strong generative model to efficiently marginalize over plausible alternative inputs, revealing concentrated pixel areas that preserve label information.
The Variation Network is a generative model able to learn high-level attributes without supervision that can then be used for controlled input manipulation.
Humans in the loop revise documents to accord with counterfactual labels, resulting resource helps to reduce reliance on spurious associations.
We propose new objective measurement for evaluating explanations based on the notion of adversarial robustness. The evaluation criteria further allows us to derive new explanations which capture pertinent features qualitatively and quantitatively.
This paper presents a GAN-based framework for learning the distribution from high-dimensional incomplete data.
We propose a new compression method, Inter-Layer Weight Prediction (ILWP) and quantization method which quantize the predicted residuals between the weights in convolution layers.
We extend a state-of-the-art technique to directly incorporate FLOPs as part of the optimization objective, and we show that, given a desired FLOPs requirement, different neural networks are successfully trained.
Granularity controled multi-domain and multimodal image to image translation method
Analysis of expressivity and generality of recurrent neural networks with ReLu nonlinearities using Tensor-Train decomposition.
We propose a new, efficient algorithm to construct adversarial examples by means of deformations, rather than additive perturbations.
Regularizing adversarial learning with an information bottleneck, applied to imitation learning, inverse reinforcement learning, and generative adversarial networks.
Simple augmentation method overcomes robustness/accuracy trade-off observed in literature and opens questions about the effect of training distribution on out-of-distribution generalization.
We use mixture density networks to do full conditional density estimation for spatial offset regression and apply it to the human pose estimation task.
Visualizing the differences between regular and relative attention for Music Transformer.
Three factors (batch size, learning rate, gradient noise) change in predictable way the properties (e.g. sharpness) of minima found by SGD.
Simple generative approach to solve the word analogy problem which yields insights into word relationships, and the problems with estimating them
This paper re-examines several common practices of setting hyper-parameters for fine-tuning.
improving deep transfer learning with regularization using attention based feature maps
Neural model predicting multi-aspect sentiments and generating a probabilistic multi-dimensional mask simultaneously. Model outperforms strong baselines and generates masks that are: strong feature predictors, meaningful, and interpretable.
Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.
A pairwise learned capsule network that performs well on face verification tasks given limited labeled data 
Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.
The paper describes methods to verify and recognize HTN plans by parsing of attribute grammars.
We explore neural architecture search for language tasks. Recurrent cell search is challenging for NMT, but attention mechanism search works. The result of attention search on translation is transferable to reading comprehension.
We propose a regularizer that improves interpolation and autoencoders and show that it also improves the learned representation for downstream tasks.
This paper presents method for stochastically generating in-between video frames from given key frames, using direct 3D convolutions.
This paper studies weakly-supervised knowledge graph alignment with adversarial training frameworks.
Multi-view learning improves unsupervised sentence representation learning
We propose a meta-learning approach for guiding visual segmentation tasks from varying amounts of supervision.
Latent optimisation improves adversarial training dynamics. We present both theoretical analysis and state-of-the-art image generation with ImageNet 128x128.
This paper talks about theoretical properties of first-order optimal point of two layer neural network in over-parametrized case
An architecture enables CNN trained on the video sequences converging rapidly 
We present a new method that combines transfer-based and scored black-box adversarial attack, improving the success rate and query efficiency of black-box adversarial attack across different network architectures.
Describe a neuro-AI interface technique to evaluate generative adversarial networks
Using adaptive sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. 
A dynamic bagging methods approach to avoiding negatve transfer in neural network few-shot transfer learning
A novel matrix completion based algorithm to model disease progression with events
Simple similarity constraints on top of multilingual NMT enables high quality translation between unseen language pairs for the first time.
The neural tangent kernel in a randomly initialized ReLU net is non-trivial fluctuations as long as the depth and width are comparable. 
We propose new tensor decompositions and associated regularizers to obtain state of the art performances on temporal knowledge base completion.
We used a CVAE type model structure to learn to directly generate slates/whole pages for recommendation systems.
Combining graph neural networks and the RNN graph generative model, we propose a novel architecture that is able to learn from a sequence of evolving graphs and predict the graph topology evolution for the future timesteps
We propose a learning-to-decompose agent that helps simple-question answerers to answer compound question over knowledge graph.
Learned energy based model with score matching
Propose a general tensor-based RBM model which can compress the model greatly at the same keep a strong model expression capacity
An actor-critic reinforcement learning approach with multi-step returns applied to autonomous driving with Carla simulator.
We propose new methods for evaluating and quantifying the quality of synthetic GAN distributions from the perspective of classification tasks
The goal of survival clustering is to map subjects into clusters. Without end-of-life signals, this is a challenging task. To address this task we propose a new loss function by modifying the Kuiper statistics.
We show how using semi-parametric prior estimations can speed up HPO significantly across datasets and metrics.
Routing procedures are not necessary for CapsNets
By setting the width or the initialization variance of each layer differently, we can actually subdue gradient explosion problems in residual networks (with fully connected layers and no batchnorm). A mathematical theory is developed that not only tells you how to do it, but also surprisingly is able to predict, after you apply such tricks, how fast your network trains to achieve a certain test set performance. This is some black magic stuff, and it's called "Deep Mean Field Theory."
Targeted communication in multi-agent cooperative reinforcement learning
We have developed an etching latte art support system which projects the making procedure directly onto a cappuccino to help the beginners to make well-balanced etching latte art.
We propose temporal self-supervisions for learning stable temporal functions with GANs.
Semi-supervised Cross-lingual Document Classification
Are HMMs a special case of RNNs? We investigate a series of architectural transformations between HMMs and RNNs, both through theoretical derivations and empirical hybridization and provide new insights.
We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.
We develop theoretical foundations for the expressive power of GNNs and design a provably most powerful GNN.
A new algorithm for online multi-task learning that learns without restarts at the task borders
Cross-Lingual Ability of Multilingual BERT: An Empirical Study
A theoretical framework for deep ReLU network that can explains multiple puzzling phenomena like over-parameterization, implicit regularization, lottery tickets, etc. 
Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.
We perform counting for visual question answering; our model produces interpretable outputs by counting directly from detected objects.
We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.
Represent sentences by composing them with Tree-LSTMs according to automatically induced parse trees.
Three new algorithms with ablation studies to prune neural network to optimize for wiring length, as opposed to number of remaining weights.
Weakly-Supervised Text-Based Video Moment Retrieval
How to use stacked generalization to improve the performance of existing transfer learning algorithms when limited labeled data is available.
This paper introduces a physics prior for Deep Learning and applies the resulting network topology for model-based control.
We improve generative models by proposing a meta-algorithm that filters new training data from the model's outputs.
We use an unrolled simulator as an end-to-end differentiable model of protein structure and show it can (sometimes) hierarchically generalize to unseen fold topologies.
Automated mice training for neuroscience with online iterative latent strategy inference for behavior prediction
We analyze recurrent networks trained on sentiment classification, and find that they all exhibit approximate line attractor dynamics when solving this task.
We built a physical simulation of a rodent, trained it to solve a set of tasks, and analyzed the resulting networks.
An extension of GANs combining optimal transport in primal form with an energy distance defined in an adversarially learned feature space.
Training an agent in a 2D virtual world for grounded language acquisition and generalization.
An RL algorithm that learns to be robust to changes in dynamics
This paper provides a game-based abstraction scheme to compute provably sound policies for POMDPs.
A toy dataset based on critical percolation in a planar graph provides an analytical window to the training dynamics of deep neural networks  
We reframe the generation problem as one of editing existing points, and as a result extrapolate better than traditional GANs.
A new approach that learns a representation for describing transition models in complex uncertaindomains using relational rules. 
We propose an end-to-end differentiable planning network for graphs. This can be applicable to many motion planning problems
We learn high-quality denoising using only single instances of corrupted images as training data.
We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations
Embedded architecture for deep learning on optimized devices for face detection and emotion recognition 
Using the same embedding across covariates doesn't make sense, we show that a tensor decomposition algorithm learns sparse covariate-specific embeddings and naturally separable topics jointly and data-efficiently.
Using linear programming we show that the computational complexity of approximate Deep Neural Network training depends polynomially on the data size for several architectures
We unify the extended Kalman filter (EKF) and the state space approach to power expectation propagation (PEP) by solving the intractable moment matching integrals in PEP via linearisation. This leads to a globally iterated extension of the EKF.
Exploring the Learnability of Learned Neural Networks
We propose a generalized evaluation methodology to interpret model biases, dataset biases, and their correlation.
Social agents learn to talk to each other in natural language towards a goal
We show that posterior collapse in linear VAEs is caused entirely by marginal log-likelihood (not ELBO). Experiments on deep VAEs suggest a similar phenomenon is at play.
This paper propose a new model  which combines multi scale information for sequence to sequence learning.
We propose a new certified adversarial training method, CROWN-IBP, that achieves state-of-the-art robustness for L_inf norm adversarial perturbations.
In structured network pruning, fine-tuning a pruned model only gives comparable performance with training it from scratch.
Interactive technique to improve brushing in dense trajectory datasets by taking into account the shape of the brush.
We develop a novel approach to model object compositionality in images in a GAN framework.
Adversarial audio discrimination using temporal dependency
 We propose a novel GAN training method by considering certain fake samples as real to alleviate mode collapse and stabilize training process.
We present a visual tool to interactively explore the latent space of an auto-encoder for peptide sequences and their attributes.
We report experiments providing strong evidence that a neuron behaves like a binary classifier during training and testing
A method for enriching and combining features to improve classification accuracy
Extend GAN architecture to obtain control over locations and identities of multiple objects within generated images.
We propose a novel end-to-end model (SPNet) to incorporate semantic scaffolds for improving abstractive dialog summarization.
We present a RL agent MINERVA which learns to walk on a knowledge graph and answer queries
An approximation of primate ventral stream as a convolutional network performs poorly on object recognition, and multiple architectural features contribute to this. 
We consider the problem of learning optimal policies in time-limited and time-unlimited domains using time-limited interactions.
One of theoretical issues in deep learning
We analyze and develop a computationally efficient implementation of Jacobian regularization that increases the classification margins of neural networks.
We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.
A novel marginalized average attentional network for weakly-supervised temporal action localization 
We propose a new auto-encoder incorporated with multiway delay-embedding transform toward interpreting deep image prior.
Ensuring that models learned in federated fashion do not reveal a client's participation.
We show how pre-training an untrained neural network with as few as 5-25 examples can improve reconstruction results in compressed sensing and semantic recovery problems like colorization.
We proposed Cooperative Training, a novel training algorithm for generative modeling of discrete data.
We introduce a method for computing an intrinsic reward for curiosity using metrics derived from sampling a latent variable model used to estimate dynamics.
We present a generative model for compositional word embeddings that captures syntactic relations, and provide empirical verification and evaluation.
We propose a hybrid model-based & model-free approach using semantic information to improve DRL generalization in man-made environments.
We use deep learning techniques to solve the sparse signal representation and recovery problem.
We present Dreamer, an agent that learns long-horizon behaviors purely by latent imagination using analytic value gradients.
We propose MULTIPOLAR, a transfer RL method that leverages a set of source policies collected under unknown diverse environmental dynamics to efficiently learn a target policy in another dynamics.
An agent trained only with curiosity, and no extrinsic reward, does surprisingly well on 54 popular environments, including the suite of Atari games, Mario etc.
for spatial transformations robust minimizer also minimizes standard accuracy; invariance-inducing regularization leads to better robustness than specialized architectures
The notion of order learning is proposed and it is applied to regression problems in computer vision
We show that neural networks operate by changing topologly of a data set and explore how architectural choices effect this change.
We use empirical tools of mode connectivity and SVCCA to investigate neural network training heuristics of learning rate restarts, warmup and knowledge distillation.
Variational Inference for infering a discrete distribution from which a low-precision neural network is derived
We propose a novel tensor based method for graph convolutional networks on dynamic graphs
"Generating new chemical materials using novel cross-domain GANs."
We provide an estimator and an estimation algorithm for a class of multi-task regression problem and provide statistical and computational analysis..
Utilized Deep Reinforcement Learning to teach agents ride-sharing fleet style coordination.
Stability of scattering transform representations of graph data to deformations of the underlying graph support.
We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.
This paper discusses different methods of pairing VO with deep learning and proposes a simultaneous prediction of corrections and uncertainty.
We have introduced Deep Density Network, a unified DNN model to estimate uncertainty for exploration/exploitation in recommendation systems.
We train RNNs on famous Twitter users to determine whether the general Twitter population is more likely to believe in climate change after a natural disaster.
Using a novel representation of symmetric linear dynamical systems with a latent state, we formulate optimal control as a convex program, giving the first polynomial-time algorithm that solves optimal control with sample complexity only polylogarithmic in the time horizon.
We model the data generator (in GAN) by means of a high-order polynomial represented by high-order tensors.
We show that deep neural networks are able to learn from data that has been diluted by an arbitrary amount of noise.
we propose a meta-learning approach for low-resource neural machine translation that can rapidly learn to translate on a new language
A method for active anomaly detection. We present a new layer that can be attached to any deep learning model designed for unsupervised anomaly detection to transform it into an active method.
We generate examples to explain a classifier desicion via interpolations in latent space. The variational auto encoder cost is extended with a functional of the classifier over the generated example path in data space.
Introduce an approach to allow agents to learn  PPDDL action models incrementally over multiple planning problems under the framework of reinforcement learning.
We propose a new DRL off-policy algorithm achieving state-of-the-art performance. 
We propose a method that can make use of the multiple passages information for open-domain QA.
Dimensionality reduction algorithm to visualise text with network information, for example an email corpus or co-authorships.
We present a framework that leverages high-fidelity computer simulations to interrogate and diagnose biases within ML classifiers. 
We present and evaluate sampling-based point cloud decoders that outperform the baseline MLP approach by better matching the semantics of point clouds.
We use deep RL to learn a policy that directs the search of a genetic algorithm to better optimize the execution cost of computation graphs, and show improved results on real-world TensorFlow graphs.
We show that with the right loss and architecture, view-predictive learning improves 3D object detection
a generative adversarial network for style modeling in a text-to-speech system
We show in a simplified learning task that over-parameterization improves generalization of a convnet that is trained with gradient descent.
Bayesian meta-learning using PAC-Bayes framework and implicit prior distributions
Position paper proposing rebellious and deceptive explanations for agents.
We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features.
We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.
We identify angle bias that causes the vanishing gradient problem in deep nets and propose an efficient method to reduce the bias.
We employ graph neural networks in the variational EM framework for efficient inference and learning of Markov Logic Networks.
A meta-reinforcement learning approach embedding a neural network controller applied to autonomous driving with Carla simulator.
Derive an information bottleneck framework in reinforcement learning and some simple relevant theories and tools.
We propose a neural module approach to continual learning using a unified visual environment with a large action space.
We propose a method of using GANs to generate high quality visual rationales to help explain model predictions. 
We propose a new algorithm leveraging the expressiveness of Generative Neural Networks to improve Evolutionary Strategies algorithms.
We compress and speed up speech recognition models on embedded devices through a trace norm regularization technique and optimized kernels.
We make theoretical justification for the concept of straight-through estimator.
With a set of modifications, under 10 LOC, to A2C you get an off-policy actor-critic that outperforms A2C and performs similarly to ACER. The modifications are large batchsizes, aggressive clamping, and policy "forcing" with gumbel noise.
Generating text using sentence embeddings from Skip-Thought Vectors with the help of Generative Adversarial Networks.
new out-of-order decoder for neural machine translation
In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.
We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.
We propose the first algorithm for verifying the robustness of Transformers.
Contrary to previous beliefs, the training performance of deep networks, when measured appropriately, is predictive of test performance, consistent with classical machine learning theory.
We propose a framework that incorporates planning for efficient exploration and learning in complex environments.
Our paper analyses the tremendous representational power of networks especially with 'skip connections', which may be used as a method  for better generalization.
This paper proposes a new  objective function to replace KL term with one that emulates maximum mean discrepancy (MMD) objective. 
We pose that generative models' likelihoods are excessively influenced by the input's complexity, and propose a way to compensate it when detecting out-of-distribution inputs
We investigate the convergence of popular optimization algorithms like Adam , RMSProp and propose new variants of these methods which provably converge to optimal solution in convex  settings. 
We present effective defenses to clean-label poisoning attacks. 
MXGNet is a multilayer, multiplex graph based architecture which achieves good performance on various diagrammatic reasoning tasks.
We propose a novel multi-task framework that learns table detection, semantic component recognition and cell type classification for spreadsheet tables with promising results.
We propose automatic metrics to holistically evaluate open-dialogue generation and they strongly correlate with human evaluation.
We devise a novel Depthwise Separable Graph Convolution (DSGC) for the generic spatial domain data, which is highly compatible with depthwise separable convolution.
We train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure, enabled by the new MAESTRO dataset.
An empirical study of variational inference based on chi-square divergence minimization, showing that minimizing the CUBO is trickier than maximizing the ELBO
We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.
Fine-tuning BERT on legal corpora provides marginal, but valuable, improvements on NLP tasks in the legal domain.
We formulate a probabilistic latent sequence model to tackle unsupervised text style transfer, and show its effectiveness across a suite of unsupervised text style transfer tasks. 
Proposes an analytically tractable model and inference procedure (misparametrized sparse regression, inferred using L_1 penalty and studied in the data-interpolation limit) to study deep-net related phenomena in the context of inverse problems. 
We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.
An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.
We proposed a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding.
A residual EBM for text whose formulation is equivalent to discriminating between human and machine generated text. We study its generalization behavior.
Pseudo-labeling has shown to be a weak alternative for semi-supervised learning. We, conversely, demonstrate that dealing with confirmation bias with several regularizations makes pseudo-labeling a suitable approach.
We show that a special goal-condition value function trained with model free methods can be used within model-based control, resulting in substantially better sample efficiency and performance.
A novel neural architecture for efficient amortized inference over latent permutations 
We propose a novel two-tower shared-bottom model architecture for transferring knowledge from rich implicit feedbacks to predict relevance for large-scale retrieval systems.
We address the task of autonomous exploration and navigation using spatial affordance maps that can be learned in a self-supervised manner, these outperform classic geometric baselines while being more sample efficient than contemporary RL algorithms
